/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041915-798m4x01
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandCatchOver2Underarm_ppo_20221020041913
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/798m4x01
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:6
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=6, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=6, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:6', seed=None, sim_device='cuda:6', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandCatchOver2Underarm', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_catch_over2underarm', 'numEnvs': 2048, 'envSpacing': 1, 'episodeLength': 75, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.1, 'orientation_scale': 2, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 2, 'fallPenalty': 0.0, 'objectType': 'egg', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandCatchOver2Underarm', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandCatchOver2Underarm_ppo_20221020041913', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 8192
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:6
Sequential(
  (0): Linear(in_features=422, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=422, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 1105 steps/s (collection: 7.993s, learning 6.833s)
               Value function loss: 7.8102
                    Surrogate loss: 0.0494
             Mean action noise std: 0.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 14.83s
                        Total time: 14.83s
                               ETA: 1482568.0s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 1562 steps/s (collection: 9.180s, learning 1.306s)
               Value function loss: 0.7144
                    Surrogate loss: -0.0275
             Mean action noise std: 0.80
                       Mean reward: 0.96
               Mean episode length: 15.33
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 10.49s
                        Total time: 25.31s
                               ETA: 1265568.3s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 1829 steps/s (collection: 8.786s, learning 0.168s)
               Value function loss: 0.3847
                    Surrogate loss: -0.0247
             Mean action noise std: 0.80
                       Mean reward: 1.59
               Mean episode length: 20.93
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 8.95s
                        Total time: 34.27s
                               ETA: 1142183.8s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 1458 steps/s (collection: 11.054s, learning 0.180s)
               Value function loss: 0.6830
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                       Mean reward: 3.24
               Mean episode length: 32.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 11.23s
                        Total time: 45.50s
                               ETA: 1137454.9s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 1238 steps/s (collection: 13.015s, learning 0.218s)
               Value function loss: 0.2772
                    Surrogate loss: -0.0200
             Mean action noise std: 0.80
                       Mean reward: 3.55
               Mean episode length: 38.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 13.23s
                        Total time: 58.73s
                               ETA: 1174608.6s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 1077 steps/s (collection: 14.973s, learning 0.238s)
               Value function loss: 0.1645
                    Surrogate loss: -0.0363
             Mean action noise std: 0.80
                       Mean reward: 3.40
               Mean episode length: 40.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 15.21s
                        Total time: 73.94s
                               ETA: 1232336.0s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 1036 steps/s (collection: 15.641s, learning 0.166s)
               Value function loss: 0.1490
                    Surrogate loss: -0.0374
             Mean action noise std: 0.80
                       Mean reward: 2.77
               Mean episode length: 29.88
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 15.81s
                        Total time: 89.75s
                               ETA: 1282078.9s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 1042 steps/s (collection: 15.532s, learning 0.178s)
               Value function loss: 0.3096
                    Surrogate loss: -0.0312
             Mean action noise std: 0.80
                       Mean reward: 3.43
               Mean episode length: 32.01
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 15.71s
                        Total time: 105.46s
                               ETA: 1318169.3s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 1006 steps/s (collection: 16.100s, learning 0.172s)
               Value function loss: 0.2219
                    Surrogate loss: -0.0418
             Mean action noise std: 0.80
                       Mean reward: 3.53
               Mean episode length: 35.53
                  Mean reward/step: 0.10
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 16.27s
                        Total time: 121.73s
                               ETA: 1352483.5s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 966 steps/s (collection: 16.784s, learning 0.173s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0429
             Mean action noise std: 0.80
                       Mean reward: 3.44
               Mean episode length: 36.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 16.96s
                        Total time: 138.69s
                               ETA: 1386773.4s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.765s, learning 0.185s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0329
             Mean action noise std: 0.80
                       Mean reward: 3.14
               Mean episode length: 30.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 16.95s
                        Total time: 155.64s
                               ETA: 1414759.0s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 958 steps/s (collection: 16.934s, learning 0.167s)
               Value function loss: 0.1654
                    Surrogate loss: -0.0379
             Mean action noise std: 0.80
                       Mean reward: 3.51
               Mean episode length: 32.11
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 17.10s
                        Total time: 172.74s
                               ETA: 1439346.8s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.721s, learning 0.166s)
               Value function loss: 0.1574
                    Surrogate loss: -0.0353
             Mean action noise std: 0.80
                       Mean reward: 3.58
               Mean episode length: 34.03
                  Mean reward/step: 0.10
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 16.89s
                        Total time: 189.63s
                               ETA: 1458504.8s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.488s, learning 0.205s)
               Value function loss: 0.0929
                    Surrogate loss: -0.0391
             Mean action noise std: 0.80
                       Mean reward: 3.56
               Mean episode length: 35.61
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 16.69s
                        Total time: 206.32s
                               ETA: 1473535.9s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.568s, learning 0.183s)
               Value function loss: 0.0872
                    Surrogate loss: -0.0525
             Mean action noise std: 0.80
                       Mean reward: 3.38
               Mean episode length: 32.18
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 16.75s
                        Total time: 223.07s
                               ETA: 1486943.1s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.551s, learning 0.163s)
               Value function loss: 0.1554
                    Surrogate loss: -0.0342
             Mean action noise std: 0.80
                       Mean reward: 3.54
               Mean episode length: 32.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 16.71s
                        Total time: 239.79s
                               ETA: 1498445.7s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.557s, learning 0.187s)
               Value function loss: 0.1628
                    Surrogate loss: -0.0354
             Mean action noise std: 0.80
                       Mean reward: 3.52
               Mean episode length: 34.14
                  Mean reward/step: 0.11
       Mean episode length/episode: 5.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 16.74s
                        Total time: 256.53s
                               ETA: 1508762.4s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.529s, learning 0.159s)
               Value function loss: 0.1074
                    Surrogate loss: -0.0453
             Mean action noise std: 0.80
                       Mean reward: 3.56
               Mean episode length: 34.89
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 16.69s
                        Total time: 273.22s
                               ETA: 1517625.3s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.324s, learning 0.166s)
               Value function loss: 0.0891
                    Surrogate loss: -0.0403
             Mean action noise std: 0.80
                       Mean reward: 3.43
               Mean episode length: 32.59
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 16.49s
                        Total time: 289.71s
                               ETA: 1524512.1s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.863s, learning 0.162s)
               Value function loss: 0.1251
                    Surrogate loss: -0.0437
             Mean action noise std: 0.80
                       Mean reward: 3.51
               Mean episode length: 32.31
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 17.02s
                        Total time: 306.73s
                               ETA: 1533380.1s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.606s, learning 0.199s)
               Value function loss: 0.1678
                    Surrogate loss: -0.0428
             Mean action noise std: 0.80
                       Mean reward: 3.69
               Mean episode length: 33.19
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 16.80s
                        Total time: 323.54s
                               ETA: 1540354.2s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.735s, learning 0.166s)
               Value function loss: 0.1183
                    Surrogate loss: -0.0367
             Mean action noise std: 0.80
                       Mean reward: 3.64
               Mean episode length: 34.15
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 16.90s
                        Total time: 340.44s
                               ETA: 1547132.8s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.314s, learning 0.174s)
               Value function loss: 0.0986
                    Surrogate loss: -0.0445
             Mean action noise std: 0.80
                       Mean reward: 3.52
               Mean episode length: 32.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 16.49s
                        Total time: 356.93s
                               ETA: 1551522.7s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.504s, learning 0.161s)
               Value function loss: 0.1301
                    Surrogate loss: -0.0435
             Mean action noise std: 0.80
                       Mean reward: 3.50
               Mean episode length: 32.37
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 16.67s
                        Total time: 373.59s
                               ETA: 1556284.4s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.107s, learning 0.162s)
               Value function loss: 0.1476
                    Surrogate loss: -0.0378
             Mean action noise std: 0.80
                       Mean reward: 3.71
               Mean episode length: 33.86
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 16.27s
                        Total time: 389.86s
                               ETA: 1559079.8s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.943s, learning 0.225s)
               Value function loss: 0.1207
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 3.69
               Mean episode length: 34.19
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 17.17s
                        Total time: 407.03s
                               ETA: 1565114.1s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 964 steps/s (collection: 16.742s, learning 0.237s)
               Value function loss: 0.1017
                    Surrogate loss: -0.0376
             Mean action noise std: 0.80
                       Mean reward: 3.66
               Mean episode length: 33.77
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 16.98s
                        Total time: 424.01s
                               ETA: 1570000.5s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.640s, learning 0.169s)
               Value function loss: 0.1114
                    Surrogate loss: -0.0475
             Mean action noise std: 0.80
                       Mean reward: 3.66
               Mean episode length: 32.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 16.81s
                        Total time: 440.82s
                               ETA: 1573929.4s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.349s, learning 0.181s)
               Value function loss: 0.1585
                    Surrogate loss: -0.0330
             Mean action noise std: 0.80
                       Mean reward: 3.75
               Mean episode length: 33.85
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 16.53s
                        Total time: 457.35s
                               ETA: 1576624.1s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.307s, learning 0.171s)
               Value function loss: 0.1341
                    Surrogate loss: -0.0304
             Mean action noise std: 0.80
                       Mean reward: 3.90
               Mean episode length: 34.31
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 16.48s
                        Total time: 473.83s
                               ETA: 1578967.2s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.177s, learning 0.158s)
               Value function loss: 0.1122
                    Surrogate loss: -0.0324
             Mean action noise std: 0.80
                       Mean reward: 3.81
               Mean episode length: 33.51
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 16.33s
                        Total time: 490.16s
                               ETA: 1580695.0s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.647s, learning 0.167s)
               Value function loss: 0.1161
                    Surrogate loss: -0.0411
             Mean action noise std: 0.80
                       Mean reward: 3.63
               Mean episode length: 32.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 16.81s
                        Total time: 506.98s
                               ETA: 1583813.2s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.881s, learning 0.178s)
               Value function loss: 0.1536
                    Surrogate loss: -0.0307
             Mean action noise std: 0.80
                       Mean reward: 3.81
               Mean episode length: 33.18
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 17.06s
                        Total time: 524.04s
                               ETA: 1587481.2s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.383s, learning 0.162s)
               Value function loss: 0.1299
                    Surrogate loss: -0.0344
             Mean action noise std: 0.80
                       Mean reward: 3.84
               Mean episode length: 33.78
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 16.55s
                        Total time: 540.58s
                               ETA: 1589421.0s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.284s, learning 0.182s)
               Value function loss: 0.1229
                    Surrogate loss: -0.0427
             Mean action noise std: 0.80
                       Mean reward: 3.82
               Mean episode length: 33.73
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 16.47s
                        Total time: 557.05s
                               ETA: 1591023.2s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.169s, learning 0.168s)
               Value function loss: 0.1293
                    Surrogate loss: -0.0388
             Mean action noise std: 0.80
                       Mean reward: 3.70
               Mean episode length: 32.62
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 16.34s
                        Total time: 573.38s
                               ETA: 1592177.9s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.348s, learning 0.167s)
               Value function loss: 0.1538
                    Surrogate loss: -0.0326
             Mean action noise std: 0.80
                       Mean reward: 3.83
               Mean episode length: 32.91
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 16.52s
                        Total time: 589.90s
                               ETA: 1593751.0s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1265 steps/s (collection: 12.768s, learning 0.182s)
               Value function loss: 0.1402
                    Surrogate loss: -0.0365
             Mean action noise std: 0.80
                       Mean reward: 3.94
               Mean episode length: 33.24
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 12.95s
                        Total time: 602.85s
                               ETA: 1585862.5s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.474s, learning 0.167s)
               Value function loss: 0.1139
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 3.76
               Mean episode length: 33.36
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 8.64s
                        Total time: 611.49s
                               ETA: 1567329.3s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.106s, learning 0.164s)
               Value function loss: 0.1175
                    Surrogate loss: -0.0378
             Mean action noise std: 0.80
                       Mean reward: 3.87
               Mean episode length: 33.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 8.27s
                        Total time: 619.76s
                               ETA: 1548798.4s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.546s, learning 0.194s)
               Value function loss: 0.1374
                    Surrogate loss: -0.0337
             Mean action noise std: 0.80
                       Mean reward: 3.93
               Mean episode length: 33.21
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 8.74s
                        Total time: 628.50s
                               ETA: 1532315.5s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.260s, learning 0.173s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0389
             Mean action noise std: 0.80
                       Mean reward: 3.99
               Mean episode length: 33.69
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 8.43s
                        Total time: 636.93s
                               ETA: 1515887.1s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.167s, learning 0.176s)
               Value function loss: 0.1236
                    Surrogate loss: -0.0407
             Mean action noise std: 0.80
                       Mean reward: 3.87
               Mean episode length: 34.02
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 8.34s
                        Total time: 645.28s
                               ETA: 1500012.6s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.068s, learning 0.162s)
               Value function loss: 0.1145
                    Surrogate loss: -0.0356
             Mean action noise std: 0.80
                       Mean reward: 3.82
               Mean episode length: 33.38
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 8.23s
                        Total time: 653.51s
                               ETA: 1484603.4s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.445s, learning 0.161s)
               Value function loss: 0.1291
                    Surrogate loss: -0.0371
             Mean action noise std: 0.80
                       Mean reward: 3.91
               Mean episode length: 33.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 8.61s
                        Total time: 662.11s
                               ETA: 1470715.3s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.404s, learning 0.166s)
               Value function loss: 0.1456
                    Surrogate loss: -0.0349
             Mean action noise std: 0.80
                       Mean reward: 3.96
               Mean episode length: 32.31
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 8.57s
                        Total time: 670.68s
                               ETA: 1457352.1s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.188s, learning 0.177s)
               Value function loss: 0.1200
                    Surrogate loss: -0.0343
             Mean action noise std: 0.80
                       Mean reward: 3.93
               Mean episode length: 32.78
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 8.37s
                        Total time: 679.05s
                               ETA: 1444121.6s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.217s, learning 0.160s)
               Value function loss: 0.1334
                    Surrogate loss: -0.0323
             Mean action noise std: 0.80
                       Mean reward: 3.90
               Mean episode length: 32.53
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 8.38s
                        Total time: 687.43s
                               ETA: 1431465.3s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.516s, learning 0.176s)
               Value function loss: 0.1387
                    Surrogate loss: -0.0330
             Mean action noise std: 0.80
                       Mean reward: 3.96
               Mean episode length: 32.50
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 8.69s
                        Total time: 696.12s
                               ETA: 1419967.3s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.345s, learning 0.163s)
               Value function loss: 0.1331
                    Surrogate loss: -0.0387
             Mean action noise std: 0.80
                       Mean reward: 3.77
               Mean episode length: 32.80
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 8.51s
                        Total time: 704.63s
                               ETA: 1408560.2s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.101s, learning 0.201s)
               Value function loss: 0.1199
                    Surrogate loss: -0.0368
             Mean action noise std: 0.80
                       Mean reward: 3.99
               Mean episode length: 33.92
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 8.30s
                        Total time: 712.93s
                               ETA: 1397197.0s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.161s, learning 0.164s)
               Value function loss: 0.1339
                    Surrogate loss: -0.0361
             Mean action noise std: 0.80
                       Mean reward: 3.99
               Mean episode length: 32.63
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 8.32s
                        Total time: 721.25s
                               ETA: 1386315.2s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.080s, learning 0.174s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0300
             Mean action noise std: 0.80
                       Mean reward: 4.07
               Mean episode length: 32.75
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 8.25s
                        Total time: 729.51s
                               ETA: 1375709.2s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.111s, learning 0.159s)
               Value function loss: 0.1269
                    Surrogate loss: -0.0367
             Mean action noise std: 0.80
                       Mean reward: 3.96
               Mean episode length: 32.86
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 8.27s
                        Total time: 737.78s
                               ETA: 1365527.3s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.874s, learning 0.181s)
               Value function loss: 0.1331
                    Surrogate loss: -0.0349
             Mean action noise std: 0.80
                       Mean reward: 3.92
               Mean episode length: 32.44
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 8.06s
                        Total time: 745.83s
                               ETA: 1355323.7s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.282s, learning 0.165s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0318
             Mean action noise std: 0.80
                       Mean reward: 3.99
               Mean episode length: 32.24
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 8.45s
                        Total time: 754.28s
                               ETA: 1346183.7s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.093s, learning 0.178s)
               Value function loss: 0.1346
                    Surrogate loss: -0.0377
             Mean action noise std: 0.80
                       Mean reward: 4.08
               Mean episode length: 32.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 8.27s
                        Total time: 762.55s
                               ETA: 1337054.8s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.493s, learning 0.169s)
               Value function loss: 0.1388
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 4.01
               Mean episode length: 32.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 8.66s
                        Total time: 771.21s
                               ETA: 1328915.5s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.237s, learning 0.192s)
               Value function loss: 0.1324
                    Surrogate loss: -0.0348
             Mean action noise std: 0.80
                       Mean reward: 4.11
               Mean episode length: 32.58
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 8.43s
                        Total time: 779.64s
                               ETA: 1320655.0s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.418s, learning 0.185s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0341
             Mean action noise std: 0.80
                       Mean reward: 4.11
               Mean episode length: 32.44
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 8.60s
                        Total time: 788.24s
                               ETA: 1312960.8s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.174s, learning 0.179s)
               Value function loss: 0.1310
                    Surrogate loss: -0.0367
             Mean action noise std: 0.80
                       Mean reward: 4.13
               Mean episode length: 32.16
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 8.35s
                        Total time: 796.59s
                               ETA: 1305109.9s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.520s, learning 0.171s)
               Value function loss: 0.1458
                    Surrogate loss: -0.0426
             Mean action noise std: 0.80
                       Mean reward: 3.98
               Mean episode length: 31.94
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 8.69s
                        Total time: 805.29s
                               ETA: 1298055.4s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.060s, learning 0.164s)
               Value function loss: 0.1401
                    Surrogate loss: -0.0457
             Mean action noise std: 0.80
                       Mean reward: 3.96
               Mean episode length: 32.54
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 8.22s
                        Total time: 813.51s
                               ETA: 1290484.4s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.913s, learning 0.166s)
               Value function loss: 0.1418
                    Surrogate loss: -0.0389
             Mean action noise std: 0.80
                       Mean reward: 4.04
               Mean episode length: 32.17
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 8.08s
                        Total time: 821.59s
                               ETA: 1282923.4s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.363s, learning 0.166s)
               Value function loss: 0.1327
                    Surrogate loss: -0.0364
             Mean action noise std: 0.80
                       Mean reward: 4.05
               Mean episode length: 32.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 8.53s
                        Total time: 830.12s
                               ETA: 1276286.4s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.078s, learning 0.165s)
               Value function loss: 0.1496
                    Surrogate loss: -0.0322
             Mean action noise std: 0.80
                       Mean reward: 3.99
               Mean episode length: 31.63
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 8.24s
                        Total time: 838.36s
                               ETA: 1269416.6s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.305s, learning 0.191s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0338
             Mean action noise std: 0.80
                       Mean reward: 4.19
               Mean episode length: 32.18
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 8.50s
                        Total time: 846.86s
                               ETA: 1263129.4s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.188s, learning 0.202s)
               Value function loss: 0.1419
                    Surrogate loss: -0.0374
             Mean action noise std: 0.80
                       Mean reward: 4.12
               Mean episode length: 32.15
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 8.39s
                        Total time: 855.25s
                               ETA: 1256870.7s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.143s, learning 0.198s)
               Value function loss: 0.1473
                    Surrogate loss: -0.0343
             Mean action noise std: 0.80
                       Mean reward: 4.13
               Mean episode length: 32.15
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 8.34s
                        Total time: 863.59s
                               ETA: 1250723.2s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.930s, learning 0.159s)
               Value function loss: 0.1453
                    Surrogate loss: -0.0402
             Mean action noise std: 0.80
                       Mean reward: 4.19
               Mean episode length: 31.88
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 8.09s
                        Total time: 871.68s
                               ETA: 1244391.3s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.318s, learning 0.180s)
               Value function loss: 0.1402
                    Surrogate loss: -0.0351
             Mean action noise std: 0.80
                       Mean reward: 4.14
               Mean episode length: 31.34
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 8.50s
                        Total time: 880.17s
                               ETA: 1238813.0s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.983s, learning 0.175s)
               Value function loss: 0.1310
                    Surrogate loss: -0.0450
             Mean action noise std: 0.80
                       Mean reward: 4.16
               Mean episode length: 32.21
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 8.16s
                        Total time: 888.33s
                               ETA: 1232916.7s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.197s, learning 0.171s)
               Value function loss: 0.1449
                    Surrogate loss: -0.0405
             Mean action noise std: 0.80
                       Mean reward: 4.09
               Mean episode length: 31.38
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 8.37s
                        Total time: 896.70s
                               ETA: 1227469.6s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.184s, learning 0.208s)
               Value function loss: 0.1527
                    Surrogate loss: -0.0363
             Mean action noise std: 0.80
                       Mean reward: 4.25
               Mean episode length: 31.93
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 8.39s
                        Total time: 905.09s
                               ETA: 1222202.4s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.901s, learning 0.214s)
               Value function loss: 0.1422
                    Surrogate loss: -0.0360
             Mean action noise std: 0.80
                       Mean reward: 4.20
               Mean episode length: 32.30
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 8.11s
                        Total time: 913.21s
                               ETA: 1216705.8s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.123s, learning 0.172s)
               Value function loss: 0.1437
                    Surrogate loss: -0.0388
             Mean action noise std: 0.80
                       Mean reward: 4.14
               Mean episode length: 31.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 8.29s
                        Total time: 921.50s
                               ETA: 1211590.4s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.127s, learning 0.161s)
               Value function loss: 0.1378
                    Surrogate loss: -0.0328
             Mean action noise std: 0.80
                       Mean reward: 4.08
               Mean episode length: 31.05
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 8.29s
                        Total time: 929.79s
                               ETA: 1206599.6s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.039s, learning 0.165s)
               Value function loss: 0.1430
                    Surrogate loss: -0.0365
             Mean action noise std: 0.80
                       Mean reward: 4.15
               Mean episode length: 32.26
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 8.20s
                        Total time: 937.99s
                               ETA: 1201627.4s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.009s, learning 0.165s)
               Value function loss: 0.1434
                    Surrogate loss: -0.0422
             Mean action noise std: 0.80
                       Mean reward: 4.22
               Mean episode length: 31.09
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 8.17s
                        Total time: 946.17s
                               ETA: 1196743.5s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.344s, learning 0.167s)
               Value function loss: 0.1371
                    Surrogate loss: -0.0420
             Mean action noise std: 0.80
                       Mean reward: 4.22
               Mean episode length: 31.59
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 8.51s
                        Total time: 954.68s
                               ETA: 1192402.8s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 2052 steps/s (collection: 7.792s, learning 0.191s)
               Value function loss: 0.1418
                    Surrogate loss: -0.0428
             Mean action noise std: 0.80
                       Mean reward: 4.25
               Mean episode length: 31.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 7.98s
                        Total time: 962.66s
                               ETA: 1187518.1s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.189s, learning 0.164s)
               Value function loss: 0.1456
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 4.24
               Mean episode length: 31.31
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 8.35s
                        Total time: 971.01s
                               ETA: 1183202.2s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.987s, learning 0.178s)
               Value function loss: 0.1464
                    Surrogate loss: -0.0312
             Mean action noise std: 0.80
                       Mean reward: 4.17
               Mean episode length: 31.28
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 8.16s
                        Total time: 979.18s
                               ETA: 1178763.7s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.082s, learning 0.164s)
               Value function loss: 0.1509
                    Surrogate loss: -0.0352
             Mean action noise std: 0.80
                       Mean reward: 4.06
               Mean episode length: 30.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 8.25s
                        Total time: 987.42s
                               ETA: 1174527.9s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.132s, learning 0.183s)
               Value function loss: 0.1494
                    Surrogate loss: -0.0362
             Mean action noise std: 0.80
                       Mean reward: 4.23
               Mean episode length: 31.61
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 8.31s
                        Total time: 995.74s
                               ETA: 1170471.5s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.227s, learning 0.162s)
               Value function loss: 0.1478
                    Surrogate loss: -0.0353
             Mean action noise std: 0.80
                       Mean reward: 4.25
               Mean episode length: 31.24
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 8.39s
                        Total time: 1004.13s
                               ETA: 1166595.9s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.006s, learning 0.204s)
               Value function loss: 0.1542
                    Surrogate loss: -0.0418
             Mean action noise std: 0.80
                       Mean reward: 4.13
               Mean episode length: 30.59
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 8.21s
                        Total time: 1012.34s
                               ETA: 1162603.6s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.160s, learning 0.161s)
               Value function loss: 0.1493
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 4.20
               Mean episode length: 30.71
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 8.32s
                        Total time: 1020.66s
                               ETA: 1158828.1s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.099s, learning 0.182s)
               Value function loss: 0.1557
                    Surrogate loss: -0.0445
             Mean action noise std: 0.80
                       Mean reward: 4.27
               Mean episode length: 30.40
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 8.28s
                        Total time: 1028.94s
                               ETA: 1155092.1s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.101s, learning 0.176s)
               Value function loss: 0.1630
                    Surrogate loss: -0.0430
             Mean action noise std: 0.80
                       Mean reward: 4.38
               Mean episode length: 31.01
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 8.28s
                        Total time: 1037.21s
                               ETA: 1151434.5s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.075s, learning 0.177s)
               Value function loss: 0.1583
                    Surrogate loss: -0.0450
             Mean action noise std: 0.80
                       Mean reward: 4.34
               Mean episode length: 30.30
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 8.25s
                        Total time: 1045.47s
                               ETA: 1147829.3s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.119s, learning 0.173s)
               Value function loss: 0.1502
                    Surrogate loss: -0.0446
             Mean action noise std: 0.80
                       Mean reward: 4.24
               Mean episode length: 30.94
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 8.29s
                        Total time: 1053.76s
                               ETA: 1144345.7s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.729s, learning 0.185s)
               Value function loss: 0.1626
                    Surrogate loss: -0.0416
             Mean action noise std: 0.80
                       Mean reward: 4.24
               Mean episode length: 30.25
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 7.91s
                        Total time: 1061.67s
                               ETA: 1140530.7s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 2033 steps/s (collection: 7.892s, learning 0.166s)
               Value function loss: 0.1610
                    Surrogate loss: -0.0458
             Mean action noise std: 0.80
                       Mean reward: 4.35
               Mean episode length: 30.39
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 8.06s
                        Total time: 1069.73s
                               ETA: 1136950.2s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.210s, learning 0.176s)
               Value function loss: 0.1589
                    Surrogate loss: -0.0436
             Mean action noise std: 0.80
                       Mean reward: 4.41
               Mean episode length: 30.94
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 8.39s
                        Total time: 1078.11s
                               ETA: 1133789.7s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.352s, learning 0.172s)
               Value function loss: 0.1655
                    Surrogate loss: -0.0413
             Mean action noise std: 0.80
                       Mean reward: 4.47
               Mean episode length: 30.14
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 8.52s
                        Total time: 1086.64s
                               ETA: 1130838.9s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.264s, learning 0.165s)
               Value function loss: 0.1713
                    Surrogate loss: -0.0411
             Mean action noise std: 0.80
                       Mean reward: 4.43
               Mean episode length: 30.51
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 8.43s
                        Total time: 1095.07s
                               ETA: 1127851.0s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.125s, learning 0.229s)
               Value function loss: 0.1783
                    Surrogate loss: -0.0440
             Mean action noise std: 0.80
                       Mean reward: 4.35
               Mean episode length: 30.73
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 8.35s
                        Total time: 1103.42s
                               ETA: 1124847.6s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.067s, learning 0.193s)
               Value function loss: 0.1667
                    Surrogate loss: -0.0458
             Mean action noise std: 0.80
                       Mean reward: 4.42
               Mean episode length: 30.17
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 8.26s
                        Total time: 1111.68s
                               ETA: 1121809.0s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.025s, learning 0.186s)
               Value function loss: 0.1636
                    Surrogate loss: -0.0450
             Mean action noise std: 0.80
                       Mean reward: 4.40
               Mean episode length: 30.28
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 8.21s
                        Total time: 1119.89s
                               ETA: 1118782.4s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.407s, learning 0.227s)
               Value function loss: 0.1781
                    Surrogate loss: -0.0418
             Mean action noise std: 0.80
                       Mean reward: 4.40
               Mean episode length: 29.82
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 8.63s
                        Total time: 1128.53s
                               ETA: 1116234.5s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.086s, learning 0.177s)
               Value function loss: 0.1765
                    Surrogate loss: -0.0454
             Mean action noise std: 0.80
                       Mean reward: 4.57
               Mean episode length: 30.26
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 8.26s
                        Total time: 1136.79s
                               ETA: 1113372.6s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.275s, learning 0.173s)
               Value function loss: 0.1643
                    Surrogate loss: -0.0384
             Mean action noise std: 0.80
                       Mean reward: 4.49
               Mean episode length: 30.12
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 8.45s
                        Total time: 1145.24s
                               ETA: 1110746.5s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 2059 steps/s (collection: 7.789s, learning 0.166s)
               Value function loss: 0.1842
                    Surrogate loss: -0.0462
             Mean action noise std: 0.80
                       Mean reward: 4.75
               Mean episode length: 29.86
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 7.95s
                        Total time: 1153.19s
                               ETA: 1107696.3s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.255s, learning 0.276s)
               Value function loss: 0.1811
                    Surrogate loss: -0.0400
             Mean action noise std: 0.80
                       Mean reward: 4.56
               Mean episode length: 30.04
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 8.53s
                        Total time: 1161.72s
                               ETA: 1105252.5s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.196s, learning 0.172s)
               Value function loss: 0.1757
                    Surrogate loss: -0.0451
             Mean action noise std: 0.80
                       Mean reward: 4.56
               Mean episode length: 29.66
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 8.37s
                        Total time: 1170.09s
                               ETA: 1102700.5s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.262s, learning 0.159s)
               Value function loss: 0.1714
                    Surrogate loss: -0.0378
             Mean action noise std: 0.80
                       Mean reward: 4.61
               Mean episode length: 29.41
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 8.42s
                        Total time: 1178.51s
                               ETA: 1100245.6s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.495s, learning 0.203s)
               Value function loss: 0.1870
                    Surrogate loss: -0.0356
             Mean action noise std: 0.80
                       Mean reward: 4.61
               Mean episode length: 29.65
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 8.70s
                        Total time: 1187.21s
                               ETA: 1098091.4s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.326s, learning 0.194s)
               Value function loss: 0.1871
                    Surrogate loss: -0.0396
             Mean action noise std: 0.80
                       Mean reward: 4.72
               Mean episode length: 30.37
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 8.52s
                        Total time: 1195.73s
                               ETA: 1095814.5s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.319s, learning 0.192s)
               Value function loss: 0.1927
                    Surrogate loss: -0.0454
             Mean action noise std: 0.80
                       Mean reward: 4.62
               Mean episode length: 29.71
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 8.51s
                        Total time: 1204.24s
                               ETA: 1093570.6s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.142s, learning 0.161s)
               Value function loss: 0.1977
                    Surrogate loss: -0.0383
             Mean action noise std: 0.80
                       Mean reward: 4.71
               Mean episode length: 29.64
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 8.30s
                        Total time: 1212.54s
                               ETA: 1091179.1s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.247s, learning 0.200s)
               Value function loss: 0.2142
                    Surrogate loss: -0.0395
             Mean action noise std: 0.80
                       Mean reward: 4.88
               Mean episode length: 29.65
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 8.45s
                        Total time: 1220.99s
                               ETA: 1088959.3s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.218s, learning 0.167s)
               Value function loss: 0.2199
                    Surrogate loss: -0.0462
             Mean action noise std: 0.80
                       Mean reward: 4.67
               Mean episode length: 29.61
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 8.38s
                        Total time: 1229.37s
                               ETA: 1086723.7s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.341s, learning 0.206s)
               Value function loss: 0.2113
                    Surrogate loss: -0.0441
             Mean action noise std: 0.80
                       Mean reward: 4.87
               Mean episode length: 29.77
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 8.55s
                        Total time: 1237.92s
                               ETA: 1084669.5s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.230s, learning 0.203s)
               Value function loss: 0.2331
                    Surrogate loss: -0.0411
             Mean action noise std: 0.80
                       Mean reward: 4.96
               Mean episode length: 29.72
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 8.43s
                        Total time: 1246.35s
                               ETA: 1082551.2s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.352s, learning 0.171s)
               Value function loss: 0.2538
                    Surrogate loss: -0.0384
             Mean action noise std: 0.80
                       Mean reward: 5.09
               Mean episode length: 30.01
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 16.52s
                        Total time: 1262.88s
                               ETA: 1087435.5s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.123s, learning 0.300s)
               Value function loss: 0.2668
                    Surrogate loss: -0.0456
             Mean action noise std: 0.80
                       Mean reward: 5.14
               Mean episode length: 29.81
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 16.42s
                        Total time: 1279.30s
                               ETA: 1092150.5s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.953s, learning 0.163s)
               Value function loss: 0.2565
                    Surrogate loss: -0.0409
             Mean action noise std: 0.80
                       Mean reward: 5.03
               Mean episode length: 29.71
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 16.12s
                        Total time: 1295.42s
                               ETA: 1096525.4s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.997s, learning 0.168s)
               Value function loss: 29.6344
                    Surrogate loss: -0.0001
             Mean action noise std: 0.80
                       Mean reward: 5.41
               Mean episode length: 30.50
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 16.16s
                        Total time: 1311.58s
                               ETA: 1100867.9s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.844s, learning 0.174s)
               Value function loss: 0.3483
                    Surrogate loss: -0.0446
             Mean action noise std: 0.80
                       Mean reward: 5.14
               Mean episode length: 30.12
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 17.02s
                        Total time: 1328.60s
                               ETA: 1105848.4s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.998s, learning 0.165s)
               Value function loss: 0.3158
                    Surrogate loss: -0.0497
             Mean action noise std: 0.80
                       Mean reward: 5.12
               Mean episode length: 29.62
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 16.16s
                        Total time: 1344.76s
                               ETA: 1110039.7s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.069s, learning 0.172s)
               Value function loss: 0.3278
                    Surrogate loss: -0.0400
             Mean action noise std: 0.80
                       Mean reward: 5.54
               Mean episode length: 30.53
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 16.24s
                        Total time: 1361.00s
                               ETA: 1114225.5s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.030s, learning 0.163s)
               Value function loss: 0.3324
                    Surrogate loss: -0.0383
             Mean action noise std: 0.80
                       Mean reward: 5.36
               Mean episode length: 30.07
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 16.19s
                        Total time: 1377.19s
                               ETA: 1118304.4s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.248s, learning 0.170s)
               Value function loss: 0.3496
                    Surrogate loss: -0.0403
             Mean action noise std: 0.80
                       Mean reward: 5.30
               Mean episode length: 30.16
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 16.42s
                        Total time: 1393.61s
                               ETA: 1122498.6s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.363s, learning 0.163s)
               Value function loss: 0.3341
                    Surrogate loss: -0.0379
             Mean action noise std: 0.80
                       Mean reward: 5.04
               Mean episode length: 29.44
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 16.53s
                        Total time: 1410.14s
                               ETA: 1126712.5s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.884s, learning 0.170s)
               Value function loss: 0.3541
                    Surrogate loss: -0.0440
             Mean action noise std: 0.80
                       Mean reward: 5.26
               Mean episode length: 30.31
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 16.05s
                        Total time: 1426.19s
                               ETA: 1130484.2s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.318s, learning 0.244s)
               Value function loss: 0.3620
                    Surrogate loss: -0.0383
             Mean action noise std: 0.80
                       Mean reward: 5.55
               Mean episode length: 30.33
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 16.56s
                        Total time: 1442.76s
                               ETA: 1134596.2s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.069s, learning 0.234s)
               Value function loss: 63.5035
                    Surrogate loss: -0.0011
             Mean action noise std: 0.80
                       Mean reward: 5.54
               Mean episode length: 30.55
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 16.30s
                        Total time: 1459.06s
                               ETA: 1138441.0s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.140s, learning 0.169s)
               Value function loss: 21.7798
                    Surrogate loss: -0.0064
             Mean action noise std: 0.80
                       Mean reward: 5.80
               Mean episode length: 30.98
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 16.31s
                        Total time: 1475.37s
                               ETA: 1142231.2s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.273s, learning 0.183s)
               Value function loss: 1.1123
                    Surrogate loss: -0.0460
             Mean action noise std: 0.80
                       Mean reward: 5.88
               Mean episode length: 31.11
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 16.46s
                        Total time: 1491.82s
                               ETA: 1146075.8s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.835s, learning 0.197s)
               Value function loss: 0.6266
                    Surrogate loss: -0.0447
             Mean action noise std: 0.80
                       Mean reward: 5.64
               Mean episode length: 30.67
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 16.03s
                        Total time: 1507.85s
                               ETA: 1149537.7s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1025 steps/s (collection: 15.809s, learning 0.167s)
               Value function loss: 0.4874
                    Surrogate loss: -0.0447
             Mean action noise std: 0.80
                       Mean reward: 5.67
               Mean episode length: 30.69
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 15.98s
                        Total time: 1523.83s
                               ETA: 1152904.8s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.166s, learning 0.202s)
               Value function loss: 0.4604
                    Surrogate loss: -0.0383
             Mean action noise std: 0.80
                       Mean reward: 5.74
               Mean episode length: 30.73
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 16.37s
                        Total time: 1540.20s
                               ETA: 1156515.3s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.611s, learning 0.203s)
               Value function loss: 14.2010
                    Surrogate loss: -0.0023
             Mean action noise std: 0.80
                       Mean reward: 5.71
               Mean episode length: 30.23
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 15.81s
                        Total time: 1556.01s
                               ETA: 1159658.3s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.145s, learning 0.197s)
               Value function loss: 0.7052
                    Surrogate loss: -0.0386
             Mean action noise std: 0.80
                       Mean reward: 6.08
               Mean episode length: 31.13
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 16.34s
                        Total time: 1572.35s
                               ETA: 1163145.7s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1039 steps/s (collection: 15.573s, learning 0.181s)
               Value function loss: 0.5398
                    Surrogate loss: -0.0417
             Mean action noise std: 0.80
                       Mean reward: 5.96
               Mean episode length: 31.27
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 15.75s
                        Total time: 1588.11s
                               ETA: 1166149.6s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1029 steps/s (collection: 15.687s, learning 0.232s)
               Value function loss: 0.4775
                    Surrogate loss: -0.0405
             Mean action noise std: 0.80
                       Mean reward: 5.65
               Mean episode length: 30.19
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 15.92s
                        Total time: 1604.03s
                               ETA: 1169230.2s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1030 steps/s (collection: 15.706s, learning 0.191s)
               Value function loss: 0.5016
                    Surrogate loss: -0.0377
             Mean action noise std: 0.80
                       Mean reward: 6.29
               Mean episode length: 31.69
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 15.90s
                        Total time: 1619.92s
                               ETA: 1172250.1s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.111s, learning 0.195s)
               Value function loss: 0.4948
                    Surrogate loss: -0.0407
             Mean action noise std: 0.80
                       Mean reward: 5.90
               Mean episode length: 30.72
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 16.31s
                        Total time: 1636.23s
                               ETA: 1175519.8s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.045s, learning 0.194s)
               Value function loss: 0.4790
                    Surrogate loss: -0.0381
             Mean action noise std: 0.80
                       Mean reward: 6.24
               Mean episode length: 31.33
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 16.24s
                        Total time: 1652.47s
                               ETA: 1178694.8s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.990s, learning 0.165s)
               Value function loss: 0.4974
                    Surrogate loss: -0.0388
             Mean action noise std: 0.80
                       Mean reward: 5.73
               Mean episode length: 30.98
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 16.16s
                        Total time: 1668.63s
                               ETA: 1181765.4s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.171s, learning 0.186s)
               Value function loss: 18.2419
                    Surrogate loss: -0.0004
             Mean action noise std: 0.80
                       Mean reward: 5.82
               Mean episode length: 31.16
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 16.36s
                        Total time: 1684.98s
                               ETA: 1184934.1s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.481s, learning 0.170s)
               Value function loss: 18.3078
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: 6.22
               Mean episode length: 31.60
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 16.65s
                        Total time: 1701.63s
                               ETA: 1188263.4s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.939s, learning 0.162s)
               Value function loss: 0.7217
                    Surrogate loss: -0.0401
             Mean action noise std: 0.80
                       Mean reward: 5.52
               Mean episode length: 30.15
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 16.10s
                        Total time: 1717.73s
                               ETA: 1191164.6s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1045 steps/s (collection: 15.486s, learning 0.181s)
               Value function loss: 0.6492
                    Surrogate loss: -0.0363
             Mean action noise std: 0.80
                       Mean reward: 6.85
               Mean episode length: 32.83
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 15.67s
                        Total time: 1733.40s
                               ETA: 1193727.3s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.222s, learning 0.193s)
               Value function loss: 12.5824
                    Surrogate loss: -0.0037
             Mean action noise std: 0.80
                       Mean reward: 5.69
               Mean episode length: 30.59
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 16.41s
                        Total time: 1749.82s
                               ETA: 1196766.0s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.606s, learning 0.221s)
               Value function loss: 18.3403
                    Surrogate loss: -0.0052
             Mean action noise std: 0.80
                       Mean reward: 5.69
               Mean episode length: 30.77
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 15.83s
                        Total time: 1765.64s
                               ETA: 1199363.9s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.852s, learning 0.187s)
               Value function loss: 0.9424
                    Surrogate loss: -0.0490
             Mean action noise std: 0.80
                       Mean reward: 5.82
               Mean episode length: 31.03
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 16.04s
                        Total time: 1781.68s
                               ETA: 1202069.3s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1063 steps/s (collection: 15.155s, learning 0.244s)
               Value function loss: 0.5851
                    Surrogate loss: -0.0479
             Mean action noise std: 0.80
                       Mean reward: 5.98
               Mean episode length: 31.73
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 15.40s
                        Total time: 1797.08s
                               ETA: 1204309.5s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1046 steps/s (collection: 15.471s, learning 0.192s)
               Value function loss: 121.0141
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: 5.69
               Mean episode length: 31.09
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 15.66s
                        Total time: 1812.74s
                               ETA: 1206695.3s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.983s, learning 0.161s)
               Value function loss: 1.1588
                    Surrogate loss: -0.0359
             Mean action noise std: 0.80
                       Mean reward: 6.48
               Mean episode length: 31.98
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 16.14s
                        Total time: 1828.89s
                               ETA: 1209367.4s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.664s, learning 0.164s)
               Value function loss: 0.7028
                    Surrogate loss: -0.0347
             Mean action noise std: 0.80
                       Mean reward: 5.51
               Mean episode length: 30.43
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 15.83s
                        Total time: 1844.72s
                               ETA: 1211796.1s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1233 steps/s (collection: 13.118s, learning 0.163s)
               Value function loss: 54.9814
                    Surrogate loss: -0.0010
             Mean action noise std: 0.80
                       Mean reward: 6.16
               Mean episode length: 31.83
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 13.28s
                        Total time: 1858.00s
                               ETA: 1212531.0s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.074s, learning 0.172s)
               Value function loss: 0.8062
                    Surrogate loss: -0.0394
             Mean action noise std: 0.80
                       Mean reward: 6.40
               Mean episode length: 31.81
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 8.25s
                        Total time: 1866.24s
                               ETA: 1209991.7s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.956s, learning 0.189s)
               Value function loss: 0.6150
                    Surrogate loss: -0.0429
             Mean action noise std: 0.80
                       Mean reward: 6.09
               Mean episode length: 31.32
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 8.14s
                        Total time: 1874.39s
                               ETA: 1207419.7s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.983s, learning 0.169s)
               Value function loss: 76.9271
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: 6.50
               Mean episode length: 32.32
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 8.15s
                        Total time: 1882.54s
                               ETA: 1204885.1s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.490s, learning 0.237s)
               Value function loss: 0.8930
                    Surrogate loss: -0.0386
             Mean action noise std: 0.80
                       Mean reward: 5.95
               Mean episode length: 31.40
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 8.73s
                        Total time: 1891.27s
                               ETA: 1202749.0s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.029s, learning 0.222s)
               Value function loss: 0.6288
                    Surrogate loss: -0.0445
             Mean action noise std: 0.80
                       Mean reward: 6.06
               Mean episode length: 31.59
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 8.25s
                        Total time: 1899.52s
                               ETA: 1200339.2s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.220s, learning 0.164s)
               Value function loss: 4.5031
                    Surrogate loss: -0.0083
             Mean action noise std: 0.80
                       Mean reward: 11.77
               Mean episode length: 32.76
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 8.38s
                        Total time: 1907.90s
                               ETA: 1198042.5s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.990s, learning 0.169s)
               Value function loss: 0.5654
                    Surrogate loss: -0.0414
             Mean action noise std: 0.80
                       Mean reward: 6.70
               Mean episode length: 32.51
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 8.16s
                        Total time: 1916.06s
                               ETA: 1195634.3s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.838s, learning 0.171s)
               Value function loss: 0.5716
                    Surrogate loss: -0.0363
             Mean action noise std: 0.80
                       Mean reward: 5.68
               Mean episode length: 31.45
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 8.01s
                        Total time: 1924.07s
                               ETA: 1193162.1s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.961s, learning 0.169s)
               Value function loss: 12.5528
                    Surrogate loss: 0.0002
             Mean action noise std: 0.80
                       Mean reward: 5.75
               Mean episode length: 30.82
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 8.13s
                        Total time: 1932.20s
                               ETA: 1190795.0s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.290s, learning 0.178s)
               Value function loss: 0.6019
                    Surrogate loss: -0.0400
             Mean action noise std: 0.80
                       Mean reward: 6.35
               Mean episode length: 32.35
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 8.47s
                        Total time: 1940.67s
                               ETA: 1188664.4s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.849s, learning 0.203s)
               Value function loss: 0.5326
                    Surrogate loss: -0.0427
             Mean action noise std: 0.80
                       Mean reward: 5.73
               Mean episode length: 30.89
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 8.05s
                        Total time: 1948.72s
                               ETA: 1186306.3s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.846s, learning 0.305s)
               Value function loss: 0.4830
                    Surrogate loss: -0.0440
             Mean action noise std: 0.80
                       Mean reward: 6.63
               Mean episode length: 33.02
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 8.15s
                        Total time: 1956.87s
                               ETA: 1184036.7s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.136s, learning 0.169s)
               Value function loss: 15.6753
                    Surrogate loss: 0.0008
             Mean action noise std: 0.80
                       Mean reward: 5.95
               Mean episode length: 31.79
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 8.31s
                        Total time: 1965.18s
                               ETA: 1181887.2s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.123s, learning 0.170s)
               Value function loss: 0.6302
                    Surrogate loss: -0.0356
             Mean action noise std: 0.80
                       Mean reward: 5.79
               Mean episode length: 31.13
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 8.29s
                        Total time: 1973.47s
                               ETA: 1179755.7s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.903s, learning 0.168s)
               Value function loss: 0.6005
                    Surrogate loss: -0.0409
             Mean action noise std: 0.80
                       Mean reward: 6.24
               Mean episode length: 32.23
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 8.07s
                        Total time: 1981.54s
                               ETA: 1177518.2s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.806s, learning 0.203s)
               Value function loss: 0.6332
                    Surrogate loss: -0.0429
             Mean action noise std: 0.80
                       Mean reward: 6.34
               Mean episode length: 31.89
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 8.01s
                        Total time: 1989.55s
                               ETA: 1175270.2s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.945s, learning 0.185s)
               Value function loss: 0.6955
                    Surrogate loss: -0.0448
             Mean action noise std: 0.80
                       Mean reward: 6.46
               Mean episode length: 32.78
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 8.13s
                        Total time: 1997.68s
                               ETA: 1173119.7s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.076s, learning 0.166s)
               Value function loss: 57.5078
                    Surrogate loss: -0.0009
             Mean action noise std: 0.80
                       Mean reward: 6.45
               Mean episode length: 32.15
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 8.24s
                        Total time: 2005.92s
                               ETA: 1171059.5s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.374s, learning 0.196s)
               Value function loss: 1.1468
                    Surrogate loss: -0.0307
             Mean action noise std: 0.80
                       Mean reward: 6.02
               Mean episode length: 31.44
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 8.57s
                        Total time: 2014.49s
                               ETA: 1169213.2s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.171s, learning 0.178s)
               Value function loss: 479.6620
                    Surrogate loss: -0.0023
             Mean action noise std: 0.80
                       Mean reward: 7.08
               Mean episode length: 33.40
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 8.35s
                        Total time: 2022.84s
                               ETA: 1167260.9s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.366s, learning 0.267s)
               Value function loss: 70.0311
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: 6.25
               Mean episode length: 32.04
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 8.63s
                        Total time: 2031.47s
                               ETA: 1165494.1s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 2001 steps/s (collection: 7.994s, learning 0.191s)
               Value function loss: 35.1252
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 7.33
               Mean episode length: 34.05
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 8.18s
                        Total time: 2039.66s
                               ETA: 1163491.2s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.225s, learning 0.159s)
               Value function loss: 16.1097
                    Surrogate loss: -0.0086
             Mean action noise std: 0.80
                       Mean reward: 7.08
               Mean episode length: 32.76
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 8.38s
                        Total time: 2048.04s
                               ETA: 1161624.1s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.916s, learning 0.161s)
               Value function loss: 36.9732
                    Surrogate loss: -0.0055
             Mean action noise std: 0.80
                       Mean reward: 10.12
               Mean episode length: 34.77
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 8.08s
                        Total time: 2056.12s
                               ETA: 1159605.0s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.034s, learning 0.174s)
               Value function loss: 149.2073
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: 7.05
               Mean episode length: 33.10
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 8.21s
                        Total time: 2064.33s
                               ETA: 1157681.9s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.879s, learning 0.174s)
               Value function loss: 182.1107
                    Surrogate loss: -0.0061
             Mean action noise std: 0.80
                       Mean reward: 6.56
               Mean episode length: 31.94
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 8.05s
                        Total time: 2072.38s
                               ETA: 1155693.3s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1999 steps/s (collection: 7.998s, learning 0.197s)
               Value function loss: 66.1227
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 11.07
               Mean episode length: 35.75
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 8.20s
                        Total time: 2080.57s
                               ETA: 1153805.9s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.266s, learning 0.193s)
               Value function loss: 82.9029
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 6.38
               Mean episode length: 31.89
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 8.46s
                        Total time: 2089.03s
                               ETA: 1152084.7s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.162s, learning 0.166s)
               Value function loss: 25.8179
                    Surrogate loss: -0.0075
             Mean action noise std: 0.80
                       Mean reward: 16.08
               Mean episode length: 30.87
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 8.33s
                        Total time: 2097.36s
                               ETA: 1150310.9s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 2039 steps/s (collection: 7.858s, learning 0.174s)
               Value function loss: 40.3483
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: 6.17
               Mean episode length: 31.53
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 8.03s
                        Total time: 2105.39s
                               ETA: 1148394.8s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.147s, learning 0.212s)
               Value function loss: 215.7872
                    Surrogate loss: -0.0047
             Mean action noise std: 0.80
                       Mean reward: 6.41
               Mean episode length: 31.72
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 8.36s
                        Total time: 2113.75s
                               ETA: 1146676.5s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.809s, learning 0.194s)
               Value function loss: 380.6457
                    Surrogate loss: -0.0044
             Mean action noise std: 0.80
                       Mean reward: 9.12
               Mean episode length: 31.87
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 8.00s
                        Total time: 2121.76s
                               ETA: 1144784.8s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.924s, learning 0.166s)
               Value function loss: 50.1172
                    Surrogate loss: -0.0174
             Mean action noise std: 0.80
                       Mean reward: 7.81
               Mean episode length: 29.44
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 8.09s
                        Total time: 2129.85s
                               ETA: 1142960.1s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.327s, learning 0.158s)
               Value function loss: 102.2000
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 13.76
               Mean episode length: 31.50
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 8.49s
                        Total time: 2138.33s
                               ETA: 1141365.8s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.997s, learning 0.166s)
               Value function loss: 96.5719
                    Surrogate loss: -0.0040
             Mean action noise std: 0.80
                       Mean reward: 6.00
               Mean episode length: 30.72
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 8.16s
                        Total time: 2146.49s
                               ETA: 1139617.3s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.097s, learning 0.204s)
               Value function loss: 87.3733
                    Surrogate loss: -0.0061
             Mean action noise std: 0.80
                       Mean reward: 9.87
               Mean episode length: 32.84
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 8.30s
                        Total time: 2154.80s
                               ETA: 1137960.1s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1996 steps/s (collection: 7.982s, learning 0.224s)
               Value function loss: 116.7042
                    Surrogate loss: -0.0074
             Mean action noise std: 0.80
                       Mean reward: 6.59
               Mean episode length: 32.13
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 8.21s
                        Total time: 2163.00s
                               ETA: 1136270.3s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.361s, learning 0.193s)
               Value function loss: 2.4003
                    Surrogate loss: -0.0237
             Mean action noise std: 0.80
                       Mean reward: 12.44
               Mean episode length: 33.26
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 8.55s
                        Total time: 2171.56s
                               ETA: 1134780.0s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.168s, learning 0.313s)
               Value function loss: 72.9874
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: 5.56
               Mean episode length: 30.39
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 8.48s
                        Total time: 2180.04s
                               ETA: 1133267.4s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 2003 steps/s (collection: 7.953s, learning 0.226s)
               Value function loss: 62.3479
                    Surrogate loss: -0.0040
             Mean action noise std: 0.80
                       Mean reward: 6.76
               Mean episode length: 32.22
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.45
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 8.18s
                        Total time: 2188.22s
                               ETA: 1131613.9s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.369s, learning 0.169s)
               Value function loss: 1.7331
                    Surrogate loss: -0.0271
             Mean action noise std: 0.80
                       Mean reward: 6.24
               Mean episode length: 30.90
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 8.54s
                        Total time: 2196.75s
                               ETA: 1130161.6s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.345s, learning 0.213s)
               Value function loss: 151.0174
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 5.33
               Mean episode length: 29.14
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 8.56s
                        Total time: 2205.31s
                               ETA: 1128734.8s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.244s, learning 0.179s)
               Value function loss: 94.3924
                    Surrogate loss: -0.0047
             Mean action noise std: 0.80
                       Mean reward: 5.47
               Mean episode length: 29.23
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 8.42s
                        Total time: 2213.73s
                               ETA: 1127253.3s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.814s, learning 0.235s)
               Value function loss: 181.4354
                    Surrogate loss: -0.0047
             Mean action noise std: 0.80
                       Mean reward: 8.83
               Mean episode length: 31.02
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 8.05s
                        Total time: 2221.78s
                               ETA: 1125597.7s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.118s, learning 0.164s)
               Value function loss: 123.5703
                    Surrogate loss: -0.0060
             Mean action noise std: 0.80
                       Mean reward: 16.54
               Mean episode length: 31.13
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 8.28s
                        Total time: 2230.06s
                               ETA: 1124076.2s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.172s, learning 0.174s)
               Value function loss: 3.0369
                    Surrogate loss: -0.0374
             Mean action noise std: 0.80
                       Mean reward: 5.98
               Mean episode length: 30.97
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 8.35s
                        Total time: 2238.41s
                               ETA: 1122602.4s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.246s, learning 0.195s)
               Value function loss: 50.3687
                    Surrogate loss: 0.0027
             Mean action noise std: 0.80
                       Mean reward: 10.62
               Mean episode length: 29.95
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 8.44s
                        Total time: 2246.85s
                               ETA: 1121190.6s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.102s, learning 0.290s)
               Value function loss: 92.6153
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 11.40
               Mean episode length: 30.55
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 8.39s
                        Total time: 2255.24s
                               ETA: 1119768.2s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.211s, learning 0.165s)
               Value function loss: 1.9308
                    Surrogate loss: -0.0216
             Mean action noise std: 0.80
                       Mean reward: 8.63
               Mean episode length: 30.29
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 8.38s
                        Total time: 2263.62s
                               ETA: 1118351.9s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.059s, learning 0.157s)
               Value function loss: 22.8060
                    Surrogate loss: 0.0000
             Mean action noise std: 0.80
                       Mean reward: 5.60
               Mean episode length: 29.94
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 8.22s
                        Total time: 2271.84s
                               ETA: 1116871.0s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.285s, learning 0.191s)
               Value function loss: 165.6212
                    Surrogate loss: -0.0031
             Mean action noise std: 0.80
                       Mean reward: 8.90
               Mean episode length: 31.30
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 8.48s
                        Total time: 2280.31s
                               ETA: 1115531.1s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.937s, learning 0.170s)
               Value function loss: 62.9947
                    Surrogate loss: -0.0080
             Mean action noise std: 0.80
                       Mean reward: 9.00
               Mean episode length: 31.55
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 8.11s
                        Total time: 2288.42s
                               ETA: 1114024.6s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.118s, learning 0.168s)
               Value function loss: 47.9180
                    Surrogate loss: -0.0026
             Mean action noise std: 0.80
                       Mean reward: 16.54
               Mean episode length: 32.02
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 8.29s
                        Total time: 2296.70s
                               ETA: 1112619.3s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.921s, learning 0.207s)
               Value function loss: 29.9170
                    Surrogate loss: -0.0081
             Mean action noise std: 0.80
                       Mean reward: 6.51
               Mean episode length: 31.75
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 8.13s
                        Total time: 2304.83s
                               ETA: 1111151.7s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.248s, learning 0.165s)
               Value function loss: 89.4730
                    Surrogate loss: -0.0026
             Mean action noise std: 0.80
                       Mean reward: 6.52
               Mean episode length: 31.85
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 8.41s
                        Total time: 2313.25s
                               ETA: 1109835.0s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.215s, learning 0.201s)
               Value function loss: 19.6640
                    Surrogate loss: -0.0095
             Mean action noise std: 0.80
                       Mean reward: 11.39
               Mean episode length: 31.54
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 8.42s
                        Total time: 2321.66s
                               ETA: 1108532.1s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.430s, learning 0.274s)
               Value function loss: 71.1491
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: 9.29
               Mean episode length: 32.35
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 8.70s
                        Total time: 2330.37s
                               ETA: 1107378.5s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.149s, learning 0.167s)
               Value function loss: 101.6316
                    Surrogate loss: -0.0055
             Mean action noise std: 0.80
                       Mean reward: 15.28
               Mean episode length: 33.94
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 8.32s
                        Total time: 2338.68s
                               ETA: 1106051.8s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.184s, learning 0.178s)
               Value function loss: 35.2617
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 9.50
               Mean episode length: 32.47
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 8.36s
                        Total time: 2347.04s
                               ETA: 1104759.9s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.257s, learning 0.161s)
               Value function loss: 14.0405
                    Surrogate loss: -0.0071
             Mean action noise std: 0.80
                       Mean reward: 9.70
               Mean episode length: 33.25
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 8.42s
                        Total time: 2355.46s
                               ETA: 1103505.7s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.530s, learning 0.191s)
               Value function loss: 143.2540
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 10.54
               Mean episode length: 34.43
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 8.72s
                        Total time: 2364.18s
                               ETA: 1102404.8s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.129s, learning 0.185s)
               Value function loss: 6.1348
                    Surrogate loss: -0.0168
             Mean action noise std: 0.80
                       Mean reward: 7.84
               Mean episode length: 34.21
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 8.31s
                        Total time: 2372.50s
                               ETA: 1101125.1s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.335s, learning 0.161s)
               Value function loss: 8.6658
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: 7.47
               Mean episode length: 33.64
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 8.50s
                        Total time: 2380.99s
                               ETA: 1099941.3s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.985s, learning 0.179s)
               Value function loss: 9.2993
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: 9.19
               Mean episode length: 32.42
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 8.16s
                        Total time: 2389.16s
                               ETA: 1098615.6s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.937s, learning 0.167s)
               Value function loss: 42.9419
                    Surrogate loss: -0.0021
             Mean action noise std: 0.80
                       Mean reward: 10.40
               Mean episode length: 34.88
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 8.10s
                        Total time: 2397.26s
                               ETA: 1097274.7s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.465s, learning 0.166s)
               Value function loss: 1.3558
                    Surrogate loss: -0.0357
             Mean action noise std: 0.80
                       Mean reward: 11.97
               Mean episode length: 33.25
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 8.63s
                        Total time: 2405.89s
                               ETA: 1096185.7s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.961s, learning 0.178s)
               Value function loss: 363.3825
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 8.42
               Mean episode length: 35.88
                  Mean reward/step: 0.41
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 8.14s
                        Total time: 2414.03s
                               ETA: 1094883.7s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.086s, learning 0.224s)
               Value function loss: 61.0386
                    Surrogate loss: -0.0049
             Mean action noise std: 0.80
                       Mean reward: 7.68
               Mean episode length: 34.45
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 8.31s
                        Total time: 2422.34s
                               ETA: 1093670.3s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.005s, learning 0.198s)
               Value function loss: 14.1937
                    Surrogate loss: -0.0105
             Mean action noise std: 0.80
                       Mean reward: 13.80
               Mean episode length: 36.64
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 8.20s
                        Total time: 2430.54s
                               ETA: 1092419.8s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.455s, learning 0.173s)
               Value function loss: 84.4900
                    Surrogate loss: -0.0010
             Mean action noise std: 0.80
                       Mean reward: 8.18
               Mean episode length: 35.17
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 8.63s
                        Total time: 2439.17s
                               ETA: 1091370.9s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.881s, learning 0.161s)
               Value function loss: 44.5238
                    Surrogate loss: -0.0065
             Mean action noise std: 0.80
                       Mean reward: 7.51
               Mean episode length: 33.85
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 8.04s
                        Total time: 2447.21s
                               ETA: 1090070.0s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.301s, learning 0.269s)
               Value function loss: 137.7807
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 24.83
               Mean episode length: 33.30
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 8.57s
                        Total time: 2455.78s
                               ETA: 1089014.7s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.116s, learning 0.183s)
               Value function loss: 76.5974
                    Surrogate loss: -0.0106
             Mean action noise std: 0.80
                       Mean reward: 12.68
               Mean episode length: 34.02
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 8.30s
                        Total time: 2464.08s
                               ETA: 1087849.1s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.403s, learning 0.156s)
               Value function loss: 24.3103
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 7.15
               Mean episode length: 33.24
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 8.56s
                        Total time: 2472.64s
                               ETA: 1086807.8s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.229s, learning 0.211s)
               Value function loss: 73.8897
                    Surrogate loss: -0.0036
             Mean action noise std: 0.80
                       Mean reward: 11.67
               Mean episode length: 31.91
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 8.44s
                        Total time: 2481.08s
                               ETA: 1085723.4s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.174s, learning 0.166s)
               Value function loss: 532.2447
                    Surrogate loss: -0.0038
             Mean action noise std: 0.80
                       Mean reward: 7.75
               Mean episode length: 33.97
                  Mean reward/step: 0.45
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 8.34s
                        Total time: 2489.42s
                               ETA: 1084605.0s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.167s, learning 0.180s)
               Value function loss: 72.1994
                    Surrogate loss: -0.0066
             Mean action noise std: 0.80
                       Mean reward: 7.24
               Mean episode length: 33.47
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 8.35s
                        Total time: 2497.77s
                               ETA: 1083499.5s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.893s, learning 0.187s)
               Value function loss: 17.2300
                    Surrogate loss: -0.0117
             Mean action noise std: 0.80
                       Mean reward: 7.56
               Mean episode length: 33.84
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 8.08s
                        Total time: 2505.85s
                               ETA: 1082288.3s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.503s, learning 0.183s)
               Value function loss: 173.6082
                    Surrogate loss: -0.0008
             Mean action noise std: 0.80
                       Mean reward: 9.95
               Mean episode length: 33.85
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 8.69s
                        Total time: 2514.53s
                               ETA: 1081347.6s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 2000 steps/s (collection: 7.918s, learning 0.273s)
               Value function loss: 114.1238
                    Surrogate loss: -0.0051
             Mean action noise std: 0.80
                       Mean reward: 22.03
               Mean episode length: 32.84
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 8.19s
                        Total time: 2522.73s
                               ETA: 1080202.9s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.290s, learning 0.196s)
               Value function loss: 164.6643
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 23.08
               Mean episode length: 34.67
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 8.49s
                        Total time: 2531.21s
                               ETA: 1079194.0s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1062 steps/s (collection: 15.237s, learning 0.182s)
               Value function loss: 101.0289
                    Surrogate loss: -0.0095
             Mean action noise std: 0.80
                       Mean reward: 10.24
               Mean episode length: 34.24
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 15.42s
                        Total time: 2546.63s
                               ETA: 1081137.0s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.143s, learning 0.189s)
               Value function loss: 66.1079
                    Surrogate loss: -0.0019
             Mean action noise std: 0.80
                       Mean reward: 12.13
               Mean episode length: 32.97
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 16.33s
                        Total time: 2562.96s
                               ETA: 1083449.3s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.243s, learning 0.201s)
               Value function loss: 125.1028
                    Surrogate loss: -0.0034
             Mean action noise std: 0.80
                       Mean reward: 7.19
               Mean episode length: 33.50
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 16.44s
                        Total time: 2579.41s
                               ETA: 1085788.9s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.803s, learning 0.161s)
               Value function loss: 38.4822
                    Surrogate loss: -0.0083
             Mean action noise std: 0.80
                       Mean reward: 7.66
               Mean episode length: 34.33
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 15.96s
                        Total time: 2595.37s
                               ETA: 1087907.7s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1030 steps/s (collection: 15.680s, learning 0.216s)
               Value function loss: 29.9925
                    Surrogate loss: 0.0006
             Mean action noise std: 0.80
                       Mean reward: 9.06
               Mean episode length: 31.94
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 15.90s
                        Total time: 2611.27s
                               ETA: 1089980.2s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.640s, learning 0.162s)
               Value function loss: 9.4503
                    Surrogate loss: -0.0118
             Mean action noise std: 0.80
                       Mean reward: 6.83
               Mean episode length: 32.25
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 15.80s
                        Total time: 2627.07s
                               ETA: 1091996.4s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.144s, learning 0.204s)
               Value function loss: 73.2249
                    Surrogate loss: -0.0027
             Mean action noise std: 0.80
                       Mean reward: 6.98
               Mean episode length: 32.53
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 16.35s
                        Total time: 2643.42s
                               ETA: 1094221.5s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.015s, learning 0.179s)
               Value function loss: 19.5559
                    Surrogate loss: -0.0079
             Mean action noise std: 0.80
                       Mean reward: 9.64
               Mean episode length: 32.81
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 16.19s
                        Total time: 2659.61s
                               ETA: 1096364.8s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.338s, learning 0.205s)
               Value function loss: 142.9656
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: 11.56
               Mean episode length: 32.50
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 16.54s
                        Total time: 2676.16s
                               ETA: 1098633.3s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.273s, learning 0.161s)
               Value function loss: 33.4050
                    Surrogate loss: -0.0074
             Mean action noise std: 0.80
                       Mean reward: 12.17
               Mean episode length: 33.46
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 16.43s
                        Total time: 2692.59s
                               ETA: 1100838.5s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1039 steps/s (collection: 15.551s, learning 0.204s)
               Value function loss: 32.9955
                    Surrogate loss: 0.0005
             Mean action noise std: 0.80
                       Mean reward: 9.47
               Mean episode length: 33.11
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 15.75s
                        Total time: 2708.34s
                               ETA: 1102749.1s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.126s, learning 0.171s)
               Value function loss: 50.2434
                    Surrogate loss: -0.0048
             Mean action noise std: 0.80
                       Mean reward: 6.19
               Mean episode length: 31.65
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 16.30s
                        Total time: 2724.64s
                               ETA: 1104864.0s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.042s, learning 0.169s)
               Value function loss: 1.5636
                    Surrogate loss: -0.0310
             Mean action noise std: 0.80
                       Mean reward: 12.41
               Mean episode length: 33.20
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 16.21s
                        Total time: 2740.85s
                               ETA: 1106926.8s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.153s, learning 0.161s)
               Value function loss: 15.5087
                    Surrogate loss: -0.0063
             Mean action noise std: 0.80
                       Mean reward: 11.99
               Mean episode length: 32.83
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 16.31s
                        Total time: 2757.17s
                               ETA: 1109014.1s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.368s, learning 0.187s)
               Value function loss: 16.1657
                    Surrogate loss: 0.0013
             Mean action noise std: 0.80
                       Mean reward: 6.52
               Mean episode length: 31.86
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 16.55s
                        Total time: 2773.72s
                               ETA: 1111181.3s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.776s, learning 0.190s)
               Value function loss: 33.7625
                    Surrogate loss: -0.0049
             Mean action noise std: 0.80
                       Mean reward: 6.55
               Mean episode length: 32.04
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 15.97s
                        Total time: 2789.69s
                               ETA: 1113096.0s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1023 steps/s (collection: 15.780s, learning 0.221s)
               Value function loss: 17.5656
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 6.79
               Mean episode length: 32.65
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 16.00s
                        Total time: 2805.69s
                               ETA: 1115009.2s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.875s, learning 0.167s)
               Value function loss: 27.5258
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                       Mean reward: 6.25
               Mean episode length: 31.37
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 16.04s
                        Total time: 2821.73s
                               ETA: 1116923.4s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.116s, learning 0.164s)
               Value function loss: 61.1207
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: 5.98
               Mean episode length: 31.29
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 16.28s
                        Total time: 2838.01s
                               ETA: 1118916.4s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.553s, learning 0.193s)
               Value function loss: 51.8271
                    Surrogate loss: -0.0003
             Mean action noise std: 0.80
                       Mean reward: 6.58
               Mean episode length: 32.28
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 16.75s
                        Total time: 2854.76s
                               ETA: 1121076.4s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.267s, learning 0.161s)
               Value function loss: 5.1037
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                       Mean reward: 7.29
               Mean episode length: 33.45
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 16.43s
                        Total time: 2871.18s
                               ETA: 1123094.6s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1009 steps/s (collection: 15.956s, learning 0.282s)
               Value function loss: 18.3757
                    Surrogate loss: 0.0010
             Mean action noise std: 0.80
                       Mean reward: 6.56
               Mean episode length: 32.33
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 16.24s
                        Total time: 2887.42s
                               ETA: 1125023.0s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.006s, learning 0.201s)
               Value function loss: 98.1577
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 14.12
               Mean episode length: 31.85
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 16.21s
                        Total time: 2903.63s
                               ETA: 1126924.5s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.014s, learning 0.204s)
               Value function loss: 15.0564
                    Surrogate loss: -0.0093
             Mean action noise std: 0.80
                       Mean reward: 6.66
               Mean episode length: 32.02
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 16.22s
                        Total time: 2919.85s
                               ETA: 1128815.0s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.168s, learning 0.193s)
               Value function loss: 4.8304
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 5.90
               Mean episode length: 31.55
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 16.36s
                        Total time: 2936.21s
                               ETA: 1130745.8s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.181s, learning 0.183s)
               Value function loss: 0.8295
                    Surrogate loss: -0.0353
             Mean action noise std: 0.80
                       Mean reward: 5.16
               Mean episode length: 29.55
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 16.36s
                        Total time: 2952.57s
                               ETA: 1132663.3s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1033 steps/s (collection: 15.685s, learning 0.164s)
               Value function loss: 4.6789
                    Surrogate loss: -0.0083
             Mean action noise std: 0.80
                       Mean reward: 8.73
               Mean episode length: 31.99
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 15.85s
                        Total time: 2968.42s
                               ETA: 1134369.2s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.051s, learning 0.164s)
               Value function loss: 30.4559
                    Surrogate loss: -0.0010
             Mean action noise std: 0.80
                       Mean reward: 12.99
               Mean episode length: 30.68
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 16.22s
                        Total time: 2984.64s
                               ETA: 1136201.1s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.306s, learning 0.224s)
               Value function loss: 58.5518
                    Surrogate loss: -0.0036
             Mean action noise std: 0.80
                       Mean reward: 9.66
               Mean episode length: 33.60
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 16.53s
                        Total time: 3001.17s
                               ETA: 1138138.3s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.067s, learning 0.211s)
               Value function loss: 60.6829
                    Surrogate loss: -0.0026
             Mean action noise std: 0.80
                       Mean reward: 6.27
               Mean episode length: 31.75
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 16.28s
                        Total time: 3017.45s
                               ETA: 1139965.8s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.275s, learning 0.162s)
               Value function loss: 13.1044
                    Surrogate loss: -0.0072
             Mean action noise std: 0.80
                       Mean reward: 11.05
               Mean episode length: 31.33
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 16.44s
                        Total time: 3033.88s
                               ETA: 1141839.2s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.060s, learning 0.209s)
               Value function loss: 17.8830
                    Surrogate loss: 0.0001
             Mean action noise std: 0.80
                       Mean reward: 6.05
               Mean episode length: 31.35
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 16.27s
                        Total time: 3050.15s
                               ETA: 1143634.9s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.964s, learning 0.189s)
               Value function loss: 81.7154
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 5.63
               Mean episode length: 30.40
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 16.15s
                        Total time: 3066.31s
                               ETA: 1145374.1s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.062s, learning 0.175s)
               Value function loss: 1.0880
                    Surrogate loss: -0.0297
             Mean action noise std: 0.80
                       Mean reward: 5.85
               Mean episode length: 30.85
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 16.24s
                        Total time: 3082.54s
                               ETA: 1147131.1s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1028 steps/s (collection: 15.752s, learning 0.186s)
               Value function loss: 24.6390
                    Surrogate loss: 0.0024
             Mean action noise std: 0.80
                       Mean reward: 6.27
               Mean episode length: 31.82
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 15.94s
                        Total time: 3098.48s
                               ETA: 1148764.0s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.796s, learning 0.162s)
               Value function loss: 48.7645
                    Surrogate loss: -0.0036
             Mean action noise std: 0.80
                       Mean reward: 6.22
               Mean episode length: 31.54
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 15.96s
                        Total time: 3114.44s
                               ETA: 1150392.2s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.898s, learning 0.192s)
               Value function loss: 62.4434
                    Surrogate loss: -0.0024
             Mean action noise std: 0.80
                       Mean reward: 5.85
               Mean episode length: 31.01
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 16.09s
                        Total time: 3130.53s
                               ETA: 1152056.9s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1103 steps/s (collection: 14.689s, learning 0.164s)
               Value function loss: 1.0400
                    Surrogate loss: -0.0284
             Mean action noise std: 0.80
                       Mean reward: 5.85
               Mean episode length: 31.21
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 14.85s
                        Total time: 3145.38s
                               ETA: 1153255.9s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 2054 steps/s (collection: 7.797s, learning 0.178s)
               Value function loss: 82.8693
                    Surrogate loss: 0.0002
             Mean action noise std: 0.80
                       Mean reward: 5.55
               Mean episode length: 30.29
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 7.97s
                        Total time: 3153.35s
                               ETA: 1151933.1s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.858s, learning 0.169s)
               Value function loss: 73.5138
                    Surrogate loss: -0.0048
             Mean action noise std: 0.80
                       Mean reward: 6.63
               Mean episode length: 32.83
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 8.03s
                        Total time: 3161.38s
                               ETA: 1150638.9s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.275s, learning 0.176s)
               Value function loss: 1.6220
                    Surrogate loss: -0.0118
             Mean action noise std: 0.80
                       Mean reward: 5.56
               Mean episode length: 30.44
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 8.45s
                        Total time: 3169.83s
                               ETA: 1149507.8s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.309s, learning 0.164s)
               Value function loss: 23.3632
                    Surrogate loss: -0.0001
             Mean action noise std: 0.80
                       Mean reward: 5.59
               Mean episode length: 30.27
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 8.47s
                        Total time: 3178.30s
                               ETA: 1148392.8s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.950s, learning 0.161s)
               Value function loss: 49.1400
                    Surrogate loss: -0.0031
             Mean action noise std: 0.80
                       Mean reward: 6.26
               Mean episode length: 31.48
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 8.11s
                        Total time: 3186.42s
                               ETA: 1147155.6s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.319s, learning 0.171s)
               Value function loss: 16.4096
                    Surrogate loss: -0.0022
             Mean action noise std: 0.80
                       Mean reward: 6.00
               Mean episode length: 31.11
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 8.49s
                        Total time: 3194.91s
                               ETA: 1146063.2s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.631s, learning 0.201s)
               Value function loss: 92.4257
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 6.23
               Mean episode length: 31.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 8.83s
                        Total time: 3203.74s
                               ETA: 1145100.6s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.213s, learning 0.172s)
               Value function loss: 38.5989
                    Surrogate loss: -0.0043
             Mean action noise std: 0.80
                       Mean reward: 8.77
               Mean episode length: 31.21
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 8.38s
                        Total time: 3212.12s
                               ETA: 1143985.7s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.531s, learning 0.182s)
               Value function loss: 19.5408
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 5.63
               Mean episode length: 30.57
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.42
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 8.71s
                        Total time: 3220.83s
                               ETA: 1142995.1s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.049s, learning 0.163s)
               Value function loss: 1.3686
                    Surrogate loss: -0.0223
             Mean action noise std: 0.80
                       Mean reward: 5.56
               Mean episode length: 30.06
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 8.21s
                        Total time: 3229.05s
                               ETA: 1141834.4s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.283s, learning 0.189s)
               Value function loss: 19.9401
                    Surrogate loss: 0.0004
             Mean action noise std: 0.80
                       Mean reward: 11.68
               Mean episode length: 31.96
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 8.47s
                        Total time: 3237.52s
                               ETA: 1140773.4s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.935s, learning 0.182s)
               Value function loss: 28.9796
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 6.00
               Mean episode length: 31.02
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 8.12s
                        Total time: 3245.64s
                               ETA: 1139595.4s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.010s, learning 0.163s)
               Value function loss: 79.8922
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 8.42
               Mean episode length: 30.94
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 8.17s
                        Total time: 3253.81s
                               ETA: 1138445.1s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.400s, learning 0.182s)
               Value function loss: 43.7061
                    Surrogate loss: -0.0043
             Mean action noise std: 0.80
                       Mean reward: 6.35
               Mean episode length: 31.67
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 8.58s
                        Total time: 3262.39s
                               ETA: 1137445.1s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.926s, learning 0.173s)
               Value function loss: 133.0245
                    Surrogate loss: -0.0017
             Mean action noise std: 0.80
                       Mean reward: 6.42
               Mean episode length: 31.77
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 8.10s
                        Total time: 3270.49s
                               ETA: 1136284.6s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.188s, learning 0.166s)
               Value function loss: 45.2484
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 6.22
               Mean episode length: 31.30
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 8.35s
                        Total time: 3278.84s
                               ETA: 1135220.1s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.149s, learning 0.197s)
               Value function loss: 1.6956
                    Surrogate loss: -0.0316
             Mean action noise std: 0.80
                       Mean reward: 6.61
               Mean episode length: 32.63
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 8.35s
                        Total time: 3287.19s
                               ETA: 1134160.2s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.524s, learning 0.190s)
               Value function loss: 215.7901
                    Surrogate loss: 0.0002
             Mean action noise std: 0.80
                       Mean reward: 8.87
               Mean episode length: 31.95
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 8.71s
                        Total time: 3295.90s
                               ETA: 1133234.2s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.202s, learning 0.165s)
               Value function loss: 15.3225
                    Surrogate loss: -0.0088
             Mean action noise std: 0.80
                       Mean reward: 6.35
               Mean episode length: 31.88
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 8.37s
                        Total time: 3304.27s
                               ETA: 1132195.4s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.866s, learning 0.163s)
               Value function loss: 70.4622
                    Surrogate loss: 0.0018
             Mean action noise std: 0.80
                       Mean reward: 6.64
               Mean episode length: 32.68
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 8.03s
                        Total time: 3312.30s
                               ETA: 1131048.1s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 2068 steps/s (collection: 7.742s, learning 0.177s)
               Value function loss: 1.3203
                    Surrogate loss: -0.0315
             Mean action noise std: 0.80
                       Mean reward: 16.51
               Mean episode length: 32.77
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 7.92s
                        Total time: 3320.22s
                               ETA: 1129871.4s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.373s, learning 0.175s)
               Value function loss: 24.8818
                    Surrogate loss: 0.0002
             Mean action noise std: 0.80
                       Mean reward: 6.13
               Mean episode length: 31.85
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 8.55s
                        Total time: 3328.77s
                               ETA: 1128916.0s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.880s, learning 0.163s)
               Value function loss: 19.1242
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 5.90
               Mean episode length: 31.55
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 8.04s
                        Total time: 3336.81s
                               ETA: 1127796.3s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.105s, learning 0.158s)
               Value function loss: 17.7251
                    Surrogate loss: -0.0029
             Mean action noise std: 0.80
                       Mean reward: 6.27
               Mean episode length: 32.01
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 8.26s
                        Total time: 3345.07s
                               ETA: 1126758.5s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 2064 steps/s (collection: 7.775s, learning 0.162s)
               Value function loss: 243.7262
                    Surrogate loss: -0.0038
             Mean action noise std: 0.80
                       Mean reward: 9.19
               Mean episode length: 32.78
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 7.94s
                        Total time: 3353.01s
                               ETA: 1125617.9s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.898s, learning 0.193s)
               Value function loss: 55.7670
                    Surrogate loss: -0.0065
             Mean action noise std: 0.80
                       Mean reward: 6.63
               Mean episode length: 32.85
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 8.09s
                        Total time: 3361.10s
                               ETA: 1124536.4s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.093s, learning 0.162s)
               Value function loss: 122.3896
                    Surrogate loss: -0.0038
             Mean action noise std: 0.80
                       Mean reward: 7.03
               Mean episode length: 33.45
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 8.26s
                        Total time: 3369.36s
                               ETA: 1123516.9s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 2089 steps/s (collection: 7.679s, learning 0.162s)
               Value function loss: 2.2988
                    Surrogate loss: -0.0281
             Mean action noise std: 0.80
                       Mean reward: 6.62
               Mean episode length: 32.41
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 7.84s
                        Total time: 3377.20s
                               ETA: 1122366.4s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.973s, learning 0.159s)
               Value function loss: 83.1567
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: 7.02
               Mean episode length: 33.63
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 8.13s
                        Total time: 3385.33s
                               ETA: 1121319.9s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.801s, learning 0.163s)
               Value function loss: 221.3872
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 6.37
               Mean episode length: 31.69
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 7.96s
                        Total time: 3393.29s
                               ETA: 1120224.9s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.841s, learning 0.222s)
               Value function loss: 143.8622
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 5.83
               Mean episode length: 30.80
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 8.06s
                        Total time: 3401.36s
                               ETA: 1119169.6s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.164s, learning 0.207s)
               Value function loss: 27.7021
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 6.20
               Mean episode length: 31.48
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 8.37s
                        Total time: 3409.73s
                               ETA: 1118222.2s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.247s, learning 0.167s)
               Value function loss: 16.5344
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 5.61
               Mean episode length: 30.77
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 8.41s
                        Total time: 3418.14s
                               ETA: 1117294.9s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1989 steps/s (collection: 7.988s, learning 0.248s)
               Value function loss: 33.9459
                    Surrogate loss: -0.0044
             Mean action noise std: 0.79
                       Mean reward: 7.16
               Mean episode length: 33.33
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 8.24s
                        Total time: 3426.38s
                               ETA: 1116316.0s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.073s, learning 0.286s)
               Value function loss: 55.4507
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 8.82
               Mean episode length: 32.24
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 8.36s
                        Total time: 3434.74s
                               ETA: 1115383.0s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.257s, learning 0.172s)
               Value function loss: 5.4611
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 5.98
               Mean episode length: 31.52
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 8.43s
                        Total time: 3443.17s
                               ETA: 1114479.0s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.995s, learning 0.168s)
               Value function loss: 1.0836
                    Surrogate loss: -0.0270
             Mean action noise std: 0.79
                       Mean reward: 6.82
               Mean episode length: 33.08
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 8.16s
                        Total time: 3451.33s
                               ETA: 1113494.5s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.125s, learning 0.187s)
               Value function loss: 43.0453
                    Surrogate loss: 0.0011
             Mean action noise std: 0.79
                       Mean reward: 6.32
               Mean episode length: 31.55
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 8.31s
                        Total time: 3459.64s
                               ETA: 1112564.5s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.401s, learning 0.182s)
               Value function loss: 92.7180
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 5.85
               Mean episode length: 30.95
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 8.58s
                        Total time: 3468.22s
                               ETA: 1111727.3s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.325s, learning 0.163s)
               Value function loss: 154.2942
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 6.28
               Mean episode length: 31.50
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 8.49s
                        Total time: 3476.71s
                               ETA: 1110864.9s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.013s, learning 0.161s)
               Value function loss: 55.8524
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 6.08
               Mean episode length: 31.49
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 8.17s
                        Total time: 3484.89s
                               ETA: 1109908.0s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 2033 steps/s (collection: 7.860s, learning 0.196s)
               Value function loss: 204.4113
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 8.84
               Mean episode length: 32.12
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 8.06s
                        Total time: 3492.94s
                               ETA: 1108919.9s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.076s, learning 0.175s)
               Value function loss: 66.8491
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 5.63
               Mean episode length: 30.34
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 8.25s
                        Total time: 3501.19s
                               ETA: 1107999.6s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 2095 steps/s (collection: 7.637s, learning 0.184s)
               Value function loss: 94.3455
                    Surrogate loss: 0.0008
             Mean action noise std: 0.79
                       Mean reward: 10.73
               Mean episode length: 30.65
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 7.82s
                        Total time: 3509.01s
                               ETA: 1106949.2s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 2051 steps/s (collection: 7.767s, learning 0.218s)
               Value function loss: 74.6133
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 6.39
               Mean episode length: 31.64
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 7.98s
                        Total time: 3517.00s
                               ETA: 1105957.1s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.273s, learning 0.158s)
               Value function loss: 15.7938
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: 15.77
               Mean episode length: 30.53
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 8.43s
                        Total time: 3525.43s
                               ETA: 1105111.1s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.170s, learning 0.193s)
               Value function loss: 1.8679
                    Surrogate loss: -0.0265
             Mean action noise std: 0.79
                       Mean reward: 6.69
               Mean episode length: 32.27
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 8.36s
                        Total time: 3533.79s
                               ETA: 1104248.9s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.241s, learning 0.170s)
               Value function loss: 18.8271
                    Surrogate loss: -0.0010
             Mean action noise std: 0.79
                       Mean reward: 5.98
               Mean episode length: 30.94
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 8.41s
                        Total time: 3542.20s
                               ETA: 1103407.2s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.769s, learning 0.201s)
               Value function loss: 95.5609
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: 6.21
               Mean episode length: 31.50
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 7.97s
                        Total time: 3550.17s
                               ETA: 1102433.6s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.254s, learning 0.245s)
               Value function loss: 179.6994
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 6.15
               Mean episode length: 31.77
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 8.50s
                        Total time: 3558.67s
                               ETA: 1101630.0s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.916s, learning 0.167s)
               Value function loss: 504.5463
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 8.76
               Mean episode length: 31.44
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 8.08s
                        Total time: 3566.76s
                               ETA: 1100702.8s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.334s, learning 0.169s)
               Value function loss: 27.8414
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: 8.53
               Mean episode length: 31.30
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 8.50s
                        Total time: 3575.26s
                               ETA: 1099910.5s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.880s, learning 0.173s)
               Value function loss: 12.5337
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 6.13
               Mean episode length: 31.20
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 8.05s
                        Total time: 3583.31s
                               ETA: 1098985.1s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.205s, learning 0.191s)
               Value function loss: 167.4767
                    Surrogate loss: 0.0003
             Mean action noise std: 0.79
                       Mean reward: 5.83
               Mean episode length: 31.09
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 8.40s
                        Total time: 3591.71s
                               ETA: 1098170.0s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.266s, learning 0.162s)
               Value function loss: 239.1031
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 14.54
               Mean episode length: 33.30
                  Mean reward/step: 0.38
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 8.43s
                        Total time: 3600.13s
                               ETA: 1097369.4s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.293s, learning 0.184s)
               Value function loss: 30.3890
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 6.94
               Mean episode length: 33.09
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 8.48s
                        Total time: 3608.61s
                               ETA: 1096589.1s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.222s, learning 0.161s)
               Value function loss: 54.1100
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 8.69
               Mean episode length: 31.53
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 8.38s
                        Total time: 3617.00s
                               ETA: 1095784.6s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.089s, learning 0.201s)
               Value function loss: 18.4973
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 6.70
               Mean episode length: 32.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 8.29s
                        Total time: 3625.29s
                               ETA: 1094957.0s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.450s, learning 0.163s)
               Value function loss: 31.7442
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 8.83
               Mean episode length: 32.02
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 8.61s
                        Total time: 3633.90s
                               ETA: 1094231.4s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.177s, learning 0.231s)
               Value function loss: 184.5365
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 6.68
               Mean episode length: 32.75
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 8.41s
                        Total time: 3642.31s
                               ETA: 1093448.8s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.166s)
               Value function loss: 72.4691
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 6.26
               Mean episode length: 31.57
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 8.47s
                        Total time: 3650.78s
                               ETA: 1092689.1s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.065s, learning 0.176s)
               Value function loss: 6.3090
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 6.58
               Mean episode length: 32.36
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 8.24s
                        Total time: 3659.02s
                               ETA: 1091865.9s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.330s, learning 0.171s)
               Value function loss: 28.2498
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 9.23
               Mean episode length: 32.50
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 8.50s
                        Total time: 3667.52s
                               ETA: 1091124.6s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.079s, learning 0.230s)
               Value function loss: 19.5089
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 5.82
               Mean episode length: 30.59
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 8.31s
                        Total time: 3675.83s
                               ETA: 1090331.0s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.098s, learning 0.188s)
               Value function loss: 25.6239
                    Surrogate loss: -0.0003
             Mean action noise std: 0.79
                       Mean reward: 14.32
               Mean episode length: 32.62
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 8.29s
                        Total time: 3684.11s
                               ETA: 1089535.0s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.025s, learning 0.186s)
               Value function loss: 161.5769
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 9.26
               Mean episode length: 32.43
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 8.21s
                        Total time: 3692.32s
                               ETA: 1088721.9s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.087s, learning 0.204s)
               Value function loss: 161.8056
                    Surrogate loss: -0.0031
             Mean action noise std: 0.79
                       Mean reward: 6.74
               Mean episode length: 32.54
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 8.29s
                        Total time: 3700.61s
                               ETA: 1087936.8s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.827s, learning 0.273s)
               Value function loss: 2.3586
                    Surrogate loss: -0.0281
             Mean action noise std: 0.79
                       Mean reward: 7.17
               Mean episode length: 33.80
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 8.10s
                        Total time: 3708.71s
                               ETA: 1087100.2s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.162s)
               Value function loss: 89.5088
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 9.82
               Mean episode length: 33.60
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 8.48s
                        Total time: 3717.19s
                               ETA: 1086378.5s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.102s, learning 0.163s)
               Value function loss: 133.0114
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 6.87
               Mean episode length: 33.18
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 8.26s
                        Total time: 3725.45s
                               ETA: 1085599.5s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.447s, learning 0.166s)
               Value function loss: 200.8327
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 6.91
               Mean episode length: 33.08
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 8.61s
                        Total time: 3734.07s
                               ETA: 1084926.2s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.212s, learning 0.171s)
               Value function loss: 23.6626
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 10.90
               Mean episode length: 31.39
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 8.38s
                        Total time: 3742.45s
                               ETA: 1084190.1s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.923s, learning 0.167s)
               Value function loss: 30.1885
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: 6.88
               Mean episode length: 32.84
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 8.09s
                        Total time: 3750.54s
                               ETA: 1083373.6s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.165s, learning 0.198s)
               Value function loss: 408.9542
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: 6.71
               Mean episode length: 32.88
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 8.36s
                        Total time: 3758.90s
                               ETA: 1082640.3s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.119s, learning 0.165s)
               Value function loss: 102.8026
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 11.37
               Mean episode length: 31.69
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 8.28s
                        Total time: 3767.19s
                               ETA: 1081888.4s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.292s, learning 0.163s)
               Value function loss: 599.1418
                    Surrogate loss: -0.0023
             Mean action noise std: 0.79
                       Mean reward: 7.36
               Mean episode length: 33.84
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 8.46s
                        Total time: 3775.64s
                               ETA: 1081189.8s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.475s, learning 0.199s)
               Value function loss: 80.2448
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 15.33
               Mean episode length: 34.46
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 9.67s
                        Total time: 3785.32s
                               ETA: 1080843.4s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.983s, learning 0.190s)
               Value function loss: 30.2688
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 6.58
               Mean episode length: 32.54
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 16.17s
                        Total time: 3801.49s
                               ETA: 1082349.3s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1063 steps/s (collection: 15.232s, learning 0.176s)
               Value function loss: 85.4767
                    Surrogate loss: 0.0006
             Mean action noise std: 0.79
                       Mean reward: 11.43
               Mean episode length: 31.60
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 15.41s
                        Total time: 3816.90s
                               ETA: 1083629.3s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.284s, learning 0.187s)
               Value function loss: 34.7132
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 7.11
               Mean episode length: 33.77
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 16.47s
                        Total time: 3833.37s
                               ETA: 1085202.9s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.262s, learning 0.164s)
               Value function loss: 2.2124
                    Surrogate loss: 0.0109
             Mean action noise std: 0.79
                       Mean reward: 6.50
               Mean episode length: 32.19
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 16.43s
                        Total time: 3849.80s
                               ETA: 1086754.7s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.187s, learning 0.193s)
               Value function loss: 194.9308
                    Surrogate loss: 0.0003
             Mean action noise std: 0.79
                       Mean reward: 6.37
               Mean episode length: 32.15
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 16.38s
                        Total time: 3866.18s
                               ETA: 1088284.7s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.020s, learning 0.208s)
               Value function loss: 62.9966
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 6.01
               Mean episode length: 31.44
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 16.23s
                        Total time: 3882.40s
                               ETA: 1089763.3s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.034s, learning 0.181s)
               Value function loss: 154.6562
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: 6.76
               Mean episode length: 32.77
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 16.21s
                        Total time: 3898.62s
                               ETA: 1091229.8s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1022 steps/s (collection: 15.846s, learning 0.173s)
               Value function loss: 19.0632
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 6.37
               Mean episode length: 31.64
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 16.02s
                        Total time: 3914.64s
                               ETA: 1092633.5s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1038 steps/s (collection: 15.612s, learning 0.159s)
               Value function loss: 158.1795
                    Surrogate loss: -0.0023
             Mean action noise std: 0.79
                       Mean reward: 6.62
               Mean episode length: 32.47
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 15.77s
                        Total time: 3930.41s
                               ETA: 1093960.1s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.175s, learning 0.168s)
               Value function loss: 16.2711
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 7.16
               Mean episode length: 33.25
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 16.34s
                        Total time: 3946.75s
                               ETA: 1095438.1s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.864s, learning 0.169s)
               Value function loss: 38.3418
                    Surrogate loss: -0.0009
             Mean action noise std: 0.79
                       Mean reward: 6.41
               Mean episode length: 31.69
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 16.03s
                        Total time: 3962.79s
                               ETA: 1096821.9s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.036s, learning 0.161s)
               Value function loss: 19.7501
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 6.22
               Mean episode length: 31.76
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 16.20s
                        Total time: 3978.98s
                               ETA: 1098243.1s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.015s, learning 0.165s)
               Value function loss: 24.2513
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 6.53
               Mean episode length: 32.02
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 16.18s
                        Total time: 3995.16s
                               ETA: 1099651.8s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1039 steps/s (collection: 15.597s, learning 0.169s)
               Value function loss: 261.3313
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 7.01
               Mean episode length: 33.03
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 15.77s
                        Total time: 4010.93s
                               ETA: 1100939.1s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1042 steps/s (collection: 15.487s, learning 0.234s)
               Value function loss: 178.9397
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 6.59
               Mean episode length: 32.49
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 15.72s
                        Total time: 4026.65s
                               ETA: 1102206.8s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1022 steps/s (collection: 15.826s, learning 0.199s)
               Value function loss: 115.6638
                    Surrogate loss: -0.0008
             Mean action noise std: 0.79
                       Mean reward: 8.82
               Mean episode length: 31.60
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 16.03s
                        Total time: 4042.67s
                               ETA: 1103550.5s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1013 steps/s (collection: 16.001s, learning 0.160s)
               Value function loss: 27.2219
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 8.70
               Mean episode length: 31.49
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 16.16s
                        Total time: 4058.84s
                               ETA: 1104923.9s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1062 steps/s (collection: 15.258s, learning 0.160s)
               Value function loss: 144.0651
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: 11.32
               Mean episode length: 31.76
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 15.42s
                        Total time: 4074.25s
                               ETA: 1106087.9s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.179s, learning 0.179s)
               Value function loss: 138.8935
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 12.08
               Mean episode length: 33.24
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 16.36s
                        Total time: 4090.61s
                               ETA: 1107500.0s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1042 steps/s (collection: 15.551s, learning 0.166s)
               Value function loss: 193.6385
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: 7.19
               Mean episode length: 32.94
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 15.72s
                        Total time: 4106.33s
                               ETA: 1108731.4s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.919s, learning 0.161s)
               Value function loss: 43.1920
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 9.24
               Mean episode length: 32.79
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 16.08s
                        Total time: 4122.41s
                               ETA: 1110053.8s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.165s, learning 0.175s)
               Value function loss: 92.4153
                    Surrogate loss: 0.0031
             Mean action noise std: 0.79
                       Mean reward: 6.94
               Mean episode length: 33.03
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 16.34s
                        Total time: 4138.75s
                               ETA: 1111438.8s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.137s, learning 0.172s)
               Value function loss: 91.1276
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: 14.52
               Mean episode length: 33.01
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 16.31s
                        Total time: 4155.06s
                               ETA: 1112807.8s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1019 steps/s (collection: 15.885s, learning 0.190s)
               Value function loss: 238.0665
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 9.37
               Mean episode length: 32.84
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 16.07s
                        Total time: 4171.13s
                               ETA: 1114106.8s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.170s, learning 0.170s)
               Value function loss: 148.2189
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: 14.84
               Mean episode length: 33.68
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 16.34s
                        Total time: 4187.47s
                               ETA: 1115469.4s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.874s, learning 0.184s)
               Value function loss: 104.2263
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 6.84
               Mean episode length: 32.73
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 16.06s
                        Total time: 4203.53s
                               ETA: 1116749.7s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.008s, learning 0.166s)
               Value function loss: 329.7181
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 16.78
               Mean episode length: 32.40
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 16.17s
                        Total time: 4219.71s
                               ETA: 1118054.0s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.260s, learning 0.190s)
               Value function loss: 6.0698
                    Surrogate loss: -0.0237
             Mean action noise std: 0.79
                       Mean reward: 6.17
               Mean episode length: 31.27
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 16.45s
                        Total time: 4236.16s
                               ETA: 1119424.3s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.009s, learning 0.187s)
               Value function loss: 165.8886
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: 5.94
               Mean episode length: 31.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 16.20s
                        Total time: 4252.35s
                               ETA: 1120720.0s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.096s, learning 0.162s)
               Value function loss: 3.4583
                    Surrogate loss: -0.0255
             Mean action noise std: 0.79
                       Mean reward: 6.16
               Mean episode length: 31.64
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 16.26s
                        Total time: 4268.61s
                               ETA: 1122025.1s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.077s, learning 0.169s)
               Value function loss: 439.5800
                    Surrogate loss: -0.0009
             Mean action noise std: 0.79
                       Mean reward: 7.06
               Mean episode length: 33.49
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 16.25s
                        Total time: 4284.86s
                               ETA: 1123320.2s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1011 steps/s (collection: 15.975s, learning 0.227s)
               Value function loss: 28.9791
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 6.24
               Mean episode length: 32.19
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 16.20s
                        Total time: 4301.06s
                               ETA: 1124596.8s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.965s, learning 0.205s)
               Value function loss: 265.6431
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: 9.33
               Mean episode length: 32.75
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 16.17s
                        Total time: 4317.23s
                               ETA: 1125858.6s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.871s, learning 0.225s)
               Value function loss: 204.6086
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: 12.00
               Mean episode length: 33.30
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 16.10s
                        Total time: 4333.33s
                               ETA: 1127094.5s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.155s, learning 0.163s)
               Value function loss: 3.8654
                    Surrogate loss: -0.0235
             Mean action noise std: 0.79
                       Mean reward: 9.45
               Mean episode length: 32.82
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.45
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 16.32s
                        Total time: 4349.64s
                               ETA: 1128381.2s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.627s, learning 0.190s)
               Value function loss: 37.0471
                    Surrogate loss: 0.0008
             Mean action noise std: 0.79
                       Mean reward: 5.74
               Mean episode length: 30.79
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 15.82s
                        Total time: 4365.46s
                               ETA: 1129531.5s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.131s, learning 0.168s)
               Value function loss: 405.5631
                    Surrogate loss: -0.0024
             Mean action noise std: 0.79
                       Mean reward: 6.03
               Mean episode length: 30.86
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 16.30s
                        Total time: 4381.76s
                               ETA: 1130800.2s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.790s, learning 0.163s)
               Value function loss: 142.7274
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 6.24
               Mean episode length: 31.46
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 11.95s
                        Total time: 4393.71s
                               ETA: 1130943.5s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.893s, learning 0.208s)
               Value function loss: 5.4593
                    Surrogate loss: -0.0253
             Mean action noise std: 0.79
                       Mean reward: 6.36
               Mean episode length: 32.36
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 8.10s
                        Total time: 4401.81s
                               ETA: 1130097.0s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.779s, learning 0.160s)
               Value function loss: 57.1840
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: 6.37
               Mean episode length: 32.08
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 7.94s
                        Total time: 4409.75s
                               ETA: 1129213.5s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 2044 steps/s (collection: 7.857s, learning 0.156s)
               Value function loss: 21.0348
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 6.26
               Mean episode length: 32.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 8.01s
                        Total time: 4417.76s
                               ETA: 1128353.4s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.262s, learning 0.186s)
               Value function loss: 43.5020
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: 6.80
               Mean episode length: 32.42
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 8.45s
                        Total time: 4426.21s
                               ETA: 1127608.4s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.952s, learning 0.197s)
               Value function loss: 19.9107
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 7.03
               Mean episode length: 33.19
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 8.15s
                        Total time: 4434.36s
                               ETA: 1126791.2s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.065s, learning 0.200s)
               Value function loss: 24.5389
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 6.00
               Mean episode length: 30.89
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 8.26s
                        Total time: 4442.62s
                               ETA: 1126007.6s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 2048 steps/s (collection: 7.808s, learning 0.191s)
               Value function loss: 576.1924
                    Surrogate loss: -0.0022
             Mean action noise std: 0.79
                       Mean reward: 6.91
               Mean episode length: 32.58
                  Mean reward/step: 0.43
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 8.00s
                        Total time: 4450.62s
                               ETA: 1125160.6s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.181s, learning 0.164s)
               Value function loss: 1026.7043
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 11.67
               Mean episode length: 32.75
                  Mean reward/step: 0.51
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 8.34s
                        Total time: 4458.97s
                               ETA: 1124405.0s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.052s, learning 0.181s)
               Value function loss: 191.0342
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 5.57
               Mean episode length: 30.36
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0239
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 8.23s
                        Total time: 4467.20s
                               ETA: 1123625.0s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.321s, learning 0.171s)
               Value function loss: 38.5884
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 7.32
               Mean episode length: 33.35
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0239
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 8.49s
                        Total time: 4475.69s
                               ETA: 1122914.2s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.833s, learning 0.207s)
               Value function loss: 243.4238
                    Surrogate loss: -0.0001
             Mean action noise std: 0.79
                       Mean reward: 12.17
               Mean episode length: 33.80
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0273
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 8.04s
                        Total time: 4483.73s
                               ETA: 1122093.6s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.030s, learning 0.162s)
               Value function loss: 70.6949
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 14.43
               Mean episode length: 33.08
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 8.19s
                        Total time: 4491.93s
                               ETA: 1121315.2s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.236s, learning 0.164s)
               Value function loss: 156.0146
                    Surrogate loss: -0.0002
             Mean action noise std: 0.79
                       Mean reward: 31.77
               Mean episode length: 32.91
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 8.40s
                        Total time: 4500.33s
                               ETA: 1120592.4s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.128s, learning 0.177s)
               Value function loss: 34.2397
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 8.28
               Mean episode length: 30.69
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 8.31s
                        Total time: 4508.63s
                               ETA: 1119849.6s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.975s, learning 0.172s)
               Value function loss: 179.4639
                    Surrogate loss: -0.0018
             Mean action noise std: 0.79
                       Mean reward: 6.32
               Mean episode length: 31.26
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 8.15s
                        Total time: 4516.78s
                               ETA: 1119071.2s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.115s, learning 0.190s)
               Value function loss: 110.8535
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: 6.67
               Mean episode length: 32.61
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 8.30s
                        Total time: 4525.08s
                               ETA: 1118335.6s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.939s, learning 0.170s)
               Value function loss: 62.5518
                    Surrogate loss: -0.0002
             Mean action noise std: 0.79
                       Mean reward: 6.37
               Mean episode length: 31.83
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 8.11s
                        Total time: 4533.19s
                               ETA: 1117555.1s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.889s, learning 0.189s)
               Value function loss: 86.4746
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: 5.69
               Mean episode length: 30.08
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 8.08s
                        Total time: 4541.27s
                               ETA: 1116771.1s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.382s, learning 0.158s)
               Value function loss: 29.8570
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 6.42
               Mean episode length: 31.76
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 8.54s
                        Total time: 4549.81s
                               ETA: 1116104.2s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 2074 steps/s (collection: 7.736s, learning 0.162s)
               Value function loss: 48.4864
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 8.85
               Mean episode length: 32.24
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 7.90s
                        Total time: 4557.71s
                               ETA: 1115283.4s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.086s, learning 0.276s)
               Value function loss: 26.7721
                    Surrogate loss: 0.0050
             Mean action noise std: 0.79
                       Mean reward: 13.97
               Mean episode length: 32.07
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 8.36s
                        Total time: 4566.07s
                               ETA: 1114579.8s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.146s, learning 0.163s)
               Value function loss: 115.5848
                    Surrogate loss: -0.0003
             Mean action noise std: 0.79
                       Mean reward: 11.86
               Mean episode length: 32.86
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 8.31s
                        Total time: 4574.38s
                               ETA: 1113866.9s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.959s, learning 0.158s)
               Value function loss: 143.5698
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 9.58
               Mean episode length: 33.25
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 8.12s
                        Total time: 4582.50s
                               ETA: 1113110.9s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.785s, learning 0.168s)
               Value function loss: 87.2015
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 6.66
               Mean episode length: 32.62
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 7.95s
                        Total time: 4590.45s
                               ETA: 1112318.6s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.159s, learning 0.168s)
               Value function loss: 58.5133
                    Surrogate loss: -0.0022
             Mean action noise std: 0.79
                       Mean reward: 9.43
               Mean episode length: 32.89
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 8.33s
                        Total time: 4598.78s
                               ETA: 1111620.4s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.071s, learning 0.177s)
               Value function loss: 19.1840
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 6.16
               Mean episode length: 31.48
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 8.25s
                        Total time: 4607.02s
                               ETA: 1110906.4s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.971s, learning 0.171s)
               Value function loss: 16.0660
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 14.17
               Mean episode length: 32.34
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 8.14s
                        Total time: 4615.17s
                               ETA: 1110170.4s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.089s, learning 0.258s)
               Value function loss: 69.5167
                    Surrogate loss: -0.0026
             Mean action noise std: 0.79
                       Mean reward: 6.04
               Mean episode length: 31.39
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 8.35s
                        Total time: 4623.51s
                               ETA: 1109487.2s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.172s, learning 0.176s)
               Value function loss: 14.9242
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 6.99
               Mean episode length: 33.25
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 8.35s
                        Total time: 4631.86s
                               ETA: 1108807.4s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.344s, learning 0.173s)
               Value function loss: 597.5801
                    Surrogate loss: -0.0034
             Mean action noise std: 0.79
                       Mean reward: 11.71
               Mean episode length: 32.46
                  Mean reward/step: 0.41
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 8.52s
                        Total time: 4640.38s
                               ETA: 1108171.1s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.207s, learning 0.185s)
               Value function loss: 164.2431
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 8.80
               Mean episode length: 31.75
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 8.39s
                        Total time: 4648.77s
                               ETA: 1107508.1s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.170s, learning 0.163s)
               Value function loss: 70.0061
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: 6.67
               Mean episode length: 32.73
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 8.33s
                        Total time: 4657.10s
                               ETA: 1106834.2s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.091s, learning 0.164s)
               Value function loss: 122.4517
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: 26.77
               Mean episode length: 32.77
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 8.25s
                        Total time: 4665.36s
                               ETA: 1106145.0s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.225s, learning 0.162s)
               Value function loss: 27.1823
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 9.85
               Mean episode length: 33.58
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 8.39s
                        Total time: 4673.74s
                               ETA: 1105490.2s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.840s, learning 0.161s)
               Value function loss: 316.8612
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 9.63
               Mean episode length: 33.89
                  Mean reward/step: 0.41
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 8.00s
                        Total time: 4681.74s
                               ETA: 1104747.4s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.962s, learning 0.166s)
               Value function loss: 113.9904
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 10.00
               Mean episode length: 33.91
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 8.13s
                        Total time: 4689.87s
                               ETA: 1104038.0s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.909s, learning 0.179s)
               Value function loss: 194.2813
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 6.78
               Mean episode length: 33.14
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 8.09s
                        Total time: 4697.96s
                               ETA: 1103322.6s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.320s, learning 0.172s)
               Value function loss: 100.0308
                    Surrogate loss: -0.0044
             Mean action noise std: 0.79
                       Mean reward: 19.73
               Mean episode length: 33.81
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 8.49s
                        Total time: 4706.45s
                               ETA: 1102705.1s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.990s, learning 0.156s)
               Value function loss: 45.1490
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: 9.61
               Mean episode length: 33.80
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 8.15s
                        Total time: 4714.60s
                               ETA: 1102009.6s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.141s, learning 0.162s)
               Value function loss: 47.5298
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: 7.58
               Mean episode length: 34.73
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 8.30s
                        Total time: 4722.90s
                               ETA: 1101353.9s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.162s)
               Value function loss: 106.8101
                    Surrogate loss: -0.0031
             Mean action noise std: 0.79
                       Mean reward: 20.60
               Mean episode length: 35.63
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 8.04s
                        Total time: 4730.94s
                               ETA: 1100640.1s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.056s, learning 0.186s)
               Value function loss: 123.5797
                    Surrogate loss: -0.0025
             Mean action noise std: 0.79
                       Mean reward: 7.59
               Mean episode length: 34.71
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 8.24s
                        Total time: 4739.18s
                               ETA: 1099976.6s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.079s, learning 0.197s)
               Value function loss: 167.2668
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 7.97
               Mean episode length: 35.41
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 8.28s
                        Total time: 4747.46s
                               ETA: 1099323.9s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.158s, learning 0.216s)
               Value function loss: 85.6037
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 7.22
               Mean episode length: 34.02
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 8.37s
                        Total time: 4755.83s
                               ETA: 1098696.8s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.828s, learning 0.177s)
               Value function loss: 87.5041
                    Surrogate loss: -0.0003
             Mean action noise std: 0.79
                       Mean reward: 10.22
               Mean episode length: 34.84
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 8.01s
                        Total time: 4763.84s
                               ETA: 1097987.7s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.057s, learning 0.214s)
               Value function loss: 973.5872
                    Surrogate loss: -0.0026
             Mean action noise std: 0.79
                       Mean reward: 14.57
               Mean episode length: 33.34
                  Mean reward/step: 0.52
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 8.27s
                        Total time: 4772.11s
                               ETA: 1097342.9s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.649s, learning 0.184s)
               Value function loss: 362.2643
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 19.91
               Mean episode length: 34.29
                  Mean reward/step: 0.49
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 8.83s
                        Total time: 4780.94s
                               ETA: 1096829.9s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.183s, learning 0.209s)
               Value function loss: 486.1194
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 7.60
               Mean episode length: 34.53
                  Mean reward/step: 0.44
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0239
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 8.39s
                        Total time: 4789.34s
                               ETA: 1096218.3s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.402s, learning 0.169s)
               Value function loss: 123.8811
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 7.27
               Mean episode length: 33.92
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 8.57s
                        Total time: 4797.91s
                               ETA: 1095650.2s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 2054 steps/s (collection: 7.806s, learning 0.169s)
               Value function loss: 454.0190
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: 14.61
               Mean episode length: 33.89
                  Mean reward/step: 0.39
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0273
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 7.97s
                        Total time: 4805.88s
                               ETA: 1094948.9s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.052s, learning 0.195s)
               Value function loss: 244.5937
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 7.19
               Mean episode length: 33.78
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0269
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 8.25s
                        Total time: 4814.13s
                               ETA: 1094312.7s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.756s, learning 0.190s)
               Value function loss: 78.6626
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 27.84
               Mean episode length: 34.92
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 7.95s
                        Total time: 4822.07s
                               ETA: 1093611.0s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.219s, learning 0.158s)
               Value function loss: 36.9183
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 12.98
               Mean episode length: 35.50
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 8.38s
                        Total time: 4830.45s
                               ETA: 1093010.0s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.119s, learning 0.207s)
               Value function loss: 37.9663
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 9.94
               Mean episode length: 34.33
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 8.33s
                        Total time: 4838.78s
                               ETA: 1092400.2s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.165s, learning 0.176s)
               Value function loss: 117.1949
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 11.60
               Mean episode length: 32.72
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 8.34s
                        Total time: 4847.12s
                               ETA: 1091796.7s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.073s, learning 0.247s)
               Value function loss: 146.1311
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 10.66
               Mean episode length: 35.56
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 8.32s
                        Total time: 4855.44s
                               ETA: 1091191.1s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.039s, learning 0.272s)
               Value function loss: 56.3637
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 7.01
               Mean episode length: 33.37
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 8.31s
                        Total time: 4863.75s
                               ETA: 1090586.1s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.157s, learning 0.164s)
               Value function loss: 127.4683
                    Surrogate loss: -0.0030
             Mean action noise std: 0.79
                       Mean reward: 16.96
               Mean episode length: 33.32
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 8.32s
                        Total time: 4872.07s
                               ETA: 1089986.1s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.289s, learning 0.179s)
               Value function loss: 317.4763
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: 15.40
               Mean episode length: 34.90
                  Mean reward/step: 0.45
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 8.47s
                        Total time: 4880.54s
                               ETA: 1089421.5s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.086s, learning 0.165s)
               Value function loss: 426.4923
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 10.13
               Mean episode length: 35.18
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 8.25s
                        Total time: 4888.79s
                               ETA: 1088811.0s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.926s, learning 0.165s)
               Value function loss: 29.2800
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 15.85
               Mean episode length: 35.70
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 8.09s
                        Total time: 4896.88s
                               ETA: 1088167.5s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.301s, learning 0.173s)
               Value function loss: 97.4138
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 23.23
               Mean episode length: 36.21
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 8.47s
                        Total time: 4905.35s
                               ETA: 1087611.9s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.398s, learning 0.177s)
               Value function loss: 721.1441
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: 6.70
               Mean episode length: 32.52
                  Mean reward/step: 0.56
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0239
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 8.58s
                        Total time: 4913.93s
                               ETA: 1087081.3s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 2083 steps/s (collection: 7.683s, learning 0.180s)
               Value function loss: 154.7987
                    Surrogate loss: -0.0055
             Mean action noise std: 0.79
                       Mean reward: 7.34
               Mean episode length: 34.12
                  Mean reward/step: 0.31
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 7.86s
                        Total time: 4921.79s
                               ETA: 1086395.6s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.041s, learning 0.191s)
               Value function loss: 386.3503
                    Surrogate loss: -0.0033
             Mean action noise std: 0.79
                       Mean reward: 7.37
               Mean episode length: 34.19
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 8.23s
                        Total time: 4930.02s
                               ETA: 1085794.3s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.269s, learning 0.179s)
               Value function loss: 137.7636
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 10.01
               Mean episode length: 34.01
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 8.45s
                        Total time: 4938.47s
                               ETA: 1085243.0s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.307s, learning 0.195s)
               Value function loss: 272.7086
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 10.10
               Mean episode length: 35.03
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0180
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 8.50s
                        Total time: 4946.98s
                               ETA: 1084706.1s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.454s, learning 0.163s)
               Value function loss: 256.9925
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 7.43
               Mean episode length: 33.69
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 8.62s
                        Total time: 4955.59s
                               ETA: 1084196.5s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.081s, learning 0.176s)
               Value function loss: 144.3487
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 15.90
               Mean episode length: 35.95
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 8.26s
                        Total time: 4963.85s
                               ETA: 1083610.5s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.039s, learning 0.180s)
               Value function loss: 423.1107
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 17.27
               Mean episode length: 33.88
                  Mean reward/step: 0.45
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 8.22s
                        Total time: 4972.07s
                               ETA: 1083018.6s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.964s, learning 0.169s)
               Value function loss: 395.9791
                    Surrogate loss: -0.0055
             Mean action noise std: 0.79
                       Mean reward: 7.45
               Mean episode length: 34.22
                  Mean reward/step: 0.50
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0234
Mean episode consecutive_successes: 0.0180
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 8.13s
                        Total time: 4980.20s
                               ETA: 1082410.7s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.452s, learning 0.171s)
               Value function loss: 774.8761
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 10.47
               Mean episode length: 35.47
                  Mean reward/step: 0.59
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 8.62s
                        Total time: 4988.82s
                               ETA: 1081911.5s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.079s, learning 0.173s)
               Value function loss: 475.7413
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 8.57
               Mean episode length: 36.89
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0317
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 8.25s
                        Total time: 4997.07s
                               ETA: 1081334.4s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.244s, learning 0.188s)
               Value function loss: 693.6717
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 10.04
               Mean episode length: 34.57
                  Mean reward/step: 0.52
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0371
Mean episode consecutive_successes: 0.0195
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 8.43s
                        Total time: 5005.51s
                               ETA: 1080798.7s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.940s, learning 0.161s)
               Value function loss: 481.4620
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 10.50
               Mean episode length: 35.29
                  Mean reward/step: 0.48
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0396
Mean episode consecutive_successes: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 8.10s
                        Total time: 5013.61s
                               ETA: 1080193.7s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.075s, learning 0.167s)
               Value function loss: 946.3987
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 14.12
               Mean episode length: 37.63
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0479
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 8.24s
                        Total time: 5021.85s
                               ETA: 1079621.7s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.443s, learning 0.168s)
               Value function loss: 652.0643
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 15.88
               Mean episode length: 36.08
                  Mean reward/step: 0.51
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 8.61s
                        Total time: 5030.46s
                               ETA: 1079131.4s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.082s, learning 0.219s)
               Value function loss: 708.5892
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 20.70
               Mean episode length: 36.21
                  Mean reward/step: 0.54
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0396
Mean episode consecutive_successes: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 8.30s
                        Total time: 5038.76s
                               ETA: 1078576.8s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.282s, learning 0.170s)
               Value function loss: 204.1971
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 7.83
               Mean episode length: 35.18
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0386
Mean episode consecutive_successes: 0.0259
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 8.45s
                        Total time: 5047.21s
                               ETA: 1078056.8s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.098s, learning 0.162s)
               Value function loss: 790.2782
                    Surrogate loss: -0.0024
             Mean action noise std: 0.79
                       Mean reward: 38.49
               Mean episode length: 36.01
                  Mean reward/step: 0.47
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0301
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 8.26s
                        Total time: 5055.47s
                               ETA: 1077498.0s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.273s, learning 0.161s)
               Value function loss: 1222.7142
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 7.42
               Mean episode length: 34.14
                  Mean reward/step: 0.66
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0303
Mean episode consecutive_successes: 0.0309
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 8.43s
                        Total time: 5063.91s
                               ETA: 1076978.6s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.142s, learning 0.186s)
               Value function loss: 575.6854
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 15.36
               Mean episode length: 35.07
                  Mean reward/step: 0.52
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0337
Mean episode consecutive_successes: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 8.33s
                        Total time: 5072.24s
                               ETA: 1076438.8s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1561 steps/s (collection: 10.262s, learning 0.230s)
               Value function loss: 150.7724
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 33.19
               Mean episode length: 35.76
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 10.49s
                        Total time: 5082.73s
                               ETA: 1076359.5s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.952s, learning 0.170s)
               Value function loss: 1031.3751
                    Surrogate loss: -0.0018
             Mean action noise std: 0.79
                       Mean reward: 22.69
               Mean episode length: 34.68
                  Mean reward/step: 0.59
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0345
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 16.12s
                        Total time: 5098.85s
                               ETA: 1077470.2s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.176s, learning 0.174s)
               Value function loss: 498.8775
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 11.22
               Mean episode length: 37.12
                  Mean reward/step: 0.48
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 16.35s
                        Total time: 5115.20s
                               ETA: 1078624.2s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.229s, learning 0.208s)
               Value function loss: 150.9628
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 7.84
               Mean episode length: 35.16
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0333
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 16.44s
                        Total time: 5131.64s
                               ETA: 1079791.5s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.582s, learning 0.172s)
               Value function loss: 1652.0101
                    Surrogate loss: -0.0033
             Mean action noise std: 0.79
                       Mean reward: 10.62
               Mean episode length: 35.58
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0322
Mean episode consecutive_successes: 0.0350
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 16.75s
                        Total time: 5148.39s
                               ETA: 1081020.5s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1034 steps/s (collection: 15.669s, learning 0.169s)
               Value function loss: 1060.0197
                    Surrogate loss: -0.0047
             Mean action noise std: 0.79
                       Mean reward: 13.00
               Mean episode length: 35.41
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0386
Mean episode consecutive_successes: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 15.84s
                        Total time: 5164.23s
                               ETA: 1082052.2s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.120s, learning 0.181s)
               Value function loss: 453.7130
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 30.52
               Mean episode length: 35.43
                  Mean reward/step: 0.51
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0444
Mean episode consecutive_successes: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 16.30s
                        Total time: 5180.53s
                               ETA: 1083176.6s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.432s, learning 0.167s)
               Value function loss: 166.9849
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 36.65
               Mean episode length: 37.36
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0381
Mean episode consecutive_successes: 0.0371
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 16.60s
                        Total time: 5197.13s
                               ETA: 1084358.4s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1024 steps/s (collection: 15.831s, learning 0.167s)
               Value function loss: 248.7138
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 18.73
               Mean episode length: 36.90
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0312
Mean episode consecutive_successes: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 16.00s
                        Total time: 5213.13s
                               ETA: 1085409.7s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.102s, learning 0.211s)
               Value function loss: 133.4248
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 10.76
               Mean episode length: 36.20
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0259
Mean episode consecutive_successes: 0.0368
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 16.31s
                        Total time: 5229.44s
                               ETA: 1086522.1s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1024 steps/s (collection: 15.824s, learning 0.172s)
               Value function loss: 691.0453
                    Surrogate loss: 0.0010
             Mean action noise std: 0.79
                       Mean reward: 24.69
               Mean episode length: 38.63
                  Mean reward/step: 0.47
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0264
Mean episode consecutive_successes: 0.0371
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 16.00s
                        Total time: 5245.43s
                               ETA: 1087564.2s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.129s, learning 0.181s)
               Value function loss: 1037.9238
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 16.31
               Mean episode length: 37.40
                  Mean reward/step: 0.61
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0283
Mean episode consecutive_successes: 0.0371
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 16.31s
                        Total time: 5261.74s
                               ETA: 1088666.7s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1030 steps/s (collection: 15.664s, learning 0.238s)
               Value function loss: 354.6971
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 6.88
               Mean episode length: 33.65
                  Mean reward/step: 0.49
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0317
Mean episode consecutive_successes: 0.0355
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 15.90s
                        Total time: 5277.65s
                               ETA: 1089680.4s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.151s, learning 0.164s)
               Value function loss: 394.2782
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 18.59
               Mean episode length: 37.09
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0347
Mean episode consecutive_successes: 0.0342
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 16.32s
                        Total time: 5293.96s
                               ETA: 1090775.1s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.002s, learning 0.181s)
               Value function loss: 161.3584
                    Surrogate loss: -0.0065
             Mean action noise std: 0.79
                       Mean reward: 61.52
               Mean episode length: 37.70
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0354
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 16.18s
                        Total time: 5310.14s
                               ETA: 1091837.8s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.050s, learning 0.194s)
               Value function loss: 324.5881
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 13.62
               Mean episode length: 36.87
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0347
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 16.24s
                        Total time: 5326.39s
                               ETA: 1092908.7s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.165s, learning 0.168s)
               Value function loss: 891.0388
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: 7.20
               Mean episode length: 34.03
                  Mean reward/step: 0.60
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0269
Mean episode consecutive_successes: 0.0357
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 16.33s
                        Total time: 5342.72s
                               ETA: 1093993.5s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.958s, learning 0.162s)
               Value function loss: 355.7612
                    Surrogate loss: -0.0031
             Mean action noise std: 0.79
                       Mean reward: 27.94
               Mean episode length: 35.06
                  Mean reward/step: 0.46
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0361
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 16.12s
                        Total time: 5358.84s
                               ETA: 1095030.0s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.880s, learning 0.170s)
               Value function loss: 187.8977
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 8.17
               Mean episode length: 35.32
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0239
Mean episode consecutive_successes: 0.0348
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 16.05s
                        Total time: 5374.89s
                               ETA: 1096048.0s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.803s, learning 0.162s)
               Value function loss: 139.1270
                    Surrogate loss: -0.0017
             Mean action noise std: 0.79
                       Mean reward: 11.33
               Mean episode length: 37.30
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0355
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 15.97s
                        Total time: 5390.86s
                               ETA: 1097044.6s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1015 steps/s (collection: 15.954s, learning 0.174s)
               Value function loss: 1580.2127
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 8.28
               Mean episode length: 36.15
                  Mean reward/step: 0.79
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 16.13s
                        Total time: 5406.98s
                               ETA: 1098070.0s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.279s, learning 0.170s)
               Value function loss: 576.9323
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 30.41
               Mean episode length: 35.53
                  Mean reward/step: 0.52
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0342
Mean episode consecutive_successes: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 16.45s
                        Total time: 5423.43s
                               ETA: 1099156.4s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.881s, learning 0.218s)
               Value function loss: 236.1625
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 15.68
               Mean episode length: 36.01
                  Mean reward/step: 0.38
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0322
Mean episode consecutive_successes: 0.0336
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 16.10s
                        Total time: 5439.53s
                               ETA: 1100167.3s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.380s, learning 0.188s)
               Value function loss: 352.3175
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 28.14
               Mean episode length: 35.85
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 16.57s
                        Total time: 5456.10s
                               ETA: 1101268.9s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.093s, learning 0.160s)
               Value function loss: 479.3371
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 33.29
               Mean episode length: 35.66
                  Mean reward/step: 0.47
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 16.25s
                        Total time: 5472.35s
                               ETA: 1102302.4s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.055s, learning 0.194s)
               Value function loss: 495.4136
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: 7.51
               Mean episode length: 34.65
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0303
Mean episode consecutive_successes: 0.0330
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 16.25s
                        Total time: 5488.60s
                               ETA: 1103331.0s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.013s, learning 0.236s)
               Value function loss: 616.1243
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: 30.49
               Mean episode length: 35.89
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0366
Mean episode consecutive_successes: 0.0344
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 16.25s
                        Total time: 5504.85s
                               ETA: 1104355.2s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.028s, learning 0.193s)
               Value function loss: 381.1198
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 18.38
               Mean episode length: 36.23
                  Mean reward/step: 0.46
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0308
Mean episode consecutive_successes: 0.0350
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 16.22s
                        Total time: 5521.07s
                               ETA: 1105369.7s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.031s, learning 0.213s)
               Value function loss: 837.9702
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 32.90
               Mean episode length: 35.33
                  Mean reward/step: 0.62
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0366
Mean episode consecutive_successes: 0.0359
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 16.24s
                        Total time: 5537.32s
                               ETA: 1106384.6s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1022 steps/s (collection: 15.803s, learning 0.228s)
               Value function loss: 907.4715
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 7.69
               Mean episode length: 34.65
                  Mean reward/step: 0.58
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0400
Mean episode consecutive_successes: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 16.03s
                        Total time: 5553.35s
                               ETA: 1107352.9s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1040 steps/s (collection: 15.569s, learning 0.175s)
               Value function loss: 247.7226
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 23.68
               Mean episode length: 36.65
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0386
Mean episode consecutive_successes: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 15.74s
                        Total time: 5569.09s
                               ETA: 1108260.1s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1011 steps/s (collection: 15.971s, learning 0.230s)
               Value function loss: 578.5011
                    Surrogate loss: -0.0018
             Mean action noise std: 0.79
                       Mean reward: 14.91
               Mean episode length: 34.11
                  Mean reward/step: 0.45
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0327
Mean episode consecutive_successes: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 16.20s
                        Total time: 5585.29s
                               ETA: 1109254.5s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.947s, learning 0.165s)
               Value function loss: 180.2726
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 14.12
               Mean episode length: 37.79
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0390
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 16.11s
                        Total time: 5601.40s
                               ETA: 1110227.2s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.025s, learning 0.164s)
               Value function loss: 935.4217
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 23.88
               Mean episode length: 37.06
                  Mean reward/step: 0.57
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0384
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 16.19s
                        Total time: 5617.59s
                               ETA: 1111211.2s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1023 steps/s (collection: 15.829s, learning 0.178s)
               Value function loss: 629.8167
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 13.52
               Mean episode length: 36.41
                  Mean reward/step: 0.55
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 16.01s
                        Total time: 5633.60s
                               ETA: 1112155.2s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1033 steps/s (collection: 15.686s, learning 0.169s)
               Value function loss: 358.3466
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 25.44
               Mean episode length: 35.42
                  Mean reward/step: 0.44
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0347
Mean episode consecutive_successes: 0.0348
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 15.85s
                        Total time: 5649.45s
                               ETA: 1113065.5s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1022 steps/s (collection: 15.850s, learning 0.173s)
               Value function loss: 304.8170
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 19.96
               Mean episode length: 34.56
                  Mean reward/step: 0.41
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0322
Mean episode consecutive_successes: 0.0349
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 16.02s
                        Total time: 5665.48s
                               ETA: 1114005.2s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.413s, learning 0.168s)
               Value function loss: 630.7771
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 25.43
               Mean episode length: 35.48
                  Mean reward/step: 0.61
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0358
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 16.58s
                        Total time: 5682.06s
                               ETA: 1115050.7s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.464s, learning 0.185s)
               Value function loss: 632.9287
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: 17.91
               Mean episode length: 35.04
                  Mean reward/step: 0.60
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0356
Mean episode consecutive_successes: 0.0367
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 11.65s
                        Total time: 5693.71s
                               ETA: 1115126.1s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.302s, learning 0.171s)
               Value function loss: 813.3407
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 23.30
               Mean episode length: 36.01
                  Mean reward/step: 0.60
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0356
Mean episode consecutive_successes: 0.0379
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 8.47s
                        Total time: 5702.18s
                               ETA: 1114580.2s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.289s, learning 0.185s)
               Value function loss: 620.4229
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 8.30
               Mean episode length: 36.01
                  Mean reward/step: 0.54
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0371
Mean episode consecutive_successes: 0.0372
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 8.47s
                        Total time: 5710.65s
                               ETA: 1114036.6s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.343s, learning 0.165s)
               Value function loss: 893.4461
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 8.40
               Mean episode length: 36.29
                  Mean reward/step: 0.50
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0420
Mean episode consecutive_successes: 0.0359
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 8.51s
                        Total time: 5719.16s
                               ETA: 1113501.8s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.150s, learning 0.177s)
               Value function loss: 378.4743
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 35.96
               Mean episode length: 35.86
                  Mean reward/step: 0.47
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0387
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 8.33s
                        Total time: 5727.49s
                               ETA: 1112933.8s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.922s, learning 0.162s)
               Value function loss: 435.6796
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: 43.29
               Mean episode length: 35.83
                  Mean reward/step: 0.63
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0283
Mean episode consecutive_successes: 0.0421
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 8.08s
                        Total time: 5735.57s
                               ETA: 1112321.0s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.251s, learning 0.227s)
               Value function loss: 751.3262
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 12.47
               Mean episode length: 34.66
                  Mean reward/step: 0.61
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0273
Mean episode consecutive_successes: 0.0429
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 8.48s
                        Total time: 5744.05s
                               ETA: 1111786.6s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.350s, learning 0.159s)
               Value function loss: 908.2924
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 30.77
               Mean episode length: 35.49
                  Mean reward/step: 0.68
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0381
Mean episode consecutive_successes: 0.0420
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 8.51s
                        Total time: 5752.56s
                               ETA: 1111260.4s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.969s, learning 0.185s)
               Value function loss: 1023.6637
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 23.42
               Mean episode length: 36.42
                  Mean reward/step: 0.65
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0391
Mean episode consecutive_successes: 0.0418
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 8.15s
                        Total time: 5760.71s
                               ETA: 1110667.8s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.957s, learning 0.158s)
               Value function loss: 584.7248
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 13.23
               Mean episode length: 35.44
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0425
Mean episode consecutive_successes: 0.0411
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 8.11s
                        Total time: 5768.83s
                               ETA: 1110069.7s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.094s, learning 0.177s)
               Value function loss: 1068.0371
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 38.49
               Mean episode length: 36.44
                  Mean reward/step: 0.70
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0522
Mean episode consecutive_successes: 0.0411
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 8.27s
                        Total time: 5777.10s
                               ETA: 1109504.0s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.031s, learning 0.163s)
               Value function loss: 1410.6691
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 8.19
               Mean episode length: 35.55
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0439
Mean episode consecutive_successes: 0.0458
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 8.19s
                        Total time: 5785.29s
                               ETA: 1108925.7s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.186s, learning 0.162s)
               Value function loss: 546.5310
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 11.97
               Mean episode length: 33.45
                  Mean reward/step: 0.59
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0522
Mean episode consecutive_successes: 0.0437
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 8.35s
                        Total time: 5793.64s
                               ETA: 1108379.1s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.184s, learning 0.215s)
               Value function loss: 702.5109
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 8.67
               Mean episode length: 36.51
                  Mean reward/step: 0.57
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0532
Mean episode consecutive_successes: 0.0437
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 8.40s
                        Total time: 5802.04s
                               ETA: 1107844.4s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.049s, learning 0.239s)
               Value function loss: 1144.2279
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 7.53
               Mean episode length: 34.16
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0586
Mean episode consecutive_successes: 0.0433
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 8.29s
                        Total time: 5810.33s
                               ETA: 1107290.5s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.409s, learning 0.165s)
               Value function loss: 520.1149
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 17.63
               Mean episode length: 34.70
                  Mean reward/step: 0.62
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0474
Mean episode consecutive_successes: 0.0479
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 8.57s
                        Total time: 5818.90s
                               ETA: 1106793.1s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.338s, learning 0.192s)
               Value function loss: 1149.9331
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 37.96
               Mean episode length: 34.72
                  Mean reward/step: 0.71
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0532
Mean episode consecutive_successes: 0.0492
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 8.53s
                        Total time: 5827.43s
                               ETA: 1106289.0s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.846s, learning 0.303s)
               Value function loss: 975.0478
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 27.29
               Mean episode length: 33.90
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0415
Mean episode consecutive_successes: 0.0526
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 8.15s
                        Total time: 5835.58s
                               ETA: 1105714.6s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.259s, learning 0.165s)
               Value function loss: 451.7644
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 12.30
               Mean episode length: 34.16
                  Mean reward/step: 0.47
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0541
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 8.42s
                        Total time: 5844.00s
                               ETA: 1105194.4s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 2075 steps/s (collection: 7.709s, learning 0.185s)
               Value function loss: 497.2957
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 20.49
               Mean episode length: 35.65
                  Mean reward/step: 0.48
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0558
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 7.89s
                        Total time: 5851.90s
                               ETA: 1104576.2s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.054s, learning 0.165s)
               Value function loss: 856.0383
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 7.68
               Mean episode length: 34.58
                  Mean reward/step: 0.52
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0527
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 8.22s
                        Total time: 5860.12s
                               ETA: 1104021.5s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.075s, learning 0.165s)
               Value function loss: 674.6020
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 7.43
               Mean episode length: 34.84
                  Mean reward/step: 0.55
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0273
Mean episode consecutive_successes: 0.0529
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 8.24s
                        Total time: 5868.36s
                               ETA: 1103472.8s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.936s, learning 0.167s)
               Value function loss: 970.2018
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: 24.67
               Mean episode length: 33.42
                  Mean reward/step: 0.48
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0308
Mean episode consecutive_successes: 0.0502
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 8.10s
                        Total time: 5876.46s
                               ETA: 1102900.6s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.134s, learning 0.178s)
               Value function loss: 501.1605
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 20.54
               Mean episode length: 35.64
                  Mean reward/step: 0.51
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0264
Mean episode consecutive_successes: 0.0512
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 8.31s
                        Total time: 5884.77s
                               ETA: 1102369.5s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.145s, learning 0.204s)
               Value function loss: 280.4361
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 27.68
               Mean episode length: 34.38
                  Mean reward/step: 0.44
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0259
Mean episode consecutive_successes: 0.0495
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 8.35s
                        Total time: 5893.12s
                               ETA: 1101847.3s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.217s, learning 0.187s)
               Value function loss: 679.6470
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 9.19
               Mean episode length: 32.99
                  Mean reward/step: 0.43
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0492
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 8.40s
                        Total time: 5901.52s
                               ETA: 1101337.4s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.068s, learning 0.170s)
               Value function loss: 851.0399
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 25.81
               Mean episode length: 35.81
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0312
Mean episode consecutive_successes: 0.0466
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 8.24s
                        Total time: 5909.76s
                               ETA: 1100798.4s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.284s, learning 0.175s)
               Value function loss: 1515.9611
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 36.97
               Mean episode length: 33.16
                  Mean reward/step: 0.69
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0312
Mean episode consecutive_successes: 0.0481
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 8.46s
                        Total time: 5918.22s
                               ETA: 1100302.5s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1991 steps/s (collection: 7.992s, learning 0.234s)
               Value function loss: 662.0986
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 10.62
               Mean episode length: 35.52
                  Mean reward/step: 0.51
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0308
Mean episode consecutive_successes: 0.0458
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 8.23s
                        Total time: 5926.45s
                               ETA: 1099765.1s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.162s)
               Value function loss: 400.6320
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 7.13
               Mean episode length: 33.25
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0342
Mean episode consecutive_successes: 0.0426
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 8.36s
                        Total time: 5934.81s
                               ETA: 1099255.2s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.361s, learning 0.182s)
               Value function loss: 473.1581
                    Surrogate loss: -0.0030
             Mean action noise std: 0.79
                       Mean reward: 82.37
               Mean episode length: 33.81
                  Mean reward/step: 0.49
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0487
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 8.54s
                        Total time: 5943.35s
                               ETA: 1098780.4s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.141s, learning 0.183s)
               Value function loss: 774.5989
                    Surrogate loss: -0.0044
             Mean action noise std: 0.79
                       Mean reward: 7.75
               Mean episode length: 34.73
                  Mean reward/step: 0.51
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0460
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 8.32s
                        Total time: 5951.68s
                               ETA: 1098266.8s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.198s, learning 0.160s)
               Value function loss: 243.5972
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 6.91
               Mean episode length: 33.31
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0432
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 8.36s
                        Total time: 5960.04s
                               ETA: 1097761.5s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.231s, learning 0.238s)
               Value function loss: 529.5562
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 9.21
               Mean episode length: 32.67
                  Mean reward/step: 0.51
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 8.47s
                        Total time: 5968.51s
                               ETA: 1097278.3s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.093s, learning 0.165s)
               Value function loss: 971.2453
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 19.51
               Mean episode length: 33.37
                  Mean reward/step: 0.59
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0425
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 8.26s
                        Total time: 5976.76s
                               ETA: 1096758.1s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.561s, learning 0.187s)
               Value function loss: 724.5353
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 6.70
               Mean episode length: 32.61
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0298
Mean episode consecutive_successes: 0.0402
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 8.75s
                        Total time: 5985.51s
                               ETA: 1096329.6s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.307s, learning 0.167s)
               Value function loss: 523.9929
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 11.72
               Mean episode length: 32.68
                  Mean reward/step: 0.48
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0407
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 8.47s
                        Total time: 5993.99s
                               ETA: 1095852.7s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.382s, learning 0.175s)
               Value function loss: 766.2704
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: 6.70
               Mean episode length: 32.84
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0273
Mean episode consecutive_successes: 0.0407
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 8.56s
                        Total time: 6002.54s
                               ETA: 1095392.4s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.185s, learning 0.163s)
               Value function loss: 537.3455
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 9.58
               Mean episode length: 33.64
                  Mean reward/step: 0.60
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0259
Mean episode consecutive_successes: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 8.35s
                        Total time: 6010.89s
                               ETA: 1094895.8s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.151s, learning 0.225s)
               Value function loss: 736.9078
                    Surrogate loss: 0.0010
             Mean action noise std: 0.79
                       Mean reward: 7.51
               Mean episode length: 34.11
                  Mean reward/step: 0.50
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0259
Mean episode consecutive_successes: 0.0414
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 8.38s
                        Total time: 6019.27s
                               ETA: 1094406.2s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.915s, learning 0.229s)
               Value function loss: 696.8459
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 11.55
               Mean episode length: 32.35
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0396
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 8.14s
                        Total time: 6027.41s
                               ETA: 1093876.1s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1991 steps/s (collection: 7.999s, learning 0.229s)
               Value function loss: 523.3857
                    Surrogate loss: -0.0055
             Mean action noise std: 0.79
                       Mean reward: 14.27
               Mean episode length: 32.83
                  Mean reward/step: 0.56
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0342
Mean episode consecutive_successes: 0.0395
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 8.23s
                        Total time: 6035.64s
                               ETA: 1093363.1s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.078s, learning 0.191s)
               Value function loss: 1477.1039
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 6.73
               Mean episode length: 32.83
                  Mean reward/step: 0.87
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0464
Mean episode consecutive_successes: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 8.27s
                        Total time: 6043.91s
                               ETA: 1092859.3s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.923s, learning 0.162s)
               Value function loss: 1064.1763
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 27.32
               Mean episode length: 34.16
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.0382
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 8.09s
                        Total time: 6051.99s
                               ETA: 1092324.2s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 2003 steps/s (collection: 7.987s, learning 0.190s)
               Value function loss: 978.2720
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 38.36
               Mean episode length: 35.51
                  Mean reward/step: 0.66
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0547
Mean episode consecutive_successes: 0.0411
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 8.18s
                        Total time: 6060.17s
                               ETA: 1091807.7s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.864s, learning 0.162s)
               Value function loss: 1445.5883
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 12.13
               Mean episode length: 33.42
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0596
Mean episode consecutive_successes: 0.0419
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 8.03s
                        Total time: 6068.20s
                               ETA: 1091265.9s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.254s, learning 0.184s)
               Value function loss: 1706.3625
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 21.85
               Mean episode length: 32.63
                  Mean reward/step: 0.91
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0684
Mean episode consecutive_successes: 0.0444
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 8.44s
                        Total time: 6076.63s
                               ETA: 1090799.8s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.883s, learning 0.162s)
               Value function loss: 955.6569
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: 39.90
               Mean episode length: 34.09
                  Mean reward/step: 0.65
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0640
Mean episode consecutive_successes: 0.0492
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 8.04s
                        Total time: 6084.68s
                               ETA: 1090265.0s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 2068 steps/s (collection: 7.765s, learning 0.155s)
               Value function loss: 1170.7556
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 14.48
               Mean episode length: 33.09
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0688
Mean episode consecutive_successes: 0.0480
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 7.92s
                        Total time: 6092.60s
                               ETA: 1089709.8s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.299s, learning 0.158s)
               Value function loss: 665.9305
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 47.01
               Mean episode length: 32.64
                  Mean reward/step: 0.60
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0610
Mean episode consecutive_successes: 0.0542
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 8.46s
                        Total time: 6101.06s
                               ETA: 1089252.5s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.259s, learning 0.182s)
               Value function loss: 484.7617
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 17.63
               Mean episode length: 34.21
                  Mean reward/step: 0.47
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0483
Mean episode consecutive_successes: 0.0550
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 8.44s
                        Total time: 6109.50s
                               ETA: 1088793.7s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.011s, learning 0.205s)
               Value function loss: 1088.0496
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 26.18
               Mean episode length: 35.98
                  Mean reward/step: 0.65
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0503
Mean episode consecutive_successes: 0.0545
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 8.22s
                        Total time: 6117.72s
                               ETA: 1088296.7s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.934s, learning 0.179s)
               Value function loss: 1073.6948
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 40.25
               Mean episode length: 34.36
                  Mean reward/step: 0.63
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0464
Mean episode consecutive_successes: 0.0576
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 8.11s
                        Total time: 6125.83s
                               ETA: 1087783.0s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.291s, learning 0.191s)
               Value function loss: 1327.4616
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 7.10
               Mean episode length: 34.05
                  Mean reward/step: 0.76
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0537
Mean episode consecutive_successes: 0.0549
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 8.48s
                        Total time: 6134.31s
                               ETA: 1087336.5s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.210s, learning 0.177s)
               Value function loss: 1818.5820
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 28.35
               Mean episode length: 36.16
                  Mean reward/step: 0.88
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0679
Mean episode consecutive_successes: 0.0537
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 8.39s
                        Total time: 6142.70s
                               ETA: 1086874.9s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.992s, learning 0.171s)
               Value function loss: 876.0384
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 20.80
               Mean episode length: 35.78
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0649
Mean episode consecutive_successes: 0.0551
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 8.16s
                        Total time: 6150.86s
                               ETA: 1086375.4s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.305s, learning 0.162s)
               Value function loss: 1254.8485
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 10.22
               Mean episode length: 34.67
                  Mean reward/step: 0.68
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0537
Mean episode consecutive_successes: 0.0581
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 8.47s
                        Total time: 6159.33s
                               ETA: 1085931.0s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.982s, learning 0.163s)
               Value function loss: 1504.0242
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 35.93
               Mean episode length: 36.15
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0625
Mean episode consecutive_successes: 0.0577
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 8.15s
                        Total time: 6167.47s
                               ETA: 1085431.6s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.377s, learning 0.185s)
               Value function loss: 1205.6125
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 30.42
               Mean episode length: 35.55
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0640
Mean episode consecutive_successes: 0.0585
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 8.56s
                        Total time: 6176.03s
                               ETA: 1085007.1s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.105s, learning 0.166s)
               Value function loss: 2370.2991
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 52.63
               Mean episode length: 34.27
                  Mean reward/step: 1.11
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0757
Mean episode consecutive_successes: 0.0607
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 8.27s
                        Total time: 6184.31s
                               ETA: 1084533.1s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 2005 steps/s (collection: 7.971s, learning 0.198s)
               Value function loss: 2134.0694
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 45.37
               Mean episode length: 35.35
                  Mean reward/step: 1.20
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0933
Mean episode consecutive_successes: 0.0617
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 8.17s
                        Total time: 6192.47s
                               ETA: 1084042.7s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.147s, learning 0.257s)
               Value function loss: 1375.0646
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 30.93
               Mean episode length: 35.93
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.68
            Mean episode successes: 0.1011
Mean episode consecutive_successes: 0.0600
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 8.40s
                        Total time: 6200.88s
                               ETA: 1083595.3s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.093s, learning 0.164s)
               Value function loss: 840.6854
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 7.09
               Mean episode length: 33.14
                  Mean reward/step: 0.65
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0854
Mean episode consecutive_successes: 0.0644
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 8.26s
                        Total time: 6209.14s
                               ETA: 1083123.7s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.451s, learning 0.181s)
               Value function loss: 765.6423
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 7.06
               Mean episode length: 33.39
                  Mean reward/step: 0.63
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0845
Mean episode consecutive_successes: 0.0643
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 8.63s
                        Total time: 6217.77s
                               ETA: 1082719.2s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.928s, learning 0.161s)
               Value function loss: 1277.7411
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 60.37
               Mean episode length: 34.65
                  Mean reward/step: 0.69
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0781
Mean episode consecutive_successes: 0.0679
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 8.09s
                        Total time: 6225.86s
                               ETA: 1082221.7s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.048s, learning 0.157s)
               Value function loss: 976.0652
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 25.40
               Mean episode length: 34.90
                  Mean reward/step: 0.71
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0649
Mean episode consecutive_successes: 0.0709
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 8.20s
                        Total time: 6234.06s
                               ETA: 1081745.8s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.258s, learning 0.192s)
               Value function loss: 1842.4586
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 57.33
               Mean episode length: 38.49
                  Mean reward/step: 0.93
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0708
Mean episode consecutive_successes: 0.0735
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 8.45s
                        Total time: 6242.51s
                               ETA: 1081314.1s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.483s, learning 0.163s)
               Value function loss: 1526.3545
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 23.25
               Mean episode length: 35.96
                  Mean reward/step: 0.99
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0796
Mean episode consecutive_successes: 0.0722
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 8.65s
                        Total time: 6251.16s
                               ETA: 1080917.8s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.208s, learning 0.253s)
               Value function loss: 938.1912
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 20.30
               Mean episode length: 34.79
                  Mean reward/step: 0.60
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0747
Mean episode consecutive_successes: 0.0710
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 8.46s
                        Total time: 6259.62s
                               ETA: 1080490.9s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.081s, learning 0.246s)
               Value function loss: 348.2640
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 35.78
               Mean episode length: 35.76
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0703
Mean episode consecutive_successes: 0.0690
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 8.33s
                        Total time: 6267.95s
                               ETA: 1080042.4s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.112s, learning 0.214s)
               Value function loss: 741.4873
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 27.84
               Mean episode length: 35.28
                  Mean reward/step: 0.59
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0591
Mean episode consecutive_successes: 0.0723
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 8.33s
                        Total time: 6276.27s
                               ETA: 1079595.1s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.172s, learning 0.262s)
               Value function loss: 1238.2042
                    Surrogate loss: -0.0055
             Mean action noise std: 0.79
                       Mean reward: 48.26
               Mean episode length: 36.10
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.0740
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 8.43s
                        Total time: 6284.71s
                               ETA: 1079167.9s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.364s, learning 0.163s)
               Value function loss: 855.4930
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 25.45
               Mean episode length: 35.39
                  Mean reward/step: 0.57
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0498
Mean episode consecutive_successes: 0.0734
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 8.53s
                        Total time: 6293.23s
                               ETA: 1078758.1s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.771s, learning 0.234s)
               Value function loss: 1056.3426
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 12.57
               Mean episode length: 34.24
                  Mean reward/step: 0.74
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0566
Mean episode consecutive_successes: 0.0714
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 9.01s
                        Total time: 6302.24s
                               ETA: 1078431.4s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.115s, learning 0.165s)
               Value function loss: 1831.5570
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 26.99
               Mean episode length: 33.37
                  Mean reward/step: 0.94
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0728
Mean episode consecutive_successes: 0.0696
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 8.28s
                        Total time: 6310.52s
                               ETA: 1077982.1s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.179s, learning 0.186s)
               Value function loss: 1767.1135
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 15.32
               Mean episode length: 35.51
                  Mean reward/step: 0.88
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0776
Mean episode consecutive_successes: 0.0683
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 8.36s
                        Total time: 6318.88s
                               ETA: 1077548.7s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.348s, learning 0.164s)
               Value function loss: 1652.3982
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 9.67
               Mean episode length: 33.75
                  Mean reward/step: 0.96
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0889
Mean episode consecutive_successes: 0.0674
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 8.51s
                        Total time: 6327.40s
                               ETA: 1077141.8s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.118s, learning 0.189s)
               Value function loss: 837.4393
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 25.15
               Mean episode length: 34.65
                  Mean reward/step: 0.61
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0933
Mean episode consecutive_successes: 0.0658
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 8.31s
                        Total time: 6335.70s
                               ETA: 1076701.4s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.619s, learning 0.226s)
               Value function loss: 1127.7219
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 9.97
               Mean episode length: 34.52
                  Mean reward/step: 0.81
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0854
Mean episode consecutive_successes: 0.0682
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 8.85s
                        Total time: 6344.55s
                               ETA: 1076353.8s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.049s, learning 0.210s)
               Value function loss: 1085.3301
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 15.36
               Mean episode length: 34.78
                  Mean reward/step: 0.72
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0850
Mean episode consecutive_successes: 0.0693
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 8.26s
                        Total time: 6352.81s
                               ETA: 1075908.2s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.972s, learning 0.168s)
               Value function loss: 767.6984
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 33.50
               Mean episode length: 36.25
                  Mean reward/step: 0.62
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0801
Mean episode consecutive_successes: 0.0699
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 8.14s
                        Total time: 6360.95s
                               ETA: 1075443.8s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.082s, learning 0.168s)
               Value function loss: 1642.0170
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 50.20
               Mean episode length: 34.60
                  Mean reward/step: 0.93
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0757
Mean episode consecutive_successes: 0.0745
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 8.25s
                        Total time: 6369.20s
                               ETA: 1074999.5s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.336s, learning 0.168s)
               Value function loss: 1180.8670
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 17.81
               Mean episode length: 34.66
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0728
Mean episode consecutive_successes: 0.0739
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 8.50s
                        Total time: 6377.70s
                               ETA: 1074599.5s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.286s, learning 0.174s)
               Value function loss: 1270.4472
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 22.17
               Mean episode length: 38.22
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0713
Mean episode consecutive_successes: 0.0753
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 8.46s
                        Total time: 6386.16s
                               ETA: 1074193.4s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.893s, learning 0.160s)
               Value function loss: 1005.4465
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 8.48
               Mean episode length: 36.21
                  Mean reward/step: 0.73
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0747
Mean episode consecutive_successes: 0.0731
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 8.05s
                        Total time: 6394.21s
                               ETA: 1073720.4s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.157s, learning 0.158s)
               Value function loss: 1311.9879
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 35.04
               Mean episode length: 34.54
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0752
Mean episode consecutive_successes: 0.0737
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 8.32s
                        Total time: 6402.53s
                               ETA: 1073292.9s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.840s, learning 0.170s)
               Value function loss: 1164.0383
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 61.57
               Mean episode length: 37.32
                  Mean reward/step: 0.82
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0801
Mean episode consecutive_successes: 0.0758
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 8.01s
                        Total time: 6410.54s
                               ETA: 1072815.7s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.064s, learning 0.162s)
               Value function loss: 2018.3861
                    Surrogate loss: -0.0044
             Mean action noise std: 0.79
                       Mean reward: 30.64
               Mean episode length: 35.58
                  Mean reward/step: 0.98
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0869
Mean episode consecutive_successes: 0.0747
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 8.23s
                        Total time: 6418.76s
                               ETA: 1072376.0s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.187s, learning 0.162s)
               Value function loss: 2596.3481
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 24.01
               Mean episode length: 36.98
                  Mean reward/step: 1.13
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1006
Mean episode consecutive_successes: 0.0739
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 8.35s
                        Total time: 6427.11s
                               ETA: 1071958.4s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.324s, learning 0.214s)
               Value function loss: 2340.0051
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 58.88
               Mean episode length: 37.15
                  Mean reward/step: 1.11
       Mean episode length/episode: 6.72
            Mean episode successes: 0.1113
Mean episode consecutive_successes: 0.0754
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 8.54s
                        Total time: 6435.65s
                               ETA: 1071573.7s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.159s, learning 0.157s)
               Value function loss: 893.4676
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 7.49
               Mean episode length: 34.48
                  Mean reward/step: 0.72
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0879
Mean episode consecutive_successes: 0.0817
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 8.32s
                        Total time: 6443.97s
                               ETA: 1071153.4s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.146s, learning 0.163s)
               Value function loss: 2061.9277
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 22.29
               Mean episode length: 38.68
                  Mean reward/step: 1.02
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0996
Mean episode consecutive_successes: 0.0801
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 8.31s
                        Total time: 6452.28s
                               ETA: 1070733.2s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.838s, learning 0.286s)
               Value function loss: 1165.8642
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 14.01
               Mean episode length: 37.02
                  Mean reward/step: 0.82
       Mean episode length/episode: 6.71
            Mean episode successes: 0.1011
Mean episode consecutive_successes: 0.0796
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 8.12s
                        Total time: 6460.40s
                               ETA: 1070283.7s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.239s, learning 0.165s)
               Value function loss: 2227.5757
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 71.34
               Mean episode length: 36.69
                  Mean reward/step: 1.16
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0952
Mean episode consecutive_successes: 0.0886
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 8.40s
                        Total time: 6468.80s
                               ETA: 1069882.1s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 2054 steps/s (collection: 7.808s, learning 0.166s)
               Value function loss: 1398.7303
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 40.72
               Mean episode length: 36.04
                  Mean reward/step: 0.81
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0918
Mean episode consecutive_successes: 0.0895
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 7.97s
                        Total time: 6476.78s
                               ETA: 1069410.9s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.265s, learning 0.194s)
               Value function loss: 1658.5101
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 30.17
               Mean episode length: 39.17
                  Mean reward/step: 0.94
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0996
Mean episode consecutive_successes: 0.0865
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 8.46s
                        Total time: 6485.24s
                               ETA: 1069021.1s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.167s)
               Value function loss: 1589.0305
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 36.66
               Mean episode length: 37.67
                  Mean reward/step: 1.11
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0977
Mean episode consecutive_successes: 0.0918
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 8.47s
                        Total time: 6493.71s
                               ETA: 1068634.5s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.439s, learning 0.249s)
               Value function loss: 1390.7776
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: 9.12
               Mean episode length: 36.79
                  Mean reward/step: 0.92
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0991
Mean episode consecutive_successes: 0.0907
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 8.69s
                        Total time: 6502.40s
                               ETA: 1068284.6s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.236s, learning 0.163s)
               Value function loss: 1718.1900
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 49.34
               Mean episode length: 37.96
                  Mean reward/step: 1.06
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1006
Mean episode consecutive_successes: 0.0944
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 8.40s
                        Total time: 6510.80s
                               ETA: 1067888.6s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.331s, learning 0.171s)
               Value function loss: 1947.6490
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 16.55
               Mean episode length: 37.22
                  Mean reward/step: 1.14
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1016
Mean episode consecutive_successes: 0.0961
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 8.50s
                        Total time: 6519.30s
                               ETA: 1067510.8s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.772s, learning 0.164s)
               Value function loss: 1608.3839
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 59.51
               Mean episode length: 38.13
                  Mean reward/step: 0.91
       Mean episode length/episode: 6.72
            Mean episode successes: 0.1035
Mean episode consecutive_successes: 0.0959
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 11.94s
                        Total time: 6531.23s
                               ETA: 1067695.6s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.401s, learning 0.163s)
               Value function loss: 1334.1868
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 16.72
               Mean episode length: 37.73
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1016
Mean episode consecutive_successes: 0.0958
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 16.56s
                        Total time: 6547.80s
                               ETA: 1068635.1s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.665s, learning 0.162s)
               Value function loss: 931.2658
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 38.83
               Mean episode length: 36.69
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0923
Mean episode consecutive_successes: 0.0976
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 15.83s
                        Total time: 6563.62s
                               ETA: 1069451.2s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.315s, learning 0.175s)
               Value function loss: 1017.9350
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 87.79
               Mean episode length: 39.34
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0708
Mean episode consecutive_successes: 0.1040
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 16.49s
                        Total time: 6580.11s
                               ETA: 1070372.5s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.110s, learning 0.164s)
               Value function loss: 1037.8573
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 21.63
               Mean episode length: 37.73
                  Mean reward/step: 0.87
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0747
Mean episode consecutive_successes: 0.1004
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 16.27s
                        Total time: 6596.39s
                               ETA: 1071255.7s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.923s, learning 0.161s)
               Value function loss: 1617.4680
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 23.14
               Mean episode length: 35.70
                  Mean reward/step: 0.92
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0786
Mean episode consecutive_successes: 0.0992
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 16.08s
                        Total time: 6612.47s
                               ETA: 1072105.1s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.182s, learning 0.188s)
               Value function loss: 2661.9631
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 25.79
               Mean episode length: 36.43
                  Mean reward/step: 1.26
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0947
Mean episode consecutive_successes: 0.0980
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 16.37s
                        Total time: 6628.84s
                               ETA: 1072997.9s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.298s, learning 0.177s)
               Value function loss: 1793.3436
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 9.90
               Mean episode length: 38.98
                  Mean reward/step: 0.89
       Mean episode length/episode: 6.74
            Mean episode successes: 0.1001
Mean episode consecutive_successes: 0.0948
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 16.48s
                        Total time: 6645.32s
                               ETA: 1073905.0s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.296s, learning 0.162s)
               Value function loss: 1144.3196
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 16.55
               Mean episode length: 37.41
                  Mean reward/step: 0.81
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0947
Mean episode consecutive_successes: 0.0959
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 16.46s
                        Total time: 6661.78s
                               ETA: 1074806.2s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.912s, learning 0.179s)
               Value function loss: 2274.8763
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 44.30
               Mean episode length: 37.83
                  Mean reward/step: 1.24
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0923
Mean episode consecutive_successes: 0.1009
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 16.09s
                        Total time: 6677.87s
                               ETA: 1075645.4s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.940s, learning 0.171s)
               Value function loss: 2461.0378
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 13.33
               Mean episode length: 35.62
                  Mean reward/step: 1.07
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0962
Mean episode consecutive_successes: 0.1005
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 16.11s
                        Total time: 6693.98s
                               ETA: 1076485.0s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.303s, learning 0.165s)
               Value function loss: 2329.2417
                    Surrogate loss: -0.0065
             Mean action noise std: 0.79
                       Mean reward: 34.25
               Mean episode length: 37.50
                  Mean reward/step: 0.97
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0952
Mean episode consecutive_successes: 0.1028
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 16.47s
                        Total time: 6710.45s
                               ETA: 1077379.1s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.695s, learning 0.163s)
               Value function loss: 1779.0436
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 41.48
               Mean episode length: 37.68
                  Mean reward/step: 0.97
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1006
Mean episode consecutive_successes: 0.1010
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 16.86s
                        Total time: 6727.30s
                               ETA: 1078332.7s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.299s, learning 0.159s)
               Value function loss: 1830.3101
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 29.70
               Mean episode length: 38.43
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0977
Mean episode consecutive_successes: 0.1003
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 16.46s
                        Total time: 6743.76s
                               ETA: 1079219.3s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.036s, learning 0.187s)
               Value function loss: 1171.9866
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 37.61
               Mean episode length: 39.15
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1001
Mean episode consecutive_successes: 0.1004
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 16.22s
                        Total time: 6759.99s
                               ETA: 1080065.3s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.340s, learning 0.163s)
               Value function loss: 976.3019
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 43.27
               Mean episode length: 35.93
                  Mean reward/step: 0.69
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0610
Mean episode consecutive_successes: 0.1112
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 16.50s
                        Total time: 6776.49s
                               ETA: 1080953.2s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.336s, learning 0.168s)
               Value function loss: 1510.9559
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 16.40
               Mean episode length: 37.23
                  Mean reward/step: 0.94
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0723
Mean episode consecutive_successes: 0.1063
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 16.50s
                        Total time: 6792.99s
                               ETA: 1081838.4s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.943s, learning 0.174s)
               Value function loss: 1451.2741
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 42.44
               Mean episode length: 39.08
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0747
Mean episode consecutive_successes: 0.1045
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 16.12s
                        Total time: 6809.11s
                               ETA: 1082659.2s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.019s, learning 0.192s)
               Value function loss: 1649.0271
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 16.12
               Mean episode length: 37.25
                  Mean reward/step: 1.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0791
Mean episode consecutive_successes: 0.1040
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 16.21s
                        Total time: 6825.32s
                               ETA: 1083492.3s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.212s, learning 0.176s)
               Value function loss: 2328.0882
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 53.30
               Mean episode length: 40.36
                  Mean reward/step: 1.29
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0908
Mean episode consecutive_successes: 0.1036
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 16.39s
                        Total time: 6841.71s
                               ETA: 1084350.7s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.027s, learning 0.160s)
               Value function loss: 1818.6636
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 29.58
               Mean episode length: 38.56
                  Mean reward/step: 1.00
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0923
Mean episode consecutive_successes: 0.1033
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 16.19s
                        Total time: 6857.90s
                               ETA: 1085174.5s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.097s, learning 0.187s)
               Value function loss: 2141.9264
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 56.84
               Mean episode length: 38.16
                  Mean reward/step: 1.22
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1104
Mean episode consecutive_successes: 0.1036
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 16.28s
                        Total time: 6874.18s
                               ETA: 1086010.9s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.272s, learning 0.205s)
               Value function loss: 1348.1769
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 37.57
               Mean episode length: 39.17
                  Mean reward/step: 0.79
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0845
Mean episode consecutive_successes: 0.1087
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 16.48s
                        Total time: 6890.66s
                               ETA: 1086875.1s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.247s, learning 0.235s)
               Value function loss: 1074.6637
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 29.24
               Mean episode length: 41.90
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0737
Mean episode consecutive_successes: 0.1106
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 16.48s
                        Total time: 6907.14s
                               ETA: 1087737.3s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.874s, learning 0.222s)
               Value function loss: 1253.8095
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 22.66
               Mean episode length: 39.50
                  Mean reward/step: 0.77
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0771
Mean episode consecutive_successes: 0.1064
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 16.10s
                        Total time: 6923.23s
                               ETA: 1088536.0s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.662s, learning 0.167s)
               Value function loss: 1417.8717
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 22.34
               Mean episode length: 38.62
                  Mean reward/step: 0.94
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0791
Mean episode consecutive_successes: 0.1074
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 16.83s
                        Total time: 6940.06s
                               ETA: 1089447.3s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.607s, learning 0.300s)
               Value function loss: 1130.0113
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 11.22
               Mean episode length: 37.06
                  Mean reward/step: 0.82
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0723
Mean episode consecutive_successes: 0.1079
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 16.91s
                        Total time: 6956.97s
                               ETA: 1090367.7s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.063s, learning 0.198s)
               Value function loss: 1639.2901
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 24.20
               Mean episode length: 37.99
                  Mean reward/step: 0.92
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0835
Mean episode consecutive_successes: 0.1042
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 16.26s
                        Total time: 6973.23s
                               ETA: 1091184.1s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.226s, learning 0.293s)
               Value function loss: 970.0676
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 71.35
               Mean episode length: 37.48
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0679
Mean episode consecutive_successes: 0.1065
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 16.52s
                        Total time: 6989.75s
                               ETA: 1092038.4s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1025 steps/s (collection: 15.814s, learning 0.166s)
               Value function loss: 2213.7268
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 27.14
               Mean episode length: 38.66
                  Mean reward/step: 1.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0806
Mean episode consecutive_successes: 0.1030
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 15.98s
                        Total time: 7005.73s
                               ETA: 1092805.7s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.922s, learning 0.166s)
               Value function loss: 1205.8621
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 17.04
               Mean episode length: 38.04
                  Mean reward/step: 0.86
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0835
Mean episode consecutive_successes: 0.1011
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 16.09s
                        Total time: 7021.82s
                               ETA: 1093587.4s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1024 steps/s (collection: 15.822s, learning 0.163s)
               Value function loss: 1591.2893
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 71.51
               Mean episode length: 37.82
                  Mean reward/step: 0.96
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0669
Mean episode consecutive_successes: 0.1066
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 15.99s
                        Total time: 7037.80s
                               ETA: 1094350.7s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1029 steps/s (collection: 15.757s, learning 0.159s)
               Value function loss: 886.4234
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 30.27
               Mean episode length: 39.63
                  Mean reward/step: 0.81
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0703
Mean episode consecutive_successes: 0.1053
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 15.92s
                        Total time: 7053.72s
                               ETA: 1095100.7s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.596s, learning 0.229s)
               Value function loss: 900.5627
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 8.92
               Mean episode length: 37.48
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0776
Mean episode consecutive_successes: 0.0998
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 16.83s
                        Total time: 7070.54s
                               ETA: 1095989.4s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.189s, learning 0.255s)
               Value function loss: 625.6847
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: 30.94
               Mean episode length: 36.37
                  Mean reward/step: 0.52
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0625
Mean episode consecutive_successes: 0.1006
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 16.44s
                        Total time: 7086.99s
                               ETA: 1096816.1s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.262s, learning 0.198s)
               Value function loss: 881.9228
                    Surrogate loss: -0.0055
             Mean action noise std: 0.79
                       Mean reward: 18.95
               Mean episode length: 37.32
                  Mean reward/step: 0.67
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0630
Mean episode consecutive_successes: 0.0984
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 16.46s
                        Total time: 7103.45s
                               ETA: 1097642.8s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.791s, learning 0.169s)
               Value function loss: 1314.9458
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 28.69
               Mean episode length: 36.83
                  Mean reward/step: 0.82
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0659
Mean episode consecutive_successes: 0.0969
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 15.96s
                        Total time: 7119.41s
                               ETA: 1098389.7s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.886s, learning 0.161s)
               Value function loss: 1960.7887
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 14.70
               Mean episode length: 39.01
                  Mean reward/step: 0.87
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0693
Mean episode consecutive_successes: 0.0938
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 16.05s
                        Total time: 7135.46s
                               ETA: 1099147.7s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.528s, learning 0.161s)
               Value function loss: 2486.6400
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 57.32
               Mean episode length: 39.09
                  Mean reward/step: 1.28
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0879
Mean episode consecutive_successes: 0.0941
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 10.69s
                        Total time: 7146.14s
                               ETA: 1099079.1s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.300s, learning 0.155s)
               Value function loss: 1265.8216
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: 54.86
               Mean episode length: 38.59
                  Mean reward/step: 0.79
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0791
Mean episode consecutive_successes: 0.0957
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 8.45s
                        Total time: 7154.60s
                               ETA: 1098667.6s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.176s, learning 0.175s)
               Value function loss: 1565.0723
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 26.80
               Mean episode length: 38.23
                  Mean reward/step: 0.94
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0811
Mean episode consecutive_successes: 0.0940
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 8.35s
                        Total time: 7162.95s
                               ETA: 1098241.5s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.840s, learning 0.186s)
               Value function loss: 2722.3640
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 84.97
               Mean episode length: 39.07
                  Mean reward/step: 1.27
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0771
Mean episode consecutive_successes: 0.1040
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 8.03s
                        Total time: 7170.98s
                               ETA: 1097767.0s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.251s, learning 0.178s)
               Value function loss: 1534.0473
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 14.76
               Mean episode length: 39.07
                  Mean reward/step: 1.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0879
Mean episode consecutive_successes: 0.0987
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 8.43s
                        Total time: 7179.41s
                               ETA: 1097355.5s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.051s, learning 0.200s)
               Value function loss: 1211.4361
                    Surrogate loss: -0.0034
             Mean action noise std: 0.79
                       Mean reward: 21.89
               Mean episode length: 37.95
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0806
Mean episode consecutive_successes: 0.1000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 8.25s
                        Total time: 7187.66s
                               ETA: 1096918.0s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.530s, learning 0.194s)
               Value function loss: 1063.3190
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 45.31
               Mean episode length: 39.41
                  Mean reward/step: 0.71
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0737
Mean episode consecutive_successes: 0.1008
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 8.72s
                        Total time: 7196.38s
                               ETA: 1096553.9s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.618s, learning 0.226s)
               Value function loss: 633.9022
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 21.83
               Mean episode length: 38.63
                  Mean reward/step: 0.63
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0410
Mean episode consecutive_successes: 0.1096
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 8.84s
                        Total time: 7205.22s
                               ETA: 1096209.1s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.727s, learning 0.235s)
               Value function loss: 1063.5983
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 15.26
               Mean episode length: 39.72
                  Mean reward/step: 0.70
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0479
Mean episode consecutive_successes: 0.1037
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 7.96s
                        Total time: 7213.19s
                               ETA: 1095731.4s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.289s, learning 0.167s)
               Value function loss: 959.9860
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 20.83
               Mean episode length: 35.89
                  Mean reward/step: 0.73
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0439
Mean episode consecutive_successes: 0.1029
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 8.46s
                        Total time: 7221.64s
                               ETA: 1095330.1s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.981s, learning 0.172s)
               Value function loss: 988.3914
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 27.12
               Mean episode length: 38.63
                  Mean reward/step: 0.66
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0493
Mean episode consecutive_successes: 0.0984
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 8.15s
                        Total time: 7229.79s
                               ETA: 1094884.1s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.096s, learning 0.189s)
               Value function loss: 852.2302
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 54.60
               Mean episode length: 38.36
                  Mean reward/step: 0.72
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0435
Mean episode consecutive_successes: 0.0993
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 8.29s
                        Total time: 7238.08s
                               ETA: 1094459.4s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.187s, learning 0.189s)
               Value function loss: 1666.1696
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 36.34
               Mean episode length: 36.91
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0527
Mean episode consecutive_successes: 0.0950
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 8.38s
                        Total time: 7246.46s
                               ETA: 1094049.7s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.448s, learning 0.166s)
               Value function loss: 1555.1533
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 10.93
               Mean episode length: 36.24
                  Mean reward/step: 0.86
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0596
Mean episode consecutive_successes: 0.0909
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 8.61s
                        Total time: 7255.07s
                               ETA: 1093677.1s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.043s, learning 0.184s)
               Value function loss: 870.6512
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 31.16
               Mean episode length: 36.97
                  Mean reward/step: 0.73
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0479
Mean episode consecutive_successes: 0.0963
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 8.23s
                        Total time: 7263.30s
                               ETA: 1093247.3s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.513s, learning 0.163s)
               Value function loss: 1673.1667
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: 24.41
               Mean episode length: 37.83
                  Mean reward/step: 0.90
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0610
Mean episode consecutive_successes: 0.0921
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 8.68s
                        Total time: 7271.97s
                               ETA: 1092886.2s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 2044 steps/s (collection: 7.852s, learning 0.162s)
               Value function loss: 1011.3450
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 55.23
               Mean episode length: 39.56
                  Mean reward/step: 0.67
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0552
Mean episode consecutive_successes: 0.0912
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 8.01s
                        Total time: 7279.99s
                               ETA: 1092426.9s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 2081 steps/s (collection: 7.707s, learning 0.163s)
               Value function loss: 722.9976
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 15.57
               Mean episode length: 35.47
                  Mean reward/step: 0.57
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0459
Mean episode consecutive_successes: 0.0907
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 7.87s
                        Total time: 7287.86s
                               ETA: 1091947.5s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.011s, learning 0.219s)
               Value function loss: 1464.6816
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 37.01
               Mean episode length: 38.08
                  Mean reward/step: 0.74
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0596
Mean episode consecutive_successes: 0.0862
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 8.23s
                        Total time: 7296.09s
                               ETA: 1091523.2s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.034s, learning 0.219s)
               Value function loss: 1338.7844
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 16.72
               Mean episode length: 37.64
                  Mean reward/step: 0.76
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0527
Mean episode consecutive_successes: 0.0871
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 8.25s
                        Total time: 7304.34s
                               ETA: 1091103.8s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.129s, learning 0.163s)
               Value function loss: 967.5640
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 104.00
               Mean episode length: 37.57
                  Mean reward/step: 0.74
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0317
Mean episode consecutive_successes: 0.0929
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 8.29s
                        Total time: 7312.63s
                               ETA: 1090691.4s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.035s, learning 0.229s)
               Value function loss: 647.2708
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 12.97
               Mean episode length: 40.37
                  Mean reward/step: 0.58
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0356
Mean episode consecutive_successes: 0.0880
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 8.26s
                        Total time: 7320.90s
                               ETA: 1090275.9s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.489s, learning 0.166s)
               Value function loss: 1030.9354
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 51.22
               Mean episode length: 37.01
                  Mean reward/step: 0.69
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0873
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 8.65s
                        Total time: 7329.55s
                               ETA: 1089919.7s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.977s, learning 0.163s)
               Value function loss: 1476.5598
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 17.19
               Mean episode length: 38.54
                  Mean reward/step: 0.97
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0483
Mean episode consecutive_successes: 0.0835
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 8.14s
                        Total time: 7337.69s
                               ETA: 1089488.1s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.143s, learning 0.200s)
               Value function loss: 1834.5039
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 54.69
               Mean episode length: 38.89
                  Mean reward/step: 1.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0664
Mean episode consecutive_successes: 0.0821
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 8.34s
                        Total time: 7346.03s
                               ETA: 1089087.9s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.946s, learning 0.201s)
               Value function loss: 1563.4915
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 64.40
               Mean episode length: 38.52
                  Mean reward/step: 0.93
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0752
Mean episode consecutive_successes: 0.0806
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 8.15s
                        Total time: 7354.18s
                               ETA: 1088660.0s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.046s, learning 0.159s)
               Value function loss: 2913.2027
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 11.42
               Mean episode length: 37.38
                  Mean reward/step: 1.28
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0835
Mean episode consecutive_successes: 0.0804
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 8.20s
                        Total time: 7362.39s
                               ETA: 1088241.7s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.909s, learning 0.184s)
               Value function loss: 2442.9757
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 22.09
               Mean episode length: 38.63
                  Mean reward/step: 1.34
       Mean episode length/episode: 6.85
            Mean episode successes: 0.1084
Mean episode consecutive_successes: 0.0788
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 8.09s
                        Total time: 7370.48s
                               ETA: 1087808.2s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.386s, learning 0.169s)
               Value function loss: 2341.3544
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 35.16
               Mean episode length: 39.80
                  Mean reward/step: 1.24
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1030
Mean episode consecutive_successes: 0.0872
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 8.55s
                        Total time: 7379.03s
                               ETA: 1087444.0s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.836s, learning 0.190s)
               Value function loss: 1950.8774
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 15.68
               Mean episode length: 35.67
                  Mean reward/step: 1.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0879
Mean episode consecutive_successes: 0.0944
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 8.03s
                        Total time: 7387.06s
                               ETA: 1087003.2s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.156s, learning 0.204s)
               Value function loss: 2291.7944
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 16.06
               Mean episode length: 36.69
                  Mean reward/step: 1.07
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0801
Mean episode consecutive_successes: 0.0987
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 8.36s
                        Total time: 7395.42s
                               ETA: 1086612.6s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.932s, learning 0.212s)
               Value function loss: 1800.6502
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 42.07
               Mean episode length: 38.60
                  Mean reward/step: 0.92
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0596
Mean episode consecutive_successes: 0.1088
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 8.14s
                        Total time: 7403.56s
                               ETA: 1086191.5s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.214s, learning 0.173s)
               Value function loss: 859.5488
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 10.90
               Mean episode length: 36.53
                  Mean reward/step: 0.63
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0552
Mean episode consecutive_successes: 0.1059
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 8.39s
                        Total time: 7411.95s
                               ETA: 1085807.1s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.865s, learning 0.182s)
               Value function loss: 1477.5014
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 58.16
               Mean episode length: 40.73
                  Mean reward/step: 0.79
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0664
Mean episode consecutive_successes: 0.1031
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 8.05s
                        Total time: 7420.00s
                               ETA: 1085374.2s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 2000 steps/s (collection: 7.989s, learning 0.202s)
               Value function loss: 2528.8846
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 57.12
               Mean episode length: 39.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0713
Mean episode consecutive_successes: 0.1036
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 8.19s
                        Total time: 7428.19s
                               ETA: 1084963.5s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.070s, learning 0.164s)
               Value function loss: 2012.2960
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 9.74
               Mean episode length: 39.01
                  Mean reward/step: 1.12
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0752
Mean episode consecutive_successes: 0.1023
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 8.23s
                        Total time: 7436.42s
                               ETA: 1084560.4s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.274s, learning 0.230s)
               Value function loss: 2060.3204
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 35.09
               Mean episode length: 38.93
                  Mean reward/step: 0.90
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0840
Mean episode consecutive_successes: 0.1000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 8.50s
                        Total time: 7444.93s
                               ETA: 1084197.6s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.815s, learning 0.206s)
               Value function loss: 970.9110
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 49.28
               Mean episode length: 38.13
                  Mean reward/step: 0.85
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0801
Mean episode consecutive_successes: 0.0998
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 8.02s
                        Total time: 7452.95s
                               ETA: 1083765.6s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.121s, learning 0.177s)
               Value function loss: 1534.1053
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 13.69
               Mean episode length: 37.19
                  Mean reward/step: 0.74
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0610
Mean episode consecutive_successes: 0.1064
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 8.30s
                        Total time: 7461.25s
                               ETA: 1083375.2s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.023s, learning 0.161s)
               Value function loss: 685.6079
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 30.91
               Mean episode length: 40.71
                  Mean reward/step: 0.72
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0610
Mean episode consecutive_successes: 0.1040
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 8.18s
                        Total time: 7469.43s
                               ETA: 1082969.3s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.214s, learning 0.186s)
               Value function loss: 1446.7435
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 10.24
               Mean episode length: 39.71
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0571
Mean episode consecutive_successes: 0.1035
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 8.40s
                        Total time: 7477.83s
                               ETA: 1082595.8s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.467s, learning 0.221s)
               Value function loss: 783.8414
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 22.10
               Mean episode length: 38.57
                  Mean reward/step: 0.66
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0391
Mean episode consecutive_successes: 0.1090
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 8.69s
                        Total time: 7486.52s
                               ETA: 1082265.0s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 2066 steps/s (collection: 7.763s, learning 0.165s)
               Value function loss: 731.2936
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 14.98
               Mean episode length: 38.90
                  Mean reward/step: 0.70
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0386
Mean episode consecutive_successes: 0.1062
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 7.93s
                        Total time: 7494.45s
                               ETA: 1081825.5s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.209s, learning 0.232s)
               Value function loss: 1365.8541
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 32.02
               Mean episode length: 38.27
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0415
Mean episode consecutive_successes: 0.1025
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 8.44s
                        Total time: 7502.89s
                               ETA: 1081461.2s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.788s, learning 0.182s)
               Value function loss: 1309.7836
                    Surrogate loss: -0.0065
             Mean action noise std: 0.79
                       Mean reward: 74.45
               Mean episode length: 37.62
                  Mean reward/step: 0.77
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0430
Mean episode consecutive_successes: 0.1000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 7.97s
                        Total time: 7510.86s
                               ETA: 1081030.2s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.001s, learning 0.268s)
               Value function loss: 499.2761
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 20.25
               Mean episode length: 38.98
                  Mean reward/step: 0.56
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0439
Mean episode consecutive_successes: 0.0958
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 8.27s
                        Total time: 7519.13s
                               ETA: 1080643.4s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.494s, learning 0.169s)
               Value function loss: 1089.2856
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 16.59
               Mean episode length: 37.45
                  Mean reward/step: 0.74
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0532
Mean episode consecutive_successes: 0.0912
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 8.66s
                        Total time: 7527.79s
                               ETA: 1080314.2s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 2052 steps/s (collection: 7.771s, learning 0.213s)
               Value function loss: 1102.9811
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 43.97
               Mean episode length: 36.55
                  Mean reward/step: 0.84
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0552
Mean episode consecutive_successes: 0.0901
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 7.98s
                        Total time: 7535.77s
                               ETA: 1079888.5s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.260s, learning 0.173s)
               Value function loss: 988.2168
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 19.65
               Mean episode length: 38.42
                  Mean reward/step: 0.74
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0430
Mean episode consecutive_successes: 0.0924
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 8.43s
                        Total time: 7544.21s
                               ETA: 1079528.3s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 2000 steps/s (collection: 7.924s, learning 0.264s)
               Value function loss: 1328.5418
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 22.28
               Mean episode length: 38.98
                  Mean reward/step: 0.85
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0527
Mean episode consecutive_successes: 0.0890
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 8.19s
                        Total time: 7552.40s
                               ETA: 1079134.2s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.393s, learning 0.160s)
               Value function loss: 895.1681
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 13.89
               Mean episode length: 37.29
                  Mean reward/step: 0.63
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0493
Mean episode consecutive_successes: 0.0892
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 8.55s
                        Total time: 7560.95s
                               ETA: 1078793.2s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.346s, learning 0.176s)
               Value function loss: 1496.3857
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 42.54
               Mean episode length: 39.11
                  Mean reward/step: 0.81
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0576
Mean episode consecutive_successes: 0.0871
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 8.52s
                        Total time: 7569.47s
                               ETA: 1078448.8s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.947s, learning 0.163s)
               Value function loss: 1367.4796
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 44.54
               Mean episode length: 38.02
                  Mean reward/step: 0.95
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0596
Mean episode consecutive_successes: 0.0860
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 8.11s
                        Total time: 7577.58s
                               ETA: 1078046.7s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.086s, learning 0.232s)
               Value function loss: 1294.7512
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 20.73
               Mean episode length: 40.65
                  Mean reward/step: 0.69
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0527
Mean episode consecutive_successes: 0.0873
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 8.32s
                        Total time: 7585.90s
                               ETA: 1077675.2s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.233s, learning 0.241s)
               Value function loss: 2083.1981
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 11.58
               Mean episode length: 36.98
                  Mean reward/step: 0.95
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0698
Mean episode consecutive_successes: 0.0821
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 8.47s
                        Total time: 7594.37s
                               ETA: 1077326.9s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.186s, learning 0.173s)
               Value function loss: 2473.5841
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 24.57
               Mean episode length: 38.98
                  Mean reward/step: 1.30
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0928
Mean episode consecutive_successes: 0.0806
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 8.36s
                        Total time: 7602.73s
                               ETA: 1076963.2s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 2049 steps/s (collection: 7.818s, learning 0.177s)
               Value function loss: 1607.9509
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 91.87
               Mean episode length: 37.91
                  Mean reward/step: 0.96
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0713
Mean episode consecutive_successes: 0.0913
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 7.99s
                        Total time: 7610.73s
                               ETA: 1076549.1s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 2030 steps/s (collection: 7.890s, learning 0.178s)
               Value function loss: 1161.4712
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 10.17
               Mean episode length: 39.64
                  Mean reward/step: 0.77
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0615
Mean episode consecutive_successes: 0.0918
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 8.07s
                        Total time: 7618.79s
                               ETA: 1076146.6s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.162s)
               Value function loss: 553.5657
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 12.61
               Mean episode length: 39.73
                  Mean reward/step: 0.50
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0547
Mean episode consecutive_successes: 0.0912
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 8.37s
                        Total time: 7627.17s
                               ETA: 1075788.0s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.177s, learning 0.194s)
               Value function loss: 1051.2158
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 42.80
               Mean episode length: 39.84
                  Mean reward/step: 0.86
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0527
Mean episode consecutive_successes: 0.0926
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 8.37s
                        Total time: 7635.54s
                               ETA: 1075430.2s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.397s, learning 0.164s)
               Value function loss: 241.5937
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 23.04
               Mean episode length: 40.50
                  Mean reward/step: 0.45
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0435
Mean episode consecutive_successes: 0.0928
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 8.56s
                        Total time: 7644.10s
                               ETA: 1075100.2s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.265s, learning 0.179s)
               Value function loss: 766.7062
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: 14.66
               Mean episode length: 39.21
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0396
Mean episode consecutive_successes: 0.0894
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 8.44s
                        Total time: 7652.54s
                               ETA: 1074754.8s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.103s, learning 0.164s)
               Value function loss: 1582.1477
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 45.53
               Mean episode length: 40.13
                  Mean reward/step: 0.85
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0464
Mean episode consecutive_successes: 0.0895
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 8.27s
                        Total time: 7660.81s
                               ETA: 1074385.4s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.160s)
               Value function loss: 849.1235
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 16.09
               Mean episode length: 40.43
                  Mean reward/step: 0.66
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0391
Mean episode consecutive_successes: 0.0884
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 8.42s
                        Total time: 7669.23s
                               ETA: 1074039.0s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.182s, learning 0.200s)
               Value function loss: 1802.4561
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 32.72
               Mean episode length: 39.61
                  Mean reward/step: 0.94
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0410
Mean episode consecutive_successes: 0.0901
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 8.38s
                        Total time: 7677.62s
                               ETA: 1073687.8s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.925s, learning 0.178s)
               Value function loss: 1937.8361
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 25.57
               Mean episode length: 40.70
                  Mean reward/step: 1.08
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0508
Mean episode consecutive_successes: 0.0885
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 8.10s
                        Total time: 7685.72s
                               ETA: 1073298.4s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.165s, learning 0.173s)
               Value function loss: 2194.0021
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 34.13
               Mean episode length: 37.96
                  Mean reward/step: 1.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0625
Mean episode consecutive_successes: 0.0874
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 8.34s
                        Total time: 7694.06s
                               ETA: 1072942.8s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.932s, learning 0.234s)
               Value function loss: 907.0583
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 43.16
               Mean episode length: 40.89
                  Mean reward/step: 0.77
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0576
Mean episode consecutive_successes: 0.0889
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 8.17s
                        Total time: 7702.22s
                               ETA: 1072564.3s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.351s, learning 0.171s)
               Value function loss: 2278.7400
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 92.89
               Mean episode length: 40.07
                  Mean reward/step: 1.11
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0620
Mean episode consecutive_successes: 0.0902
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 8.52s
                        Total time: 7710.75s
                               ETA: 1072236.4s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.034s, learning 0.207s)
               Value function loss: 1962.7802
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 27.31
               Mean episode length: 39.09
                  Mean reward/step: 1.08
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0811
Mean episode consecutive_successes: 0.0869
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 8.24s
                        Total time: 7718.99s
                               ETA: 1071870.4s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.097s, learning 0.175s)
               Value function loss: 2034.8968
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 72.99
               Mean episode length: 40.33
                  Mean reward/step: 1.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0894
Mean episode consecutive_successes: 0.0901
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 8.27s
                        Total time: 7727.26s
                               ETA: 1071509.6s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.403s, learning 0.308s)
               Value function loss: 2241.5519
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 27.43
               Mean episode length: 39.08
                  Mean reward/step: 1.13
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0938
Mean episode consecutive_successes: 0.0884
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 8.71s
                        Total time: 7735.97s
                               ETA: 1071210.7s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.093s, learning 0.169s)
               Value function loss: 1193.4300
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 25.16
               Mean episode length: 39.78
                  Mean reward/step: 0.79
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0767
Mean episode consecutive_successes: 0.0942
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 8.26s
                        Total time: 7744.23s
                               ETA: 1070850.5s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.828s, learning 0.220s)
               Value function loss: 1095.8084
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 30.52
               Mean episode length: 40.04
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0713
Mean episode consecutive_successes: 0.0956
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 8.05s
                        Total time: 7752.28s
                               ETA: 1070461.7s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.749s, learning 0.221s)
               Value function loss: 2089.8995
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 37.65
               Mean episode length: 39.31
                  Mean reward/step: 1.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0674
Mean episode consecutive_successes: 0.0988
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 7.97s
                        Total time: 7760.25s
                               ETA: 1070063.1s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.323s, learning 0.178s)
               Value function loss: 1547.2428
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 8.96
               Mean episode length: 37.83
                  Mean reward/step: 0.76
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0630
Mean episode consecutive_successes: 0.0972
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 8.50s
                        Total time: 7768.75s
                               ETA: 1069738.9s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.009s, learning 0.274s)
               Value function loss: 1453.6426
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 42.16
               Mean episode length: 38.09
                  Mean reward/step: 1.00
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0713
Mean episode consecutive_successes: 0.0970
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 8.28s
                        Total time: 7777.04s
                               ETA: 1069385.4s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.502s, learning 0.173s)
               Value function loss: 502.0831
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 11.60
               Mean episode length: 41.82
                  Mean reward/step: 0.56
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0684
Mean episode consecutive_successes: 0.0927
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 8.67s
                        Total time: 7785.71s
                               ETA: 1069086.7s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.027s, learning 0.171s)
               Value function loss: 1562.1115
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 11.73
               Mean episode length: 37.27
                  Mean reward/step: 0.97
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0596
Mean episode consecutive_successes: 0.0960
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 8.20s
                        Total time: 7793.91s
                               ETA: 1068723.5s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.419s, learning 0.161s)
               Value function loss: 1807.2039
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 18.63
               Mean episode length: 36.60
                  Mean reward/step: 0.91
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0703
Mean episode consecutive_successes: 0.0926
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 8.58s
                        Total time: 7802.49s
                               ETA: 1068413.4s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.165s, learning 0.170s)
               Value function loss: 1910.4262
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 19.88
               Mean episode length: 38.33
                  Mean reward/step: 0.99
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0864
Mean episode consecutive_successes: 0.0884
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 8.34s
                        Total time: 7810.82s
                               ETA: 1068070.8s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.025s, learning 0.174s)
               Value function loss: 1655.1036
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 32.24
               Mean episode length: 38.46
                  Mean reward/step: 0.96
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0918
Mean episode consecutive_successes: 0.0886
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 8.20s
                        Total time: 7819.02s
                               ETA: 1067710.5s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.125s, learning 0.164s)
               Value function loss: 1480.5135
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 22.97
               Mean episode length: 40.06
                  Mean reward/step: 0.96
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0854
Mean episode consecutive_successes: 0.0921
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 8.29s
                        Total time: 7827.31s
                               ETA: 1067363.3s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.194s, learning 0.183s)
               Value function loss: 3141.6781
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 38.07
               Mean episode length: 40.40
                  Mean reward/step: 1.31
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0811
Mean episode consecutive_successes: 0.1014
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 8.38s
                        Total time: 7835.69s
                               ETA: 1067029.3s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.261s, learning 0.210s)
               Value function loss: 2904.3135
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 22.29
               Mean episode length: 38.46
                  Mean reward/step: 1.32
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0981
Mean episode consecutive_successes: 0.0987
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 8.47s
                        Total time: 7844.16s
                               ETA: 1066708.8s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.388s, learning 0.169s)
               Value function loss: 2718.1771
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 34.42
               Mean episode length: 38.25
                  Mean reward/step: 1.20
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1162
Mean episode consecutive_successes: 0.0965
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 8.56s
                        Total time: 7852.72s
                               ETA: 1066400.9s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.779s, learning 0.167s)
               Value function loss: 1571.0964
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 32.40
               Mean episode length: 39.05
                  Mean reward/step: 1.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1108
Mean episode consecutive_successes: 0.1002
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 7.95s
                        Total time: 7860.66s
                               ETA: 1066010.9s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.934s, learning 0.176s)
               Value function loss: 1449.7016
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 59.34
               Mean episode length: 37.19
                  Mean reward/step: 0.86
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1001
Mean episode consecutive_successes: 0.1029
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 8.11s
                        Total time: 7868.77s
                               ETA: 1065644.1s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.302s, learning 0.195s)
               Value function loss: 954.0269
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 70.16
               Mean episode length: 39.73
                  Mean reward/step: 0.72
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0664
Mean episode consecutive_successes: 0.1148
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 8.50s
                        Total time: 7877.27s
                               ETA: 1065330.7s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.213s, learning 0.186s)
               Value function loss: 1136.5574
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 39.44
               Mean episode length: 37.75
                  Mean reward/step: 0.76
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0640
Mean episode consecutive_successes: 0.1119
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 16.40s
                        Total time: 7893.67s
                               ETA: 1066085.3s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.207s, learning 0.159s)
               Value function loss: 1891.6650
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 25.35
               Mean episode length: 39.64
                  Mean reward/step: 1.18
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0703
Mean episode consecutive_successes: 0.1116
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 16.37s
                        Total time: 7910.03s
                               ETA: 1066833.4s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.485s, learning 0.196s)
               Value function loss: 1830.6410
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 17.73
               Mean episode length: 40.14
                  Mean reward/step: 0.88
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0659
Mean episode consecutive_successes: 0.1107
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 16.68s
                        Total time: 7926.71s
                               ETA: 1067622.0s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.012s, learning 0.166s)
               Value function loss: 1403.3027
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 37.47
               Mean episode length: 39.29
                  Mean reward/step: 0.79
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0737
Mean episode consecutive_successes: 0.1073
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 16.18s
                        Total time: 7942.89s
                               ETA: 1068340.6s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1019 steps/s (collection: 15.840s, learning 0.238s)
               Value function loss: 2354.5959
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 39.57
               Mean episode length: 38.43
                  Mean reward/step: 1.31
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0796
Mean episode consecutive_successes: 0.1083
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 16.08s
                        Total time: 7958.97s
                               ETA: 1069043.8s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.260s, learning 0.194s)
               Value function loss: 1492.3376
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 18.06
               Mean episode length: 40.75
                  Mean reward/step: 1.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0967
Mean episode consecutive_successes: 0.1032
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 16.45s
                        Total time: 7975.42s
                               ETA: 1069795.4s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1011 steps/s (collection: 15.889s, learning 0.303s)
               Value function loss: 1633.6747
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 11.65
               Mean episode length: 37.85
                  Mean reward/step: 0.92
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0840
Mean episode consecutive_successes: 0.1063
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 16.19s
                        Total time: 7991.62s
                               ETA: 1070509.9s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1039 steps/s (collection: 15.583s, learning 0.176s)
               Value function loss: 2045.5283
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 42.65
               Mean episode length: 39.47
                  Mean reward/step: 1.16
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0938
Mean episode consecutive_successes: 0.1070
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 15.76s
                        Total time: 8007.38s
                               ETA: 1071164.5s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.114s, learning 0.172s)
               Value function loss: 2865.2388
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 25.99
               Mean episode length: 41.65
                  Mean reward/step: 1.25
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1055
Mean episode consecutive_successes: 0.1053
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 16.29s
                        Total time: 8023.66s
                               ETA: 1071887.7s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.625s, learning 0.205s)
               Value function loss: 2526.3578
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 10.11
               Mean episode length: 38.95
                  Mean reward/step: 1.22
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0928
Mean episode consecutive_successes: 0.1130
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 15.83s
                        Total time: 8039.49s
                               ETA: 1072548.0s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1029 steps/s (collection: 15.666s, learning 0.241s)
               Value function loss: 3003.3143
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 76.32
               Mean episode length: 41.24
                  Mean reward/step: 1.52
       Mean episode length/episode: 6.75
            Mean episode successes: 0.1035
Mean episode consecutive_successes: 0.1168
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 15.91s
                        Total time: 8055.40s
                               ETA: 1073216.8s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.987s, learning 0.158s)
               Value function loss: 1885.2734
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 106.16
               Mean episode length: 40.61
                  Mean reward/step: 1.18
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1040
Mean episode consecutive_successes: 0.1204
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 16.15s
                        Total time: 8071.54s
                               ETA: 1073915.5s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1012 steps/s (collection: 15.998s, learning 0.184s)
               Value function loss: 2889.9921
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 40.76
               Mean episode length: 40.65
                  Mean reward/step: 1.28
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1021
Mean episode consecutive_successes: 0.1223
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 16.18s
                        Total time: 8087.73s
                               ETA: 1074617.2s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.205s, learning 0.173s)
               Value function loss: 2383.5897
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 25.83
               Mean episode length: 36.09
                  Mean reward/step: 1.22
       Mean episode length/episode: 6.74
            Mean episode successes: 0.1143
Mean episode consecutive_successes: 0.1195
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 16.38s
                        Total time: 8104.10s
                               ETA: 1075343.0s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.057s, learning 0.164s)
               Value function loss: 1703.7145
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 44.56
               Mean episode length: 38.42
                  Mean reward/step: 1.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1138
Mean episode consecutive_successes: 0.1207
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 16.22s
                        Total time: 8120.32s
                               ETA: 1076046.0s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.796s, learning 0.161s)
               Value function loss: 2073.6161
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 80.67
               Mean episode length: 39.99
                  Mean reward/step: 1.08
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0991
Mean episode consecutive_successes: 0.1261
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 15.96s
                        Total time: 8136.28s
                               ETA: 1076712.0s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.238s, learning 0.170s)
               Value function loss: 1705.9771
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 26.60
               Mean episode length: 37.58
                  Mean reward/step: 0.89
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0879
Mean episode consecutive_successes: 0.1268
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 16.41s
                        Total time: 8152.69s
                               ETA: 1077435.9s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.040s, learning 0.194s)
               Value function loss: 867.4957
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 68.11
               Mean episode length: 40.57
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0718
Mean episode consecutive_successes: 0.1286
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 16.23s
                        Total time: 8168.92s
                               ETA: 1078134.8s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1031 steps/s (collection: 15.719s, learning 0.167s)
               Value function loss: 1772.8115
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 67.99
               Mean episode length: 39.41
                  Mean reward/step: 1.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0723
Mean episode consecutive_successes: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 15.89s
                        Total time: 8184.81s
                               ETA: 1078786.0s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.121s, learning 0.189s)
               Value function loss: 1916.8630
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 19.48
               Mean episode length: 38.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0889
Mean episode consecutive_successes: 0.1222
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 16.31s
                        Total time: 8201.12s
                               ETA: 1079491.2s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.191s, learning 0.190s)
               Value function loss: 2774.6743
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 33.67
               Mean episode length: 36.68
                  Mean reward/step: 1.34
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0977
Mean episode consecutive_successes: 0.1227
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 16.38s
                        Total time: 8217.50s
                               ETA: 1080203.8s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1033 steps/s (collection: 15.683s, learning 0.166s)
               Value function loss: 1552.5453
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 44.51
               Mean episode length: 38.70
                  Mean reward/step: 1.06
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0981
Mean episode consecutive_successes: 0.1221
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 15.85s
                        Total time: 8233.35s
                               ETA: 1080844.8s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1015 steps/s (collection: 15.912s, learning 0.216s)
               Value function loss: 2187.6345
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 24.94
               Mean episode length: 39.77
                  Mean reward/step: 1.09
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0996
Mean episode consecutive_successes: 0.1218
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 16.13s
                        Total time: 8249.48s
                               ETA: 1081520.6s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.965s, learning 0.189s)
               Value function loss: 1651.7364
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 24.50
               Mean episode length: 38.09
                  Mean reward/step: 0.96
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0972
Mean episode consecutive_successes: 0.1218
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 16.15s
                        Total time: 8265.63s
                               ETA: 1082197.9s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.954s, learning 0.211s)
               Value function loss: 1480.8607
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 41.27
               Mean episode length: 41.15
                  Mean reward/step: 1.01
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0947
Mean episode consecutive_successes: 0.1239
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 16.17s
                        Total time: 8281.80s
                               ETA: 1082874.9s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.086s, learning 0.206s)
               Value function loss: 2262.6727
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 18.20
               Mean episode length: 40.54
                  Mean reward/step: 1.10
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0967
Mean episode consecutive_successes: 0.1228
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 16.29s
                        Total time: 8298.09s
                               ETA: 1083566.5s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.208s, learning 0.281s)
               Value function loss: 2881.7448
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 37.52
               Mean episode length: 39.04
                  Mean reward/step: 1.33
       Mean episode length/episode: 6.91
            Mean episode successes: 0.1089
Mean episode consecutive_successes: 0.1239
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 16.49s
                        Total time: 8314.58s
                               ETA: 1084282.1s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.904s, learning 0.180s)
               Value function loss: 1906.7369
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 67.95
               Mean episode length: 39.76
                  Mean reward/step: 1.07
       Mean episode length/episode: 6.72
            Mean episode successes: 0.1035
Mean episode consecutive_successes: 0.1272
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 16.08s
                        Total time: 8330.66s
                               ETA: 1084943.0s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.191s, learning 0.255s)
               Value function loss: 1435.3615
                    Surrogate loss: -0.0030
             Mean action noise std: 0.79
                       Mean reward: 22.29
               Mean episode length: 38.90
                  Mean reward/step: 0.81
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0977
Mean episode consecutive_successes: 0.1248
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 16.45s
                        Total time: 8347.11s
                               ETA: 1085649.1s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.113s, learning 0.200s)
               Value function loss: 1444.4154
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 50.45
               Mean episode length: 39.89
                  Mean reward/step: 0.78
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0947
Mean episode consecutive_successes: 0.1245
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 16.31s
                        Total time: 8363.42s
                               ETA: 1086336.0s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1019 steps/s (collection: 15.806s, learning 0.271s)
               Value function loss: 959.6405
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 37.72
               Mean episode length: 40.20
                  Mean reward/step: 0.69
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0728
Mean episode consecutive_successes: 0.1283
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 16.08s
                        Total time: 8379.50s
                               ETA: 1086990.6s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.959s, learning 0.190s)
               Value function loss: 2251.1106
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 65.72
               Mean episode length: 40.39
                  Mean reward/step: 1.02
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0649
Mean episode consecutive_successes: 0.1340
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 16.15s
                        Total time: 8395.65s
                               ETA: 1087652.7s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.153s, learning 0.305s)
               Value function loss: 2200.5082
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 62.22
               Mean episode length: 43.84
                  Mean reward/step: 1.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0620
Mean episode consecutive_successes: 0.1304
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 16.46s
                        Total time: 8412.10s
                               ETA: 1088353.0s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.045s, learning 0.198s)
               Value function loss: 3936.6744
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 18.15
               Mean episode length: 40.64
                  Mean reward/step: 1.59
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0850
Mean episode consecutive_successes: 0.1278
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 16.24s
                        Total time: 8428.35s
                               ETA: 1089023.6s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.176s, learning 0.301s)
               Value function loss: 1222.7612
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 27.32
               Mean episode length: 38.30
                  Mean reward/step: 0.77
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0830
Mean episode consecutive_successes: 0.1263
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 16.48s
                        Total time: 8444.82s
                               ETA: 1089722.7s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1012 steps/s (collection: 15.879s, learning 0.309s)
               Value function loss: 1948.5476
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 37.65
               Mean episode length: 39.50
                  Mean reward/step: 0.97
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0762
Mean episode consecutive_successes: 0.1275
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 16.19s
                        Total time: 8461.01s
                               ETA: 1090382.7s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1037 steps/s (collection: 15.612s, learning 0.181s)
               Value function loss: 2465.6056
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 12.57
               Mean episode length: 39.82
                  Mean reward/step: 1.17
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0737
Mean episode consecutive_successes: 0.1284
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 15.79s
                        Total time: 8476.81s
                               ETA: 1090990.2s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1213 steps/s (collection: 13.315s, learning 0.185s)
               Value function loss: 1148.5693
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 70.26
               Mean episode length: 39.80
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.1314
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 13.50s
                        Total time: 8490.31s
                               ETA: 1091301.2s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.157s, learning 0.177s)
               Value function loss: 1128.3146
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 18.77
               Mean episode length: 37.28
                  Mean reward/step: 0.64
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0562
Mean episode consecutive_successes: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 8.33s
                        Total time: 8498.64s
                               ETA: 1090948.2s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.953s, learning 0.182s)
               Value function loss: 1475.5668
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 16.80
               Mean episode length: 38.34
                  Mean reward/step: 1.02
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0620
Mean episode consecutive_successes: 0.1245
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 8.13s
                        Total time: 8506.77s
                               ETA: 1090570.6s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.481s, learning 0.209s)
               Value function loss: 3381.2386
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 17.74
               Mean episode length: 40.13
                  Mean reward/step: 1.48
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0815
Mean episode consecutive_successes: 0.1227
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 8.69s
                        Total time: 8515.46s
                               ETA: 1090265.1s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.144s, learning 0.229s)
               Value function loss: 2115.8326
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 76.54
               Mean episode length: 41.78
                  Mean reward/step: 1.25
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0889
Mean episode consecutive_successes: 0.1236
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 8.37s
                        Total time: 8523.84s
                               ETA: 1089919.8s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.429s, learning 0.187s)
               Value function loss: 2296.2570
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 76.57
               Mean episode length: 41.11
                  Mean reward/step: 1.18
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0913
Mean episode consecutive_successes: 0.1281
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 8.62s
                        Total time: 8532.45s
                               ETA: 1089606.3s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.594s, learning 0.169s)
               Value function loss: 1997.3100
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 21.33
               Mean episode length: 37.36
                  Mean reward/step: 1.21
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0874
Mean episode consecutive_successes: 0.1271
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 8.76s
                        Total time: 8541.22s
                               ETA: 1089312.4s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 2058 steps/s (collection: 7.795s, learning 0.163s)
               Value function loss: 1770.4050
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 19.62
               Mean episode length: 39.09
                  Mean reward/step: 1.07
       Mean episode length/episode: 6.78
            Mean episode successes: 0.1108
Mean episode consecutive_successes: 0.1190
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 7.96s
                        Total time: 8549.17s
                               ETA: 1088916.8s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.451s, learning 0.247s)
               Value function loss: 2162.0471
                    Surrogate loss: -0.0124
             Mean action noise std: 0.79
                       Mean reward: 55.29
               Mean episode length: 39.87
                  Mean reward/step: 1.24
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1221
Mean episode consecutive_successes: 0.1177
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 8.70s
                        Total time: 8557.87s
                               ETA: 1088616.2s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.172s)
               Value function loss: 2447.3787
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 57.41
               Mean episode length: 43.46
                  Mean reward/step: 1.35
       Mean episode length/episode: 6.91
            Mean episode successes: 0.1182
Mean episode consecutive_successes: 0.1256
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 8.55s
                        Total time: 8566.42s
                               ETA: 1088297.0s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.254s, learning 0.169s)
               Value function loss: 2679.4726
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 45.15
               Mean episode length: 39.90
                  Mean reward/step: 1.29
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1182
Mean episode consecutive_successes: 0.1271
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 8.42s
                        Total time: 8574.84s
                               ETA: 1087963.0s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.407s, learning 0.190s)
               Value function loss: 2501.9028
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 58.11
               Mean episode length: 40.35
                  Mean reward/step: 1.16
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1274
Mean episode consecutive_successes: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 8.60s
                        Total time: 8583.44s
                               ETA: 1087652.0s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.027s, learning 0.231s)
               Value function loss: 2052.6761
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 32.64
               Mean episode length: 39.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1294
Mean episode consecutive_successes: 0.1255
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 8.26s
                        Total time: 8591.70s
                               ETA: 1087298.7s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.106s, learning 0.169s)
               Value function loss: 2300.6707
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 66.96
               Mean episode length: 42.66
                  Mean reward/step: 1.29
       Mean episode length/episode: 6.78
            Mean episode successes: 0.1016
Mean episode consecutive_successes: 0.1386
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 8.28s
                        Total time: 8599.97s
                               ETA: 1086948.6s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.219s, learning 0.179s)
               Value function loss: 3150.9495
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 67.28
               Mean episode length: 38.94
                  Mean reward/step: 1.45
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1167
Mean episode consecutive_successes: 0.1370
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 8.40s
                        Total time: 8608.37s
                               ETA: 1086615.0s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.657s, learning 0.178s)
               Value function loss: 3944.1813
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 43.95
               Mean episode length: 37.51
                  Mean reward/step: 1.73
       Mean episode length/episode: 6.78
            Mean episode successes: 0.1338
Mean episode consecutive_successes: 0.1381
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 8.84s
                        Total time: 8617.20s
                               ETA: 1086337.2s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.427s, learning 0.162s)
               Value function loss: 2108.3326
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 13.03
               Mean episode length: 40.32
                  Mean reward/step: 1.17
       Mean episode length/episode: 6.74
            Mean episode successes: 0.1299
Mean episode consecutive_successes: 0.1371
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 8.59s
                        Total time: 8625.79s
                               ETA: 1086029.1s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.287s, learning 0.210s)
               Value function loss: 2605.3052
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 52.41
               Mean episode length: 39.74
                  Mean reward/step: 1.11
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1333
Mean episode consecutive_successes: 0.1389
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 8.50s
                        Total time: 8634.29s
                               ETA: 1085710.1s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.253s, learning 0.191s)
               Value function loss: 2391.3205
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 13.50
               Mean episode length: 36.85
                  Mean reward/step: 1.12
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1118
Mean episode consecutive_successes: 0.1440
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 8.44s
                        Total time: 8642.74s
                               ETA: 1085385.3s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.180s, learning 0.175s)
               Value function loss: 3583.5191
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 33.40
               Mean episode length: 41.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 6.85
            Mean episode successes: 0.1328
Mean episode consecutive_successes: 0.1422
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 8.35s
                        Total time: 8651.09s
                               ETA: 1085050.1s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.191s, learning 0.172s)
               Value function loss: 2687.3323
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 27.17
               Mean episode length: 43.08
                  Mean reward/step: 1.36
       Mean episode length/episode: 6.83
            Mean episode successes: 0.1270
Mean episode consecutive_successes: 0.1463
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 8.36s
                        Total time: 8659.45s
                               ETA: 1084716.7s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.896s, learning 0.158s)
               Value function loss: 3386.3320
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: 54.08
               Mean episode length: 41.86
                  Mean reward/step: 1.61
       Mean episode length/episode: 6.75
            Mean episode successes: 0.1235
Mean episode consecutive_successes: 0.1532
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 8.05s
                        Total time: 8667.51s
                               ETA: 1084345.5s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.568s, learning 0.167s)
               Value function loss: 2813.8023
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 52.94
               Mean episode length: 39.60
                  Mean reward/step: 1.43
       Mean episode length/episode: 6.85
            Mean episode successes: 0.1416
Mean episode consecutive_successes: 0.1500
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 8.74s
                        Total time: 8676.24s
                               ETA: 1084060.4s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.419s, learning 0.166s)
               Value function loss: 3042.6124
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 84.33
               Mean episode length: 42.72
                  Mean reward/step: 1.34
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1489
Mean episode consecutive_successes: 0.1483
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 8.59s
                        Total time: 8684.83s
                               ETA: 1083757.2s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.475s, learning 0.191s)
               Value function loss: 3760.0956
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 35.57
               Mean episode length: 40.13
                  Mean reward/step: 1.55
       Mean episode length/episode: 6.76
            Mean episode successes: 0.1484
Mean episode consecutive_successes: 0.1518
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 8.67s
                        Total time: 8693.49s
                               ETA: 1083464.9s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.265s, learning 0.158s)
               Value function loss: 2464.3978
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 64.09
               Mean episode length: 41.55
                  Mean reward/step: 1.15
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1387
Mean episode consecutive_successes: 0.1542
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 8.42s
                        Total time: 8701.92s
                               ETA: 1083142.9s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.142s, learning 0.205s)
               Value function loss: 3030.2286
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 69.57
               Mean episode length: 38.53
                  Mean reward/step: 1.39
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1436
Mean episode consecutive_successes: 0.1560
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 8.35s
                        Total time: 8710.26s
                               ETA: 1082812.3s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.143s, learning 0.207s)
               Value function loss: 3030.5271
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 122.88
               Mean episode length: 39.73
                  Mean reward/step: 1.26
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1426
Mean episode consecutive_successes: 0.1622
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 8.35s
                        Total time: 8718.61s
                               ETA: 1082482.9s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.220s, learning 0.177s)
               Value function loss: 2079.2757
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 47.17
               Mean episode length: 38.24
                  Mean reward/step: 1.14
       Mean episode length/episode: 6.76
            Mean episode successes: 0.1226
Mean episode consecutive_successes: 0.1609
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 8.40s
                        Total time: 8727.01s
                               ETA: 1082160.3s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.906s, learning 0.200s)
               Value function loss: 2877.8347
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 78.77
               Mean episode length: 41.14
                  Mean reward/step: 1.30
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1025
Mean episode consecutive_successes: 0.1689
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 8.11s
                        Total time: 8735.12s
                               ETA: 1081802.3s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.169s, learning 0.162s)
               Value function loss: 3004.8575
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 51.12
               Mean episode length: 41.51
                  Mean reward/step: 1.36
       Mean episode length/episode: 6.85
            Mean episode successes: 0.1011
Mean episode consecutive_successes: 0.1714
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 8.33s
                        Total time: 8743.45s
                               ETA: 1081473.0s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.257s, learning 0.176s)
               Value function loss: 2431.3996
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 42.96
               Mean episode length: 39.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.1118
Mean episode consecutive_successes: 0.1646
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 8.43s
                        Total time: 8751.88s
                               ETA: 1081157.1s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.218s, learning 0.186s)
               Value function loss: 3703.2649
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 20.64
               Mean episode length: 40.35
                  Mean reward/step: 1.74
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1382
Mean episode consecutive_successes: 0.1592
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 8.40s
                        Total time: 8760.28s
                               ETA: 1080838.2s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.021s, learning 0.172s)
               Value function loss: 2467.1346
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 132.46
               Mean episode length: 43.30
                  Mean reward/step: 1.23
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1147
Mean episode consecutive_successes: 0.1705
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 8.19s
                        Total time: 8768.48s
                               ETA: 1080494.2s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.213s, learning 0.167s)
               Value function loss: 2787.8370
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 45.62
               Mean episode length: 39.93
                  Mean reward/step: 1.32
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1064
Mean episode consecutive_successes: 0.1724
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 8.38s
                        Total time: 8776.86s
                               ETA: 1080174.1s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.198s, learning 0.265s)
               Value function loss: 2966.9833
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 20.47
               Mean episode length: 40.18
                  Mean reward/step: 1.38
       Mean episode length/episode: 6.83
            Mean episode successes: 0.1167
Mean episode consecutive_successes: 0.1677
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 8.46s
                        Total time: 8785.32s
                               ETA: 1079864.9s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.043s, learning 0.171s)
               Value function loss: 3022.5831
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 41.11
               Mean episode length: 40.39
                  Mean reward/step: 1.45
       Mean episode length/episode: 6.74
            Mean episode successes: 0.1191
Mean episode consecutive_successes: 0.1694
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 8.21s
                        Total time: 8793.53s
                               ETA: 1079525.9s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.996s, learning 0.163s)
               Value function loss: 4378.7310
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 127.20
               Mean episode length: 43.47
                  Mean reward/step: 1.89
       Mean episode length/episode: 6.78
            Mean episode successes: 0.1143
Mean episode consecutive_successes: 0.1762
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 8.16s
                        Total time: 8801.69s
                               ETA: 1079181.0s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.063s, learning 0.166s)
               Value function loss: 3207.1813
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 56.46
               Mean episode length: 41.24
                  Mean reward/step: 1.66
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1274
Mean episode consecutive_successes: 0.1768
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 8.23s
                        Total time: 8809.92s
                               ETA: 1078845.5s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.535s, learning 0.217s)
               Value function loss: 3114.9742
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 104.26
               Mean episode length: 42.02
                  Mean reward/step: 1.48
       Mean episode length/episode: 6.73
            Mean episode successes: 0.1377
Mean episode consecutive_successes: 0.1758
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 8.75s
                        Total time: 8818.67s
                               ETA: 1078574.7s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 2059 steps/s (collection: 7.775s, learning 0.179s)
               Value function loss: 2580.6319
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 40.79
               Mean episode length: 40.77
                  Mean reward/step: 1.31
       Mean episode length/episode: 6.73
            Mean episode successes: 0.1387
Mean episode consecutive_successes: 0.1728
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 7.95s
                        Total time: 8826.63s
                               ETA: 1078207.2s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.164s)
               Value function loss: 3280.1603
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 70.64
               Mean episode length: 40.31
                  Mean reward/step: 1.44
       Mean episode length/episode: 6.85
            Mean episode successes: 0.1460
Mean episode consecutive_successes: 0.1718
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 8.38s
                        Total time: 8835.01s
                               ETA: 1077892.8s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.050s, learning 0.166s)
               Value function loss: 2619.1197
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 73.80
               Mean episode length: 41.30
                  Mean reward/step: 1.40
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1289
Mean episode consecutive_successes: 0.1799
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 8.22s
                        Total time: 8843.22s
                               ETA: 1077558.9s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.364s, learning 0.205s)
               Value function loss: 3671.1072
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 85.63
               Mean episode length: 40.47
                  Mean reward/step: 1.79
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1260
Mean episode consecutive_successes: 0.1894
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 8.57s
                        Total time: 8851.79s
                               ETA: 1077268.6s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.339s, learning 0.170s)
               Value function loss: 4427.3457
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 32.54
               Mean episode length: 38.64
                  Mean reward/step: 1.91
       Mean episode length/episode: 6.70
            Mean episode successes: 0.1445
Mean episode consecutive_successes: 0.1834
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 8.51s
                        Total time: 8860.30s
                               ETA: 1076972.0s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.596s, learning 0.173s)
               Value function loss: 4524.6817
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 70.09
               Mean episode length: 39.28
                  Mean reward/step: 1.86
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1729
Mean episode consecutive_successes: 0.1807
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 8.77s
                        Total time: 8869.07s
                               ETA: 1076707.4s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.924s, learning 0.156s)
               Value function loss: 3333.4338
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 68.31
               Mean episode length: 40.84
                  Mean reward/step: 1.59
       Mean episode length/episode: 6.93
            Mean episode successes: 0.1724
Mean episode consecutive_successes: 0.1827
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 8.08s
                        Total time: 8877.15s
                               ETA: 1076360.0s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.061s, learning 0.162s)
               Value function loss: 2450.6381
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 156.33
               Mean episode length: 40.96
                  Mean reward/step: 1.26
       Mean episode length/episode: 6.76
            Mean episode successes: 0.1226
Mean episode consecutive_successes: 0.1987
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 8.22s
                        Total time: 8885.37s
                               ETA: 1076030.7s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.275s, learning 0.187s)
               Value function loss: 2189.1349
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 31.78
               Mean episode length: 38.68
                  Mean reward/step: 1.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.1104
Mean episode consecutive_successes: 0.1976
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 8.46s
                        Total time: 8893.84s
                               ETA: 1075731.1s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.189s, learning 0.165s)
               Value function loss: 1393.7784
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 81.81
               Mean episode length: 42.63
                  Mean reward/step: 0.90
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0854
Mean episode consecutive_successes: 0.2029
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 8.35s
                        Total time: 8902.19s
                               ETA: 1075419.2s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.119s, learning 0.161s)
               Value function loss: 3503.2667
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 32.56
               Mean episode length: 38.97
                  Mean reward/step: 1.45
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0986
Mean episode consecutive_successes: 0.1940
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 8.28s
                        Total time: 8910.47s
                               ETA: 1075099.1s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.220s, learning 0.174s)
               Value function loss: 4140.9547
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 25.90
               Mean episode length: 40.30
                  Mean reward/step: 1.72
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1050
Mean episode consecutive_successes: 0.1929
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 8.39s
                        Total time: 8918.86s
                               ETA: 1074793.5s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.469s, learning 0.169s)
               Value function loss: 4387.1299
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 24.72
               Mean episode length: 38.63
                  Mean reward/step: 1.88
       Mean episode length/episode: 6.83
            Mean episode successes: 0.1431
Mean episode consecutive_successes: 0.1834
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 8.64s
                        Total time: 8927.50s
                               ETA: 1074518.0s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.054s, learning 0.183s)
               Value function loss: 4513.7408
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 62.72
               Mean episode length: 39.20
                  Mean reward/step: 2.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.1846
Mean episode consecutive_successes: 0.1779
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 8.24s
                        Total time: 8935.74s
                               ETA: 1074194.9s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.139s, learning 0.167s)
               Value function loss: 3451.4530
                    Surrogate loss: -0.0149
             Mean action noise std: 0.79
                       Mean reward: 18.08
               Mean episode length: 40.31
                  Mean reward/step: 1.56
       Mean episode length/episode: 6.88
            Mean episode successes: 0.1851
Mean episode consecutive_successes: 0.1762
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 8.31s
                        Total time: 8944.05s
                               ETA: 1073881.0s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.094s, learning 0.193s)
               Value function loss: 3851.1664
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 133.82
               Mean episode length: 41.01
                  Mean reward/step: 1.86
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1826
Mean episode consecutive_successes: 0.1889
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 8.29s
                        Total time: 8952.33s
                               ETA: 1073565.5s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.160s)
               Value function loss: 2879.6713
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 70.47
               Mean episode length: 40.55
                  Mean reward/step: 1.28
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1562
Mean episode consecutive_successes: 0.1962
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 8.18s
                        Total time: 8960.52s
                               ETA: 1073238.2s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.258s, learning 0.192s)
               Value function loss: 3180.3943
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 78.18
               Mean episode length: 40.40
                  Mean reward/step: 1.38
       Mean episode length/episode: 6.69
            Mean episode successes: 0.1226
Mean episode consecutive_successes: 0.2039
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 8.45s
                        Total time: 8968.97s
                               ETA: 1072943.6s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.033s, learning 0.185s)
               Value function loss: 3085.1044
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 107.71
               Mean episode length: 43.50
                  Mean reward/step: 1.46
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1377
Mean episode consecutive_successes: 0.2004
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 8.22s
                        Total time: 8977.18s
                               ETA: 1072622.1s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.131s, learning 0.266s)
               Value function loss: 1703.2769
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 22.63
               Mean episode length: 40.07
                  Mean reward/step: 0.98
       Mean episode length/episode: 6.92
            Mean episode successes: 0.1226
Mean episode consecutive_successes: 0.1963
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 8.40s
                        Total time: 8985.58s
                               ETA: 1072322.7s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.438s, learning 0.226s)
               Value function loss: 3246.9475
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 29.02
               Mean episode length: 37.13
                  Mean reward/step: 1.49
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1167
Mean episode consecutive_successes: 0.1986
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 8.66s
                        Total time: 8994.25s
                               ETA: 1072055.7s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.056s, learning 0.275s)
               Value function loss: 3750.1816
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 35.37
               Mean episode length: 40.34
                  Mean reward/step: 1.62
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1211
Mean episode consecutive_successes: 0.1990
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 8.33s
                        Total time: 9002.58s
                               ETA: 1071749.7s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.249s, learning 0.289s)
               Value function loss: 3208.4171
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 12.56
               Mean episode length: 39.80
                  Mean reward/step: 1.60
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1255
Mean episode consecutive_successes: 0.1962
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 8.54s
                        Total time: 9011.11s
                               ETA: 1071469.1s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.260s, learning 0.191s)
               Value function loss: 3232.4232
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 65.51
               Mean episode length: 40.46
                  Mean reward/step: 1.54
       Mean episode length/episode: 6.88
            Mean episode successes: 0.1602
Mean episode consecutive_successes: 0.1881
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 8.45s
                        Total time: 9019.57s
                               ETA: 1071178.8s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.132s, learning 0.174s)
               Value function loss: 2825.7132
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 92.14
               Mean episode length: 38.26
                  Mean reward/step: 1.48
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1533
Mean episode consecutive_successes: 0.1890
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 8.31s
                        Total time: 9027.87s
                               ETA: 1070871.9s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.289s, learning 0.184s)
               Value function loss: 4346.8467
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 69.14
               Mean episode length: 41.60
                  Mean reward/step: 1.81
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1650
Mean episode consecutive_successes: 0.1910
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 8.47s
                        Total time: 9036.35s
                               ETA: 1070585.6s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.156s, learning 0.168s)
               Value function loss: 3444.7124
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 43.98
               Mean episode length: 41.23
                  Mean reward/step: 1.75
       Mean episode length/episode: 6.83
            Mean episode successes: 0.1592
Mean episode consecutive_successes: 0.1947
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 8.32s
                        Total time: 9044.67s
                               ETA: 1070282.4s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.212s, learning 0.169s)
               Value function loss: 4348.2505
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 19.95
               Mean episode length: 39.15
                  Mean reward/step: 1.77
       Mean episode length/episode: 6.91
            Mean episode successes: 0.1763
Mean episode consecutive_successes: 0.1910
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 8.38s
                        Total time: 9053.05s
                               ETA: 1069986.5s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.017s, learning 0.179s)
               Value function loss: 3627.3168
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 61.01
               Mean episode length: 40.30
                  Mean reward/step: 1.71
       Mean episode length/episode: 6.78
            Mean episode successes: 0.1689
Mean episode consecutive_successes: 0.1965
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 8.20s
                        Total time: 9061.25s
                               ETA: 1069669.5s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.092s, learning 0.158s)
               Value function loss: 3498.3220
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 93.30
               Mean episode length: 40.14
                  Mean reward/step: 1.49
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1597
Mean episode consecutive_successes: 0.2029
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 8.25s
                        Total time: 9069.50s
                               ETA: 1069359.6s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 2059 steps/s (collection: 7.788s, learning 0.166s)
               Value function loss: 3777.6907
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 97.38
               Mean episode length: 39.26
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.75
            Mean episode successes: 0.1455
Mean episode consecutive_successes: 0.2094
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 7.95s
                        Total time: 9077.45s
                               ETA: 1069015.6s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.225s, learning 0.194s)
               Value function loss: 3538.1420
                    Surrogate loss: -0.0150
             Mean action noise std: 0.79
                       Mean reward: 49.08
               Mean episode length: 37.31
                  Mean reward/step: 1.66
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1392
Mean episode consecutive_successes: 0.2144
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 8.42s
                        Total time: 9085.87s
                               ETA: 1068727.0s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.235s, learning 0.164s)
               Value function loss: 3600.9501
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 57.58
               Mean episode length: 39.31
                  Mean reward/step: 1.63
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1655
Mean episode consecutive_successes: 0.2055
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 8.40s
                        Total time: 9094.27s
                               ETA: 1068436.8s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.202s, learning 0.163s)
               Value function loss: 5764.4080
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 47.17
               Mean episode length: 38.25
                  Mean reward/step: 2.37
       Mean episode length/episode: 6.92
            Mean episode successes: 0.1763
Mean episode consecutive_successes: 0.2126
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 8.37s
                        Total time: 9102.64s
                               ETA: 1068143.2s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.978s, learning 0.162s)
               Value function loss: 3533.0768
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 86.32
               Mean episode length: 41.94
                  Mean reward/step: 1.63
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1777
Mean episode consecutive_successes: 0.2097
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 8.14s
                        Total time: 9110.78s
                               ETA: 1067823.9s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.038s, learning 0.161s)
               Value function loss: 3283.8532
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 79.63
               Mean episode length: 38.87
                  Mean reward/step: 1.32
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1587
Mean episode consecutive_successes: 0.2144
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 8.20s
                        Total time: 9118.98s
                               ETA: 1067512.2s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.486s, learning 0.205s)
               Value function loss: 4375.7444
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 40.91
               Mean episode length: 41.01
                  Mean reward/step: 1.86
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1753
Mean episode consecutive_successes: 0.2106
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 8.69s
                        Total time: 9127.67s
                               ETA: 1067258.8s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.096s, learning 0.197s)
               Value function loss: 4956.4711
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 35.59
               Mean episode length: 39.67
                  Mean reward/step: 1.98
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1831
Mean episode consecutive_successes: 0.2132
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 8.29s
                        Total time: 9135.96s
                               ETA: 1066959.6s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.078s, learning 0.171s)
               Value function loss: 5105.7398
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 119.14
               Mean episode length: 41.91
                  Mean reward/step: 2.01
       Mean episode length/episode: 6.83
            Mean episode successes: 0.1973
Mean episode consecutive_successes: 0.2203
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 8.25s
                        Total time: 9144.21s
                               ETA: 1066655.8s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.098s, learning 0.185s)
               Value function loss: 3927.2117
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 34.85
               Mean episode length: 39.29
                  Mean reward/step: 1.55
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1880
Mean episode consecutive_successes: 0.2165
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 8.28s
                        Total time: 9152.49s
                               ETA: 1066356.6s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.914s, learning 0.165s)
               Value function loss: 3224.1262
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 66.04
               Mean episode length: 40.96
                  Mean reward/step: 1.52
       Mean episode length/episode: 6.90
            Mean episode successes: 0.1802
Mean episode consecutive_successes: 0.2191
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 8.08s
                        Total time: 9160.57s
                               ETA: 1066034.4s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 2002 steps/s (collection: 7.975s, learning 0.207s)
               Value function loss: 4132.9633
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 74.28
               Mean episode length: 42.19
                  Mean reward/step: 1.58
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1733
Mean episode consecutive_successes: 0.2214
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 8.18s
                        Total time: 9168.75s
                               ETA: 1065725.0s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.235s, learning 0.175s)
               Value function loss: 4297.6450
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 179.50
               Mean episode length: 42.95
                  Mean reward/step: 1.84
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1865
Mean episode consecutive_successes: 0.2224
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 8.41s
                        Total time: 9177.16s
                               ETA: 1065442.7s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.091s, learning 0.188s)
               Value function loss: 5177.8590
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 56.03
               Mean episode length: 40.90
                  Mean reward/step: 2.21
       Mean episode length/episode: 6.85
            Mean episode successes: 0.1880
Mean episode consecutive_successes: 0.2233
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 8.28s
                        Total time: 9185.44s
                               ETA: 1065145.9s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.092s, learning 0.162s)
               Value function loss: 4998.1962
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 103.54
               Mean episode length: 40.60
                  Mean reward/step: 2.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1992
Mean episode consecutive_successes: 0.2263
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 8.25s
                        Total time: 9193.70s
                               ETA: 1064846.8s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.945s, learning 0.188s)
               Value function loss: 4372.8783
                    Surrogate loss: -0.0124
             Mean action noise std: 0.79
                       Mean reward: 85.78
               Mean episode length: 40.11
                  Mean reward/step: 1.74
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1943
Mean episode consecutive_successes: 0.2290
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 8.13s
                        Total time: 9201.83s
                               ETA: 1064534.5s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.028s, learning 0.160s)
               Value function loss: 4514.3183
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 103.31
               Mean episode length: 39.80
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1904
Mean episode consecutive_successes: 0.2325
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 8.19s
                        Total time: 9210.02s
                               ETA: 1064229.2s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.241s, learning 0.170s)
               Value function loss: 5775.1338
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 42.52
               Mean episode length: 43.27
                  Mean reward/step: 2.29
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2183
Mean episode consecutive_successes: 0.2287
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 8.41s
                        Total time: 9218.43s
                               ETA: 1063950.3s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.832s, learning 0.253s)
               Value function loss: 4230.8756
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 118.54
               Mean episode length: 40.28
                  Mean reward/step: 1.87
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2197
Mean episode consecutive_successes: 0.2313
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 8.08s
                        Total time: 9226.51s
                               ETA: 1063634.4s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.012s, learning 0.254s)
               Value function loss: 4363.0566
                    Surrogate loss: -0.0150
             Mean action noise std: 0.79
                       Mean reward: 43.33
               Mean episode length: 40.76
                  Mean reward/step: 1.63
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1978
Mean episode consecutive_successes: 0.2391
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 8.27s
                        Total time: 9234.78s
                               ETA: 1063340.1s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.104s, learning 0.178s)
               Value function loss: 4388.7834
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 81.40
               Mean episode length: 41.02
                  Mean reward/step: 1.77
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1934
Mean episode consecutive_successes: 0.2415
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 8.28s
                        Total time: 9243.06s
                               ETA: 1063048.4s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.098s, learning 0.160s)
               Value function loss: 4539.4427
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 61.18
               Mean episode length: 40.80
                  Mean reward/step: 1.76
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2090
Mean episode consecutive_successes: 0.2336
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 8.26s
                        Total time: 9251.32s
                               ETA: 1062754.5s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 2097 steps/s (collection: 7.651s, learning 0.159s)
               Value function loss: 4280.8298
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 59.70
               Mean episode length: 42.48
                  Mean reward/step: 1.62
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2007
Mean episode consecutive_successes: 0.2366
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 7.81s
                        Total time: 9259.13s
                               ETA: 1062409.9s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.168s, learning 0.162s)
               Value function loss: 3392.8262
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 126.01
               Mean episode length: 40.71
                  Mean reward/step: 1.50
       Mean episode length/episode: 6.76
            Mean episode successes: 0.1914
Mean episode consecutive_successes: 0.2441
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 8.33s
                        Total time: 9267.46s
                               ETA: 1062125.6s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.104s, learning 0.166s)
               Value function loss: 3811.9424
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 25.98
               Mean episode length: 41.06
                  Mean reward/step: 1.59
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1714
Mean episode consecutive_successes: 0.2421
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 8.27s
                        Total time: 9275.73s
                               ETA: 1061835.1s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.215s, learning 0.206s)
               Value function loss: 4744.6845
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 35.76
               Mean episode length: 40.38
                  Mean reward/step: 2.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1934
Mean episode consecutive_successes: 0.2378
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 8.42s
                        Total time: 9284.15s
                               ETA: 1061562.5s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.759s, learning 0.193s)
               Value function loss: 2156.9183
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 77.85
               Mean episode length: 40.17
                  Mean reward/step: 1.22
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1509
Mean episode consecutive_successes: 0.2475
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 7.95s
                        Total time: 9292.10s
                               ETA: 1061237.0s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.204s, learning 0.160s)
               Value function loss: 3241.0611
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 70.51
               Mean episode length: 40.19
                  Mean reward/step: 1.47
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1284
Mean episode consecutive_successes: 0.2514
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 8.36s
                        Total time: 9300.46s
                               ETA: 1060959.2s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.339s, learning 0.165s)
               Value function loss: 4099.0210
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 49.56
               Mean episode length: 42.05
                  Mean reward/step: 1.77
       Mean episode length/episode: 6.83
            Mean episode successes: 0.1431
Mean episode consecutive_successes: 0.2470
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 8.50s
                        Total time: 9308.97s
                               ETA: 1060698.1s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.307s, learning 0.162s)
               Value function loss: 2718.7354
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 51.10
               Mean episode length: 41.16
                  Mean reward/step: 1.27
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1182
Mean episode consecutive_successes: 0.2508
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 8.47s
                        Total time: 9317.44s
                               ETA: 1060433.4s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.109s, learning 0.216s)
               Value function loss: 2200.4194
                    Surrogate loss: -0.0165
             Mean action noise std: 0.79
                       Mean reward: 37.27
               Mean episode length: 43.07
                  Mean reward/step: 1.18
       Mean episode length/episode: 6.92
            Mean episode successes: 0.1235
Mean episode consecutive_successes: 0.2410
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 8.32s
                        Total time: 9325.76s
                               ETA: 1060153.0s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.291s, learning 0.174s)
               Value function loss: 2831.8123
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 107.96
               Mean episode length: 43.48
                  Mean reward/step: 1.51
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1265
Mean episode consecutive_successes: 0.2393
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 8.47s
                        Total time: 9334.23s
                               ETA: 1059889.2s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.094s, learning 0.178s)
               Value function loss: 3963.2652
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 44.61
               Mean episode length: 42.81
                  Mean reward/step: 1.75
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1416
Mean episode consecutive_successes: 0.2313
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 8.27s
                        Total time: 9342.50s
                               ETA: 1059604.1s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.326s, learning 0.173s)
               Value function loss: 4801.2937
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 110.07
               Mean episode length: 43.17
                  Mean reward/step: 1.84
       Mean episode length/episode: 6.78
            Mean episode successes: 0.1636
Mean episode consecutive_successes: 0.2305
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 8.50s
                        Total time: 9351.00s
                               ETA: 1059345.2s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.089s, learning 0.162s)
               Value function loss: 3267.6217
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 21.53
               Mean episode length: 41.49
                  Mean reward/step: 1.49
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1621
Mean episode consecutive_successes: 0.2227
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 8.25s
                        Total time: 9359.25s
                               ETA: 1059058.9s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.047s, learning 0.163s)
               Value function loss: 3817.0278
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 26.90
               Mean episode length: 41.82
                  Mean reward/step: 1.69
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1758
Mean episode consecutive_successes: 0.2190
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 8.21s
                        Total time: 9367.46s
                               ETA: 1058768.6s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.080s, learning 0.159s)
               Value function loss: 5305.7881
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 48.99
               Mean episode length: 41.50
                  Mean reward/step: 2.20
       Mean episode length/episode: 6.83
            Mean episode successes: 0.2070
Mean episode consecutive_successes: 0.2143
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 8.24s
                        Total time: 9375.70s
                               ETA: 1058482.2s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.054s, learning 0.158s)
               Value function loss: 4132.9456
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 30.20
               Mean episode length: 38.82
                  Mean reward/step: 1.80
       Mean episode length/episode: 6.73
            Mean episode successes: 0.2075
Mean episode consecutive_successes: 0.2139
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 8.21s
                        Total time: 9383.91s
                               ETA: 1058193.4s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.937s, learning 0.187s)
               Value function loss: 3934.7657
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 44.04
               Mean episode length: 40.59
                  Mean reward/step: 1.84
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2070
Mean episode consecutive_successes: 0.2182
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 8.12s
                        Total time: 9392.03s
                               ETA: 1057895.3s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.027s, learning 0.211s)
               Value function loss: 3607.4228
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 86.60
               Mean episode length: 41.27
                  Mean reward/step: 1.56
       Mean episode length/episode: 6.94
            Mean episode successes: 0.1831
Mean episode consecutive_successes: 0.2294
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 8.24s
                        Total time: 9400.27s
                               ETA: 1057610.7s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.161s)
               Value function loss: 3748.8230
                    Surrogate loss: -0.0174
             Mean action noise std: 0.79
                       Mean reward: 115.70
               Mean episode length: 39.83
                  Mean reward/step: 1.72
       Mean episode length/episode: 6.87
            Mean episode successes: 0.1987
Mean episode consecutive_successes: 0.2289
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 8.43s
                        Total time: 9408.70s
                               ETA: 1057347.9s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 2089 steps/s (collection: 7.601s, learning 0.238s)
               Value function loss: 5815.8549
                    Surrogate loss: -0.0158
             Mean action noise std: 0.79
                       Mean reward: 123.82
               Mean episode length: 41.63
                  Mean reward/step: 2.33
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1973
Mean episode consecutive_successes: 0.2369
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 7.84s
                        Total time: 9416.54s
                               ETA: 1057019.8s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.608s, learning 0.177s)
               Value function loss: 3523.7734
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 58.13
               Mean episode length: 40.15
                  Mean reward/step: 1.74
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2046
Mean episode consecutive_successes: 0.2329
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 8.79s
                        Total time: 9425.32s
                               ETA: 1056798.5s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.097s, learning 0.179s)
               Value function loss: 3396.4416
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 89.27
               Mean episode length: 41.46
                  Mean reward/step: 1.55
       Mean episode length/episode: 6.77
            Mean episode successes: 0.1885
Mean episode consecutive_successes: 0.2351
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 8.28s
                        Total time: 9433.60s
                               ETA: 1056520.6s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.309s, learning 0.170s)
               Value function loss: 4515.8924
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 106.82
               Mean episode length: 41.56
                  Mean reward/step: 1.83
       Mean episode length/episode: 6.85
            Mean episode successes: 0.1924
Mean episode consecutive_successes: 0.2356
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 8.48s
                        Total time: 9442.08s
                               ETA: 1056266.0s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.113s, learning 0.185s)
               Value function loss: 5139.4140
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 115.29
               Mean episode length: 43.68
                  Mean reward/step: 2.24
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2119
Mean episode consecutive_successes: 0.2389
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 8.30s
                        Total time: 9450.38s
                               ETA: 1055991.8s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.222s, learning 0.162s)
               Value function loss: 4069.0341
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 71.51
               Mean episode length: 41.34
                  Mean reward/step: 1.60
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1846
Mean episode consecutive_successes: 0.2408
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 8.38s
                        Total time: 9458.76s
                               ETA: 1055727.8s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.212s, learning 0.161s)
               Value function loss: 4603.2639
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 85.56
               Mean episode length: 39.59
                  Mean reward/step: 1.90
       Mean episode length/episode: 6.74
            Mean episode successes: 0.1948
Mean episode consecutive_successes: 0.2412
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 8.37s
                        Total time: 9467.14s
                               ETA: 1055463.2s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.096s, learning 0.218s)
               Value function loss: 3558.0653
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 100.74
               Mean episode length: 40.01
                  Mean reward/step: 1.63
       Mean episode length/episode: 6.92
            Mean episode successes: 0.2051
Mean episode consecutive_successes: 0.2360
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 16.31s
                        Total time: 9483.45s
                               ETA: 1056083.4s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.226s, learning 0.161s)
               Value function loss: 2681.9930
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 132.90
               Mean episode length: 38.85
                  Mean reward/step: 1.25
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1704
Mean episode consecutive_successes: 0.2431
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 16.39s
                        Total time: 9499.84s
                               ETA: 1056710.3s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.152s, learning 0.165s)
               Value function loss: 4578.1230
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 58.39
               Mean episode length: 39.95
                  Mean reward/step: 1.87
       Mean episode length/episode: 6.71
            Mean episode successes: 0.1592
Mean episode consecutive_successes: 0.2461
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 16.32s
                        Total time: 9516.15s
                               ETA: 1057327.9s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1028 steps/s (collection: 15.761s, learning 0.171s)
               Value function loss: 3499.3125
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 37.76
               Mean episode length: 39.52
                  Mean reward/step: 1.81
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1655
Mean episode consecutive_successes: 0.2431
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 15.93s
                        Total time: 9532.09s
                               ETA: 1057901.4s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1051 steps/s (collection: 15.429s, learning 0.159s)
               Value function loss: 2521.7453
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 125.93
               Mean episode length: 39.92
                  Mean reward/step: 1.18
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1504
Mean episode consecutive_successes: 0.2445
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 15.59s
                        Total time: 9547.67s
                               ETA: 1058435.4s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1022 steps/s (collection: 15.846s, learning 0.174s)
               Value function loss: 2986.8170
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 41.05
               Mean episode length: 35.95
                  Mean reward/step: 1.53
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1187
Mean episode consecutive_successes: 0.2505
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 16.02s
                        Total time: 9563.69s
                               ETA: 1059016.1s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1024 steps/s (collection: 15.827s, learning 0.164s)
               Value function loss: 3673.3135
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 79.62
               Mean episode length: 37.40
                  Mean reward/step: 1.61
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1362
Mean episode consecutive_successes: 0.2441
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 15.99s
                        Total time: 9579.69s
                               ETA: 1059592.3s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.036s, learning 0.236s)
               Value function loss: 4412.3271
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 57.87
               Mean episode length: 40.11
                  Mean reward/step: 1.84
       Mean episode length/episode: 6.84
            Mean episode successes: 0.1494
Mean episode consecutive_successes: 0.2373
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 16.27s
                        Total time: 9595.96s
                               ETA: 1060198.2s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.011s, learning 0.282s)
               Value function loss: 3949.1855
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 20.21
               Mean episode length: 38.76
                  Mean reward/step: 1.58
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1680
Mean episode consecutive_successes: 0.2290
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 16.29s
                        Total time: 9612.25s
                               ETA: 1060805.0s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.520s, learning 0.166s)
               Value function loss: 5401.2048
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 27.04
               Mean episode length: 38.03
                  Mean reward/step: 2.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.1948
Mean episode consecutive_successes: 0.2227
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 16.69s
                        Total time: 9628.94s
                               ETA: 1061453.6s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.350s, learning 0.161s)
               Value function loss: 6623.8492
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 67.41
               Mean episode length: 38.47
                  Mean reward/step: 2.45
       Mean episode length/episode: 6.88
            Mean episode successes: 0.2285
Mean episode consecutive_successes: 0.2219
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 16.51s
                        Total time: 9645.45s
                               ETA: 1062081.6s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.225s, learning 0.176s)
               Value function loss: 4565.9772
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 80.44
               Mean episode length: 39.35
                  Mean reward/step: 2.10
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2461
Mean episode consecutive_successes: 0.2238
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 16.40s
                        Total time: 9661.85s
                               ETA: 1062696.0s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.882s, learning 0.168s)
               Value function loss: 4650.0265
                    Surrogate loss: -0.0159
             Mean action noise std: 0.79
                       Mean reward: 25.23
               Mean episode length: 39.03
                  Mean reward/step: 2.20
       Mean episode length/episode: 6.79
            Mean episode successes: 0.2246
Mean episode consecutive_successes: 0.2347
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 16.05s
                        Total time: 9677.90s
                               ETA: 1063270.5s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.042s, learning 0.160s)
               Value function loss: 4636.0480
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: 66.28
               Mean episode length: 40.93
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2427
Mean episode consecutive_successes: 0.2311
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 16.20s
                        Total time: 9694.10s
                               ETA: 1063860.2s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.398s, learning 0.167s)
               Value function loss: 5142.0293
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 60.40
               Mean episode length: 39.26
                  Mean reward/step: 2.16
       Mean episode length/episode: 6.79
            Mean episode successes: 0.2329
Mean episode consecutive_successes: 0.2389
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 16.57s
                        Total time: 9710.66s
                               ETA: 1064488.6s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1032 steps/s (collection: 15.711s, learning 0.159s)
               Value function loss: 5159.9057
                    Surrogate loss: -0.0168
             Mean action noise std: 0.79
                       Mean reward: 120.69
               Mean episode length: 39.65
                  Mean reward/step: 2.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2373
Mean episode consecutive_successes: 0.2438
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 15.87s
                        Total time: 9726.53s
                               ETA: 1065039.5s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.264s, learning 0.160s)
               Value function loss: 5739.0426
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 26.50
               Mean episode length: 40.95
                  Mean reward/step: 2.19
       Mean episode length/episode: 6.75
            Mean episode successes: 0.2168
Mean episode consecutive_successes: 0.2510
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 16.42s
                        Total time: 9742.96s
                               ETA: 1065649.5s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.146s, learning 0.170s)
               Value function loss: 4164.0925
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 84.31
               Mean episode length: 41.43
                  Mean reward/step: 1.73
       Mean episode length/episode: 6.75
            Mean episode successes: 0.1914
Mean episode consecutive_successes: 0.2585
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 16.32s
                        Total time: 9759.28s
                               ETA: 1066246.6s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.757s, learning 0.206s)
               Value function loss: 3892.1383
                    Surrogate loss: -0.0168
             Mean action noise std: 0.79
                       Mean reward: 158.05
               Mean episode length: 39.05
                  Mean reward/step: 1.64
       Mean episode length/episode: 6.82
            Mean episode successes: 0.1973
Mean episode consecutive_successes: 0.2603
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 15.96s
                        Total time: 9775.24s
                               ETA: 1066803.6s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.027s, learning 0.185s)
               Value function loss: 2599.9178
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 74.96
               Mean episode length: 38.17
                  Mean reward/step: 1.46
       Mean episode length/episode: 6.79
            Mean episode successes: 0.1792
Mean episode consecutive_successes: 0.2575
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 16.21s
                        Total time: 9791.45s
                               ETA: 1067386.5s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1042 steps/s (collection: 15.546s, learning 0.177s)
               Value function loss: 4465.2379
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 68.13
               Mean episode length: 39.30
                  Mean reward/step: 2.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.1992
Mean episode consecutive_successes: 0.2519
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 15.72s
                        Total time: 9807.17s
                               ETA: 1067914.9s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.881s, learning 0.176s)
               Value function loss: 4510.4681
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 75.03
               Mean episode length: 38.51
                  Mean reward/step: 2.17
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2080
Mean episode consecutive_successes: 0.2494
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 16.06s
                        Total time: 9823.23s
                               ETA: 1068478.4s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1062 steps/s (collection: 15.253s, learning 0.171s)
               Value function loss: 6591.3378
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 44.97
               Mean episode length: 38.85
                  Mean reward/step: 2.67
       Mean episode length/episode: 6.77
            Mean episode successes: 0.2061
Mean episode consecutive_successes: 0.2575
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 15.42s
                        Total time: 9838.65s
                               ETA: 1068971.9s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1032 steps/s (collection: 15.685s, learning 0.181s)
               Value function loss: 5340.7553
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 32.22
               Mean episode length: 37.97
                  Mean reward/step: 2.33
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2271
Mean episode consecutive_successes: 0.2563
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 15.87s
                        Total time: 9854.52s
                               ETA: 1069512.2s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.882s, learning 0.267s)
               Value function loss: 7549.4516
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 54.88
               Mean episode length: 38.14
                  Mean reward/step: 2.85
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2642
Mean episode consecutive_successes: 0.2562
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 16.15s
                        Total time: 9870.67s
                               ETA: 1070082.1s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.294s, learning 0.164s)
               Value function loss: 6914.1291
                    Surrogate loss: -0.0174
             Mean action noise std: 0.79
                       Mean reward: 64.92
               Mean episode length: 38.02
                  Mean reward/step: 2.60
       Mean episode length/episode: 6.77
            Mean episode successes: 0.2856
Mean episode consecutive_successes: 0.2534
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 16.46s
                        Total time: 9887.13s
                               ETA: 1070684.0s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.031s, learning 0.232s)
               Value function loss: 5157.6708
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 108.13
               Mean episode length: 39.22
                  Mean reward/step: 2.20
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2549
Mean episode consecutive_successes: 0.2692
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 16.26s
                        Total time: 9903.39s
                               ETA: 1071263.5s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.258s, learning 0.260s)
               Value function loss: 6196.2733
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 161.12
               Mean episode length: 39.94
                  Mean reward/step: 2.38
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2559
Mean episode consecutive_successes: 0.2824
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 16.52s
                        Total time: 9919.91s
                               ETA: 1071869.3s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.800s, learning 0.164s)
               Value function loss: 5240.9077
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 77.25
               Mean episode length: 38.35
                  Mean reward/step: 2.51
       Mean episode length/episode: 6.83
            Mean episode successes: 0.2529
Mean episode consecutive_successes: 0.2824
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 15.96s
                        Total time: 9935.87s
                               ETA: 1072413.9s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.064s, learning 0.167s)
               Value function loss: 5199.9345
                    Surrogate loss: -0.0191
             Mean action noise std: 0.79
                       Mean reward: 115.95
               Mean episode length: 40.41
                  Mean reward/step: 2.11
       Mean episode length/episode: 6.83
            Mean episode successes: 0.2441
Mean episode consecutive_successes: 0.2926
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 16.23s
                        Total time: 9952.10s
                               ETA: 1072986.0s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.005s, learning 0.210s)
               Value function loss: 6374.0978
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 219.38
               Mean episode length: 40.86
                  Mean reward/step: 2.31
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2593
Mean episode consecutive_successes: 0.2941
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 16.21s
                        Total time: 9968.32s
                               ETA: 1073555.2s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.926s, learning 0.247s)
               Value function loss: 6425.1217
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 123.65
               Mean episode length: 40.35
                  Mean reward/step: 2.50
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2554
Mean episode consecutive_successes: 0.2969
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 16.17s
                        Total time: 9984.49s
                               ETA: 1074118.7s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.006s, learning 0.217s)
               Value function loss: 6998.3180
                    Surrogate loss: -0.0192
             Mean action noise std: 0.79
                       Mean reward: 176.57
               Mean episode length: 41.07
                  Mean reward/step: 2.52
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2388
Mean episode consecutive_successes: 0.3051
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 16.22s
                        Total time: 10000.71s
                               ETA: 1074686.1s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.796s, learning 0.161s)
               Value function loss: 7085.0782
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 99.00
               Mean episode length: 40.75
                  Mean reward/step: 2.77
       Mean episode length/episode: 6.90
            Mean episode successes: 0.2612
Mean episode consecutive_successes: 0.3058
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 15.96s
                        Total time: 10016.67s
                               ETA: 1075223.8s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.168s, learning 0.170s)
               Value function loss: 5850.1528
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 169.36
               Mean episode length: 41.52
                  Mean reward/step: 2.40
       Mean episode length/episode: 6.75
            Mean episode successes: 0.2529
Mean episode consecutive_successes: 0.3130
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 16.34s
                        Total time: 10033.01s
                               ETA: 1075801.2s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.606s, learning 0.202s)
               Value function loss: 7338.7865
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 68.89
               Mean episode length: 41.46
                  Mean reward/step: 2.78
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2627
Mean episode consecutive_successes: 0.3128
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 15.81s
                        Total time: 10048.82s
                               ETA: 1076320.4s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1022 steps/s (collection: 15.860s, learning 0.171s)
               Value function loss: 6679.7881
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 50.36
               Mean episode length: 39.43
                  Mean reward/step: 2.65
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2749
Mean episode consecutive_successes: 0.3112
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 16.03s
                        Total time: 10064.85s
                               ETA: 1076862.5s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1237 steps/s (collection: 13.058s, learning 0.183s)
               Value function loss: 6336.4359
                    Surrogate loss: -0.0174
             Mean action noise std: 0.79
                       Mean reward: 35.56
               Mean episode length: 40.32
                  Mean reward/step: 2.23
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2754
Mean episode consecutive_successes: 0.3118
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 13.24s
                        Total time: 10078.09s
                               ETA: 1077105.1s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.323s, learning 0.182s)
               Value function loss: 6545.3700
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 80.42
               Mean episode length: 39.22
                  Mean reward/step: 2.55
       Mean episode length/episode: 6.74
            Mean episode successes: 0.2764
Mean episode consecutive_successes: 0.3139
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 8.50s
                        Total time: 10086.59s
                               ETA: 1076841.6s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.099s, learning 0.176s)
               Value function loss: 7954.2295
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 172.79
               Mean episode length: 42.62
                  Mean reward/step: 2.93
       Mean episode length/episode: 6.75
            Mean episode successes: 0.3130
Mean episode consecutive_successes: 0.3191
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 8.27s
                        Total time: 10094.87s
                               ETA: 1076554.0s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.371s, learning 0.186s)
               Value function loss: 5846.1563
                    Surrogate loss: -0.0207
             Mean action noise std: 0.79
                       Mean reward: 127.20
               Mean episode length: 37.98
                  Mean reward/step: 2.31
       Mean episode length/episode: 6.93
            Mean episode successes: 0.3062
Mean episode consecutive_successes: 0.3191
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 8.56s
                        Total time: 10103.42s
                               ETA: 1076297.2s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.965s, learning 0.165s)
               Value function loss: 6536.5401
                    Surrogate loss: -0.0184
             Mean action noise std: 0.79
                       Mean reward: 188.87
               Mean episode length: 40.64
                  Mean reward/step: 2.68
       Mean episode length/episode: 6.88
            Mean episode successes: 0.2881
Mean episode consecutive_successes: 0.3318
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 8.13s
                        Total time: 10111.56s
                               ETA: 1075995.5s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 2068 steps/s (collection: 7.734s, learning 0.188s)
               Value function loss: 7819.1518
                    Surrogate loss: -0.0194
             Mean action noise std: 0.79
                       Mean reward: 144.35
               Mean episode length: 42.14
                  Mean reward/step: 2.94
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3354
Mean episode consecutive_successes: 0.3234
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 7.92s
                        Total time: 10119.48s
                               ETA: 1075672.1s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.310s, learning 0.268s)
               Value function loss: 6780.1621
                    Surrogate loss: -0.0187
             Mean action noise std: 0.79
                       Mean reward: 201.32
               Mean episode length: 40.74
                  Mean reward/step: 2.64
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2896
Mean episode consecutive_successes: 0.3495
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 8.58s
                        Total time: 10128.06s
                               ETA: 1075419.3s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.155s, learning 0.159s)
               Value function loss: 6138.5056
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 32.62
               Mean episode length: 38.55
                  Mean reward/step: 2.42
       Mean episode length/episode: 6.72
            Mean episode successes: 0.2612
Mean episode consecutive_successes: 0.3485
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 8.31s
                        Total time: 10136.37s
                               ETA: 1075138.9s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.090s, learning 0.162s)
               Value function loss: 7496.8525
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 106.20
               Mean episode length: 40.02
                  Mean reward/step: 2.63
       Mean episode length/episode: 6.85
            Mean episode successes: 0.2803
Mean episode consecutive_successes: 0.3516
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 8.25s
                        Total time: 10144.62s
                               ETA: 1074852.5s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.141s, learning 0.158s)
               Value function loss: 5336.1984
                    Surrogate loss: -0.0165
             Mean action noise std: 0.79
                       Mean reward: 107.69
               Mean episode length: 41.65
                  Mean reward/step: 2.39
       Mean episode length/episode: 6.79
            Mean episode successes: 0.2290
Mean episode consecutive_successes: 0.3659
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 8.30s
                        Total time: 10152.92s
                               ETA: 1074571.7s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.255s, learning 0.170s)
               Value function loss: 4739.6253
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 94.97
               Mean episode length: 38.29
                  Mean reward/step: 2.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2275
Mean episode consecutive_successes: 0.3607
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 8.42s
                        Total time: 10161.35s
                               ETA: 1074304.7s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.836s, learning 0.169s)
               Value function loss: 6078.2311
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 67.22
               Mean episode length: 42.26
                  Mean reward/step: 2.17
       Mean episode length/episode: 6.75
            Mean episode successes: 0.2261
Mean episode consecutive_successes: 0.3547
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 8.01s
                        Total time: 10169.35s
                               ETA: 1073994.0s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.027s, learning 0.170s)
               Value function loss: 8308.1625
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 56.28
               Mean episode length: 36.14
                  Mean reward/step: 2.71
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2461
Mean episode consecutive_successes: 0.3504
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 8.20s
                        Total time: 10177.55s
                               ETA: 1073704.2s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.068s, learning 0.158s)
               Value function loss: 5763.8086
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 108.28
               Mean episode length: 39.87
                  Mean reward/step: 2.45
       Mean episode length/episode: 6.85
            Mean episode successes: 0.2671
Mean episode consecutive_successes: 0.3446
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 8.23s
                        Total time: 10185.77s
                               ETA: 1073418.1s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.923s, learning 0.159s)
               Value function loss: 5396.7584
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 146.71
               Mean episode length: 40.58
                  Mean reward/step: 2.16
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2485
Mean episode consecutive_successes: 0.3467
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 8.08s
                        Total time: 10193.86s
                               ETA: 1073117.3s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.167s, learning 0.272s)
               Value function loss: 6234.2382
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 87.44
               Mean episode length: 42.60
                  Mean reward/step: 2.10
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2603
Mean episode consecutive_successes: 0.3367
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 8.44s
                        Total time: 10202.30s
                               ETA: 1072854.8s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1979 steps/s (collection: 7.986s, learning 0.290s)
               Value function loss: 5588.2180
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 35.72
               Mean episode length: 39.19
                  Mean reward/step: 2.24
       Mean episode length/episode: 6.83
            Mean episode successes: 0.2300
Mean episode consecutive_successes: 0.3464
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 8.28s
                        Total time: 10210.57s
                               ETA: 1072575.6s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.283s, learning 0.158s)
               Value function loss: 6050.1864
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 79.86
               Mean episode length: 37.88
                  Mean reward/step: 2.31
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2344
Mean episode consecutive_successes: 0.3470
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 8.44s
                        Total time: 10219.01s
                               ETA: 1072314.3s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.247s, learning 0.164s)
               Value function loss: 5158.0452
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 187.17
               Mean episode length: 42.57
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.2241
Mean episode consecutive_successes: 0.3495
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 8.41s
                        Total time: 10227.42s
                               ETA: 1072050.5s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.484s, learning 0.158s)
               Value function loss: 7146.7075
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 80.67
               Mean episode length: 39.67
                  Mean reward/step: 2.39
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2446
Mean episode consecutive_successes: 0.3378
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 8.64s
                        Total time: 10236.07s
                               ETA: 1071811.4s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.883s, learning 0.164s)
               Value function loss: 8014.5067
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 160.35
               Mean episode length: 42.43
                  Mean reward/step: 2.88
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2534
Mean episode consecutive_successes: 0.3402
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 8.05s
                        Total time: 10244.11s
                               ETA: 1071510.4s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 2030 steps/s (collection: 7.881s, learning 0.190s)
               Value function loss: 7920.2809
                    Surrogate loss: -0.0180
             Mean action noise std: 0.79
                       Mean reward: 55.17
               Mean episode length: 38.80
                  Mean reward/step: 3.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2856
Mean episode consecutive_successes: 0.3386
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 8.07s
                        Total time: 10252.18s
                               ETA: 1071212.6s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.864s, learning 0.237s)
               Value function loss: 6983.5266
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 169.48
               Mean episode length: 41.82
                  Mean reward/step: 2.52
       Mean episode length/episode: 6.85
            Mean episode successes: 0.2896
Mean episode consecutive_successes: 0.3397
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 8.10s
                        Total time: 10260.28s
                               ETA: 1070918.5s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.052s, learning 0.238s)
               Value function loss: 5502.1982
                    Surrogate loss: -0.0207
             Mean action noise std: 0.79
                       Mean reward: 162.57
               Mean episode length: 43.04
                  Mean reward/step: 2.28
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2788
Mean episode consecutive_successes: 0.3484
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 8.29s
                        Total time: 10268.57s
                               ETA: 1070644.8s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.356s, learning 0.167s)
               Value function loss: 4875.0344
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 161.36
               Mean episode length: 45.53
                  Mean reward/step: 1.74
       Mean episode length/episode: 6.89
            Mean episode successes: 0.2505
Mean episode consecutive_successes: 0.3488
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 8.52s
                        Total time: 10277.10s
                               ETA: 1070395.9s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.585s, learning 0.230s)
               Value function loss: 7043.6758
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 66.52
               Mean episode length: 41.42
                  Mean reward/step: 2.45
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2373
Mean episode consecutive_successes: 0.3543
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 8.81s
                        Total time: 10285.91s
                               ETA: 1070177.8s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.351s, learning 0.175s)
               Value function loss: 5516.6441
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 69.14
               Mean episode length: 41.15
                  Mean reward/step: 2.17
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2329
Mean episode consecutive_successes: 0.3529
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 8.53s
                        Total time: 10294.44s
                               ETA: 1069930.1s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.031s, learning 0.186s)
               Value function loss: 5106.2641
                    Surrogate loss: -0.0180
             Mean action noise std: 0.79
                       Mean reward: 147.03
               Mean episode length: 41.85
                  Mean reward/step: 2.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2134
Mean episode consecutive_successes: 0.3619
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 8.22s
                        Total time: 10302.65s
                               ETA: 1069651.0s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.146s, learning 0.188s)
               Value function loss: 5084.6682
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 121.94
               Mean episode length: 42.21
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.91
            Mean episode successes: 0.2075
Mean episode consecutive_successes: 0.3549
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 8.33s
                        Total time: 10310.99s
                               ETA: 1069384.5s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.771s, learning 0.192s)
               Value function loss: 7396.3765
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 76.61
               Mean episode length: 41.48
                  Mean reward/step: 2.63
       Mean episode length/episode: 6.91
            Mean episode successes: 0.2510
Mean episode consecutive_successes: 0.3412
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 7.96s
                        Total time: 10318.95s
                               ETA: 1069080.1s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.035s, learning 0.186s)
               Value function loss: 7830.8708
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 133.10
               Mean episode length: 39.32
                  Mean reward/step: 2.94
       Mean episode length/episode: 6.75
            Mean episode successes: 0.2930
Mean episode consecutive_successes: 0.3327
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 8.22s
                        Total time: 10327.17s
                               ETA: 1068803.0s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.949s, learning 0.159s)
               Value function loss: 8532.0950
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 104.42
               Mean episode length: 41.45
                  Mean reward/step: 3.12
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2959
Mean episode consecutive_successes: 0.3383
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 8.11s
                        Total time: 10335.28s
                               ETA: 1068514.9s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.298s, learning 0.163s)
               Value function loss: 6936.0959
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 111.46
               Mean episode length: 40.84
                  Mean reward/step: 2.72
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3276
Mean episode consecutive_successes: 0.3354
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 8.46s
                        Total time: 10343.74s
                               ETA: 1068263.7s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.996s, learning 0.163s)
               Value function loss: 6926.3705
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 123.14
               Mean episode length: 39.48
                  Mean reward/step: 2.52
       Mean episode length/episode: 6.89
            Mean episode successes: 0.2905
Mean episode consecutive_successes: 0.3535
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 8.16s
                        Total time: 10351.90s
                               ETA: 1067982.0s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.006s, learning 0.167s)
               Value function loss: 7035.9361
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 65.19
               Mean episode length: 42.21
                  Mean reward/step: 2.52
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3091
Mean episode consecutive_successes: 0.3476
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 8.17s
                        Total time: 10360.08s
                               ETA: 1067702.2s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.945s, learning 0.185s)
               Value function loss: 7171.7055
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 171.91
               Mean episode length: 41.23
                  Mean reward/step: 2.46
       Mean episode length/episode: 6.72
            Mean episode successes: 0.2832
Mean episode consecutive_successes: 0.3544
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 8.13s
                        Total time: 10368.21s
                               ETA: 1067418.6s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.209s, learning 0.210s)
               Value function loss: 5706.9776
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 126.98
               Mean episode length: 41.67
                  Mean reward/step: 2.29
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2715
Mean episode consecutive_successes: 0.3564
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 8.42s
                        Total time: 10376.62s
                               ETA: 1067165.2s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.296s, learning 0.162s)
               Value function loss: 5278.7563
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 144.10
               Mean episode length: 41.31
                  Mean reward/step: 2.16
       Mean episode length/episode: 6.89
            Mean episode successes: 0.2524
Mean episode consecutive_successes: 0.3616
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 8.46s
                        Total time: 10385.08s
                               ETA: 1066916.3s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 2093 steps/s (collection: 7.665s, learning 0.162s)
               Value function loss: 7891.2026
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: 136.13
               Mean episode length: 40.74
                  Mean reward/step: 2.63
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2607
Mean episode consecutive_successes: 0.3632
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 7.83s
                        Total time: 10392.91s
                               ETA: 1066603.3s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 2051 steps/s (collection: 7.821s, learning 0.167s)
               Value function loss: 7438.8131
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 75.91
               Mean episode length: 40.03
                  Mean reward/step: 2.91
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2876
Mean episode consecutive_successes: 0.3541
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 7.99s
                        Total time: 10400.90s
                               ETA: 1066307.4s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.952s, learning 0.175s)
               Value function loss: 7105.6691
                    Surrogate loss: -0.0207
             Mean action noise std: 0.79
                       Mean reward: 105.00
               Mean episode length: 38.13
                  Mean reward/step: 2.90
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3247
Mean episode consecutive_successes: 0.3487
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 8.13s
                        Total time: 10409.02s
                               ETA: 1066026.2s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.236s, learning 0.171s)
               Value function loss: 6948.6235
                    Surrogate loss: -0.0195
             Mean action noise std: 0.79
                       Mean reward: 169.09
               Mean episode length: 41.62
                  Mean reward/step: 2.93
       Mean episode length/episode: 6.89
            Mean episode successes: 0.3311
Mean episode consecutive_successes: 0.3530
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 8.41s
                        Total time: 10417.43s
                               ETA: 1065774.3s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.256s, learning 0.171s)
               Value function loss: 8278.8837
                    Surrogate loss: -0.0149
             Mean action noise std: 0.79
                       Mean reward: 80.90
               Mean episode length: 40.05
                  Mean reward/step: 3.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3315
Mean episode consecutive_successes: 0.3574
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 8.43s
                        Total time: 10425.86s
                               ETA: 1065525.0s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.074s, learning 0.194s)
               Value function loss: 7265.4724
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 114.85
               Mean episode length: 42.28
                  Mean reward/step: 2.59
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3267
Mean episode consecutive_successes: 0.3587
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 8.27s
                        Total time: 10434.13s
                               ETA: 1065259.8s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.794s, learning 0.237s)
               Value function loss: 9126.7215
                    Surrogate loss: -0.0182
             Mean action noise std: 0.79
                       Mean reward: 250.45
               Mean episode length: 43.30
                  Mean reward/step: 3.40
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3438
Mean episode consecutive_successes: 0.3785
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 8.03s
                        Total time: 10442.16s
                               ETA: 1064971.0s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.100s, learning 0.163s)
               Value function loss: 7599.6491
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 54.34
               Mean episode length: 41.56
                  Mean reward/step: 3.18
       Mean episode length/episode: 6.75
            Mean episode successes: 0.3242
Mean episode consecutive_successes: 0.3805
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 8.26s
                        Total time: 10450.42s
                               ETA: 1064706.4s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.262s, learning 0.160s)
               Value function loss: 7989.1155
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 196.41
               Mean episode length: 40.85
                  Mean reward/step: 3.01
       Mean episode length/episode: 6.90
            Mean episode successes: 0.3711
Mean episode consecutive_successes: 0.3768
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 8.42s
                        Total time: 10458.84s
                               ETA: 1064458.6s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.449s, learning 0.161s)
               Value function loss: 7689.5501
                    Surrogate loss: -0.0199
             Mean action noise std: 0.79
                       Mean reward: 180.41
               Mean episode length: 43.51
                  Mean reward/step: 3.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3896
Mean episode consecutive_successes: 0.3760
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 8.61s
                        Total time: 10467.45s
                               ETA: 1064230.4s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.307s, learning 0.246s)
               Value function loss: 7167.0173
                    Surrogate loss: -0.0199
             Mean action noise std: 0.79
                       Mean reward: 143.94
               Mean episode length: 40.92
                  Mean reward/step: 2.67
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3340
Mean episode consecutive_successes: 0.3886
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 8.55s
                        Total time: 10476.01s
                               ETA: 1063996.9s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.271s, learning 0.175s)
               Value function loss: 9169.7114
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 233.02
               Mean episode length: 43.23
                  Mean reward/step: 3.26
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3433
Mean episode consecutive_successes: 0.4019
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 8.45s
                        Total time: 10484.45s
                               ETA: 1063752.9s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.962s, learning 0.178s)
               Value function loss: 10456.8386
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 56.04
               Mean episode length: 40.31
                  Mean reward/step: 3.73
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3389
Mean episode consecutive_successes: 0.4053
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 8.14s
                        Total time: 10492.59s
                               ETA: 1063478.3s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.313s, learning 0.155s)
               Value function loss: 6919.7083
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 104.60
               Mean episode length: 41.54
                  Mean reward/step: 2.43
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3418
Mean episode consecutive_successes: 0.4040
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 8.47s
                        Total time: 10501.06s
                               ETA: 1063237.5s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.967s, learning 0.186s)
               Value function loss: 7619.5273
                    Surrogate loss: -0.0144
             Mean action noise std: 0.79
                       Mean reward: 80.08
               Mean episode length: 43.09
                  Mean reward/step: 2.93
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3506
Mean episode consecutive_successes: 0.3989
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 8.15s
                        Total time: 10509.21s
                               ETA: 1062965.5s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.160s, learning 0.185s)
               Value function loss: 6945.1378
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 154.62
               Mean episode length: 42.35
                  Mean reward/step: 2.62
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3555
Mean episode consecutive_successes: 0.4018
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 8.34s
                        Total time: 10517.56s
                               ETA: 1062713.3s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.991s, learning 0.170s)
               Value function loss: 8488.2728
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 99.13
               Mean episode length: 41.33
                  Mean reward/step: 3.19
       Mean episode length/episode: 6.89
            Mean episode successes: 0.2930
Mean episode consecutive_successes: 0.4264
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 8.16s
                        Total time: 10525.72s
                               ETA: 1062443.1s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.109s, learning 0.172s)
               Value function loss: 8510.3766
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 83.64
               Mean episode length: 40.54
                  Mean reward/step: 3.12
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3218
Mean episode consecutive_successes: 0.4198
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 8.28s
                        Total time: 10534.00s
                               ETA: 1062185.5s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.438s, learning 0.163s)
               Value function loss: 8053.9855
                    Surrogate loss: -0.0144
             Mean action noise std: 0.79
                       Mean reward: 190.37
               Mean episode length: 43.09
                  Mean reward/step: 2.86
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3276
Mean episode consecutive_successes: 0.4192
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 8.60s
                        Total time: 10542.60s
                               ETA: 1061960.6s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.336s, learning 0.196s)
               Value function loss: 10109.2929
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 76.59
               Mean episode length: 40.85
                  Mean reward/step: 3.64
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3584
Mean episode consecutive_successes: 0.4170
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 8.53s
                        Total time: 10551.13s
                               ETA: 1061729.2s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.534s, learning 0.158s)
               Value function loss: 9168.8019
                    Surrogate loss: -0.0198
             Mean action noise std: 0.79
                       Mean reward: 134.79
               Mean episode length: 43.12
                  Mean reward/step: 3.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.3335
Mean episode consecutive_successes: 0.4316
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 8.69s
                        Total time: 10559.82s
                               ETA: 1061514.3s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.049s, learning 0.185s)
               Value function loss: 7747.1057
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 141.69
               Mean episode length: 41.37
                  Mean reward/step: 2.73
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3477
Mean episode consecutive_successes: 0.4284
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 8.23s
                        Total time: 10568.06s
                               ETA: 1061253.8s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.959s, learning 0.166s)
               Value function loss: 6972.8612
                    Surrogate loss: -0.0168
             Mean action noise std: 0.79
                       Mean reward: 121.26
               Mean episode length: 44.70
                  Mean reward/step: 2.38
       Mean episode length/episode: 6.85
            Mean episode successes: 0.3232
Mean episode consecutive_successes: 0.4292
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 8.13s
                        Total time: 10576.18s
                               ETA: 1060983.0s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.183s, learning 0.160s)
               Value function loss: 6351.2814
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 142.59
               Mean episode length: 43.03
                  Mean reward/step: 2.28
       Mean episode length/episode: 6.94
            Mean episode successes: 0.3135
Mean episode consecutive_successes: 0.4309
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 8.34s
                        Total time: 10584.53s
                               ETA: 1060734.5s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.055s, learning 0.170s)
               Value function loss: 6324.2578
                    Surrogate loss: -0.0189
             Mean action noise std: 0.79
                       Mean reward: 138.34
               Mean episode length: 43.86
                  Mean reward/step: 2.31
       Mean episode length/episode: 6.85
            Mean episode successes: 0.2949
Mean episode consecutive_successes: 0.4304
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 8.23s
                        Total time: 10592.75s
                               ETA: 1060474.7s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.015s, learning 0.175s)
               Value function loss: 7780.8858
                    Surrogate loss: -0.0180
             Mean action noise std: 0.79
                       Mean reward: 131.93
               Mean episode length: 41.23
                  Mean reward/step: 2.76
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2881
Mean episode consecutive_successes: 0.4289
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 8.19s
                        Total time: 10600.94s
                               ETA: 1060211.9s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.036s, learning 0.187s)
               Value function loss: 7347.3332
                    Surrogate loss: -0.0185
             Mean action noise std: 0.79
                       Mean reward: 57.64
               Mean episode length: 42.19
                  Mean reward/step: 2.81
       Mean episode length/episode: 6.92
            Mean episode successes: 0.3018
Mean episode consecutive_successes: 0.4237
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 8.22s
                        Total time: 10609.16s
                               ETA: 1059953.0s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.272s, learning 0.185s)
               Value function loss: 9371.0813
                    Surrogate loss: -0.0159
             Mean action noise std: 0.79
                       Mean reward: 102.04
               Mean episode length: 42.07
                  Mean reward/step: 3.35
       Mean episode length/episode: 6.90
            Mean episode successes: 0.3232
Mean episode consecutive_successes: 0.4253
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 8.46s
                        Total time: 10617.62s
                               ETA: 1059717.9s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.279s, learning 0.219s)
               Value function loss: 8176.0058
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 79.41
               Mean episode length: 41.79
                  Mean reward/step: 3.15
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3433
Mean episode consecutive_successes: 0.4177
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 8.50s
                        Total time: 10626.12s
                               ETA: 1059487.3s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.234s, learning 0.196s)
               Value function loss: 9615.6848
                    Surrogate loss: -0.0194
             Mean action noise std: 0.79
                       Mean reward: 80.87
               Mean episode length: 39.97
                  Mean reward/step: 3.54
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3789
Mean episode consecutive_successes: 0.4132
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 8.43s
                        Total time: 10634.55s
                               ETA: 1059250.4s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.399s, learning 0.257s)
               Value function loss: 9119.9635
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 154.23
               Mean episode length: 41.19
                  Mean reward/step: 3.02
       Mean episode length/episode: 6.93
            Mean episode successes: 0.3765
Mean episode consecutive_successes: 0.4227
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 8.66s
                        Total time: 10643.21s
                               ETA: 1059036.4s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.263s, learning 0.163s)
               Value function loss: 8220.8284
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 113.19
               Mean episode length: 43.95
                  Mean reward/step: 2.96
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3701
Mean episode consecutive_successes: 0.4281
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 8.43s
                        Total time: 10651.63s
                               ETA: 1058800.0s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.232s, learning 0.167s)
               Value function loss: 8818.0936
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 170.87
               Mean episode length: 39.86
                  Mean reward/step: 3.18
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3477
Mean episode consecutive_successes: 0.4378
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 8.40s
                        Total time: 10660.03s
                               ETA: 1058561.4s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.092s, learning 0.167s)
               Value function loss: 11222.7437
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 152.85
               Mean episode length: 43.43
                  Mean reward/step: 3.86
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3970
Mean episode consecutive_successes: 0.4321
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 8.26s
                        Total time: 10668.29s
                               ETA: 1058309.4s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.007s, learning 0.204s)
               Value function loss: 8981.3329
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 179.53
               Mean episode length: 41.52
                  Mean reward/step: 3.36
       Mean episode length/episode: 6.91
            Mean episode successes: 0.3784
Mean episode consecutive_successes: 0.4450
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 8.21s
                        Total time: 10676.50s
                               ETA: 1058053.1s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.161s)
               Value function loss: 8900.4046
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 109.76
               Mean episode length: 41.87
                  Mean reward/step: 2.99
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3857
Mean episode consecutive_successes: 0.4476
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 8.36s
                        Total time: 10684.86s
                               ETA: 1057811.7s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.243s, learning 0.158s)
               Value function loss: 8967.4312
                    Surrogate loss: -0.0204
             Mean action noise std: 0.79
                       Mean reward: 219.53
               Mean episode length: 41.93
                  Mean reward/step: 3.21
       Mean episode length/episode: 6.91
            Mean episode successes: 0.3647
Mean episode consecutive_successes: 0.4575
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 8.40s
                        Total time: 10693.26s
                               ETA: 1057575.0s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.274s, learning 0.158s)
               Value function loss: 8150.3622
                    Surrogate loss: -0.0217
             Mean action noise std: 0.79
                       Mean reward: 129.03
               Mean episode length: 40.68
                  Mean reward/step: 2.93
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3613
Mean episode consecutive_successes: 0.4552
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 8.43s
                        Total time: 10701.69s
                               ETA: 1057341.9s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.166s)
               Value function loss: 9851.9032
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 75.61
               Mean episode length: 43.56
                  Mean reward/step: 3.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.3486
Mean episode consecutive_successes: 0.4600
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 8.39s
                        Total time: 10710.08s
                               ETA: 1057105.4s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.157s, learning 0.164s)
               Value function loss: 7183.7935
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 250.50
               Mean episode length: 43.47
                  Mean reward/step: 2.61
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3091
Mean episode consecutive_successes: 0.4794
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 8.32s
                        Total time: 10718.40s
                               ETA: 1056862.3s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.831s, learning 0.201s)
               Value function loss: 7110.9648
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 130.65
               Mean episode length: 43.16
                  Mean reward/step: 2.51
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2847
Mean episode consecutive_successes: 0.4739
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 8.03s
                        Total time: 10726.44s
                               ETA: 1056591.2s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.219s, learning 0.168s)
               Value function loss: 6819.3497
                    Surrogate loss: -0.0187
             Mean action noise std: 0.79
                       Mean reward: 157.71
               Mean episode length: 42.59
                  Mean reward/step: 2.52
       Mean episode length/episode: 6.90
            Mean episode successes: 0.2769
Mean episode consecutive_successes: 0.4737
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 8.39s
                        Total time: 10734.82s
                               ETA: 1056355.7s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.056s, learning 0.207s)
               Value function loss: 7118.6588
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 169.79
               Mean episode length: 41.93
                  Mean reward/step: 2.81
       Mean episode length/episode: 6.85
            Mean episode successes: 0.2812
Mean episode consecutive_successes: 0.4613
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 8.26s
                        Total time: 10743.09s
                               ETA: 1056108.2s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.193s, learning 0.195s)
               Value function loss: 7808.4729
                    Surrogate loss: -0.0229
             Mean action noise std: 0.79
                       Mean reward: 140.57
               Mean episode length: 43.28
                  Mean reward/step: 2.81
       Mean episode length/episode: 6.85
            Mean episode successes: 0.3062
Mean episode consecutive_successes: 0.4463
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 8.39s
                        Total time: 10751.47s
                               ETA: 1055873.6s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.283s, learning 0.165s)
               Value function loss: 8750.8685
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 107.35
               Mean episode length: 41.74
                  Mean reward/step: 3.15
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3203
Mean episode consecutive_successes: 0.4424
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 8.45s
                        Total time: 10759.92s
                               ETA: 1055645.2s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.079s, learning 0.164s)
               Value function loss: 10500.2358
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 271.20
               Mean episode length: 43.86
                  Mean reward/step: 3.61
       Mean episode length/episode: 6.85
            Mean episode successes: 0.3638
Mean episode consecutive_successes: 0.4443
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 8.24s
                        Total time: 10768.16s
                               ETA: 1055397.4s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.158s, learning 0.180s)
               Value function loss: 8279.1062
                    Surrogate loss: -0.0184
             Mean action noise std: 0.79
                       Mean reward: 164.85
               Mean episode length: 41.83
                  Mean reward/step: 2.86
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3345
Mean episode consecutive_successes: 0.4474
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 8.34s
                        Total time: 10776.50s
                               ETA: 1055159.2s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.984s, learning 0.166s)
               Value function loss: 7936.2291
                    Surrogate loss: -0.0198
             Mean action noise std: 0.79
                       Mean reward: 72.48
               Mean episode length: 38.15
                  Mean reward/step: 2.88
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3403
Mean episode consecutive_successes: 0.4389
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 8.15s
                        Total time: 10784.65s
                               ETA: 1054903.2s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.347s, learning 0.170s)
               Value function loss: 8736.6306
                    Surrogate loss: -0.0195
             Mean action noise std: 0.79
                       Mean reward: 217.13
               Mean episode length: 41.44
                  Mean reward/step: 2.85
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3325
Mean episode consecutive_successes: 0.4482
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 8.52s
                        Total time: 10793.17s
                               ETA: 1054683.5s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.058s, learning 0.169s)
               Value function loss: 8729.1410
                    Surrogate loss: -0.0215
             Mean action noise std: 0.79
                       Mean reward: 118.23
               Mean episode length: 43.25
                  Mean reward/step: 3.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3169
Mean episode consecutive_successes: 0.4473
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 8.23s
                        Total time: 10801.40s
                               ETA: 1054435.7s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.136s, learning 0.186s)
               Value function loss: 8027.6733
                    Surrogate loss: -0.0178
             Mean action noise std: 0.79
                       Mean reward: 62.95
               Mean episode length: 39.48
                  Mean reward/step: 2.61
       Mean episode length/episode: 6.92
            Mean episode successes: 0.3418
Mean episode consecutive_successes: 0.4335
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 8.32s
                        Total time: 10809.72s
                               ETA: 1054197.9s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.847s, learning 0.162s)
               Value function loss: 7936.7781
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 66.20
               Mean episode length: 40.63
                  Mean reward/step: 2.67
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3428
Mean episode consecutive_successes: 0.4305
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 8.01s
                        Total time: 10817.73s
                               ETA: 1053930.0s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.952s, learning 0.162s)
               Value function loss: 6082.6433
                    Surrogate loss: -0.0185
             Mean action noise std: 0.79
                       Mean reward: 74.44
               Mean episode length: 41.47
                  Mean reward/step: 2.20
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2974
Mean episode consecutive_successes: 0.4396
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 8.11s
                        Total time: 10825.84s
                               ETA: 1053672.8s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.920s, learning 0.171s)
               Value function loss: 5628.4973
                    Surrogate loss: -0.0213
             Mean action noise std: 0.79
                       Mean reward: 79.33
               Mean episode length: 41.28
                  Mean reward/step: 2.13
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2651
Mean episode consecutive_successes: 0.4414
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 8.09s
                        Total time: 10833.93s
                               ETA: 1053413.8s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.462s, learning 0.188s)
               Value function loss: 5376.1027
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 48.88
               Mean episode length: 40.69
                  Mean reward/step: 1.91
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2280
Mean episode consecutive_successes: 0.4435
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 8.65s
                        Total time: 10842.58s
                               ETA: 1053209.6s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.087s, learning 0.206s)
               Value function loss: 4986.9339
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 87.68
               Mean episode length: 43.58
                  Mean reward/step: 2.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.1963
Mean episode consecutive_successes: 0.4435
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 8.29s
                        Total time: 10850.88s
                               ETA: 1052971.2s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.164s)
               Value function loss: 5667.5241
                    Surrogate loss: -0.0205
             Mean action noise std: 0.79
                       Mean reward: 144.39
               Mean episode length: 41.28
                  Mean reward/step: 2.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.2104
Mean episode consecutive_successes: 0.4338
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 8.29s
                        Total time: 10859.17s
                               ETA: 1052733.4s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.047s, learning 0.170s)
               Value function loss: 5953.8562
                    Surrogate loss: -0.0189
             Mean action noise std: 0.79
                       Mean reward: 18.98
               Mean episode length: 41.30
                  Mean reward/step: 2.63
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2319
Mean episode consecutive_successes: 0.4115
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 8.22s
                        Total time: 10867.39s
                               ETA: 1052488.5s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.162s)
               Value function loss: 6883.2914
                    Surrogate loss: -0.0180
             Mean action noise std: 0.79
                       Mean reward: 62.21
               Mean episode length: 37.40
                  Mean reward/step: 2.62
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2485
Mean episode consecutive_successes: 0.4029
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 8.45s
                        Total time: 10875.84s
                               ETA: 1052266.6s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.374s, learning 0.166s)
               Value function loss: 7403.6248
                    Surrogate loss: -0.0195
             Mean action noise std: 0.79
                       Mean reward: 173.04
               Mean episode length: 43.44
                  Mean reward/step: 2.59
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2769
Mean episode consecutive_successes: 0.3958
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 8.54s
                        Total time: 10884.38s
                               ETA: 1052053.9s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.332s, learning 0.174s)
               Value function loss: 6941.5751
                    Surrogate loss: -0.0185
             Mean action noise std: 0.79
                       Mean reward: 138.03
               Mean episode length: 43.21
                  Mean reward/step: 2.54
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2651
Mean episode consecutive_successes: 0.3958
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 8.51s
                        Total time: 10892.89s
                               ETA: 1051838.3s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.134s, learning 0.161s)
               Value function loss: 7198.4469
                    Surrogate loss: -0.0190
             Mean action noise std: 0.79
                       Mean reward: 66.55
               Mean episode length: 41.20
                  Mean reward/step: 2.69
       Mean episode length/episode: 6.83
            Mean episode successes: 0.2759
Mean episode consecutive_successes: 0.3886
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 8.30s
                        Total time: 10901.18s
                               ETA: 1051602.7s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.062s, learning 0.172s)
               Value function loss: 7345.8645
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 160.72
               Mean episode length: 39.25
                  Mean reward/step: 2.82
       Mean episode length/episode: 6.88
            Mean episode successes: 0.2466
Mean episode consecutive_successes: 0.4038
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 8.23s
                        Total time: 10909.41s
                               ETA: 1051361.7s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.501s, learning 0.168s)
               Value function loss: 6209.2991
                    Surrogate loss: -0.0204
             Mean action noise std: 0.79
                       Mean reward: 21.00
               Mean episode length: 39.78
                  Mean reward/step: 2.59
       Mean episode length/episode: 6.86
            Mean episode successes: 0.2612
Mean episode consecutive_successes: 0.3971
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 8.67s
                        Total time: 10918.08s
                               ETA: 1051163.0s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.066s, learning 0.166s)
               Value function loss: 6268.3732
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 82.04
               Mean episode length: 41.86
                  Mean reward/step: 2.40
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2773
Mean episode consecutive_successes: 0.3921
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 8.23s
                        Total time: 10926.32s
                               ETA: 1050922.7s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.036s, learning 0.192s)
               Value function loss: 8098.1190
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 81.14
               Mean episode length: 40.18
                  Mean reward/step: 2.93
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2842
Mean episode consecutive_successes: 0.3939
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 8.23s
                        Total time: 10934.55s
                               ETA: 1050682.4s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.320s, learning 0.191s)
               Value function loss: 7082.2447
                    Surrogate loss: -0.0203
             Mean action noise std: 0.79
                       Mean reward: 139.15
               Mean episode length: 41.53
                  Mean reward/step: 2.94
       Mean episode length/episode: 6.89
            Mean episode successes: 0.2837
Mean episode consecutive_successes: 0.3965
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 8.51s
                        Total time: 10943.06s
                               ETA: 1050469.7s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.144s, learning 0.165s)
               Value function loss: 8049.4795
                    Surrogate loss: -0.0208
             Mean action noise std: 0.79
                       Mean reward: 187.30
               Mean episode length: 41.99
                  Mean reward/step: 3.25
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2988
Mean episode consecutive_successes: 0.4003
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 8.31s
                        Total time: 10951.36s
                               ETA: 1050238.0s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.998s, learning 0.164s)
               Value function loss: 8753.9385
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 66.82
               Mean episode length: 40.81
                  Mean reward/step: 3.29
       Mean episode length/episode: 6.90
            Mean episode successes: 0.3433
Mean episode consecutive_successes: 0.3900
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 8.16s
                        Total time: 10959.53s
                               ETA: 1049992.7s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.338s, learning 0.214s)
               Value function loss: 12642.2087
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 108.90
               Mean episode length: 40.77
                  Mean reward/step: 4.43
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3647
Mean episode consecutive_successes: 0.4035
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 8.55s
                        Total time: 10968.08s
                               ETA: 1049785.2s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.010s, learning 0.240s)
               Value function loss: 11191.8820
                    Surrogate loss: -0.0206
             Mean action noise std: 0.79
                       Mean reward: 178.66
               Mean episode length: 39.84
                  Mean reward/step: 4.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4189
Mean episode consecutive_successes: 0.4080
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 8.25s
                        Total time: 10976.33s
                               ETA: 1049549.2s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.320s, learning 0.168s)
               Value function loss: 9433.1796
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 82.08
               Mean episode length: 40.97
                  Mean reward/step: 3.10
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4023
Mean episode consecutive_successes: 0.4091
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 8.49s
                        Total time: 10984.82s
                               ETA: 1049336.4s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.267s, learning 0.193s)
               Value function loss: 8070.5746
                    Surrogate loss: -0.0210
             Mean action noise std: 0.79
                       Mean reward: 191.47
               Mean episode length: 40.57
                  Mean reward/step: 2.79
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3765
Mean episode consecutive_successes: 0.4227
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 8.46s
                        Total time: 10993.28s
                               ETA: 1049121.3s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.370s, learning 0.165s)
               Value function loss: 7157.1195
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 187.25
               Mean episode length: 41.92
                  Mean reward/step: 2.70
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3511
Mean episode consecutive_successes: 0.4351
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 8.54s
                        Total time: 11001.81s
                               ETA: 1048913.8s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.535s, learning 0.236s)
               Value function loss: 7794.4109
                    Surrogate loss: -0.0200
             Mean action noise std: 0.79
                       Mean reward: 83.73
               Mean episode length: 40.27
                  Mean reward/step: 2.78
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3330
Mean episode consecutive_successes: 0.4390
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 16.77s
                        Total time: 11018.58s
                               ETA: 1049491.1s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.179s, learning 0.194s)
               Value function loss: 7159.6470
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 57.77
               Mean episode length: 38.67
                  Mean reward/step: 2.67
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3027
Mean episode consecutive_successes: 0.4417
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 16.37s
                        Total time: 11034.96s
                               ETA: 1050029.3s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.810s, learning 0.168s)
               Value function loss: 9602.3001
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 91.61
               Mean episode length: 40.95
                  Mean reward/step: 3.64
       Mean episode length/episode: 6.91
            Mean episode successes: 0.3433
Mean episode consecutive_successes: 0.4392
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 15.98s
                        Total time: 11050.94s
                               ETA: 1050528.9s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.035s, learning 0.185s)
               Value function loss: 9185.6126
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 129.04
               Mean episode length: 40.91
                  Mean reward/step: 3.60
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3457
Mean episode consecutive_successes: 0.4451
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 16.22s
                        Total time: 11067.16s
                               ETA: 1051050.5s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.878s, learning 0.188s)
               Value function loss: 8385.6566
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 162.38
               Mean episode length: 42.49
                  Mean reward/step: 2.99
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3501
Mean episode consecutive_successes: 0.4485
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 17.07s
                        Total time: 11084.22s
                               ETA: 1051651.4s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.397s, learning 0.164s)
               Value function loss: 9358.3804
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 204.82
               Mean episode length: 42.01
                  Mean reward/step: 3.37
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3208
Mean episode consecutive_successes: 0.4571
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 16.56s
                        Total time: 11100.78s
                               ETA: 1052203.2s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.358s, learning 0.187s)
               Value function loss: 9141.6148
                    Surrogate loss: -0.0158
             Mean action noise std: 0.79
                       Mean reward: 121.22
               Mean episode length: 39.68
                  Mean reward/step: 3.19
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3330
Mean episode consecutive_successes: 0.4568
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 16.55s
                        Total time: 11117.33s
                               ETA: 1052752.4s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.033s, learning 0.188s)
               Value function loss: 10660.2753
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 175.05
               Mean episode length: 41.97
                  Mean reward/step: 3.54
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3086
Mean episode consecutive_successes: 0.4742
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 16.22s
                        Total time: 11133.55s
                               ETA: 1053269.8s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.154s, learning 0.173s)
               Value function loss: 8308.5660
                    Surrogate loss: -0.0212
             Mean action noise std: 0.79
                       Mean reward: 81.55
               Mean episode length: 40.79
                  Mean reward/step: 2.91
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3350
Mean episode consecutive_successes: 0.4613
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 16.33s
                        Total time: 11149.88s
                               ETA: 1053796.3s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.172s, learning 0.172s)
               Value function loss: 9764.8677
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 106.70
               Mean episode length: 41.08
                  Mean reward/step: 3.35
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3218
Mean episode consecutive_successes: 0.4687
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 16.34s
                        Total time: 11166.22s
                               ETA: 1054323.4s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.392s, learning 0.191s)
               Value function loss: 9878.8581
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 154.88
               Mean episode length: 41.52
                  Mean reward/step: 3.48
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3623
Mean episode consecutive_successes: 0.4587
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 16.58s
                        Total time: 11182.80s
                               ETA: 1054872.0s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.090s, learning 0.164s)
               Value function loss: 9401.7323
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 144.83
               Mean episode length: 41.48
                  Mean reward/step: 3.53
       Mean episode length/episode: 6.89
            Mean episode successes: 0.3677
Mean episode consecutive_successes: 0.4653
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 16.25s
                        Total time: 11199.06s
                               ETA: 1055388.4s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.037s, learning 0.185s)
               Value function loss: 12109.0999
                    Surrogate loss: -0.0203
             Mean action noise std: 0.79
                       Mean reward: 131.74
               Mean episode length: 40.80
                  Mean reward/step: 4.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4072
Mean episode consecutive_successes: 0.4564
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 16.22s
                        Total time: 11215.28s
                               ETA: 1055901.0s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.287s, learning 0.202s)
               Value function loss: 11749.5029
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 147.48
               Mean episode length: 42.11
                  Mean reward/step: 3.90
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4106
Mean episode consecutive_successes: 0.4681
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 16.49s
                        Total time: 11231.77s
                               ETA: 1056437.5s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.665s, learning 0.163s)
               Value function loss: 13242.3445
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: 128.04
               Mean episode length: 43.46
                  Mean reward/step: 4.15
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4429
Mean episode consecutive_successes: 0.4638
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 15.83s
                        Total time: 11247.60s
                               ETA: 1056910.9s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.231s, learning 0.190s)
               Value function loss: 13857.0184
                    Surrogate loss: -0.0200
             Mean action noise std: 0.79
                       Mean reward: 70.83
               Mean episode length: 39.75
                  Mean reward/step: 4.17
       Mean episode length/episode: 6.89
            Mean episode successes: 0.4614
Mean episode consecutive_successes: 0.4693
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 16.42s
                        Total time: 11264.02s
                               ETA: 1057439.0s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.121s, learning 0.166s)
               Value function loss: 14432.4643
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 144.10
               Mean episode length: 40.76
                  Mean reward/step: 4.71
       Mean episode length/episode: 6.76
            Mean episode successes: 0.4995
Mean episode consecutive_successes: 0.4723
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 16.29s
                        Total time: 11280.30s
                               ETA: 1057953.6s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.899s, learning 0.162s)
               Value function loss: 13007.6607
                    Surrogate loss: -0.0198
             Mean action noise std: 0.79
                       Mean reward: 241.04
               Mean episode length: 39.88
                  Mean reward/step: 3.76
       Mean episode length/episode: 6.89
            Mean episode successes: 0.4966
Mean episode consecutive_successes: 0.4899
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 16.06s
                        Total time: 11296.36s
                               ETA: 1058445.9s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.885s, learning 0.209s)
               Value function loss: 10541.9870
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 226.92
               Mean episode length: 41.11
                  Mean reward/step: 3.15
       Mean episode length/episode: 6.85
            Mean episode successes: 0.3940
Mean episode consecutive_successes: 0.5198
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 16.09s
                        Total time: 11312.46s
                               ETA: 1058940.3s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.019s, learning 0.236s)
               Value function loss: 9418.7703
                    Surrogate loss: -0.0199
             Mean action noise std: 0.79
                       Mean reward: 160.26
               Mean episode length: 42.03
                  Mean reward/step: 2.85
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3848
Mean episode consecutive_successes: 0.5102
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 16.26s
                        Total time: 11328.71s
                               ETA: 1059448.9s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.103s, learning 0.208s)
               Value function loss: 9713.5616
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 184.30
               Mean episode length: 40.88
                  Mean reward/step: 2.57
       Mean episode length/episode: 6.77
            Mean episode successes: 0.3428
Mean episode consecutive_successes: 0.5204
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 16.31s
                        Total time: 11345.03s
                               ETA: 1059961.8s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.899s, learning 0.198s)
               Value function loss: 10996.1034
                    Surrogate loss: -0.0207
             Mean action noise std: 0.79
                       Mean reward: 201.98
               Mean episode length: 40.86
                  Mean reward/step: 3.64
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3555
Mean episode consecutive_successes: 0.5220
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 16.10s
                        Total time: 11361.12s
                               ETA: 1060453.5s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.272s, learning 0.162s)
               Value function loss: 7908.0369
                    Surrogate loss: -0.0193
             Mean action noise std: 0.79
                       Mean reward: 87.43
               Mean episode length: 41.57
                  Mean reward/step: 2.41
       Mean episode length/episode: 6.90
            Mean episode successes: 0.2856
Mean episode consecutive_successes: 0.5283
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 16.43s
                        Total time: 11377.56s
                               ETA: 1060975.8s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.097s, learning 0.190s)
               Value function loss: 10221.7736
                    Surrogate loss: -0.0200
             Mean action noise std: 0.79
                       Mean reward: 137.02
               Mean episode length: 41.19
                  Mean reward/step: 3.34
       Mean episode length/episode: 6.73
            Mean episode successes: 0.3330
Mean episode consecutive_successes: 0.5074
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 16.29s
                        Total time: 11393.84s
                               ETA: 1061483.4s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.181s, learning 0.185s)
               Value function loss: 10175.4269
                    Surrogate loss: -0.0208
             Mean action noise std: 0.79
                       Mean reward: 190.66
               Mean episode length: 43.44
                  Mean reward/step: 3.63
       Mean episode length/episode: 6.94
            Mean episode successes: 0.3516
Mean episode consecutive_successes: 0.5072
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 16.37s
                        Total time: 11410.21s
                               ETA: 1061997.5s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.229s, learning 0.168s)
               Value function loss: 12760.2259
                    Surrogate loss: -0.0189
             Mean action noise std: 0.79
                       Mean reward: 108.98
               Mean episode length: 40.74
                  Mean reward/step: 4.25
       Mean episode length/episode: 6.93
            Mean episode successes: 0.4058
Mean episode consecutive_successes: 0.4944
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 16.40s
                        Total time: 11426.61s
                               ETA: 1062513.2s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.180s, learning 0.166s)
               Value function loss: 11537.4964
                    Surrogate loss: -0.0202
             Mean action noise std: 0.79
                       Mean reward: 164.22
               Mean episode length: 40.99
                  Mean reward/step: 4.27
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4487
Mean episode consecutive_successes: 0.4899
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 16.35s
                        Total time: 11442.95s
                               ETA: 1063023.3s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.008s, learning 0.159s)
               Value function loss: 11847.5700
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 56.28
               Mean episode length: 39.90
                  Mean reward/step: 3.73
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4585
Mean episode consecutive_successes: 0.4862
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 16.17s
                        Total time: 11459.12s
                               ETA: 1063515.8s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.203s, learning 0.162s)
               Value function loss: 12904.7928
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 127.52
               Mean episode length: 42.09
                  Mean reward/step: 4.34
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4468
Mean episode consecutive_successes: 0.5098
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 16.37s
                        Total time: 11475.48s
                               ETA: 1064025.8s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.209s, learning 0.159s)
               Value function loss: 10250.6832
                    Surrogate loss: -0.0210
             Mean action noise std: 0.79
                       Mean reward: 244.04
               Mean episode length: 44.16
                  Mean reward/step: 3.59
       Mean episode length/episode: 6.97
            Mean episode successes: 0.3857
Mean episode consecutive_successes: 0.5438
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 16.37s
                        Total time: 11491.85s
                               ETA: 1064534.9s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.322s, learning 0.160s)
               Value function loss: 10026.8236
                    Surrogate loss: -0.0194
             Mean action noise std: 0.79
                       Mean reward: 83.82
               Mean episode length: 40.54
                  Mean reward/step: 3.55
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3862
Mean episode consecutive_successes: 0.5414
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 16.48s
                        Total time: 11508.33s
                               ETA: 1065053.7s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.872s, learning 0.165s)
               Value function loss: 7453.5704
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 130.70
               Mean episode length: 42.77
                  Mean reward/step: 2.75
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3438
Mean episode consecutive_successes: 0.5544
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 16.04s
                        Total time: 11524.37s
                               ETA: 1065530.3s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.479s, learning 0.186s)
               Value function loss: 6416.9159
                    Surrogate loss: -0.0218
             Mean action noise std: 0.79
                       Mean reward: 117.09
               Mean episode length: 41.67
                  Mean reward/step: 2.36
       Mean episode length/episode: 6.73
            Mean episode successes: 0.2896
Mean episode consecutive_successes: 0.5515
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 16.66s
                        Total time: 11541.03s
                               ETA: 1066064.0s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.241s, learning 0.160s)
               Value function loss: 8225.0663
                    Surrogate loss: -0.0191
             Mean action noise std: 0.79
                       Mean reward: 18.89
               Mean episode length: 40.58
                  Mean reward/step: 2.90
       Mean episode length/episode: 6.88
            Mean episode successes: 0.2881
Mean episode consecutive_successes: 0.5379
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 16.40s
                        Total time: 11557.44s
                               ETA: 1066572.4s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.833s, learning 0.164s)
               Value function loss: 6938.0601
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 255.13
               Mean episode length: 42.23
                  Mean reward/step: 2.75
       Mean episode length/episode: 6.85
            Mean episode successes: 0.2568
Mean episode consecutive_successes: 0.5485
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 17.00s
                        Total time: 11574.43s
                               ETA: 1067134.7s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.158s, learning 0.170s)
               Value function loss: 8180.5937
                    Surrogate loss: -0.0198
             Mean action noise std: 0.79
                       Mean reward: 56.97
               Mean episode length: 36.95
                  Mean reward/step: 2.74
       Mean episode length/episode: 6.81
            Mean episode successes: 0.2642
Mean episode consecutive_successes: 0.5294
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 16.33s
                        Total time: 11590.76s
                               ETA: 1067634.2s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.394s, learning 0.163s)
               Value function loss: 10589.3331
                    Surrogate loss: -0.0191
             Mean action noise std: 0.79
                       Mean reward: 107.80
               Mean episode length: 38.23
                  Mean reward/step: 3.79
       Mean episode length/episode: 6.76
            Mean episode successes: 0.3008
Mean episode consecutive_successes: 0.5152
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 16.56s
                        Total time: 11607.32s
                               ETA: 1068154.0s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1211 steps/s (collection: 13.363s, learning 0.160s)
               Value function loss: 9630.8414
                    Surrogate loss: -0.0202
             Mean action noise std: 0.79
                       Mean reward: 102.54
               Mean episode length: 38.06
                  Mean reward/step: 3.47
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2861
Mean episode consecutive_successes: 0.5227
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 13.52s
                        Total time: 11620.84s
                               ETA: 1068393.8s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.390s, learning 0.182s)
               Value function loss: 11157.0900
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 71.41
               Mean episode length: 40.16
                  Mean reward/step: 3.60
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3311
Mean episode consecutive_successes: 0.5084
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 8.57s
                        Total time: 11629.41s
                               ETA: 1068178.3s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.405s, learning 0.229s)
               Value function loss: 8435.1586
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 190.37
               Mean episode length: 41.78
                  Mean reward/step: 2.79
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3149
Mean episode consecutive_successes: 0.5077
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 8.63s
                        Total time: 11638.05s
                               ETA: 1067969.0s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.515s, learning 0.171s)
               Value function loss: 8884.0080
                    Surrogate loss: -0.0207
             Mean action noise std: 0.79
                       Mean reward: 118.67
               Mean episode length: 39.66
                  Mean reward/step: 3.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3022
Mean episode consecutive_successes: 0.5067
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 8.69s
                        Total time: 11646.73s
                               ETA: 1067764.8s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.291s, learning 0.167s)
               Value function loss: 8812.7252
                    Surrogate loss: -0.0206
             Mean action noise std: 0.79
                       Mean reward: 149.71
               Mean episode length: 40.95
                  Mean reward/step: 2.99
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3081
Mean episode consecutive_successes: 0.5024
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 8.46s
                        Total time: 11655.19s
                               ETA: 1067540.1s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.010s, learning 0.167s)
               Value function loss: 10114.1196
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 81.31
               Mean episode length: 39.43
                  Mean reward/step: 3.36
       Mean episode length/episode: 6.74
            Mean episode successes: 0.3149
Mean episode consecutive_successes: 0.4937
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 8.18s
                        Total time: 11663.37s
                               ETA: 1067290.0s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.167s, learning 0.157s)
               Value function loss: 8844.7715
                    Surrogate loss: -0.0221
             Mean action noise std: 0.79
                       Mean reward: 114.45
               Mean episode length: 41.05
                  Mean reward/step: 3.02
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3467
Mean episode consecutive_successes: 0.4823
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 8.32s
                        Total time: 11671.69s
                               ETA: 1067053.8s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.417s, learning 0.175s)
               Value function loss: 8950.4992
                    Surrogate loss: -0.0189
             Mean action noise std: 0.79
                       Mean reward: 197.01
               Mean episode length: 40.81
                  Mean reward/step: 3.24
       Mean episode length/episode: 6.76
            Mean episode successes: 0.3228
Mean episode consecutive_successes: 0.4855
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 8.59s
                        Total time: 11680.29s
                               ETA: 1066842.5s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.093s, learning 0.188s)
               Value function loss: 8707.5509
                    Surrogate loss: -0.0167
             Mean action noise std: 0.79
                       Mean reward: 149.94
               Mean episode length: 41.37
                  Mean reward/step: 3.39
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3174
Mean episode consecutive_successes: 0.4951
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 8.28s
                        Total time: 11688.57s
                               ETA: 1066603.3s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.364s, learning 0.163s)
               Value function loss: 8622.9111
                    Surrogate loss: -0.0207
             Mean action noise std: 0.79
                       Mean reward: 79.46
               Mean episode length: 41.02
                  Mean reward/step: 3.56
       Mean episode length/episode: 6.77
            Mean episode successes: 0.3071
Mean episode consecutive_successes: 0.4942
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 8.53s
                        Total time: 11697.09s
                               ETA: 1066386.8s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.431s, learning 0.168s)
               Value function loss: 10871.1898
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 55.82
               Mean episode length: 39.13
                  Mean reward/step: 3.48
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3398
Mean episode consecutive_successes: 0.4837
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 8.60s
                        Total time: 11705.69s
                               ETA: 1066177.4s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.058s, learning 0.234s)
               Value function loss: 8045.4806
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 144.67
               Mean episode length: 41.39
                  Mean reward/step: 3.14
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3613
Mean episode consecutive_successes: 0.4750
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 8.29s
                        Total time: 11713.98s
                               ETA: 1065940.3s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.414s, learning 0.168s)
               Value function loss: 10831.4422
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 115.29
               Mean episode length: 37.77
                  Mean reward/step: 3.57
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3901
Mean episode consecutive_successes: 0.4702
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 8.58s
                        Total time: 11722.57s
                               ETA: 1065730.0s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.078s, learning 0.167s)
               Value function loss: 9925.6870
                    Surrogate loss: -0.0223
             Mean action noise std: 0.79
                       Mean reward: 176.34
               Mean episode length: 39.81
                  Mean reward/step: 3.59
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3745
Mean episode consecutive_successes: 0.4864
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 8.24s
                        Total time: 11730.81s
                               ETA: 1065489.5s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.251s, learning 0.229s)
               Value function loss: 8603.2127
                    Surrogate loss: -0.0187
             Mean action noise std: 0.79
                       Mean reward: 117.16
               Mean episode length: 41.12
                  Mean reward/step: 2.92
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3560
Mean episode consecutive_successes: 0.4942
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 8.48s
                        Total time: 11739.29s
                               ETA: 1065270.7s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.256s, learning 0.165s)
               Value function loss: 7432.0889
                    Surrogate loss: -0.0226
             Mean action noise std: 0.79
                       Mean reward: 36.96
               Mean episode length: 41.08
                  Mean reward/step: 2.81
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3213
Mean episode consecutive_successes: 0.4926
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 8.42s
                        Total time: 11747.71s
                               ETA: 1065047.0s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.466s, learning 0.165s)
               Value function loss: 8243.3868
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 46.55
               Mean episode length: 40.63
                  Mean reward/step: 3.01
       Mean episode length/episode: 6.90
            Mean episode successes: 0.3608
Mean episode consecutive_successes: 0.4761
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 8.63s
                        Total time: 11756.34s
                               ETA: 1064842.6s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.009s, learning 0.160s)
               Value function loss: 6684.9026
                    Surrogate loss: -0.0212
             Mean action noise std: 0.79
                       Mean reward: 107.61
               Mean episode length: 42.60
                  Mean reward/step: 2.69
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3564
Mean episode consecutive_successes: 0.4721
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 8.17s
                        Total time: 11764.51s
                               ETA: 1064596.8s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.075s, learning 0.162s)
               Value function loss: 10563.2161
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: 145.07
               Mean episode length: 38.02
                  Mean reward/step: 3.91
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3535
Mean episode consecutive_successes: 0.4860
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 8.24s
                        Total time: 11772.75s
                               ETA: 1064357.6s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.855s, learning 0.166s)
               Value function loss: 9465.0086
                    Surrogate loss: -0.0209
             Mean action noise std: 0.79
                       Mean reward: 140.51
               Mean episode length: 43.00
                  Mean reward/step: 3.39
       Mean episode length/episode: 6.85
            Mean episode successes: 0.3921
Mean episode consecutive_successes: 0.4749
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 8.02s
                        Total time: 11780.77s
                               ETA: 1064099.3s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.238s, learning 0.211s)
               Value function loss: 8847.6637
                    Surrogate loss: -0.0214
             Mean action noise std: 0.79
                       Mean reward: 50.56
               Mean episode length: 38.93
                  Mean reward/step: 3.16
       Mean episode length/episode: 6.73
            Mean episode successes: 0.3853
Mean episode consecutive_successes: 0.4709
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 8.45s
                        Total time: 11789.22s
                               ETA: 1063880.2s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.308s, learning 0.163s)
               Value function loss: 9490.6683
                    Surrogate loss: -0.0204
             Mean action noise std: 0.79
                       Mean reward: 115.59
               Mean episode length: 38.66
                  Mean reward/step: 3.19
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4175
Mean episode consecutive_successes: 0.4614
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 8.47s
                        Total time: 11797.69s
                               ETA: 1063663.3s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.050s, learning 0.158s)
               Value function loss: 8899.9557
                    Surrogate loss: -0.0216
             Mean action noise std: 0.79
                       Mean reward: 245.77
               Mean episode length: 38.96
                  Mean reward/step: 3.12
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4033
Mean episode consecutive_successes: 0.4772
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 8.21s
                        Total time: 11805.90s
                               ETA: 1063423.2s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.334s, learning 0.174s)
               Value function loss: 8920.4083
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 95.95
               Mean episode length: 39.71
                  Mean reward/step: 2.83
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3560
Mean episode consecutive_successes: 0.4819
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 8.51s
                        Total time: 11814.41s
                               ETA: 1063210.6s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.139s, learning 0.165s)
               Value function loss: 9165.2948
                    Surrogate loss: -0.0222
             Mean action noise std: 0.79
                       Mean reward: 133.12
               Mean episode length: 43.30
                  Mean reward/step: 3.19
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3594
Mean episode consecutive_successes: 0.4815
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 8.30s
                        Total time: 11822.71s
                               ETA: 1062979.9s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.173s, learning 0.200s)
               Value function loss: 7706.4517
                    Surrogate loss: -0.0231
             Mean action noise std: 0.79
                       Mean reward: 113.35
               Mean episode length: 43.14
                  Mean reward/step: 2.78
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3550
Mean episode consecutive_successes: 0.4783
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 8.37s
                        Total time: 11831.08s
                               ETA: 1062755.9s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.303s, learning 0.225s)
               Value function loss: 8171.1220
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 203.57
               Mean episode length: 39.20
                  Mean reward/step: 3.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3350
Mean episode consecutive_successes: 0.4865
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 8.53s
                        Total time: 11839.61s
                               ETA: 1062546.1s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.256s, learning 0.211s)
               Value function loss: 10409.4689
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 126.14
               Mean episode length: 43.83
                  Mean reward/step: 3.56
       Mean episode length/episode: 6.90
            Mean episode successes: 0.3677
Mean episode consecutive_successes: 0.4745
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 8.47s
                        Total time: 11848.08s
                               ETA: 1062331.3s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.303s, learning 0.165s)
               Value function loss: 6888.3689
                    Surrogate loss: -0.0199
             Mean action noise std: 0.79
                       Mean reward: 117.84
               Mean episode length: 41.56
                  Mean reward/step: 2.40
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3506
Mean episode consecutive_successes: 0.4744
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 8.47s
                        Total time: 11856.55s
                               ETA: 1062116.8s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.097s, learning 0.212s)
               Value function loss: 9286.4304
                    Surrogate loss: -0.0202
             Mean action noise std: 0.79
                       Mean reward: 194.79
               Mean episode length: 45.37
                  Mean reward/step: 3.10
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3701
Mean episode consecutive_successes: 0.4678
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 8.31s
                        Total time: 11864.86s
                               ETA: 1061888.5s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.136s, learning 0.161s)
               Value function loss: 7898.5509
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 50.80
               Mean episode length: 39.65
                  Mean reward/step: 2.59
       Mean episode length/episode: 6.85
            Mean episode successes: 0.3545
Mean episode consecutive_successes: 0.4587
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 8.30s
                        Total time: 11873.15s
                               ETA: 1061659.6s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.831s, learning 0.165s)
               Value function loss: 8239.2943
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 267.19
               Mean episode length: 41.79
                  Mean reward/step: 3.20
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3804
Mean episode consecutive_successes: 0.4619
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 9.00s
                        Total time: 11882.15s
                               ETA: 1061493.5s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.977s, learning 0.165s)
               Value function loss: 7472.9945
                    Surrogate loss: -0.0168
             Mean action noise std: 0.79
                       Mean reward: 69.41
               Mean episode length: 41.58
                  Mean reward/step: 2.68
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3403
Mean episode consecutive_successes: 0.4607
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 8.14s
                        Total time: 11890.29s
                               ETA: 1061251.4s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.232s, learning 0.172s)
               Value function loss: 6982.9013
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 65.90
               Mean episode length: 39.79
                  Mean reward/step: 2.39
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3027
Mean episode consecutive_successes: 0.4644
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 8.40s
                        Total time: 11898.70s
                               ETA: 1061033.1s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.152s, learning 0.173s)
               Value function loss: 9886.9336
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 90.07
               Mean episode length: 43.44
                  Mean reward/step: 3.45
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3228
Mean episode consecutive_successes: 0.4602
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 8.32s
                        Total time: 11907.02s
                               ETA: 1060808.2s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.485s, learning 0.163s)
               Value function loss: 7952.4834
                    Surrogate loss: -0.0212
             Mean action noise std: 0.79
                       Mean reward: 90.36
               Mean episode length: 42.57
                  Mean reward/step: 3.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.3340
Mean episode consecutive_successes: 0.4591
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 8.65s
                        Total time: 11915.67s
                               ETA: 1060612.4s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.119s, learning 0.165s)
               Value function loss: 6958.1011
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 150.82
               Mean episode length: 39.40
                  Mean reward/step: 2.49
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3325
Mean episode consecutive_successes: 0.4561
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 8.28s
                        Total time: 11923.95s
                               ETA: 1060384.6s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.377s, learning 0.179s)
               Value function loss: 8002.6467
                    Surrogate loss: -0.0210
             Mean action noise std: 0.79
                       Mean reward: 43.21
               Mean episode length: 39.70
                  Mean reward/step: 2.92
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3467
Mean episode consecutive_successes: 0.4427
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 8.56s
                        Total time: 11932.51s
                               ETA: 1060181.4s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.954s, learning 0.169s)
               Value function loss: 7541.1985
                    Surrogate loss: -0.0206
             Mean action noise std: 0.79
                       Mean reward: 179.13
               Mean episode length: 40.86
                  Mean reward/step: 2.76
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3496
Mean episode consecutive_successes: 0.4437
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 8.12s
                        Total time: 11940.63s
                               ETA: 1059940.0s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.247s, learning 0.170s)
               Value function loss: 6074.7996
                    Surrogate loss: -0.0210
             Mean action noise std: 0.79
                       Mean reward: 110.60
               Mean episode length: 38.95
                  Mean reward/step: 2.18
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3184
Mean episode consecutive_successes: 0.4441
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 8.42s
                        Total time: 11949.05s
                               ETA: 1059725.1s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.321s, learning 0.160s)
               Value function loss: 5467.8694
                    Surrogate loss: -0.0227
             Mean action noise std: 0.79
                       Mean reward: 216.49
               Mean episode length: 40.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 6.88
            Mean episode successes: 0.2432
Mean episode consecutive_successes: 0.4694
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 8.48s
                        Total time: 11957.53s
                               ETA: 1059516.3s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.197s, learning 0.171s)
               Value function loss: 7918.1877
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 33.60
               Mean episode length: 39.76
                  Mean reward/step: 2.72
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2217
Mean episode consecutive_successes: 0.4598
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 8.37s
                        Total time: 11965.90s
                               ETA: 1059297.8s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.251s, learning 0.166s)
               Value function loss: 7816.2956
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 58.03
               Mean episode length: 39.63
                  Mean reward/step: 2.99
       Mean episode length/episode: 6.91
            Mean episode successes: 0.2656
Mean episode consecutive_successes: 0.4464
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 8.42s
                        Total time: 11974.31s
                               ETA: 1059084.1s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.426s, learning 0.164s)
               Value function loss: 9658.7545
                    Surrogate loss: -0.0211
             Mean action noise std: 0.79
                       Mean reward: 162.60
               Mean episode length: 41.49
                  Mean reward/step: 3.82
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3115
Mean episode consecutive_successes: 0.4413
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 8.59s
                        Total time: 11982.90s
                               ETA: 1058886.0s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.689s, learning 0.191s)
               Value function loss: 12682.5487
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 126.04
               Mean episode length: 39.90
                  Mean reward/step: 4.11
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3472
Mean episode consecutive_successes: 0.4383
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 8.88s
                        Total time: 11991.78s
                               ETA: 1058713.8s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.422s, learning 0.165s)
               Value function loss: 12430.0160
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 126.53
               Mean episode length: 39.74
                  Mean reward/step: 4.60
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4385
Mean episode consecutive_successes: 0.4300
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 8.59s
                        Total time: 12000.37s
                               ETA: 1058516.1s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.371s, learning 0.161s)
               Value function loss: 12396.7126
                    Surrogate loss: -0.0182
             Mean action noise std: 0.79
                       Mean reward: 112.60
               Mean episode length: 38.19
                  Mean reward/step: 4.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.4609
Mean episode consecutive_successes: 0.4339
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 8.53s
                        Total time: 12008.90s
                               ETA: 1058314.0s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.389s, learning 0.163s)
               Value function loss: 11857.8003
                    Surrogate loss: -0.0198
             Mean action noise std: 0.79
                       Mean reward: 235.28
               Mean episode length: 42.34
                  Mean reward/step: 4.31
       Mean episode length/episode: 6.85
            Mean episode successes: 0.4312
Mean episode consecutive_successes: 0.4648
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 8.55s
                        Total time: 12017.45s
                               ETA: 1058113.9s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.137s, learning 0.205s)
               Value function loss: 8665.5323
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 63.91
               Mean episode length: 39.67
                  Mean reward/step: 2.82
       Mean episode length/episode: 6.78
            Mean episode successes: 0.4062
Mean episode consecutive_successes: 0.4600
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 8.34s
                        Total time: 12025.80s
                               ETA: 1057895.7s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.041s, learning 0.156s)
               Value function loss: 9433.2742
                    Surrogate loss: -0.0220
             Mean action noise std: 0.79
                       Mean reward: 105.30
               Mean episode length: 42.06
                  Mean reward/step: 3.13
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3711
Mean episode consecutive_successes: 0.4702
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 8.20s
                        Total time: 12033.99s
                               ETA: 1057665.1s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.204s, learning 0.186s)
               Value function loss: 12518.2861
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 167.12
               Mean episode length: 40.99
                  Mean reward/step: 4.21
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3828
Mean episode consecutive_successes: 0.4824
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 8.39s
                        Total time: 12042.39s
                               ETA: 1057451.9s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.281s, learning 0.165s)
               Value function loss: 13983.3562
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 134.18
               Mean episode length: 41.38
                  Mean reward/step: 4.71
       Mean episode length/episode: 6.97
            Mean episode successes: 0.4556
Mean episode consecutive_successes: 0.4791
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 8.45s
                        Total time: 12050.83s
                               ETA: 1057244.0s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.056s, learning 0.170s)
               Value function loss: 14513.8877
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 228.37
               Mean episode length: 43.37
                  Mean reward/step: 5.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.4541
Mean episode consecutive_successes: 0.5011
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 8.23s
                        Total time: 12059.06s
                               ETA: 1057017.1s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.231s, learning 0.165s)
               Value function loss: 12982.2333
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 223.93
               Mean episode length: 40.66
                  Mean reward/step: 4.71
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4858
Mean episode consecutive_successes: 0.5070
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 8.40s
                        Total time: 12067.45s
                               ETA: 1056805.5s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.567s, learning 0.158s)
               Value function loss: 11323.3032
                    Surrogate loss: -0.0213
             Mean action noise std: 0.78
                       Mean reward: 169.15
               Mean episode length: 44.82
                  Mean reward/step: 4.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.5171
Mean episode consecutive_successes: 0.5054
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 8.72s
                        Total time: 12076.18s
                               ETA: 1056622.9s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.239s, learning 0.163s)
               Value function loss: 9892.0428
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 218.26
               Mean episode length: 43.49
                  Mean reward/step: 3.00
       Mean episode length/episode: 6.85
            Mean episode successes: 0.4673
Mean episode consecutive_successes: 0.5252
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 8.40s
                        Total time: 12084.58s
                               ETA: 1056412.5s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.001s, learning 0.167s)
               Value function loss: 11298.5421
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 97.00
               Mean episode length: 41.12
                  Mean reward/step: 3.44
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4189
Mean episode consecutive_successes: 0.5298
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 8.17s
                        Total time: 12092.75s
                               ETA: 1056182.0s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.160s)
               Value function loss: 9463.5229
                    Surrogate loss: -0.0213
             Mean action noise std: 0.78
                       Mean reward: 119.27
               Mean episode length: 40.36
                  Mean reward/step: 3.19
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3662
Mean episode consecutive_successes: 0.5442
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 8.45s
                        Total time: 12101.20s
                               ETA: 1055976.3s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.809s, learning 0.164s)
               Value function loss: 9892.5219
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 208.70
               Mean episode length: 39.82
                  Mean reward/step: 3.46
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3286
Mean episode consecutive_successes: 0.5621
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 7.97s
                        Total time: 12109.17s
                               ETA: 1055729.6s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.111s, learning 0.218s)
               Value function loss: 10308.4676
                    Surrogate loss: -0.0224
             Mean action noise std: 0.78
                       Mean reward: 136.62
               Mean episode length: 39.83
                  Mean reward/step: 3.72
       Mean episode length/episode: 6.91
            Mean episode successes: 0.3584
Mean episode consecutive_successes: 0.5520
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 8.33s
                        Total time: 12117.50s
                               ETA: 1055514.2s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.426s, learning 0.159s)
               Value function loss: 14186.1055
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 225.95
               Mean episode length: 43.54
                  Mean reward/step: 4.55
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3770
Mean episode consecutive_successes: 0.5551
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 8.59s
                        Total time: 12126.08s
                               ETA: 1055321.6s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.299s, learning 0.162s)
               Value function loss: 14519.9282
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 125.59
               Mean episode length: 43.01
                  Mean reward/step: 4.83
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4316
Mean episode consecutive_successes: 0.5423
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 8.46s
                        Total time: 12134.55s
                               ETA: 1055118.5s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.056s, learning 0.158s)
               Value function loss: 13116.1724
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 185.93
               Mean episode length: 39.49
                  Mean reward/step: 4.64
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4307
Mean episode consecutive_successes: 0.5605
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 8.21s
                        Total time: 12142.76s
                               ETA: 1054894.3s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.623s, learning 0.186s)
               Value function loss: 14239.6885
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 189.15
               Mean episode length: 40.54
                  Mean reward/step: 4.57
       Mean episode length/episode: 6.85
            Mean episode successes: 0.4688
Mean episode consecutive_successes: 0.5654
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 8.81s
                        Total time: 12151.57s
                               ETA: 1054722.0s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.200s, learning 0.167s)
               Value function loss: 12728.0756
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 79.65
               Mean episode length: 41.30
                  Mean reward/step: 4.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4937
Mean episode consecutive_successes: 0.5517
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 8.37s
                        Total time: 12159.93s
                               ETA: 1054511.7s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.312s, learning 0.170s)
               Value function loss: 13712.3282
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 223.40
               Mean episode length: 44.08
                  Mean reward/step: 4.33
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4590
Mean episode consecutive_successes: 0.5814
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 8.48s
                        Total time: 12168.42s
                               ETA: 1054311.7s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.167s, learning 0.161s)
               Value function loss: 15382.1277
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 294.09
               Mean episode length: 40.60
                  Mean reward/step: 5.14
       Mean episode length/episode: 6.94
            Mean episode successes: 0.4858
Mean episode consecutive_successes: 0.5969
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 8.33s
                        Total time: 12176.74s
                               ETA: 1054098.7s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.080s, learning 0.170s)
               Value function loss: 17115.9045
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 274.00
               Mean episode length: 44.40
                  Mean reward/step: 5.49
       Mean episode length/episode: 6.91
            Mean episode successes: 0.5322
Mean episode consecutive_successes: 0.5989
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 8.25s
                        Total time: 12184.99s
                               ETA: 1053879.4s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.255s, learning 0.209s)
               Value function loss: 15602.9252
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 321.42
               Mean episode length: 40.06
                  Mean reward/step: 4.90
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4937
Mean episode consecutive_successes: 0.6265
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 8.46s
                        Total time: 12193.46s
                               ETA: 1053678.8s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.100s, learning 0.203s)
               Value function loss: 14651.7862
                    Surrogate loss: -0.0241
             Mean action noise std: 0.78
                       Mean reward: 109.45
               Mean episode length: 41.50
                  Mean reward/step: 4.83
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5171
Mean episode consecutive_successes: 0.6193
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 8.30s
                        Total time: 12201.76s
                               ETA: 1053464.8s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.887s, learning 0.168s)
               Value function loss: 18098.7372
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 268.54
               Mean episode length: 43.42
                  Mean reward/step: 5.70
       Mean episode length/episode: 6.78
            Mean episode successes: 0.5532
Mean episode consecutive_successes: 0.6294
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 8.06s
                        Total time: 12209.82s
                               ETA: 1053229.7s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.371s, learning 0.164s)
               Value function loss: 15516.0065
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 122.16
               Mean episode length: 41.80
                  Mean reward/step: 5.44
       Mean episode length/episode: 6.92
            Mean episode successes: 0.5835
Mean episode consecutive_successes: 0.6355
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 8.54s
                        Total time: 12218.35s
                               ETA: 1053036.4s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.271s, learning 0.184s)
               Value function loss: 16528.8137
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 219.62
               Mean episode length: 41.89
                  Mean reward/step: 5.23
       Mean episode length/episode: 6.87
            Mean episode successes: 0.5879
Mean episode consecutive_successes: 0.6467
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 8.46s
                        Total time: 12226.81s
                               ETA: 1052836.6s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.352s, learning 0.163s)
               Value function loss: 14205.1148
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 168.40
               Mean episode length: 39.68
                  Mean reward/step: 4.56
       Mean episode length/episode: 6.86
            Mean episode successes: 0.5869
Mean episode consecutive_successes: 0.6634
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 8.52s
                        Total time: 12235.32s
                               ETA: 1052642.2s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.273s, learning 0.179s)
               Value function loss: 15243.5717
                    Surrogate loss: -0.0245
             Mean action noise std: 0.78
                       Mean reward: 230.78
               Mean episode length: 43.59
                  Mean reward/step: 4.59
       Mean episode length/episode: 6.84
            Mean episode successes: 0.5278
Mean episode consecutive_successes: 0.6857
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 8.45s
                        Total time: 12243.77s
                               ETA: 1052442.8s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.174s, learning 0.163s)
               Value function loss: 15489.5359
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 206.12
               Mean episode length: 40.42
                  Mean reward/step: 4.45
       Mean episode length/episode: 6.90
            Mean episode successes: 0.5156
Mean episode consecutive_successes: 0.6952
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 8.34s
                        Total time: 12252.11s
                               ETA: 1052233.8s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.003s, learning 0.217s)
               Value function loss: 15147.5285
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: 175.62
               Mean episode length: 39.48
                  Mean reward/step: 5.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4536
Mean episode consecutive_successes: 0.7238
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 8.22s
                        Total time: 12260.33s
                               ETA: 1052015.1s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.406s, learning 0.164s)
               Value function loss: 15687.8824
                    Surrogate loss: -0.0188
             Mean action noise std: 0.78
                       Mean reward: 143.05
               Mean episode length: 42.58
                  Mean reward/step: 5.38
       Mean episode length/episode: 6.95
            Mean episode successes: 0.4663
Mean episode consecutive_successes: 0.7318
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 8.57s
                        Total time: 12268.90s
                               ETA: 1051826.7s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.184s, learning 0.163s)
               Value function loss: 14805.8466
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 173.51
               Mean episode length: 43.08
                  Mean reward/step: 4.34
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4780
Mean episode consecutive_successes: 0.7209
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 8.35s
                        Total time: 12277.25s
                               ETA: 1051619.6s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.967s, learning 0.171s)
               Value function loss: 15811.6262
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 205.10
               Mean episode length: 42.78
                  Mean reward/step: 5.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4668
Mean episode consecutive_successes: 0.7301
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 8.14s
                        Total time: 12285.38s
                               ETA: 1051394.9s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.284s, learning 0.159s)
               Value function loss: 15556.5368
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 184.74
               Mean episode length: 41.18
                  Mean reward/step: 5.49
       Mean episode length/episode: 6.90
            Mean episode successes: 0.5166
Mean episode consecutive_successes: 0.7284
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 8.44s
                        Total time: 12293.83s
                               ETA: 1051196.7s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.462s, learning 0.161s)
               Value function loss: 18426.8542
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 183.57
               Mean episode length: 43.71
                  Mean reward/step: 5.47
       Mean episode length/episode: 6.79
            Mean episode successes: 0.5234
Mean episode consecutive_successes: 0.7320
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 8.62s
                        Total time: 12302.45s
                               ETA: 1051014.1s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.037s, learning 0.187s)
               Value function loss: 16203.5689
                    Surrogate loss: -0.0198
             Mean action noise std: 0.78
                       Mean reward: 238.57
               Mean episode length: 39.70
                  Mean reward/step: 5.21
       Mean episode length/episode: 6.84
            Mean episode successes: 0.5088
Mean episode consecutive_successes: 0.7536
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 8.22s
                        Total time: 12310.67s
                               ETA: 1050797.9s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.315s, learning 0.172s)
               Value function loss: 15612.8050
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 152.41
               Mean episode length: 45.80
                  Mean reward/step: 4.62
       Mean episode length/episode: 6.78
            Mean episode successes: 0.5024
Mean episode consecutive_successes: 0.7444
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 8.49s
                        Total time: 12319.16s
                               ETA: 1050604.4s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.279s, learning 0.167s)
               Value function loss: 16584.3208
                    Surrogate loss: -0.0255
             Mean action noise std: 0.78
                       Mean reward: 50.82
               Mean episode length: 39.13
                  Mean reward/step: 5.56
       Mean episode length/episode: 6.86
            Mean episode successes: 0.5337
Mean episode consecutive_successes: 0.7360
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 8.45s
                        Total time: 12327.61s
                               ETA: 1050407.7s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.276s, learning 0.168s)
               Value function loss: 15327.6924
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 134.82
               Mean episode length: 41.71
                  Mean reward/step: 4.98
       Mean episode length/episode: 6.88
            Mean episode successes: 0.5166
Mean episode consecutive_successes: 0.7485
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 8.44s
                        Total time: 12336.05s
                               ETA: 1050211.2s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.165s, learning 0.168s)
               Value function loss: 14256.1758
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 425.07
               Mean episode length: 41.83
                  Mean reward/step: 4.51
       Mean episode length/episode: 6.84
            Mean episode successes: 0.5381
Mean episode consecutive_successes: 0.7548
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 8.33s
                        Total time: 12344.38s
                               ETA: 1050005.6s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.356s, learning 0.168s)
               Value function loss: 13315.3167
                    Surrogate loss: -0.0193
             Mean action noise std: 0.78
                       Mean reward: 127.05
               Mean episode length: 41.55
                  Mean reward/step: 4.02
       Mean episode length/episode: 6.77
            Mean episode successes: 0.4634
Mean episode consecutive_successes: 0.7602
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 8.52s
                        Total time: 12352.91s
                               ETA: 1049816.5s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.268s, learning 0.186s)
               Value function loss: 15059.3740
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 127.15
               Mean episode length: 40.90
                  Mean reward/step: 4.69
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4531
Mean episode consecutive_successes: 0.7626
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 8.45s
                        Total time: 12361.36s
                               ETA: 1049621.9s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.227s, learning 0.163s)
               Value function loss: 13299.3061
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 74.91
               Mean episode length: 41.96
                  Mean reward/step: 4.36
       Mean episode length/episode: 6.92
            Mean episode successes: 0.4575
Mean episode consecutive_successes: 0.7517
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 8.39s
                        Total time: 12369.75s
                               ETA: 1049422.1s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.042s, learning 0.169s)
               Value function loss: 14807.6699
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 129.65
               Mean episode length: 41.73
                  Mean reward/step: 4.78
       Mean episode length/episode: 6.94
            Mean episode successes: 0.4751
Mean episode consecutive_successes: 0.7513
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 8.21s
                        Total time: 12377.96s
                               ETA: 1049207.5s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.452s, learning 0.171s)
               Value function loss: 13518.5630
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 150.06
               Mean episode length: 42.46
                  Mean reward/step: 4.86
       Mean episode length/episode: 6.75
            Mean episode successes: 0.4888
Mean episode consecutive_successes: 0.7408
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 8.62s
                        Total time: 12386.58s
                               ETA: 1049028.0s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.267s, learning 0.206s)
               Value function loss: 15996.8327
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 292.39
               Mean episode length: 42.37
                  Mean reward/step: 5.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.5112
Mean episode consecutive_successes: 0.7463
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 8.47s
                        Total time: 12395.06s
                               ETA: 1048836.3s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.528s, learning 0.168s)
               Value function loss: 15016.1739
                    Surrogate loss: -0.0232
             Mean action noise std: 0.78
                       Mean reward: 74.37
               Mean episode length: 41.52
                  Mean reward/step: 4.62
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4995
Mean episode consecutive_successes: 0.7400
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 8.70s
                        Total time: 12403.75s
                               ETA: 1048663.6s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.737s, learning 0.258s)
               Value function loss: 16357.8105
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 258.69
               Mean episode length: 39.80
                  Mean reward/step: 5.10
       Mean episode length/episode: 6.78
            Mean episode successes: 0.4717
Mean episode consecutive_successes: 0.7596
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 9.00s
                        Total time: 12412.75s
                               ETA: 1048516.6s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.172s)
               Value function loss: 14750.3199
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 60.28
               Mean episode length: 38.49
                  Mean reward/step: 4.76
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4702
Mean episode consecutive_successes: 0.7519
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 8.46s
                        Total time: 12421.21s
                               ETA: 1048324.4s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.551s, learning 0.162s)
               Value function loss: 17217.3106
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 204.68
               Mean episode length: 41.90
                  Mean reward/step: 5.20
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4463
Mean episode consecutive_successes: 0.7740
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 8.71s
                        Total time: 12429.92s
                               ETA: 1048154.1s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.239s, learning 0.167s)
               Value function loss: 15307.5512
                    Surrogate loss: -0.0246
             Mean action noise std: 0.78
                       Mean reward: 214.03
               Mean episode length: 40.20
                  Mean reward/step: 4.91
       Mean episode length/episode: 6.91
            Mean episode successes: 0.4683
Mean episode consecutive_successes: 0.7793
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 8.41s
                        Total time: 12438.33s
                               ETA: 1047958.2s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.071s, learning 0.175s)
               Value function loss: 16967.8209
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 126.38
               Mean episode length: 40.10
                  Mean reward/step: 5.40
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4839
Mean episode consecutive_successes: 0.7678
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 8.25s
                        Total time: 12446.57s
                               ETA: 1047749.1s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.030s, learning 0.161s)
               Value function loss: 14560.1421
                    Surrogate loss: -0.0198
             Mean action noise std: 0.78
                       Mean reward: 337.55
               Mean episode length: 41.46
                  Mean reward/step: 5.29
       Mean episode length/episode: 6.87
            Mean episode successes: 0.5215
Mean episode consecutive_successes: 0.7689
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 8.19s
                        Total time: 12454.76s
                               ETA: 1047535.7s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.274s, learning 0.208s)
               Value function loss: 15597.6176
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 200.69
               Mean episode length: 42.34
                  Mean reward/step: 5.51
       Mean episode length/episode: 6.88
            Mean episode successes: 0.5405
Mean episode consecutive_successes: 0.7572
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 8.48s
                        Total time: 12463.25s
                               ETA: 1047347.1s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.108s, learning 0.213s)
               Value function loss: 14497.6612
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 103.04
               Mean episode length: 42.88
                  Mean reward/step: 4.23
       Mean episode length/episode: 6.87
            Mean episode successes: 0.5327
Mean episode consecutive_successes: 0.7539
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 8.32s
                        Total time: 12471.57s
                               ETA: 1047145.3s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.233s, learning 0.159s)
               Value function loss: 15048.7709
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 375.34
               Mean episode length: 42.83
                  Mean reward/step: 4.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.5000
Mean episode consecutive_successes: 0.7700
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 8.39s
                        Total time: 12479.96s
                               ETA: 1046949.8s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.023s, learning 0.181s)
               Value function loss: 13684.1936
                    Surrogate loss: -0.0239
             Mean action noise std: 0.78
                       Mean reward: 426.12
               Mean episode length: 43.94
                  Mean reward/step: 4.88
       Mean episode length/episode: 6.75
            Mean episode successes: 0.4595
Mean episode consecutive_successes: 0.7758
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 8.20s
                        Total time: 12488.16s
                               ETA: 1046738.9s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.112s, learning 0.209s)
               Value function loss: 10906.0906
                    Surrogate loss: -0.0228
             Mean action noise std: 0.78
                       Mean reward: 276.69
               Mean episode length: 40.87
                  Mean reward/step: 3.86
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4346
Mean episode consecutive_successes: 0.7714
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 8.32s
                        Total time: 12496.48s
                               ETA: 1046538.2s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.995s, learning 0.163s)
               Value function loss: 11269.9629
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 81.12
               Mean episode length: 39.57
                  Mean reward/step: 3.74
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4180
Mean episode consecutive_successes: 0.7564
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 8.16s
                        Total time: 12504.64s
                               ETA: 1046324.0s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.248s, learning 0.161s)
               Value function loss: 10578.8136
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 81.26
               Mean episode length: 39.85
                  Mean reward/step: 3.42
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3887
Mean episode consecutive_successes: 0.7487
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 8.41s
                        Total time: 12513.05s
                               ETA: 1046131.3s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.650s, learning 0.160s)
               Value function loss: 13292.8115
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 111.68
               Mean episode length: 40.73
                  Mean reward/step: 4.17
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3906
Mean episode consecutive_successes: 0.7379
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 10.81s
                        Total time: 12523.86s
                               ETA: 1046139.4s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.893s, learning 0.165s)
               Value function loss: 17082.6479
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 169.65
               Mean episode length: 40.77
                  Mean reward/step: 5.47
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4756
Mean episode consecutive_successes: 0.7103
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 16.06s
                        Total time: 12539.92s
                               ETA: 1046585.4s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.944s, learning 0.183s)
               Value function loss: 12046.3400
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 264.51
               Mean episode length: 41.37
                  Mean reward/step: 4.35
       Mean episode length/episode: 6.85
            Mean episode successes: 0.5010
Mean episode consecutive_successes: 0.7037
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 16.13s
                        Total time: 12556.05s
                               ETA: 1047036.4s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.239s, learning 0.168s)
               Value function loss: 13685.6817
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 126.37
               Mean episode length: 39.90
                  Mean reward/step: 4.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.4780
Mean episode consecutive_successes: 0.6965
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 16.41s
                        Total time: 12572.45s
                               ETA: 1047510.0s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.913s, learning 0.166s)
               Value function loss: 13819.1560
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 236.57
               Mean episode length: 40.92
                  Mean reward/step: 4.20
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4580
Mean episode consecutive_successes: 0.7060
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 16.08s
                        Total time: 12588.53s
                               ETA: 1047955.5s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.708s, learning 0.164s)
               Value function loss: 14299.5661
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 180.02
               Mean episode length: 41.76
                  Mean reward/step: 4.55
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4526
Mean episode consecutive_successes: 0.7097
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 15.87s
                        Total time: 12604.40s
                               ETA: 1048383.0s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.997s, learning 0.172s)
               Value function loss: 13674.4619
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 106.46
               Mean episode length: 40.28
                  Mean reward/step: 4.35
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4707
Mean episode consecutive_successes: 0.6973
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 16.17s
                        Total time: 12620.57s
                               ETA: 1048834.3s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.166s, learning 0.166s)
               Value function loss: 12554.1667
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 174.08
               Mean episode length: 40.23
                  Mean reward/step: 4.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.4600
Mean episode consecutive_successes: 0.6891
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 16.33s
                        Total time: 12636.90s
                               ETA: 1049298.4s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.772s, learning 0.168s)
               Value function loss: 12724.6018
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 247.68
               Mean episode length: 41.56
                  Mean reward/step: 4.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4346
Mean episode consecutive_successes: 0.6986
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 15.94s
                        Total time: 12652.84s
                               ETA: 1049729.3s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.547s, learning 0.162s)
               Value function loss: 14842.0563
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 171.06
               Mean episode length: 39.59
                  Mean reward/step: 4.64
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4502
Mean episode consecutive_successes: 0.6905
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 16.71s
                        Total time: 12669.55s
                               ETA: 1050223.0s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.398s, learning 0.168s)
               Value function loss: 14407.9269
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 201.05
               Mean episode length: 38.76
                  Mean reward/step: 4.78
       Mean episode length/episode: 6.83
            Mean episode successes: 0.4678
Mean episode consecutive_successes: 0.6876
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 16.57s
                        Total time: 12686.12s
                               ETA: 1050704.1s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.213s, learning 0.173s)
               Value function loss: 11481.8050
                    Surrogate loss: -0.0246
             Mean action noise std: 0.78
                       Mean reward: 67.59
               Mean episode length: 37.49
                  Mean reward/step: 3.83
       Mean episode length/episode: 6.76
            Mean episode successes: 0.4473
Mean episode consecutive_successes: 0.6790
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 16.39s
                        Total time: 12702.50s
                               ETA: 1051169.5s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.490s, learning 0.157s)
               Value function loss: 14237.5500
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 299.29
               Mean episode length: 41.04
                  Mean reward/step: 4.27
       Mean episode length/episode: 6.91
            Mean episode successes: 0.4570
Mean episode consecutive_successes: 0.6798
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 16.65s
                        Total time: 12719.15s
                               ETA: 1051655.7s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.810s, learning 0.173s)
               Value function loss: 10924.9727
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 187.35
               Mean episode length: 41.45
                  Mean reward/step: 3.91
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4717
Mean episode consecutive_successes: 0.6708
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 15.98s
                        Total time: 12735.14s
                               ETA: 1052086.1s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.483s, learning 0.179s)
               Value function loss: 12535.3627
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 143.89
               Mean episode length: 40.22
                  Mean reward/step: 4.14
       Mean episode length/episode: 6.76
            Mean episode successes: 0.4272
Mean episode consecutive_successes: 0.6791
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 16.66s
                        Total time: 12751.80s
                               ETA: 1052571.8s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.233s, learning 0.183s)
               Value function loss: 11043.5615
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 188.65
               Mean episode length: 39.88
                  Mean reward/step: 4.00
       Mean episode length/episode: 6.83
            Mean episode successes: 0.4365
Mean episode consecutive_successes: 0.6722
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 16.42s
                        Total time: 12768.21s
                               ETA: 1053036.5s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.327s, learning 0.162s)
               Value function loss: 12419.4824
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 179.06
               Mean episode length: 40.44
                  Mean reward/step: 4.37
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4351
Mean episode consecutive_successes: 0.6657
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 16.49s
                        Total time: 12784.70s
                               ETA: 1053506.3s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.935s, learning 0.167s)
               Value function loss: 13754.5194
                    Surrogate loss: -0.0230
             Mean action noise std: 0.78
                       Mean reward: 182.01
               Mean episode length: 40.44
                  Mean reward/step: 4.33
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4463
Mean episode consecutive_successes: 0.6641
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 16.10s
                        Total time: 12800.80s
                               ETA: 1053943.5s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.281s, learning 0.208s)
               Value function loss: 13622.4889
                    Surrogate loss: -0.0231
             Mean action noise std: 0.78
                       Mean reward: 163.24
               Mean episode length: 39.12
                  Mean reward/step: 4.74
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4546
Mean episode consecutive_successes: 0.6673
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 16.49s
                        Total time: 12817.29s
                               ETA: 1054411.7s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.174s, learning 0.159s)
               Value function loss: 14855.8215
                    Surrogate loss: -0.0238
             Mean action noise std: 0.78
                       Mean reward: 148.86
               Mean episode length: 40.24
                  Mean reward/step: 5.27
       Mean episode length/episode: 6.87
            Mean episode successes: 0.5132
Mean episode consecutive_successes: 0.6566
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 16.33s
                        Total time: 12833.63s
                               ETA: 1054866.4s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.930s, learning 0.203s)
               Value function loss: 14356.9069
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 346.03
               Mean episode length: 43.72
                  Mean reward/step: 4.77
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4639
Mean episode consecutive_successes: 0.6922
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 16.13s
                        Total time: 12849.76s
                               ETA: 1055303.8s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.495s, learning 0.264s)
               Value function loss: 13979.3119
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 247.16
               Mean episode length: 41.19
                  Mean reward/step: 4.36
       Mean episode length/episode: 6.89
            Mean episode successes: 0.4551
Mean episode consecutive_successes: 0.6896
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 16.76s
                        Total time: 12866.52s
                               ETA: 1055791.8s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.894s, learning 0.260s)
               Value function loss: 15781.5201
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 281.96
               Mean episode length: 44.96
                  Mean reward/step: 4.83
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4697
Mean episode consecutive_successes: 0.6891
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 16.15s
                        Total time: 12882.67s
                               ETA: 1056229.3s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.098s, learning 0.164s)
               Value function loss: 13428.5472
                    Surrogate loss: -0.0235
             Mean action noise std: 0.78
                       Mean reward: 130.87
               Mean episode length: 43.55
                  Mean reward/step: 4.19
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4561
Mean episode consecutive_successes: 0.6885
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 16.26s
                        Total time: 12898.93s
                               ETA: 1056675.0s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.763s, learning 0.255s)
               Value function loss: 10965.6255
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 140.47
               Mean episode length: 42.33
                  Mean reward/step: 3.99
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4761
Mean episode consecutive_successes: 0.6786
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 16.02s
                        Total time: 12914.95s
                               ETA: 1057099.9s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.766s, learning 0.172s)
               Value function loss: 17965.8295
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 169.95
               Mean episode length: 41.39
                  Mean reward/step: 5.12
       Mean episode length/episode: 6.81
            Mean episode successes: 0.4653
Mean episode consecutive_successes: 0.6867
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 15.94s
                        Total time: 12930.89s
                               ETA: 1057517.6s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.025s, learning 0.203s)
               Value function loss: 14561.9579
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 148.26
               Mean episode length: 42.75
                  Mean reward/step: 5.29
       Mean episode length/episode: 6.81
            Mean episode successes: 0.5176
Mean episode consecutive_successes: 0.6800
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 16.23s
                        Total time: 12947.12s
                               ETA: 1057958.3s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.384s, learning 0.238s)
               Value function loss: 13410.7763
                    Surrogate loss: -0.0244
             Mean action noise std: 0.78
                       Mean reward: 232.93
               Mean episode length: 42.50
                  Mean reward/step: 4.37
       Mean episode length/episode: 6.89
            Mean episode successes: 0.4922
Mean episode consecutive_successes: 0.6925
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 16.62s
                        Total time: 12963.74s
                               ETA: 1058430.4s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.771s, learning 0.160s)
               Value function loss: 13968.8384
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 210.45
               Mean episode length: 42.73
                  Mean reward/step: 4.01
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4634
Mean episode consecutive_successes: 0.6984
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 15.93s
                        Total time: 12979.67s
                               ETA: 1058845.3s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.759s, learning 0.163s)
               Value function loss: 15584.7448
                    Surrogate loss: -0.0247
             Mean action noise std: 0.78
                       Mean reward: 240.11
               Mean episode length: 42.01
                  Mean reward/step: 5.47
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5249
Mean episode consecutive_successes: 0.6991
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 15.92s
                        Total time: 12995.59s
                               ETA: 1059258.8s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.921s, learning 0.173s)
               Value function loss: 16433.6285
                    Surrogate loss: -0.0221
             Mean action noise std: 0.78
                       Mean reward: 122.62
               Mean episode length: 41.58
                  Mean reward/step: 5.44
       Mean episode length/episode: 6.78
            Mean episode successes: 0.5532
Mean episode consecutive_successes: 0.6881
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 16.09s
                        Total time: 13011.69s
                               ETA: 1059685.5s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.048s, learning 0.162s)
               Value function loss: 13975.9539
                    Surrogate loss: -0.0227
             Mean action noise std: 0.78
                       Mean reward: 300.53
               Mean episode length: 41.47
                  Mean reward/step: 4.30
       Mean episode length/episode: 6.84
            Mean episode successes: 0.5112
Mean episode consecutive_successes: 0.7029
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 16.21s
                        Total time: 13027.90s
                               ETA: 1060121.0s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.012s, learning 0.245s)
               Value function loss: 13843.5245
                    Surrogate loss: -0.0236
             Mean action noise std: 0.78
                       Mean reward: 233.67
               Mean episode length: 43.93
                  Mean reward/step: 4.20
       Mean episode length/episode: 6.95
            Mean episode successes: 0.5264
Mean episode consecutive_successes: 0.7058
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 16.26s
                        Total time: 13044.15s
                               ETA: 1060559.5s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.456s, learning 0.158s)
               Value function loss: 14089.0767
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 313.47
               Mean episode length: 42.55
                  Mean reward/step: 4.50
       Mean episode length/episode: 6.86
            Mean episode successes: 0.5225
Mean episode consecutive_successes: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 16.61s
                        Total time: 13060.77s
                               ETA: 1061026.3s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.733s, learning 0.178s)
               Value function loss: 14970.3743
                    Surrogate loss: -0.0232
             Mean action noise std: 0.78
                       Mean reward: 189.30
               Mean episode length: 40.46
                  Mean reward/step: 4.54
       Mean episode length/episode: 6.76
            Mean episode successes: 0.4683
Mean episode consecutive_successes: 0.7177
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 15.91s
                        Total time: 13076.68s
                               ETA: 1061435.3s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.520s, learning 0.165s)
               Value function loss: 13550.1932
                    Surrogate loss: -0.0213
             Mean action noise std: 0.78
                       Mean reward: 121.63
               Mean episode length: 44.73
                  Mean reward/step: 4.46
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4849
Mean episode consecutive_successes: 0.7025
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 15.68s
                        Total time: 13092.36s
                               ETA: 1061825.1s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.186s, learning 0.168s)
               Value function loss: 12151.2699
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 55.05
               Mean episode length: 38.37
                  Mean reward/step: 4.35
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4805
Mean episode consecutive_successes: 0.7025
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 16.35s
                        Total time: 13108.72s
                               ETA: 1062268.6s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.153s, learning 0.160s)
               Value function loss: 16662.7936
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 164.49
               Mean episode length: 41.21
                  Mean reward/step: 5.16
       Mean episode length/episode: 6.87
            Mean episode successes: 0.5381
Mean episode consecutive_successes: 0.6971
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 16.31s
                        Total time: 13125.03s
                               ETA: 1062707.9s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.353s, learning 0.185s)
               Value function loss: 15719.6697
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 257.20
               Mean episode length: 41.41
                  Mean reward/step: 4.74
       Mean episode length/episode: 6.74
            Mean episode successes: 0.5444
Mean episode consecutive_successes: 0.6964
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 11.54s
                        Total time: 13136.57s
                               ETA: 1062760.2s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.071s, learning 0.162s)
               Value function loss: 13998.1918
                    Surrogate loss: -0.0227
             Mean action noise std: 0.78
                       Mean reward: 324.07
               Mean episode length: 44.20
                  Mean reward/step: 3.95
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4751
Mean episode consecutive_successes: 0.7195
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 8.23s
                        Total time: 13144.80s
                               ETA: 1062545.3s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.153s, learning 0.271s)
               Value function loss: 14983.8783
                    Surrogate loss: -0.0234
             Mean action noise std: 0.78
                       Mean reward: 179.33
               Mean episode length: 40.52
                  Mean reward/step: 4.62
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4277
Mean episode consecutive_successes: 0.7332
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 8.42s
                        Total time: 13153.22s
                               ETA: 1062346.1s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.166s)
               Value function loss: 15660.6019
                    Surrogate loss: -0.0237
             Mean action noise std: 0.78
                       Mean reward: 216.24
               Mean episode length: 39.88
                  Mean reward/step: 4.92
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4634
Mean episode consecutive_successes: 0.7272
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 8.39s
                        Total time: 13161.62s
                               ETA: 1062144.6s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.282s, learning 0.163s)
               Value function loss: 15968.1283
                    Surrogate loss: -0.0247
             Mean action noise std: 0.78
                       Mean reward: 131.66
               Mean episode length: 40.07
                  Mean reward/step: 5.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4956
Mean episode consecutive_successes: 0.7081
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 8.44s
                        Total time: 13170.06s
                               ETA: 1061947.7s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.163s)
               Value function loss: 14144.3134
                    Surrogate loss: -0.0244
             Mean action noise std: 0.78
                       Mean reward: 217.29
               Mean episode length: 45.52
                  Mean reward/step: 4.26
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4756
Mean episode consecutive_successes: 0.7138
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 8.48s
                        Total time: 13178.54s
                               ETA: 1061753.7s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.751s, learning 0.170s)
               Value function loss: 14068.5516
                    Surrogate loss: -0.0224
             Mean action noise std: 0.78
                       Mean reward: 136.51
               Mean episode length: 40.08
                  Mean reward/step: 4.25
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4785
Mean episode consecutive_successes: 0.7061
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 7.92s
                        Total time: 13186.46s
                               ETA: 1061515.3s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.702s, learning 0.214s)
               Value function loss: 11591.6686
                    Surrogate loss: -0.0261
             Mean action noise std: 0.78
                       Mean reward: 276.20
               Mean episode length: 43.38
                  Mean reward/step: 3.70
       Mean episode length/episode: 6.72
            Mean episode successes: 0.4800
Mean episode consecutive_successes: 0.6976
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 8.92s
                        Total time: 13195.37s
                               ETA: 1061357.2s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.337s, learning 0.269s)
               Value function loss: 10976.1573
                    Surrogate loss: -0.0238
             Mean action noise std: 0.78
                       Mean reward: 231.20
               Mean episode length: 43.14
                  Mean reward/step: 3.48
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4614
Mean episode consecutive_successes: 0.6842
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 8.61s
                        Total time: 13203.98s
                               ETA: 1061174.5s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.173s, learning 0.164s)
               Value function loss: 11607.7486
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 322.65
               Mean episode length: 41.30
                  Mean reward/step: 3.66
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4170
Mean episode consecutive_successes: 0.7142
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 8.34s
                        Total time: 13212.32s
                               ETA: 1060970.6s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.739s, learning 0.169s)
               Value function loss: 10155.8835
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 133.33
               Mean episode length: 39.15
                  Mean reward/step: 3.52
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3755
Mean episode consecutive_successes: 0.7090
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 7.91s
                        Total time: 13220.23s
                               ETA: 1060732.5s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.351s, learning 0.188s)
               Value function loss: 10761.1906
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 103.46
               Mean episode length: 38.85
                  Mean reward/step: 3.64
       Mean episode length/episode: 6.72
            Mean episode successes: 0.3755
Mean episode consecutive_successes: 0.6886
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 8.54s
                        Total time: 13228.77s
                               ETA: 1060545.4s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.100s, learning 0.163s)
               Value function loss: 12661.6820
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 184.35
               Mean episode length: 41.01
                  Mean reward/step: 4.10
       Mean episode length/episode: 6.77
            Mean episode successes: 0.3672
Mean episode consecutive_successes: 0.6966
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 8.26s
                        Total time: 13237.03s
                               ETA: 1060336.4s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.131s, learning 0.156s)
               Value function loss: 9822.7006
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 208.99
               Mean episode length: 39.95
                  Mean reward/step: 3.78
       Mean episode length/episode: 6.89
            Mean episode successes: 0.3599
Mean episode consecutive_successes: 0.6880
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 8.29s
                        Total time: 13245.32s
                               ETA: 1060129.7s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.160s, learning 0.163s)
               Value function loss: 11133.0080
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 63.58
               Mean episode length: 39.58
                  Mean reward/step: 3.48
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3662
Mean episode consecutive_successes: 0.6661
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 8.32s
                        Total time: 13253.64s
                               ETA: 1059926.1s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.309s, learning 0.195s)
               Value function loss: 11225.2527
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 69.05
               Mean episode length: 40.01
                  Mean reward/step: 3.32
       Mean episode length/episode: 6.89
            Mean episode successes: 0.3799
Mean episode consecutive_successes: 0.6498
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 8.50s
                        Total time: 13262.14s
                               ETA: 1059737.4s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.167s, learning 0.161s)
               Value function loss: 10406.7711
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 117.47
               Mean episode length: 42.13
                  Mean reward/step: 3.58
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3584
Mean episode consecutive_successes: 0.6487
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 8.33s
                        Total time: 13270.47s
                               ETA: 1059534.9s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.360s, learning 0.161s)
               Value function loss: 11833.3214
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 160.02
               Mean episode length: 41.82
                  Mean reward/step: 3.50
       Mean episode length/episode: 6.85
            Mean episode successes: 0.3486
Mean episode consecutive_successes: 0.6469
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 8.52s
                        Total time: 13278.99s
                               ETA: 1059348.2s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.946s, learning 0.161s)
               Value function loss: 12045.5210
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 66.21
               Mean episode length: 39.99
                  Mean reward/step: 3.75
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3760
Mean episode consecutive_successes: 0.6258
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 8.11s
                        Total time: 13287.10s
                               ETA: 1059128.7s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.995s, learning 0.159s)
               Value function loss: 12666.6479
                    Surrogate loss: -0.0257
             Mean action noise std: 0.78
                       Mean reward: 79.66
               Mean episode length: 38.02
                  Mean reward/step: 3.92
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4321
Mean episode consecutive_successes: 0.6073
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 8.15s
                        Total time: 13295.25s
                               ETA: 1058913.3s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.905s, learning 0.161s)
               Value function loss: 11346.9745
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 119.58
               Mean episode length: 41.40
                  Mean reward/step: 3.64
       Mean episode length/episode: 6.77
            Mean episode successes: 0.4268
Mean episode consecutive_successes: 0.6005
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 8.07s
                        Total time: 13303.32s
                               ETA: 1058691.2s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.109s, learning 0.186s)
               Value function loss: 10659.2738
                    Surrogate loss: -0.0239
             Mean action noise std: 0.78
                       Mean reward: 158.80
               Mean episode length: 40.59
                  Mean reward/step: 3.59
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4224
Mean episode consecutive_successes: 0.5990
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 8.29s
                        Total time: 13311.61s
                               ETA: 1058487.6s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.997s, learning 0.164s)
               Value function loss: 9471.9769
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 253.50
               Mean episode length: 43.62
                  Mean reward/step: 3.47
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4170
Mean episode consecutive_successes: 0.5945
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 8.16s
                        Total time: 13319.77s
                               ETA: 1058273.8s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.295s, learning 0.163s)
               Value function loss: 9613.3485
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 184.00
               Mean episode length: 40.07
                  Mean reward/step: 2.89
       Mean episode length/episode: 6.86
            Mean episode successes: 0.4004
Mean episode consecutive_successes: 0.5925
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 8.46s
                        Total time: 13328.23s
                               ETA: 1058083.8s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.164s)
               Value function loss: 8479.4164
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 285.87
               Mean episode length: 42.95
                  Mean reward/step: 3.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3740
Mean episode consecutive_successes: 0.5977
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 8.37s
                        Total time: 13336.61s
                               ETA: 1057887.5s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.871s, learning 0.162s)
               Value function loss: 8297.1719
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 92.30
               Mean episode length: 42.23
                  Mean reward/step: 2.70
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3027
Mean episode consecutive_successes: 0.5937
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 8.03s
                        Total time: 13344.64s
                               ETA: 1057664.4s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.050s, learning 0.193s)
               Value function loss: 9150.8188
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 244.37
               Mean episode length: 41.42
                  Mean reward/step: 3.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3208
Mean episode consecutive_successes: 0.5815
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 8.24s
                        Total time: 13352.88s
                               ETA: 1057458.4s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.311s, learning 0.160s)
               Value function loss: 8916.1302
                    Surrogate loss: -0.0213
             Mean action noise std: 0.78
                       Mean reward: 49.29
               Mean episode length: 41.03
                  Mean reward/step: 3.44
       Mean episode length/episode: 6.94
            Mean episode successes: 0.3491
Mean episode consecutive_successes: 0.5609
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 8.47s
                        Total time: 13361.35s
                               ETA: 1057270.6s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.072s, learning 0.158s)
               Value function loss: 11505.8464
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 183.41
               Mean episode length: 43.06
                  Mean reward/step: 4.02
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4053
Mean episode consecutive_successes: 0.5521
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 8.23s
                        Total time: 13369.58s
                               ETA: 1057064.1s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.866s, learning 0.160s)
               Value function loss: 11013.0133
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: 98.90
               Mean episode length: 40.66
                  Mean reward/step: 4.20
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4067
Mean episode consecutive_successes: 0.5493
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 8.03s
                        Total time: 13377.61s
                               ETA: 1056841.9s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.075s, learning 0.230s)
               Value function loss: 9583.0502
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 84.56
               Mean episode length: 40.88
                  Mean reward/step: 3.45
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3833
Mean episode consecutive_successes: 0.5554
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 8.30s
                        Total time: 13385.91s
                               ETA: 1056642.0s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.666s, learning 0.197s)
               Value function loss: 10909.7985
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 58.15
               Mean episode length: 38.49
                  Mean reward/step: 3.98
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4048
Mean episode consecutive_successes: 0.5476
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 8.86s
                        Total time: 13394.78s
                               ETA: 1056486.4s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.614s, learning 0.199s)
               Value function loss: 10865.6286
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 114.18
               Mean episode length: 40.62
                  Mean reward/step: 3.63
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3696
Mean episode consecutive_successes: 0.5587
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 8.81s
                        Total time: 13403.59s
                               ETA: 1056327.2s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.091s, learning 0.179s)
               Value function loss: 9454.2578
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 111.06
               Mean episode length: 38.90
                  Mean reward/step: 2.97
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3813
Mean episode consecutive_successes: 0.5444
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 8.27s
                        Total time: 13411.86s
                               ETA: 1056125.4s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.137s, learning 0.160s)
               Value function loss: 11425.8939
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 144.33
               Mean episode length: 41.13
                  Mean reward/step: 3.54
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3706
Mean episode consecutive_successes: 0.5491
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 8.30s
                        Total time: 13420.16s
                               ETA: 1055926.0s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.000s, learning 0.166s)
               Value function loss: 10086.9916
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 143.17
               Mean episode length: 42.75
                  Mean reward/step: 3.34
       Mean episode length/episode: 6.89
            Mean episode successes: 0.3516
Mean episode consecutive_successes: 0.5563
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 8.17s
                        Total time: 13428.33s
                               ETA: 1055716.6s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.143s, learning 0.165s)
               Value function loss: 10767.5079
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 194.19
               Mean episode length: 40.49
                  Mean reward/step: 3.56
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3477
Mean episode consecutive_successes: 0.5574
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 8.31s
                        Total time: 13436.63s
                               ETA: 1055518.7s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.367s, learning 0.171s)
               Value function loss: 9859.4448
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 111.01
               Mean episode length: 43.61
                  Mean reward/step: 3.43
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3535
Mean episode consecutive_successes: 0.5457
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 8.54s
                        Total time: 13445.17s
                               ETA: 1055339.1s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.254s, learning 0.166s)
               Value function loss: 8573.8751
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 219.01
               Mean episode length: 39.72
                  Mean reward/step: 2.90
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3667
Mean episode consecutive_successes: 0.5340
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 8.42s
                        Total time: 13453.59s
                               ETA: 1055150.6s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.305s, learning 0.172s)
               Value function loss: 7558.6988
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 114.97
               Mean episode length: 42.10
                  Mean reward/step: 2.54
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2983
Mean episode consecutive_successes: 0.5449
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 8.48s
                        Total time: 13462.07s
                               ETA: 1054966.7s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.107s, learning 0.232s)
               Value function loss: 8457.8775
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 120.02
               Mean episode length: 42.34
                  Mean reward/step: 3.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3286
Mean episode consecutive_successes: 0.5264
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 8.34s
                        Total time: 13470.41s
                               ETA: 1054772.4s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.371s, learning 0.159s)
               Value function loss: 8880.6479
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 213.61
               Mean episode length: 39.66
                  Mean reward/step: 2.91
       Mean episode length/episode: 6.77
            Mean episode successes: 0.2969
Mean episode consecutive_successes: 0.5329
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 8.53s
                        Total time: 13478.94s
                               ETA: 1054593.4s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.857s, learning 0.187s)
               Value function loss: 9025.1772
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 79.17
               Mean episode length: 41.43
                  Mean reward/step: 3.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2988
Mean episode consecutive_successes: 0.5178
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 8.04s
                        Total time: 13486.98s
                               ETA: 1054376.5s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.291s, learning 0.175s)
               Value function loss: 8456.9097
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 115.81
               Mean episode length: 38.94
                  Mean reward/step: 2.97
       Mean episode length/episode: 6.89
            Mean episode successes: 0.3115
Mean episode consecutive_successes: 0.5089
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 8.47s
                        Total time: 13495.45s
                               ETA: 1054193.1s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.173s, learning 0.172s)
               Value function loss: 8451.7618
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 78.78
               Mean episode length: 39.44
                  Mean reward/step: 3.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3359
Mean episode consecutive_successes: 0.4931
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 8.35s
                        Total time: 13503.79s
                               ETA: 1054000.5s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.224s, learning 0.160s)
               Value function loss: 6800.2908
                    Surrogate loss: -0.0209
             Mean action noise std: 0.78
                       Mean reward: 122.16
               Mean episode length: 41.06
                  Mean reward/step: 2.63
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3433
Mean episode consecutive_successes: 0.4822
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 8.38s
                        Total time: 13512.18s
                               ETA: 1053811.1s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.163s)
               Value function loss: 7666.4496
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 154.89
               Mean episode length: 41.35
                  Mean reward/step: 2.69
       Mean episode length/episode: 6.77
            Mean episode successes: 0.3086
Mean episode consecutive_successes: 0.4853
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 8.43s
                        Total time: 13520.61s
                               ETA: 1053625.9s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.326s, learning 0.202s)
               Value function loss: 6435.9741
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 138.17
               Mean episode length: 38.99
                  Mean reward/step: 2.13
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2959
Mean episode consecutive_successes: 0.4793
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 8.53s
                        Total time: 13529.14s
                               ETA: 1053448.4s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.312s, learning 0.185s)
               Value function loss: 7318.4432
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 122.11
               Mean episode length: 41.22
                  Mean reward/step: 2.42
       Mean episode length/episode: 6.80
            Mean episode successes: 0.2793
Mean episode consecutive_successes: 0.4694
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 8.50s
                        Total time: 13537.64s
                               ETA: 1053268.6s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.302s, learning 0.164s)
               Value function loss: 7664.0837
                    Surrogate loss: -0.0206
             Mean action noise std: 0.78
                       Mean reward: 57.84
               Mean episode length: 38.19
                  Mean reward/step: 2.45
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2910
Mean episode consecutive_successes: 0.4543
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 8.47s
                        Total time: 13546.10s
                               ETA: 1053086.8s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.169s)
               Value function loss: 7968.9969
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 183.64
               Mean episode length: 39.69
                  Mean reward/step: 3.12
       Mean episode length/episode: 6.76
            Mean episode successes: 0.3096
Mean episode consecutive_successes: 0.4547
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 8.43s
                        Total time: 13554.53s
                               ETA: 1052902.3s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.198s, learning 0.167s)
               Value function loss: 7894.9392
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 136.45
               Mean episode length: 40.04
                  Mean reward/step: 2.89
       Mean episode length/episode: 6.79
            Mean episode successes: 0.2983
Mean episode consecutive_successes: 0.4501
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 8.37s
                        Total time: 13562.90s
                               ETA: 1052713.3s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.139s, learning 0.220s)
               Value function loss: 9613.2969
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 111.33
               Mean episode length: 40.09
                  Mean reward/step: 3.35
       Mean episode length/episode: 6.90
            Mean episode successes: 0.3315
Mean episode consecutive_successes: 0.4369
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 8.36s
                        Total time: 13571.26s
                               ETA: 1052523.9s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.334s, learning 0.207s)
               Value function loss: 9969.5051
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 105.70
               Mean episode length: 39.25
                  Mean reward/step: 3.22
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3433
Mean episode consecutive_successes: 0.4396
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 8.54s
                        Total time: 13579.80s
                               ETA: 1052349.0s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.303s, learning 0.190s)
               Value function loss: 10246.2795
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 82.55
               Mean episode length: 38.14
                  Mean reward/step: 3.68
       Mean episode length/episode: 6.75
            Mean episode successes: 0.3594
Mean episode consecutive_successes: 0.4358
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 8.49s
                        Total time: 13588.29s
                               ETA: 1052170.6s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.022s, learning 0.164s)
               Value function loss: 10721.5214
                    Surrogate loss: -0.0225
             Mean action noise std: 0.78
                       Mean reward: 190.15
               Mean episode length: 41.73
                  Mean reward/step: 3.17
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3589
Mean episode consecutive_successes: 0.4447
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 8.19s
                        Total time: 13596.48s
                               ETA: 1051968.7s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.010s, learning 0.171s)
               Value function loss: 9016.0694
                    Surrogate loss: -0.0258
             Mean action noise std: 0.78
                       Mean reward: 129.35
               Mean episode length: 40.82
                  Mean reward/step: 2.94
       Mean episode length/episode: 6.91
            Mean episode successes: 0.3589
Mean episode consecutive_successes: 0.4438
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 8.18s
                        Total time: 13604.66s
                               ETA: 1051766.7s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.389s, learning 0.226s)
               Value function loss: 8142.3473
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 65.64
               Mean episode length: 38.01
                  Mean reward/step: 2.46
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3203
Mean episode consecutive_successes: 0.4424
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 8.61s
                        Total time: 13613.27s
                               ETA: 1051598.6s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.447s, learning 0.175s)
               Value function loss: 7052.8532
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 83.54
               Mean episode length: 43.24
                  Mean reward/step: 2.73
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3237
Mean episode consecutive_successes: 0.4373
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 8.62s
                        Total time: 13621.89s
                               ETA: 1051431.2s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.416s, learning 0.156s)
               Value function loss: 9082.5006
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 219.62
               Mean episode length: 41.14
                  Mean reward/step: 3.12
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3428
Mean episode consecutive_successes: 0.4383
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 8.57s
                        Total time: 13630.47s
                               ETA: 1051260.3s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.452s, learning 0.161s)
               Value function loss: 10109.5588
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 96.72
               Mean episode length: 41.50
                  Mean reward/step: 3.17
       Mean episode length/episode: 6.77
            Mean episode successes: 0.3276
Mean episode consecutive_successes: 0.4388
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 8.61s
                        Total time: 13639.08s
                               ETA: 1051092.7s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.162s, learning 0.162s)
               Value function loss: 8149.5844
                    Surrogate loss: -0.0234
             Mean action noise std: 0.78
                       Mean reward: 171.77
               Mean episode length: 39.89
                  Mean reward/step: 3.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3091
Mean episode consecutive_successes: 0.4498
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 8.32s
                        Total time: 13647.40s
                               ETA: 1050903.2s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.155s, learning 0.163s)
               Value function loss: 9320.3377
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 72.07
               Mean episode length: 41.18
                  Mean reward/step: 3.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3228
Mean episode consecutive_successes: 0.4406
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 8.32s
                        Total time: 13655.72s
                               ETA: 1050713.5s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.081s, learning 0.232s)
               Value function loss: 9034.5569
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 76.59
               Mean episode length: 36.18
                  Mean reward/step: 3.14
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3306
Mean episode consecutive_successes: 0.4418
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 8.31s
                        Total time: 13664.03s
                               ETA: 1050523.7s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.382s, learning 0.169s)
               Value function loss: 7985.1406
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 71.75
               Mean episode length: 40.62
                  Mean reward/step: 2.72
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3208
Mean episode consecutive_successes: 0.4427
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 8.55s
                        Total time: 13672.58s
                               ETA: 1050352.4s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.265s, learning 0.168s)
               Value function loss: 8088.9675
                    Surrogate loss: -0.0253
             Mean action noise std: 0.78
                       Mean reward: 66.91
               Mean episode length: 36.03
                  Mean reward/step: 2.76
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3066
Mean episode consecutive_successes: 0.4449
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 8.43s
                        Total time: 13681.02s
                               ETA: 1050172.4s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.152s, learning 0.163s)
               Value function loss: 8134.1485
                    Surrogate loss: -0.0232
             Mean action noise std: 0.78
                       Mean reward: 49.49
               Mean episode length: 37.05
                  Mean reward/step: 2.77
       Mean episode length/episode: 6.75
            Mean episode successes: 0.3130
Mean episode consecutive_successes: 0.4348
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 8.32s
                        Total time: 13689.33s
                               ETA: 1049983.6s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.932s, learning 0.170s)
               Value function loss: 8447.9213
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 103.67
               Mean episode length: 38.97
                  Mean reward/step: 2.58
       Mean episode length/episode: 6.71
            Mean episode successes: 0.2651
Mean episode consecutive_successes: 0.4442
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 8.10s
                        Total time: 13697.44s
                               ETA: 1049778.7s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.184s, learning 0.234s)
               Value function loss: 7919.3927
                    Surrogate loss: -0.0246
             Mean action noise std: 0.78
                       Mean reward: 32.61
               Mean episode length: 38.11
                  Mean reward/step: 2.88
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2593
Mean episode consecutive_successes: 0.4422
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 8.42s
                        Total time: 13705.85s
                               ETA: 1049598.3s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.984s, learning 0.175s)
               Value function loss: 7642.8250
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 181.28
               Mean episode length: 39.55
                  Mean reward/step: 2.68
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2510
Mean episode consecutive_successes: 0.4454
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 8.16s
                        Total time: 13714.01s
                               ETA: 1049398.4s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.238s, learning 0.171s)
               Value function loss: 8117.6030
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 93.54
               Mean episode length: 39.10
                  Mean reward/step: 2.65
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2578
Mean episode consecutive_successes: 0.4363
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 8.41s
                        Total time: 13722.42s
                               ETA: 1049217.8s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.310s, learning 0.200s)
               Value function loss: 6848.5273
                    Surrogate loss: -0.0215
             Mean action noise std: 0.78
                       Mean reward: 34.36
               Mean episode length: 36.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 6.71
            Mean episode successes: 0.2485
Mean episode consecutive_successes: 0.4272
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 8.51s
                        Total time: 13730.93s
                               ETA: 1049045.2s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.485s, learning 0.194s)
               Value function loss: 6296.4728
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 88.17
               Mean episode length: 37.71
                  Mean reward/step: 2.29
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2432
Mean episode consecutive_successes: 0.4228
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 8.68s
                        Total time: 13739.61s
                               ETA: 1048885.8s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.122s, learning 0.205s)
               Value function loss: 6748.1213
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 146.84
               Mean episode length: 40.68
                  Mean reward/step: 2.49
       Mean episode length/episode: 6.84
            Mean episode successes: 0.2627
Mean episode consecutive_successes: 0.4152
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 8.33s
                        Total time: 13747.94s
                               ETA: 1048699.8s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.213s, learning 0.189s)
               Value function loss: 5386.1859
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 81.00
               Mean episode length: 38.92
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2139
Mean episode consecutive_successes: 0.4165
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 8.40s
                        Total time: 13756.34s
                               ETA: 1048519.8s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.095s, learning 0.172s)
               Value function loss: 8608.6715
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 33.29
               Mean episode length: 39.22
                  Mean reward/step: 3.15
       Mean episode length/episode: 6.77
            Mean episode successes: 0.2373
Mean episode consecutive_successes: 0.4097
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 8.27s
                        Total time: 13764.61s
                               ETA: 1048329.8s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.160s, learning 0.192s)
               Value function loss: 6657.3999
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 133.91
               Mean episode length: 39.89
                  Mean reward/step: 2.66
       Mean episode length/episode: 6.81
            Mean episode successes: 0.2476
Mean episode consecutive_successes: 0.4093
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 8.35s
                        Total time: 13772.96s
                               ETA: 1048146.5s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.618s, learning 0.192s)
               Value function loss: 7819.8068
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 40.30
               Mean episode length: 38.69
                  Mean reward/step: 2.93
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2847
Mean episode consecutive_successes: 0.3915
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 8.81s
                        Total time: 13781.77s
                               ETA: 1047998.3s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.955s, learning 0.169s)
               Value function loss: 6103.1411
                    Surrogate loss: -0.0209
             Mean action noise std: 0.78
                       Mean reward: 76.60
               Mean episode length: 36.53
                  Mean reward/step: 2.40
       Mean episode length/episode: 6.71
            Mean episode successes: 0.2881
Mean episode consecutive_successes: 0.3842
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 8.12s
                        Total time: 13789.89s
                               ETA: 1047798.3s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.039s, learning 0.176s)
               Value function loss: 6187.5674
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 75.52
               Mean episode length: 38.04
                  Mean reward/step: 2.28
       Mean episode length/episode: 6.81
            Mean episode successes: 0.2700
Mean episode consecutive_successes: 0.3873
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 8.22s
                        Total time: 13798.11s
                               ETA: 1047605.4s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.845s, learning 0.213s)
               Value function loss: 5494.1341
                    Surrogate loss: -0.0241
             Mean action noise std: 0.78
                       Mean reward: 50.81
               Mean episode length: 38.76
                  Mean reward/step: 2.06
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2690
Mean episode consecutive_successes: 0.3764
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 8.06s
                        Total time: 13806.17s
                               ETA: 1047400.9s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.430s, learning 0.201s)
               Value function loss: 7156.3166
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 43.25
               Mean episode length: 39.34
                  Mean reward/step: 2.78
       Mean episode length/episode: 6.91
            Mean episode successes: 0.2412
Mean episode consecutive_successes: 0.3936
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 8.63s
                        Total time: 13814.80s
                               ETA: 1047240.1s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.263s, learning 0.185s)
               Value function loss: 6295.7222
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 199.53
               Mean episode length: 41.30
                  Mean reward/step: 2.28
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2534
Mean episode consecutive_successes: 0.3922
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 8.45s
                        Total time: 13823.24s
                               ETA: 1047065.6s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.016s, learning 0.166s)
               Value function loss: 5305.5627
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 76.02
               Mean episode length: 34.96
                  Mean reward/step: 2.19
       Mean episode length/episode: 6.79
            Mean episode successes: 0.2515
Mean episode consecutive_successes: 0.3798
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 8.18s
                        Total time: 13831.43s
                               ETA: 1046871.4s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.329s, learning 0.165s)
               Value function loss: 5333.2052
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 71.11
               Mean episode length: 39.16
                  Mean reward/step: 2.23
       Mean episode length/episode: 6.72
            Mean episode successes: 0.2217
Mean episode consecutive_successes: 0.3832
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 8.49s
                        Total time: 13839.92s
                               ETA: 1046701.0s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.124s, learning 0.161s)
               Value function loss: 6734.9302
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 140.09
               Mean episode length: 42.36
                  Mean reward/step: 2.45
       Mean episode length/episode: 6.90
            Mean episode successes: 0.2324
Mean episode consecutive_successes: 0.3836
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 8.28s
                        Total time: 13848.20s
                               ETA: 1046515.0s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.995s, learning 0.173s)
               Value function loss: 5549.1609
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 115.07
               Mean episode length: 42.23
                  Mean reward/step: 2.34
       Mean episode length/episode: 6.74
            Mean episode successes: 0.2275
Mean episode consecutive_successes: 0.3780
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 8.17s
                        Total time: 13856.37s
                               ETA: 1046320.5s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.913s, learning 0.196s)
               Value function loss: 5172.3268
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: 77.00
               Mean episode length: 40.75
                  Mean reward/step: 1.99
       Mean episode length/episode: 6.79
            Mean episode successes: 0.2119
Mean episode consecutive_successes: 0.3721
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 8.11s
                        Total time: 13864.48s
                               ETA: 1046121.8s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.610s, learning 0.173s)
               Value function loss: 7271.4748
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 111.77
               Mean episode length: 40.75
                  Mean reward/step: 2.73
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2109
Mean episode consecutive_successes: 0.3716
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 8.78s
                        Total time: 13873.27s
                               ETA: 1045974.2s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.107s, learning 0.196s)
               Value function loss: 5921.1908
                    Surrogate loss: -0.0188
             Mean action noise std: 0.78
                       Mean reward: 60.53
               Mean episode length: 38.88
                  Mean reward/step: 2.40
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2314
Mean episode consecutive_successes: 0.3655
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 8.30s
                        Total time: 13881.57s
                               ETA: 1045790.7s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.369s, learning 0.245s)
               Value function loss: 8496.0496
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 113.48
               Mean episode length: 39.25
                  Mean reward/step: 3.11
       Mean episode length/episode: 6.78
            Mean episode successes: 0.2769
Mean episode consecutive_successes: 0.3568
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 8.61s
                        Total time: 13890.18s
                               ETA: 1045630.8s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.020s, learning 0.226s)
               Value function loss: 7182.1589
                    Surrogate loss: -0.0209
             Mean action noise std: 0.78
                       Mean reward: 84.56
               Mean episode length: 37.23
                  Mean reward/step: 2.89
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3086
Mean episode consecutive_successes: 0.3453
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 8.25s
                        Total time: 13898.43s
                               ETA: 1045443.5s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.095s, learning 0.188s)
               Value function loss: 7305.5056
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 85.36
               Mean episode length: 38.23
                  Mean reward/step: 2.74
       Mean episode length/episode: 6.74
            Mean episode successes: 0.3247
Mean episode consecutive_successes: 0.3437
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 8.28s
                        Total time: 13906.71s
                               ETA: 1045259.2s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.281s, learning 0.155s)
               Value function loss: 7992.4245
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 147.33
               Mean episode length: 41.22
                  Mean reward/step: 2.51
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3101
Mean episode consecutive_successes: 0.3493
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 8.44s
                        Total time: 13915.15s
                               ETA: 1045086.8s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.356s, learning 0.185s)
               Value function loss: 8311.5411
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 139.06
               Mean episode length: 40.33
                  Mean reward/step: 3.18
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3179
Mean episode consecutive_successes: 0.3575
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 8.54s
                        Total time: 13923.69s
                               ETA: 1044922.4s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.989s, learning 0.159s)
               Value function loss: 10665.5222
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 68.84
               Mean episode length: 39.78
                  Mean reward/step: 3.72
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3330
Mean episode consecutive_successes: 0.3632
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 8.15s
                        Total time: 13931.83s
                               ETA: 1044728.8s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.376s, learning 0.170s)
               Value function loss: 8798.7241
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 131.54
               Mean episode length: 40.22
                  Mean reward/step: 3.33
       Mean episode length/episode: 6.69
            Mean episode successes: 0.3423
Mean episode consecutive_successes: 0.3661
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 8.55s
                        Total time: 13940.38s
                               ETA: 1044565.3s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.043s, learning 0.180s)
               Value function loss: 7280.6209
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 158.27
               Mean episode length: 38.96
                  Mean reward/step: 2.52
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3398
Mean episode consecutive_successes: 0.3685
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 8.22s
                        Total time: 13948.60s
                               ETA: 1044377.9s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.215s, learning 0.170s)
               Value function loss: 8968.2505
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 90.48
               Mean episode length: 38.94
                  Mean reward/step: 2.92
       Mean episode length/episode: 6.77
            Mean episode successes: 0.3379
Mean episode consecutive_successes: 0.3729
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 12.38s
                        Total time: 13960.99s
                               ETA: 1044502.1s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.212s, learning 0.170s)
               Value function loss: 9550.7158
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 229.30
               Mean episode length: 41.26
                  Mean reward/step: 3.51
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3599
Mean episode consecutive_successes: 0.3837
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 16.38s
                        Total time: 13977.37s
                               ETA: 1044925.0s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.251s, learning 0.163s)
               Value function loss: 7960.5029
                    Surrogate loss: -0.0244
             Mean action noise std: 0.78
                       Mean reward: 276.48
               Mean episode length: 39.72
                  Mean reward/step: 2.72
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3374
Mean episode consecutive_successes: 0.3922
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 16.41s
                        Total time: 13993.79s
                               ETA: 1045349.5s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.444s, learning 0.159s)
               Value function loss: 7729.5396
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 132.62
               Mean episode length: 37.65
                  Mean reward/step: 2.43
       Mean episode length/episode: 6.73
            Mean episode successes: 0.2759
Mean episode consecutive_successes: 0.4028
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 16.60s
                        Total time: 14010.39s
                               ETA: 1045787.6s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.559s, learning 0.167s)
               Value function loss: 7376.0518
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 45.53
               Mean episode length: 39.38
                  Mean reward/step: 2.86
       Mean episode length/episode: 6.71
            Mean episode successes: 0.3262
Mean episode consecutive_successes: 0.3816
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 16.73s
                        Total time: 14027.11s
                               ETA: 1046234.0s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.723s, learning 0.157s)
               Value function loss: 9149.8179
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 176.45
               Mean episode length: 40.57
                  Mean reward/step: 3.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3257
Mean episode consecutive_successes: 0.3851
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 15.88s
                        Total time: 14042.99s
                               ETA: 1046616.7s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.437s, learning 0.159s)
               Value function loss: 8576.8760
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 140.42
               Mean episode length: 38.32
                  Mean reward/step: 3.30
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3535
Mean episode consecutive_successes: 0.3855
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 16.60s
                        Total time: 14059.59s
                               ETA: 1047052.2s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.137s, learning 0.280s)
               Value function loss: 8301.2941
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 188.08
               Mean episode length: 38.73
                  Mean reward/step: 3.01
       Mean episode length/episode: 6.74
            Mean episode successes: 0.3135
Mean episode consecutive_successes: 0.4062
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 16.42s
                        Total time: 14076.01s
                               ETA: 1047473.6s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.076s, learning 0.204s)
               Value function loss: 8155.3415
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 53.03
               Mean episode length: 38.58
                  Mean reward/step: 3.13
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3086
Mean episode consecutive_successes: 0.4058
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 16.28s
                        Total time: 14092.29s
                               ETA: 1047884.2s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1010 steps/s (collection: 15.964s, learning 0.253s)
               Value function loss: 8299.1743
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 104.45
               Mean episode length: 36.96
                  Mean reward/step: 3.07
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3169
Mean episode consecutive_successes: 0.4081
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 16.22s
                        Total time: 14108.50s
                               ETA: 1048289.4s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.373s, learning 0.172s)
               Value function loss: 9458.3127
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 223.37
               Mean episode length: 38.97
                  Mean reward/step: 2.78
       Mean episode length/episode: 6.76
            Mean episode successes: 0.3232
Mean episode consecutive_successes: 0.4080
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 16.54s
                        Total time: 14125.05s
                               ETA: 1048718.4s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.688s, learning 0.258s)
               Value function loss: 6750.1882
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 43.19
               Mean episode length: 39.28
                  Mean reward/step: 2.27
       Mean episode length/episode: 6.76
            Mean episode successes: 0.2676
Mean episode consecutive_successes: 0.4122
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 15.95s
                        Total time: 14140.99s
                               ETA: 1049102.3s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1039 steps/s (collection: 15.568s, learning 0.187s)
               Value function loss: 6531.5578
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 95.82
               Mean episode length: 39.42
                  Mean reward/step: 2.24
       Mean episode length/episode: 6.87
            Mean episode successes: 0.2529
Mean episode consecutive_successes: 0.4117
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 15.75s
                        Total time: 14156.75s
                               ETA: 1049471.4s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.047s, learning 0.165s)
               Value function loss: 7889.6168
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 132.49
               Mean episode length: 42.15
                  Mean reward/step: 3.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.2739
Mean episode consecutive_successes: 0.4083
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 16.21s
                        Total time: 14172.96s
                               ETA: 1049873.8s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.296s, learning 0.163s)
               Value function loss: 7997.8524
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 87.12
               Mean episode length: 37.00
                  Mean reward/step: 2.71
       Mean episode length/episode: 6.75
            Mean episode successes: 0.2793
Mean episode consecutive_successes: 0.4001
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 16.46s
                        Total time: 14189.42s
                               ETA: 1050293.8s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.435s, learning 0.158s)
               Value function loss: 9548.2697
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 238.11
               Mean episode length: 38.23
                  Mean reward/step: 3.44
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3140
Mean episode consecutive_successes: 0.4016
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 16.59s
                        Total time: 14206.01s
                               ETA: 1050723.1s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.979s, learning 0.168s)
               Value function loss: 10936.5483
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 270.63
               Mean episode length: 42.73
                  Mean reward/step: 3.81
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3066
Mean episode consecutive_successes: 0.4105
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 16.15s
                        Total time: 14222.16s
                               ETA: 1051118.8s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.236s, learning 0.236s)
               Value function loss: 10735.0373
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 139.01
               Mean episode length: 40.32
                  Mean reward/step: 3.78
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3721
Mean episode consecutive_successes: 0.3990
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 16.47s
                        Total time: 14238.63s
                               ETA: 1051537.9s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.382s, learning 0.189s)
               Value function loss: 9099.0719
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 47.36
               Mean episode length: 37.79
                  Mean reward/step: 3.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3535
Mean episode consecutive_successes: 0.4074
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 16.57s
                        Total time: 14255.20s
                               ETA: 1051963.7s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.624s, learning 0.207s)
               Value function loss: 10460.0677
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 123.72
               Mean episode length: 40.14
                  Mean reward/step: 3.28
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3667
Mean episode consecutive_successes: 0.4143
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 15.83s
                        Total time: 14271.03s
                               ETA: 1052334.1s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.174s, learning 0.159s)
               Value function loss: 9561.4158
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 165.35
               Mean episode length: 42.31
                  Mean reward/step: 3.28
       Mean episode length/episode: 6.89
            Mean episode successes: 0.3784
Mean episode consecutive_successes: 0.4143
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 16.33s
                        Total time: 14287.37s
                               ETA: 1052741.0s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.321s, learning 0.181s)
               Value function loss: 7723.6996
                    Surrogate loss: -0.0241
             Mean action noise std: 0.78
                       Mean reward: 130.33
               Mean episode length: 38.22
                  Mean reward/step: 2.72
       Mean episode length/episode: 6.66
            Mean episode successes: 0.3066
Mean episode consecutive_successes: 0.4290
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 16.50s
                        Total time: 14303.87s
                               ETA: 1053159.7s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.834s, learning 0.180s)
               Value function loss: 9065.4361
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: 123.09
               Mean episode length: 43.14
                  Mean reward/step: 3.11
       Mean episode length/episode: 6.81
            Mean episode successes: 0.3198
Mean episode consecutive_successes: 0.4275
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 16.01s
                        Total time: 14319.88s
                               ETA: 1053541.8s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.016s, learning 0.219s)
               Value function loss: 10104.8358
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 156.74
               Mean episode length: 41.40
                  Mean reward/step: 3.53
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3203
Mean episode consecutive_successes: 0.4324
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 16.24s
                        Total time: 14336.12s
                               ETA: 1053939.7s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.291s, learning 0.205s)
               Value function loss: 10217.3156
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 76.75
               Mean episode length: 37.23
                  Mean reward/step: 3.43
       Mean episode length/episode: 6.91
            Mean episode successes: 0.3457
Mean episode consecutive_successes: 0.4275
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 16.50s
                        Total time: 14352.61s
                               ETA: 1054356.0s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.838s, learning 0.162s)
               Value function loss: 9392.1994
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 122.54
               Mean episode length: 37.65
                  Mean reward/step: 3.39
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3833
Mean episode consecutive_successes: 0.4214
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 16.00s
                        Total time: 14368.61s
                               ETA: 1054735.4s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.958s, learning 0.164s)
               Value function loss: 9290.7596
                    Surrogate loss: -0.0192
             Mean action noise std: 0.78
                       Mean reward: 164.62
               Mean episode length: 41.39
                  Mean reward/step: 3.33
       Mean episode length/episode: 6.77
            Mean episode successes: 0.3887
Mean episode consecutive_successes: 0.4247
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 16.12s
                        Total time: 14384.74s
                               ETA: 1055123.1s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.085s, learning 0.168s)
               Value function loss: 9732.1780
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 113.62
               Mean episode length: 40.16
                  Mean reward/step: 3.08
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3804
Mean episode consecutive_successes: 0.4281
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 16.25s
                        Total time: 14400.99s
                               ETA: 1055519.7s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.956s, learning 0.214s)
               Value function loss: 9428.5983
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 88.62
               Mean episode length: 39.25
                  Mean reward/step: 3.11
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3394
Mean episode consecutive_successes: 0.4480
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 16.17s
                        Total time: 14417.16s
                               ETA: 1055909.7s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.275s, learning 0.185s)
               Value function loss: 9235.5559
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 65.95
               Mean episode length: 39.38
                  Mean reward/step: 3.68
       Mean episode length/episode: 6.90
            Mean episode successes: 0.3389
Mean episode consecutive_successes: 0.4640
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 16.46s
                        Total time: 14433.62s
                               ETA: 1056320.4s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.228s, learning 0.161s)
               Value function loss: 10026.7151
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 163.77
               Mean episode length: 40.31
                  Mean reward/step: 3.64
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3452
Mean episode consecutive_successes: 0.4729
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 16.39s
                        Total time: 14450.01s
                               ETA: 1056725.2s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.118s, learning 0.156s)
               Value function loss: 11697.4863
                    Surrogate loss: -0.0230
             Mean action noise std: 0.78
                       Mean reward: 101.07
               Mean episode length: 39.57
                  Mean reward/step: 4.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.3198
Mean episode consecutive_successes: 0.4853
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 16.27s
                        Total time: 14466.28s
                               ETA: 1057120.9s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.324s, learning 0.165s)
               Value function loss: 9295.7502
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 29.68
               Mean episode length: 37.43
                  Mean reward/step: 3.73
       Mean episode length/episode: 6.78
            Mean episode successes: 0.3535
Mean episode consecutive_successes: 0.4764
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 16.49s
                        Total time: 14482.77s
                               ETA: 1057531.7s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.621s, learning 0.228s)
               Value function loss: 11184.0526
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 275.71
               Mean episode length: 43.02
                  Mean reward/step: 3.82
       Mean episode length/episode: 6.85
            Mean episode successes: 0.4043
Mean episode consecutive_successes: 0.4750
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 16.85s
                        Total time: 14499.62s
                               ETA: 1057968.2s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.234s, learning 0.160s)
               Value function loss: 9089.6861
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 117.03
               Mean episode length: 40.83
                  Mean reward/step: 3.40
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3691
Mean episode consecutive_successes: 0.4811
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 16.39s
                        Total time: 14516.01s
                               ETA: 1058370.8s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.777s, learning 0.164s)
               Value function loss: 10440.3124
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 200.04
               Mean episode length: 37.75
                  Mean reward/step: 3.59
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3613
Mean episode consecutive_successes: 0.4919
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 15.94s
                        Total time: 14531.95s
                               ETA: 1058739.8s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.949s, learning 0.165s)
               Value function loss: 10805.3917
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 265.55
               Mean episode length: 42.72
                  Mean reward/step: 3.81
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3809
Mean episode consecutive_successes: 0.4954
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 16.11s
                        Total time: 14548.07s
                               ETA: 1059120.8s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.028s, learning 0.162s)
               Value function loss: 10023.1927
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 144.33
               Mean episode length: 40.36
                  Mean reward/step: 3.78
       Mean episode length/episode: 6.80
            Mean episode successes: 0.3477
Mean episode consecutive_successes: 0.5044
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 16.19s
                        Total time: 14564.26s
                               ETA: 1059506.8s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1785 steps/s (collection: 8.997s, learning 0.177s)
               Value function loss: 9349.1690
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 34.66
               Mean episode length: 37.20
                  Mean reward/step: 2.83
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3188
Mean episode consecutive_successes: 0.5045
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 9.17s
                        Total time: 14573.43s
                               ETA: 1059382.2s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.626s, learning 0.157s)
               Value function loss: 8709.5800
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 146.22
               Mean episode length: 39.44
                  Mean reward/step: 2.96
       Mean episode length/episode: 6.84
            Mean episode successes: 0.3115
Mean episode consecutive_successes: 0.5059
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 8.78s
                        Total time: 14582.22s
                               ETA: 1059229.4s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.031s, learning 0.164s)
               Value function loss: 10661.5460
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 88.80
               Mean episode length: 40.25
                  Mean reward/step: 3.58
       Mean episode length/episode: 6.72
            Mean episode successes: 0.3130
Mean episode consecutive_successes: 0.5008
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 8.19s
                        Total time: 14590.41s
                               ETA: 1059034.0s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.307s, learning 0.232s)
               Value function loss: 11621.3481
                    Surrogate loss: -0.0206
             Mean action noise std: 0.78
                       Mean reward: 131.31
               Mean episode length: 39.40
                  Mean reward/step: 3.79
       Mean episode length/episode: 6.79
            Mean episode successes: 0.3296
Mean episode consecutive_successes: 0.5034
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 8.54s
                        Total time: 14598.95s
                               ETA: 1058863.9s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.056s, learning 0.170s)
               Value function loss: 8476.7032
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 200.35
               Mean episode length: 42.58
                  Mean reward/step: 3.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3179
Mean episode consecutive_successes: 0.5079
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 8.23s
                        Total time: 14607.18s
                               ETA: 1058671.4s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.800s, learning 0.184s)
               Value function loss: 10576.9249
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 108.30
               Mean episode length: 38.66
                  Mean reward/step: 3.40
       Mean episode length/episode: 6.87
            Mean episode successes: 0.3281
Mean episode consecutive_successes: 0.4960
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 8.98s
                        Total time: 14616.16s
                               ETA: 1058534.1s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.298s, learning 0.168s)
               Value function loss: 9580.8050
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 167.27
               Mean episode length: 41.94
                  Mean reward/step: 3.59
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3311
Mean episode consecutive_successes: 0.5032
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 8.47s
                        Total time: 14624.62s
                               ETA: 1058359.3s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.111s, learning 0.162s)
               Value function loss: 8498.5858
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 108.31
               Mean episode length: 39.29
                  Mean reward/step: 2.69
       Mean episode length/episode: 6.74
            Mean episode successes: 0.2876
Mean episode consecutive_successes: 0.5015
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 8.27s
                        Total time: 14632.90s
                               ETA: 1058170.9s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.289s, learning 0.159s)
               Value function loss: 10968.7740
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 60.25
               Mean episode length: 41.79
                  Mean reward/step: 3.81
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3306
Mean episode consecutive_successes: 0.4895
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 8.45s
                        Total time: 14641.35s
                               ETA: 1057995.5s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.174s, learning 0.249s)
               Value function loss: 10521.1774
                    Surrogate loss: -0.0213
             Mean action noise std: 0.78
                       Mean reward: 167.75
               Mean episode length: 42.36
                  Mean reward/step: 3.59
       Mean episode length/episode: 6.93
            Mean episode successes: 0.3555
Mean episode consecutive_successes: 0.4926
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 8.42s
                        Total time: 14649.77s
                               ETA: 1057818.4s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.305s, learning 0.201s)
               Value function loss: 11697.7104
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 56.01
               Mean episode length: 39.02
                  Mean reward/step: 3.86
       Mean episode length/episode: 6.85
            Mean episode successes: 0.4014
Mean episode consecutive_successes: 0.4759
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 8.51s
                        Total time: 14658.28s
                               ETA: 1057647.7s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.005s, learning 0.228s)
               Value function loss: 10944.0657
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 169.76
               Mean episode length: 41.67
                  Mean reward/step: 4.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4043
Mean episode consecutive_successes: 0.4908
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 8.23s
                        Total time: 14666.51s
                               ETA: 1057457.4s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.205s, learning 0.166s)
               Value function loss: 12007.7999
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 57.26
               Mean episode length: 37.28
                  Mean reward/step: 4.29
       Mean episode length/episode: 6.83
            Mean episode successes: 0.3848
Mean episode consecutive_successes: 0.5012
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 8.37s
                        Total time: 14674.88s
                               ETA: 1057277.4s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.330s, learning 0.181s)
               Value function loss: 11483.8707
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 86.46
               Mean episode length: 40.07
                  Mean reward/step: 4.25
       Mean episode length/episode: 6.92
            Mean episode successes: 0.4019
Mean episode consecutive_successes: 0.5103
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 8.51s
                        Total time: 14683.39s
                               ETA: 1057107.7s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.423s, learning 0.168s)
               Value function loss: 13595.1270
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 262.96
               Mean episode length: 42.40
                  Mean reward/step: 4.34
       Mean episode length/episode: 6.78
            Mean episode successes: 0.4097
Mean episode consecutive_successes: 0.5221
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 8.59s
                        Total time: 14691.98s
                               ETA: 1056944.0s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.217s, learning 0.211s)
               Value function loss: 13505.2391
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 204.70
               Mean episode length: 41.23
                  Mean reward/step: 5.16
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4331
Mean episode consecutive_successes: 0.5374
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 8.43s
                        Total time: 14700.41s
                               ETA: 1056768.8s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.260s, learning 0.161s)
               Value function loss: 15834.0216
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 172.00
               Mean episode length: 40.70
                  Mean reward/step: 5.79
       Mean episode length/episode: 6.86
            Mean episode successes: 0.5098
Mean episode consecutive_successes: 0.5386
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 8.42s
                        Total time: 14708.83s
                               ETA: 1056593.3s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.037s, learning 0.218s)
               Value function loss: 14797.8300
                    Surrogate loss: -0.0193
             Mean action noise std: 0.78
                       Mean reward: 200.00
               Mean episode length: 41.99
                  Mean reward/step: 5.22
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5010
Mean episode consecutive_successes: 0.5607
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 8.26s
                        Total time: 14717.09s
                               ETA: 1056406.2s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.256s, learning 0.162s)
               Value function loss: 13669.6046
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 196.81
               Mean episode length: 40.52
                  Mean reward/step: 4.48
       Mean episode length/episode: 6.76
            Mean episode successes: 0.4790
Mean episode consecutive_successes: 0.5733
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 8.42s
                        Total time: 14725.50s
                               ETA: 1056231.0s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.151s, learning 0.159s)
               Value function loss: 13736.5736
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 203.45
               Mean episode length: 39.26
                  Mean reward/step: 4.48
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4819
Mean episode consecutive_successes: 0.5804
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 8.31s
                        Total time: 14733.81s
                               ETA: 1056048.3s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.283s, learning 0.187s)
               Value function loss: 13584.4200
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 107.69
               Mean episode length: 37.70
                  Mean reward/step: 4.44
       Mean episode length/episode: 6.83
            Mean episode successes: 0.5010
Mean episode consecutive_successes: 0.5856
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 8.47s
                        Total time: 14742.28s
                               ETA: 1055877.3s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.036s, learning 0.166s)
               Value function loss: 12108.2218
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 226.76
               Mean episode length: 41.11
                  Mean reward/step: 3.94
       Mean episode length/episode: 6.78
            Mean episode successes: 0.4165
Mean episode consecutive_successes: 0.6152
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 8.20s
                        Total time: 14750.49s
                               ETA: 1055687.4s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.440s, learning 0.247s)
               Value function loss: 12272.8872
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 164.98
               Mean episode length: 41.75
                  Mean reward/step: 4.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4033
Mean episode consecutive_successes: 0.6238
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 8.69s
                        Total time: 14759.17s
                               ETA: 1055532.4s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.132s, learning 0.205s)
               Value function loss: 12747.1634
                    Surrogate loss: -0.0253
             Mean action noise std: 0.78
                       Mean reward: 254.92
               Mean episode length: 41.38
                  Mean reward/step: 4.17
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3970
Mean episode consecutive_successes: 0.6274
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 8.34s
                        Total time: 14767.51s
                               ETA: 1055352.6s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.406s, learning 0.184s)
               Value function loss: 12697.7802
                    Surrogate loss: -0.0247
             Mean action noise std: 0.78
                       Mean reward: 164.09
               Mean episode length: 40.40
                  Mean reward/step: 4.29
       Mean episode length/episode: 6.82
            Mean episode successes: 0.3862
Mean episode consecutive_successes: 0.6377
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 8.59s
                        Total time: 14776.10s
                               ETA: 1055191.1s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.981s, learning 0.158s)
               Value function loss: 12202.8584
                    Surrogate loss: -0.0227
             Mean action noise std: 0.78
                       Mean reward: 207.92
               Mean episode length: 43.36
                  Mean reward/step: 4.21
       Mean episode length/episode: 6.77
            Mean episode successes: 0.4009
Mean episode consecutive_successes: 0.6298
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 8.14s
                        Total time: 14784.24s
                               ETA: 1054997.7s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.903s, learning 0.157s)
               Value function loss: 13015.8195
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 225.52
               Mean episode length: 42.29
                  Mean reward/step: 4.50
       Mean episode length/episode: 6.86
            Mean episode successes: 0.3989
Mean episode consecutive_successes: 0.6370
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 8.06s
                        Total time: 14792.30s
                               ETA: 1054798.9s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.161s)
               Value function loss: 12802.8832
                    Surrogate loss: -0.0199
             Mean action noise std: 0.78
                       Mean reward: 107.39
               Mean episode length: 42.06
                  Mean reward/step: 4.23
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4111
Mean episode consecutive_successes: 0.6328
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 8.47s
                        Total time: 14800.77s
                               ETA: 1054629.8s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.364s, learning 0.161s)
               Value function loss: 11569.5026
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 226.45
               Mean episode length: 40.20
                  Mean reward/step: 4.22
       Mean episode length/episode: 6.80
            Mean episode successes: 0.4023
Mean episode consecutive_successes: 0.6437
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 8.53s
                        Total time: 14809.30s
                               ETA: 1054464.6s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.702s, learning 0.167s)
               Value function loss: 12951.4345
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: 164.23
               Mean episode length: 40.19
                  Mean reward/step: 4.32
       Mean episode length/episode: 6.88
            Mean episode successes: 0.3838
Mean episode consecutive_successes: 0.6461
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 8.87s
                        Total time: 14818.17s
                               ETA: 1054324.2s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.163s)
               Value function loss: 13967.3965
                    Surrogate loss: -0.0225
             Mean action noise std: 0.78
                       Mean reward: 185.94
               Mean episode length: 38.95
                  Mean reward/step: 4.78
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4648
Mean episode consecutive_successes: 0.6246
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 8.36s
                        Total time: 14826.53s
                               ETA: 1054148.1s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.999s, learning 0.200s)
               Value function loss: 13364.3349
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 195.62
               Mean episode length: 43.16
                  Mean reward/step: 4.43
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4727
Mean episode consecutive_successes: 0.6249
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 8.20s
                        Total time: 14834.73s
                               ETA: 1053960.4s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.262s, learning 0.242s)
               Value function loss: 12497.1423
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 116.65
               Mean episode length: 40.44
                  Mean reward/step: 4.25
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4526
Mean episode consecutive_successes: 0.6296
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 8.50s
                        Total time: 14843.23s
                               ETA: 1053794.7s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.148s, learning 0.179s)
               Value function loss: 16044.1418
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 148.21
               Mean episode length: 43.66
                  Mean reward/step: 5.44
       Mean episode length/episode: 6.85
            Mean episode successes: 0.5269
Mean episode consecutive_successes: 0.6164
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 8.33s
                        Total time: 14851.56s
                               ETA: 1053616.6s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.302s, learning 0.157s)
               Value function loss: 17350.8466
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 252.25
               Mean episode length: 41.20
                  Mean reward/step: 5.84
       Mean episode length/episode: 6.80
            Mean episode successes: 0.5728
Mean episode consecutive_successes: 0.6270
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 8.46s
                        Total time: 14860.02s
                               ETA: 1053448.2s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.596s, learning 0.189s)
               Value function loss: 13321.7410
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 182.49
               Mean episode length: 42.01
                  Mean reward/step: 5.20
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5342
Mean episode consecutive_successes: 0.6529
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 8.78s
                        Total time: 14868.80s
                               ETA: 1053303.0s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.321s, learning 0.186s)
               Value function loss: 13386.1846
                    Surrogate loss: -0.0198
             Mean action noise std: 0.78
                       Mean reward: 137.52
               Mean episode length: 41.86
                  Mean reward/step: 4.44
       Mean episode length/episode: 6.96
            Mean episode successes: 0.5332
Mean episode consecutive_successes: 0.6663
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 8.51s
                        Total time: 14877.31s
                               ETA: 1053138.4s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.080s, learning 0.163s)
               Value function loss: 11423.7649
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 140.33
               Mean episode length: 42.54
                  Mean reward/step: 3.90
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4341
Mean episode consecutive_successes: 0.6925
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 8.24s
                        Total time: 14885.55s
                               ETA: 1052955.4s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.170s, learning 0.184s)
               Value function loss: 11599.2953
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 257.44
               Mean episode length: 42.02
                  Mean reward/step: 3.94
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4458
Mean episode consecutive_successes: 0.6865
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 8.35s
                        Total time: 14893.91s
                               ETA: 1052780.4s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.406s, learning 0.157s)
               Value function loss: 12924.4337
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 217.88
               Mean episode length: 42.29
                  Mean reward/step: 4.66
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4507
Mean episode consecutive_successes: 0.6912
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 8.56s
                        Total time: 14902.47s
                               ETA: 1052620.4s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.338s, learning 0.159s)
               Value function loss: 11907.7546
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 90.60
               Mean episode length: 38.70
                  Mean reward/step: 4.48
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4395
Mean episode consecutive_successes: 0.6871
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 8.50s
                        Total time: 14910.97s
                               ETA: 1052456.0s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.151s, learning 0.157s)
               Value function loss: 12037.1819
                    Surrogate loss: -0.0203
             Mean action noise std: 0.78
                       Mean reward: 48.42
               Mean episode length: 38.88
                  Mean reward/step: 4.26
       Mean episode length/episode: 6.73
            Mean episode successes: 0.4492
Mean episode consecutive_successes: 0.6737
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 8.31s
                        Total time: 14919.28s
                               ETA: 1052278.5s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.330s, learning 0.209s)
               Value function loss: 13568.6321
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 121.82
               Mean episode length: 40.73
                  Mean reward/step: 5.11
       Mean episode length/episode: 6.83
            Mean episode successes: 0.4795
Mean episode consecutive_successes: 0.6682
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 8.54s
                        Total time: 14927.82s
                               ETA: 1052117.5s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.799s, learning 0.174s)
               Value function loss: 17571.2396
                    Surrogate loss: -0.0224
             Mean action noise std: 0.78
                       Mean reward: 260.63
               Mean episode length: 42.44
                  Mean reward/step: 5.85
       Mean episode length/episode: 6.94
            Mean episode successes: 0.5273
Mean episode consecutive_successes: 0.6881
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 7.97s
                        Total time: 14935.79s
                               ETA: 1051916.9s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.277s, learning 0.285s)
               Value function loss: 16589.6800
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 228.52
               Mean episode length: 43.30
                  Mean reward/step: 6.02
       Mean episode length/episode: 6.92
            Mean episode successes: 0.4912
Mean episode consecutive_successes: 0.7186
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 8.56s
                        Total time: 14944.35s
                               ETA: 1051758.0s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.179s, learning 0.185s)
               Value function loss: 13996.9329
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 259.96
               Mean episode length: 42.10
                  Mean reward/step: 5.10
       Mean episode length/episode: 6.78
            Mean episode successes: 0.4834
Mean episode consecutive_successes: 0.7274
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 8.36s
                        Total time: 14952.71s
                               ETA: 1051585.4s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.442s, learning 0.158s)
               Value function loss: 14907.0910
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 187.17
               Mean episode length: 41.60
                  Mean reward/step: 4.99
       Mean episode length/episode: 6.80
            Mean episode successes: 0.4888
Mean episode consecutive_successes: 0.7218
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 8.60s
                        Total time: 14961.32s
                               ETA: 1051429.6s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.870s, learning 0.159s)
               Value function loss: 13348.6769
                    Surrogate loss: -0.0198
             Mean action noise std: 0.78
                       Mean reward: 210.90
               Mean episode length: 39.52
                  Mean reward/step: 4.61
       Mean episode length/episode: 6.92
            Mean episode successes: 0.4766
Mean episode consecutive_successes: 0.7335
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 8.03s
                        Total time: 14969.34s
                               ETA: 1051233.9s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.960s, learning 0.157s)
               Value function loss: 14083.8053
                    Surrogate loss: -0.0192
             Mean action noise std: 0.78
                       Mean reward: 254.15
               Mean episode length: 39.93
                  Mean reward/step: 4.96
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4912
Mean episode consecutive_successes: 0.7332
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 8.12s
                        Total time: 14977.46s
                               ETA: 1051044.6s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.137s, learning 0.166s)
               Value function loss: 14209.8283
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 165.99
               Mean episode length: 43.51
                  Mean reward/step: 5.02
       Mean episode length/episode: 6.83
            Mean episode successes: 0.5010
Mean episode consecutive_successes: 0.7264
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 8.30s
                        Total time: 14985.76s
                               ETA: 1050868.6s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.401s, learning 0.164s)
               Value function loss: 15699.5329
                    Surrogate loss: -0.0227
             Mean action noise std: 0.78
                       Mean reward: 389.96
               Mean episode length: 41.72
                  Mean reward/step: 5.44
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4932
Mean episode consecutive_successes: 0.7518
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 8.56s
                        Total time: 14994.33s
                               ETA: 1050711.3s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.968s, learning 0.162s)
               Value function loss: 13334.1082
                    Surrogate loss: -0.0228
             Mean action noise std: 0.78
                       Mean reward: 93.37
               Mean episode length: 43.67
                  Mean reward/step: 4.18
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4141
Mean episode consecutive_successes: 0.7596
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 8.13s
                        Total time: 15002.46s
                               ETA: 1050523.7s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.220s, learning 0.170s)
               Value function loss: 14387.8301
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 130.51
               Mean episode length: 38.48
                  Mean reward/step: 5.15
       Mean episode length/episode: 6.87
            Mean episode successes: 0.4473
Mean episode consecutive_successes: 0.7535
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 8.39s
                        Total time: 15010.85s
                               ETA: 1050354.5s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.108s, learning 0.168s)
               Value function loss: 14613.0528
                    Surrogate loss: -0.0193
             Mean action noise std: 0.78
                       Mean reward: 374.45
               Mean episode length: 40.31
                  Mean reward/step: 5.26
       Mean episode length/episode: 6.88
            Mean episode successes: 0.4868
Mean episode consecutive_successes: 0.7527
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 8.28s
                        Total time: 15019.12s
                               ETA: 1050177.6s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.037s, learning 0.205s)
               Value function loss: 18757.7307
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 119.80
               Mean episode length: 41.59
                  Mean reward/step: 6.18
       Mean episode length/episode: 6.85
            Mean episode successes: 0.5620
Mean episode consecutive_successes: 0.7290
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 8.24s
                        Total time: 15027.37s
                               ETA: 1049998.6s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.305s, learning 0.183s)
               Value function loss: 17041.8515
                    Surrogate loss: -0.0262
             Mean action noise std: 0.78
                       Mean reward: 149.84
               Mean episode length: 41.27
                  Mean reward/step: 5.64
       Mean episode length/episode: 6.88
            Mean episode successes: 0.5688
Mean episode consecutive_successes: 0.7400
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 8.49s
                        Total time: 15035.85s
                               ETA: 1049837.0s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.110s, learning 0.184s)
               Value function loss: 19850.3207
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 444.75
               Mean episode length: 45.55
                  Mean reward/step: 6.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.5332
Mean episode consecutive_successes: 0.7961
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 8.29s
                        Total time: 15044.15s
                               ETA: 1049662.1s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.399s, learning 0.166s)
               Value function loss: 15106.0553
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 147.64
               Mean episode length: 37.85
                  Mean reward/step: 5.77
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5410
Mean episode consecutive_successes: 0.7861
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 8.56s
                        Total time: 15052.71s
                               ETA: 1049506.2s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.952s, learning 0.170s)
               Value function loss: 17238.2240
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 232.58
               Mean episode length: 41.94
                  Mean reward/step: 5.52
       Mean episode length/episode: 6.85
            Mean episode successes: 0.5474
Mean episode consecutive_successes: 0.7980
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 8.12s
                        Total time: 15060.83s
                               ETA: 1049319.7s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.174s, learning 0.164s)
               Value function loss: 15635.2471
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 319.37
               Mean episode length: 40.17
                  Mean reward/step: 5.87
       Mean episode length/episode: 6.84
            Mean episode successes: 0.5044
Mean episode consecutive_successes: 0.8181
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 8.34s
                        Total time: 15069.17s
                               ETA: 1049148.5s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.167s, learning 0.306s)
               Value function loss: 16356.8090
                    Surrogate loss: -0.0241
             Mean action noise std: 0.78
                       Mean reward: 286.66
               Mean episode length: 44.20
                  Mean reward/step: 5.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.4941
Mean episode consecutive_successes: 0.8222
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 8.47s
                        Total time: 15077.64s
                               ETA: 1048986.9s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.546s, learning 0.161s)
               Value function loss: 14890.3900
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 261.48
               Mean episode length: 39.48
                  Mean reward/step: 4.49
       Mean episode length/episode: 6.77
            Mean episode successes: 0.4873
Mean episode consecutive_successes: 0.8218
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 8.71s
                        Total time: 15086.35s
                               ETA: 1048841.9s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.457s, learning 0.212s)
               Value function loss: 13824.0567
                    Surrogate loss: -0.0168
             Mean action noise std: 0.78
                       Mean reward: 332.50
               Mean episode length: 41.42
                  Mean reward/step: 4.94
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4458
Mean episode consecutive_successes: 0.8354
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 8.67s
                        Total time: 15095.02s
                               ETA: 1048694.3s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.602s, learning 0.165s)
               Value function loss: 14456.8632
                    Surrogate loss: -0.0188
             Mean action noise std: 0.78
                       Mean reward: 193.03
               Mean episode length: 38.08
                  Mean reward/step: 4.91
       Mean episode length/episode: 6.92
            Mean episode successes: 0.4585
Mean episode consecutive_successes: 0.8157
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 8.77s
                        Total time: 15103.79s
                               ETA: 1048553.8s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.851s, learning 0.156s)
               Value function loss: 14600.2193
                    Surrogate loss: -0.0272
             Mean action noise std: 0.78
                       Mean reward: 277.17
               Mean episode length: 40.90
                  Mean reward/step: 5.70
       Mean episode length/episode: 6.82
            Mean episode successes: 0.4731
Mean episode consecutive_successes: 0.8129
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 8.01s
                        Total time: 15111.79s
                               ETA: 1048360.7s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.173s)
               Value function loss: 13261.8651
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 103.01
               Mean episode length: 38.06
                  Mean reward/step: 4.40
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4951
Mean episode consecutive_successes: 0.7929
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 8.53s
                        Total time: 15120.33s
                               ETA: 1048204.4s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.004s, learning 0.254s)
               Value function loss: 13731.6934
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 310.32
               Mean episode length: 41.73
                  Mean reward/step: 5.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.5049
Mean episode consecutive_successes: 0.8025
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 8.26s
                        Total time: 15128.58s
                               ETA: 1048029.2s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.036s, learning 0.166s)
               Value function loss: 16617.6106
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: 274.46
               Mean episode length: 40.35
                  Mean reward/step: 5.15
       Mean episode length/episode: 6.89
            Mean episode successes: 0.4805
Mean episode consecutive_successes: 0.8081
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 8.20s
                        Total time: 15136.79s
                               ETA: 1047850.4s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.317s, learning 0.167s)
               Value function loss: 16635.9048
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 224.75
               Mean episode length: 41.46
                  Mean reward/step: 5.38
       Mean episode length/episode: 6.92
            Mean episode successes: 0.4648
Mean episode consecutive_successes: 0.8154
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 8.48s
                        Total time: 15145.27s
                               ETA: 1047691.3s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.880s, learning 0.167s)
               Value function loss: 16076.2903
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 165.39
               Mean episode length: 42.92
                  Mean reward/step: 5.55
       Mean episode length/episode: 6.90
            Mean episode successes: 0.4512
Mean episode consecutive_successes: 0.8215
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 8.05s
                        Total time: 15153.32s
                               ETA: 1047502.2s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.939s, learning 0.165s)
               Value function loss: 15339.7393
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 155.09
               Mean episode length: 42.21
                  Mean reward/step: 5.76
       Mean episode length/episode: 6.79
            Mean episode successes: 0.4956
Mean episode consecutive_successes: 0.8041
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 8.10s
                        Total time: 15161.42s
                               ETA: 1047317.3s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.199s, learning 0.159s)
               Value function loss: 14526.1631
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 422.15
               Mean episode length: 41.36
                  Mean reward/step: 5.18
       Mean episode length/episode: 6.85
            Mean episode successes: 0.5029
Mean episode consecutive_successes: 0.8096
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 8.36s
                        Total time: 15169.78s
                               ETA: 1047150.2s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.445s, learning 0.188s)
               Value function loss: 16041.5908
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 285.33
               Mean episode length: 42.22
                  Mean reward/step: 5.75
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5000
Mean episode consecutive_successes: 0.8193
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 8.63s
                        Total time: 15178.41s
                               ETA: 1047002.3s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.796s, learning 0.162s)
               Value function loss: 17061.5512
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: 243.18
               Mean episode length: 43.03
                  Mean reward/step: 5.66
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5244
Mean episode consecutive_successes: 0.8155
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 7.96s
                        Total time: 15186.37s
                               ETA: 1046808.1s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.232s, learning 0.229s)
               Value function loss: 15886.2274
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 252.66
               Mean episode length: 42.14
                  Mean reward/step: 5.26
       Mean episode length/episode: 6.90
            Mean episode successes: 0.5151
Mean episode consecutive_successes: 0.8267
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 8.46s
                        Total time: 15194.83s
                               ETA: 1046648.8s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.215s, learning 0.163s)
               Value function loss: 17231.7332
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 159.75
               Mean episode length: 41.47
                  Mean reward/step: 5.89
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5103
Mean episode consecutive_successes: 0.8261
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 8.38s
                        Total time: 15203.21s
                               ETA: 1046483.9s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.158s)
               Value function loss: 17932.4477
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 267.56
               Mean episode length: 41.32
                  Mean reward/step: 6.60
       Mean episode length/episode: 6.81
            Mean episode successes: 0.5557
Mean episode consecutive_successes: 0.8265
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 8.30s
                        Total time: 15211.51s
                               ETA: 1046314.2s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.118s, learning 0.187s)
               Value function loss: 18766.5018
                    Surrogate loss: -0.0231
             Mean action noise std: 0.78
                       Mean reward: 365.12
               Mean episode length: 45.45
                  Mean reward/step: 6.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.5723
Mean episode consecutive_successes: 0.8414
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 8.31s
                        Total time: 15219.82s
                               ETA: 1046144.9s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.326s, learning 0.181s)
               Value function loss: 17761.1362
                    Surrogate loss: -0.0241
             Mean action noise std: 0.78
                       Mean reward: 172.40
               Mean episode length: 37.43
                  Mean reward/step: 5.87
       Mean episode length/episode: 6.81
            Mean episode successes: 0.5356
Mean episode consecutive_successes: 0.8452
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 8.51s
                        Total time: 15228.32s
                               ETA: 1045989.6s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1992 steps/s (collection: 7.972s, learning 0.252s)
               Value function loss: 15091.8506
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 163.36
               Mean episode length: 39.24
                  Mean reward/step: 5.41
       Mean episode length/episode: 6.87
            Mean episode successes: 0.5249
Mean episode consecutive_successes: 0.8585
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 8.22s
                        Total time: 15236.55s
                               ETA: 1045815.1s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.404s, learning 0.187s)
               Value function loss: 16013.6992
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 459.44
               Mean episode length: 40.73
                  Mean reward/step: 5.46
       Mean episode length/episode: 6.85
            Mean episode successes: 0.4609
Mean episode consecutive_successes: 0.8880
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 8.59s
                        Total time: 15245.14s
                               ETA: 1045666.0s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.161s, learning 0.163s)
               Value function loss: 15426.6392
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 221.14
               Mean episode length: 43.75
                  Mean reward/step: 5.24
       Mean episode length/episode: 6.80
            Mean episode successes: 0.4604
Mean episode consecutive_successes: 0.8808
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 8.32s
                        Total time: 15253.46s
                               ETA: 1045498.7s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.496s, learning 0.237s)
               Value function loss: 16667.4987
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 220.13
               Mean episode length: 42.60
                  Mean reward/step: 5.96
       Mean episode length/episode: 6.84
            Mean episode successes: 0.4878
Mean episode consecutive_successes: 0.8753
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 8.73s
                        Total time: 15262.20s
                               ETA: 1045359.7s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.286s, learning 0.155s)
               Value function loss: 18047.0554
                    Surrogate loss: -0.0203
             Mean action noise std: 0.78
                       Mean reward: 295.47
               Mean episode length: 42.90
                  Mean reward/step: 6.44
       Mean episode length/episode: 6.89
            Mean episode successes: 0.5420
Mean episode consecutive_successes: 0.8732
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 8.44s
                        Total time: 15270.64s
                               ETA: 1045201.0s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.473s, learning 0.211s)
               Value function loss: 19630.1780
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 208.47
               Mean episode length: 43.54
                  Mean reward/step: 5.95
       Mean episode length/episode: 6.84
            Mean episode successes: 0.5015
Mean episode consecutive_successes: 0.8875
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 8.68s
                        Total time: 15279.32s
                               ETA: 1045059.0s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.553s, learning 0.166s)
               Value function loss: 18699.8817
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 328.74
               Mean episode length: 43.55
                  Mean reward/step: 5.99
       Mean episode length/episode: 6.84
            Mean episode successes: 0.5127
Mean episode consecutive_successes: 0.8914
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 8.72s
                        Total time: 15288.04s
                               ETA: 1044919.6s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.934s, learning 0.175s)
               Value function loss: 18234.1968
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 153.65
               Mean episode length: 39.72
                  Mean reward/step: 5.98
       Mean episode length/episode: 6.91
            Mean episode successes: 0.5488
Mean episode consecutive_successes: 0.8839
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 8.11s
                        Total time: 15296.15s
                               ETA: 1044738.7s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.189s, learning 0.235s)
               Value function loss: 18857.4886
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 257.61
               Mean episode length: 42.16
                  Mean reward/step: 6.30
       Mean episode length/episode: 6.88
            Mean episode successes: 0.5952
Mean episode consecutive_successes: 0.8830
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 8.42s
                        Total time: 15304.58s
                               ETA: 1044579.7s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.236s, learning 0.160s)
               Value function loss: 17995.8947
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 335.55
               Mean episode length: 42.23
                  Mean reward/step: 5.54
       Mean episode length/episode: 6.89
            Mean episode successes: 0.5591
Mean episode consecutive_successes: 0.8928
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 8.40s
                        Total time: 15312.97s
                               ETA: 1044418.8s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.839s, learning 0.162s)
               Value function loss: 18438.7948
                    Surrogate loss: -0.0224
             Mean action noise std: 0.78
                       Mean reward: 152.59
               Mean episode length: 42.39
                  Mean reward/step: 5.93
       Mean episode length/episode: 6.85
            Mean episode successes: 0.5815
Mean episode consecutive_successes: 0.8836
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 8.00s
                        Total time: 15320.97s
                               ETA: 1044231.3s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.220s, learning 0.190s)
               Value function loss: 21135.1751
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 148.81
               Mean episode length: 40.78
                  Mean reward/step: 6.96
       Mean episode length/episode: 6.86
            Mean episode successes: 0.5566
Mean episode consecutive_successes: 0.9088
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 8.41s
                        Total time: 15329.38s
                               ETA: 1044071.8s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.299s, learning 0.159s)
               Value function loss: 21688.6539
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 235.22
               Mean episode length: 42.37
                  Mean reward/step: 7.69
       Mean episode length/episode: 6.82
            Mean episode successes: 0.5928
Mean episode consecutive_successes: 0.9189
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 8.46s
                        Total time: 15337.84s
                               ETA: 1043915.9s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.895s, learning 0.160s)
               Value function loss: 19790.2603
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 137.17
               Mean episode length: 41.44
                  Mean reward/step: 6.22
       Mean episode length/episode: 6.90
            Mean episode successes: 0.6421
Mean episode consecutive_successes: 0.9068
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 8.05s
                        Total time: 15345.90s
                               ETA: 1043732.7s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.068s, learning 0.167s)
               Value function loss: 20534.1101
                    Surrogate loss: -0.0228
             Mean action noise std: 0.78
                       Mean reward: 291.42
               Mean episode length: 39.67
                  Mean reward/step: 6.38
       Mean episode length/episode: 6.86
            Mean episode successes: 0.6289
Mean episode consecutive_successes: 0.9268
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 8.23s
                        Total time: 15354.13s
                               ETA: 1043562.0s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.014s, learning 0.191s)
               Value function loss: 21868.0868
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 276.66
               Mean episode length: 44.47
                  Mean reward/step: 7.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.6353
Mean episode consecutive_successes: 0.9445
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 8.20s
                        Total time: 15362.33s
                               ETA: 1043389.4s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.491s, learning 0.208s)
               Value function loss: 19443.3731
                    Surrogate loss: -0.0250
             Mean action noise std: 0.78
                       Mean reward: 260.98
               Mean episode length: 43.67
                  Mean reward/step: 6.58
       Mean episode length/episode: 6.84
            Mean episode successes: 0.6333
Mean episode consecutive_successes: 0.9537
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 10.70s
                        Total time: 15373.03s
                               ETA: 1043386.5s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.993s, learning 0.163s)
               Value function loss: 22197.6272
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 275.00
               Mean episode length: 42.00
                  Mean reward/step: 6.88
       Mean episode length/episode: 6.82
            Mean episode successes: 0.6157
Mean episode consecutive_successes: 0.9679
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 16.16s
                        Total time: 15389.19s
                               ETA: 1043753.5s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.988s, learning 0.164s)
               Value function loss: 22505.2486
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 310.28
               Mean episode length: 42.08
                  Mean reward/step: 7.30
       Mean episode length/episode: 6.81
            Mean episode successes: 0.5918
Mean episode consecutive_successes: 0.9908
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 16.15s
                        Total time: 15405.34s
                               ETA: 1044119.8s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.874s, learning 0.163s)
               Value function loss: 24106.3732
                    Surrogate loss: -0.0245
             Mean action noise std: 0.78
                       Mean reward: 308.52
               Mean episode length: 43.25
                  Mean reward/step: 7.19
       Mean episode length/episode: 6.91
            Mean episode successes: 0.5957
Mean episode consecutive_successes: 1.0087
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 16.04s
                        Total time: 15421.38s
                               ETA: 1044477.8s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.949s, learning 0.164s)
               Value function loss: 24879.6119
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 316.87
               Mean episode length: 40.97
                  Mean reward/step: 7.77
       Mean episode length/episode: 6.93
            Mean episode successes: 0.6387
Mean episode consecutive_successes: 1.0092
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 16.11s
                        Total time: 15437.49s
                               ETA: 1044840.4s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.996s, learning 0.173s)
               Value function loss: 24175.5601
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 191.61
               Mean episode length: 40.82
                  Mean reward/step: 7.86
       Mean episode length/episode: 6.85
            Mean episode successes: 0.6885
Mean episode consecutive_successes: 1.0091
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 16.17s
                        Total time: 15453.66s
                               ETA: 1045206.2s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.728s, learning 0.301s)
               Value function loss: 26532.3830
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 400.07
               Mean episode length: 42.77
                  Mean reward/step: 8.70
       Mean episode length/episode: 6.85
            Mean episode successes: 0.7290
Mean episode consecutive_successes: 1.0206
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 17.03s
                        Total time: 15470.69s
                               ETA: 1045629.7s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.025s, learning 0.241s)
               Value function loss: 26396.5931
                    Surrogate loss: -0.0209
             Mean action noise std: 0.78
                       Mean reward: 311.43
               Mean episode length: 44.69
                  Mean reward/step: 8.99
       Mean episode length/episode: 6.88
            Mean episode successes: 0.7485
Mean episode consecutive_successes: 1.0513
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 16.27s
                        Total time: 15486.95s
                               ETA: 1046001.0s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.485s, learning 0.262s)
               Value function loss: 26787.1327
                    Surrogate loss: -0.0227
             Mean action noise std: 0.78
                       Mean reward: 331.04
               Mean episode length: 43.73
                  Mean reward/step: 8.66
       Mean episode length/episode: 6.98
            Mean episode successes: 0.7651
Mean episode consecutive_successes: 1.0880
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 16.75s
                        Total time: 15503.70s
                               ETA: 1046404.3s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1008 steps/s (collection: 15.955s, learning 0.287s)
               Value function loss: 27009.3273
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 232.10
               Mean episode length: 41.03
                  Mean reward/step: 8.33
       Mean episode length/episode: 6.89
            Mean episode successes: 0.7729
Mean episode consecutive_successes: 1.1052
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 16.24s
                        Total time: 15519.94s
                               ETA: 1046772.9s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.114s, learning 0.240s)
               Value function loss: 28066.7733
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 338.68
               Mean episode length: 44.44
                  Mean reward/step: 9.22
       Mean episode length/episode: 6.87
            Mean episode successes: 0.7632
Mean episode consecutive_successes: 1.1476
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 16.35s
                        Total time: 15536.30s
                               ETA: 1047148.6s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.054s, learning 0.252s)
               Value function loss: 27502.5330
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 397.38
               Mean episode length: 45.23
                  Mean reward/step: 8.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.7275
Mean episode consecutive_successes: 1.1789
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 16.31s
                        Total time: 15552.60s
                               ETA: 1047520.5s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.919s, learning 0.258s)
               Value function loss: 24581.2965
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 339.29
               Mean episode length: 45.32
                  Mean reward/step: 6.90
       Mean episode length/episode: 6.86
            Mean episode successes: 0.6895
Mean episode consecutive_successes: 1.1880
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 16.18s
                        Total time: 15568.78s
                               ETA: 1047883.2s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.909s, learning 0.165s)
               Value function loss: 22522.5076
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 284.41
               Mean episode length: 41.36
                  Mean reward/step: 6.61
       Mean episode length/episode: 6.85
            Mean episode successes: 0.6157
Mean episode consecutive_successes: 1.2024
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 16.07s
                        Total time: 15584.86s
                               ETA: 1048238.5s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1010 steps/s (collection: 15.979s, learning 0.241s)
               Value function loss: 27286.8991
                    Surrogate loss: -0.0235
             Mean action noise std: 0.78
                       Mean reward: 281.28
               Mean episode length: 43.16
                  Mean reward/step: 8.30
       Mean episode length/episode: 6.89
            Mean episode successes: 0.6445
Mean episode consecutive_successes: 1.2028
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 16.22s
                        Total time: 15601.08s
                               ETA: 1048603.0s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.775s, learning 0.161s)
               Value function loss: 25918.5379
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 355.32
               Mean episode length: 42.52
                  Mean reward/step: 8.06
       Mean episode length/episode: 6.78
            Mean episode successes: 0.6904
Mean episode consecutive_successes: 1.1999
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 15.94s
                        Total time: 15617.01s
                               ETA: 1048948.0s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.093s, learning 0.164s)
               Value function loss: 26869.0336
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 346.85
               Mean episode length: 40.34
                  Mean reward/step: 8.23
       Mean episode length/episode: 6.93
            Mean episode successes: 0.6816
Mean episode consecutive_successes: 1.2142
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 16.26s
                        Total time: 15633.27s
                               ETA: 1049313.9s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.181s, learning 0.159s)
               Value function loss: 31335.9501
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 414.76
               Mean episode length: 40.98
                  Mean reward/step: 9.63
       Mean episode length/episode: 6.84
            Mean episode successes: 0.7637
Mean episode consecutive_successes: 1.2113
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 16.34s
                        Total time: 15649.61s
                               ETA: 1049685.0s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.018s, learning 0.163s)
               Value function loss: 30381.6428
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 398.06
               Mean episode length: 42.44
                  Mean reward/step: 9.80
       Mean episode length/episode: 6.92
            Mean episode successes: 0.7881
Mean episode consecutive_successes: 1.2355
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 16.18s
                        Total time: 15665.79s
                               ETA: 1050044.8s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.031s, learning 0.163s)
               Value function loss: 28687.1829
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 500.65
               Mean episode length: 43.01
                  Mean reward/step: 8.84
       Mean episode length/episode: 6.89
            Mean episode successes: 0.7798
Mean episode consecutive_successes: 1.2585
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 16.19s
                        Total time: 15681.98s
                               ETA: 1050405.0s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.215s, learning 0.159s)
               Value function loss: 26429.1788
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 301.80
               Mean episode length: 40.24
                  Mean reward/step: 8.30
       Mean episode length/episode: 6.82
            Mean episode successes: 0.8208
Mean episode consecutive_successes: 1.2397
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 16.37s
                        Total time: 15698.36s
                               ETA: 1050776.8s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.188s, learning 0.159s)
               Value function loss: 29676.3267
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 529.78
               Mean episode length: 40.93
                  Mean reward/step: 8.83
       Mean episode length/episode: 6.87
            Mean episode successes: 0.8188
Mean episode consecutive_successes: 1.2658
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 16.35s
                        Total time: 15714.70s
                               ETA: 1051146.3s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.430s, learning 0.161s)
               Value function loss: 29332.2801
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 372.94
               Mean episode length: 42.32
                  Mean reward/step: 8.80
       Mean episode length/episode: 6.91
            Mean episode successes: 0.7578
Mean episode consecutive_successes: 1.2866
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 16.59s
                        Total time: 15731.30s
                               ETA: 1051531.5s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.090s, learning 0.186s)
               Value function loss: 27497.1373
                    Surrogate loss: -0.0261
             Mean action noise std: 0.78
                       Mean reward: 448.18
               Mean episode length: 42.61
                  Mean reward/step: 8.48
       Mean episode length/episode: 6.83
            Mean episode successes: 0.7129
Mean episode consecutive_successes: 1.3168
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 16.28s
                        Total time: 15747.57s
                               ETA: 1051895.1s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.109s, learning 0.180s)
               Value function loss: 29741.0411
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 454.12
               Mean episode length: 44.01
                  Mean reward/step: 8.38
       Mean episode length/episode: 6.88
            Mean episode successes: 0.7324
Mean episode consecutive_successes: 1.3156
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 16.29s
                        Total time: 15763.86s
                               ETA: 1052259.1s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.991s, learning 0.163s)
               Value function loss: 25951.7271
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 350.64
               Mean episode length: 43.03
                  Mean reward/step: 8.33
       Mean episode length/episode: 6.85
            Mean episode successes: 0.7393
Mean episode consecutive_successes: 1.2992
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 16.15s
                        Total time: 15780.02s
                               ETA: 1052613.5s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.359s, learning 0.175s)
               Value function loss: 26625.9932
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 285.00
               Mean episode length: 41.84
                  Mean reward/step: 7.50
       Mean episode length/episode: 6.80
            Mean episode successes: 0.7358
Mean episode consecutive_successes: 1.2947
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 16.53s
                        Total time: 15796.55s
                               ETA: 1052992.8s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.680s, learning 0.164s)
               Value function loss: 24980.9282
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 361.30
               Mean episode length: 44.15
                  Mean reward/step: 7.17
       Mean episode length/episode: 6.78
            Mean episode successes: 0.6235
Mean episode consecutive_successes: 1.3150
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 16.84s
                        Total time: 15813.39s
                               ETA: 1053392.3s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.135s, learning 0.155s)
               Value function loss: 22662.7383
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 428.55
               Mean episode length: 43.87
                  Mean reward/step: 7.12
       Mean episode length/episode: 6.91
            Mean episode successes: 0.6602
Mean episode consecutive_successes: 1.3035
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 16.29s
                        Total time: 15829.68s
                               ETA: 1053754.3s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.131s, learning 0.186s)
               Value function loss: 27070.5668
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 398.33
               Mean episode length: 42.39
                  Mean reward/step: 9.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.7085
Mean episode consecutive_successes: 1.2971
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 16.32s
                        Total time: 15846.00s
                               ETA: 1054117.5s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.204s, learning 0.196s)
               Value function loss: 24467.8628
                    Surrogate loss: -0.0227
             Mean action noise std: 0.78
                       Mean reward: 303.16
               Mean episode length: 42.33
                  Mean reward/step: 7.89
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7261
Mean episode consecutive_successes: 1.2884
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 16.40s
                        Total time: 15862.40s
                               ETA: 1054485.8s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.264s, learning 0.162s)
               Value function loss: 23302.3283
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 315.15
               Mean episode length: 42.49
                  Mean reward/step: 7.22
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7280
Mean episode consecutive_successes: 1.2810
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 16.43s
                        Total time: 15878.83s
                               ETA: 1054855.2s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.654s, learning 0.160s)
               Value function loss: 25230.0085
                    Surrogate loss: -0.0206
             Mean action noise std: 0.78
                       Mean reward: 440.63
               Mean episode length: 42.67
                  Mean reward/step: 7.68
       Mean episode length/episode: 6.97
            Mean episode successes: 0.6519
Mean episode consecutive_successes: 1.3213
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 16.81s
                        Total time: 15895.64s
                               ETA: 1055249.9s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.172s, learning 0.170s)
               Value function loss: 25550.6061
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 211.06
               Mean episode length: 43.96
                  Mean reward/step: 7.85
       Mean episode length/episode: 6.90
            Mean episode successes: 0.6943
Mean episode consecutive_successes: 1.3004
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 16.34s
                        Total time: 15911.98s
                               ETA: 1055612.7s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.165s, learning 0.171s)
               Value function loss: 27281.6574
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 455.76
               Mean episode length: 43.31
                  Mean reward/step: 8.12
       Mean episode length/episode: 6.85
            Mean episode successes: 0.6997
Mean episode consecutive_successes: 1.3058
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 16.34s
                        Total time: 15928.32s
                               ETA: 1055974.7s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.908s, learning 0.162s)
               Value function loss: 28135.3973
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 325.87
               Mean episode length: 43.48
                  Mean reward/step: 7.37
       Mean episode length/episode: 6.81
            Mean episode successes: 0.5962
Mean episode consecutive_successes: 1.3256
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 16.07s
                        Total time: 15944.39s
                               ETA: 1056318.4s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.175s, learning 0.170s)
               Value function loss: 25061.4764
                    Surrogate loss: -0.0225
             Mean action noise std: 0.78
                       Mean reward: 389.56
               Mean episode length: 44.61
                  Mean reward/step: 7.81
       Mean episode length/episode: 6.89
            Mean episode successes: 0.6338
Mean episode consecutive_successes: 1.3192
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 16.35s
                        Total time: 15960.73s
                               ETA: 1056680.0s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.028s, learning 0.173s)
               Value function loss: 26852.1415
                    Surrogate loss: -0.0225
             Mean action noise std: 0.78
                       Mean reward: 346.38
               Mean episode length: 43.80
                  Mean reward/step: 8.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.6172
Mean episode consecutive_successes: 1.3328
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 16.20s
                        Total time: 15976.93s
                               ETA: 1057031.4s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.378s, learning 0.163s)
               Value function loss: 27464.3737
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 366.98
               Mean episode length: 41.04
                  Mean reward/step: 8.40
       Mean episode length/episode: 6.92
            Mean episode successes: 0.6611
Mean episode consecutive_successes: 1.3229
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 11.54s
                        Total time: 15988.48s
                               ETA: 1057074.3s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.370s, learning 0.236s)
               Value function loss: 27961.8623
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 227.22
               Mean episode length: 41.47
                  Mean reward/step: 8.39
       Mean episode length/episode: 6.92
            Mean episode successes: 0.6914
Mean episode consecutive_successes: 1.3089
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 8.61s
                        Total time: 15997.08s
                               ETA: 1056923.3s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.241s, learning 0.167s)
               Value function loss: 28650.8307
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 251.65
               Mean episode length: 41.29
                  Mean reward/step: 9.52
       Mean episode length/episode: 6.80
            Mean episode successes: 0.7651
Mean episode consecutive_successes: 1.3064
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 8.41s
                        Total time: 16005.49s
                               ETA: 1056759.3s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.310s, learning 0.194s)
               Value function loss: 30189.3123
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: 593.38
               Mean episode length: 46.57
                  Mean reward/step: 9.13
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8096
Mean episode consecutive_successes: 1.3200
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 8.50s
                        Total time: 16014.00s
                               ETA: 1056601.9s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.450s, learning 0.161s)
               Value function loss: 30315.9526
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 348.40
               Mean episode length: 43.56
                  Mean reward/step: 9.45
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8789
Mean episode consecutive_successes: 1.3059
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 8.61s
                        Total time: 16022.61s
                               ETA: 1056451.7s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.202s, learning 0.189s)
               Value function loss: 31382.9735
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 199.61
               Mean episode length: 41.67
                  Mean reward/step: 9.70
       Mean episode length/episode: 6.74
            Mean episode successes: 0.8062
Mean episode consecutive_successes: 1.3327
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 8.39s
                        Total time: 16031.00s
                               ETA: 1056287.2s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.848s, learning 0.170s)
               Value function loss: 30732.9942
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 350.55
               Mean episode length: 43.18
                  Mean reward/step: 8.95
       Mean episode length/episode: 6.93
            Mean episode successes: 0.7729
Mean episode consecutive_successes: 1.3790
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 8.02s
                        Total time: 16039.02s
                               ETA: 1056098.4s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.218s, learning 0.171s)
               Value function loss: 31311.4976
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 197.73
               Mean episode length: 42.56
                  Mean reward/step: 9.83
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8223
Mean episode consecutive_successes: 1.3649
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 8.39s
                        Total time: 16047.40s
                               ETA: 1055934.2s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.213s, learning 0.229s)
               Value function loss: 30725.1942
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 259.46
               Mean episode length: 40.82
                  Mean reward/step: 9.25
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7856
Mean episode consecutive_successes: 1.3909
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 8.44s
                        Total time: 16055.85s
                               ETA: 1055773.7s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.951s, learning 0.195s)
               Value function loss: 32963.1765
                    Surrogate loss: -0.0209
             Mean action noise std: 0.78
                       Mean reward: 372.89
               Mean episode length: 42.33
                  Mean reward/step: 9.22
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8159
Mean episode consecutive_successes: 1.3888
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 8.15s
                        Total time: 16063.99s
                               ETA: 1055593.9s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.128s, learning 0.164s)
               Value function loss: 31259.5270
                    Surrogate loss: -0.0230
             Mean action noise std: 0.78
                       Mean reward: 356.12
               Mean episode length: 44.05
                  Mean reward/step: 8.96
       Mean episode length/episode: 6.97
            Mean episode successes: 0.8105
Mean episode consecutive_successes: 1.4188
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 8.29s
                        Total time: 16072.28s
                               ETA: 1055424.0s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.752s, learning 0.160s)
               Value function loss: 30570.5303
                    Surrogate loss: -0.0250
             Mean action noise std: 0.78
                       Mean reward: 274.99
               Mean episode length: 41.88
                  Mean reward/step: 9.85
       Mean episode length/episode: 6.87
            Mean episode successes: 0.8237
Mean episode consecutive_successes: 1.4237
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 7.91s
                        Total time: 16080.19s
                               ETA: 1055229.3s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.537s, learning 0.168s)
               Value function loss: 28193.5945
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 406.47
               Mean episode length: 44.35
                  Mean reward/step: 9.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.8535
Mean episode consecutive_successes: 1.4210
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 8.71s
                        Total time: 16088.90s
                               ETA: 1055086.9s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.247s, learning 0.209s)
               Value function loss: 28765.5409
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 269.51
               Mean episode length: 41.42
                  Mean reward/step: 8.99
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8467
Mean episode consecutive_successes: 1.4266
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 8.46s
                        Total time: 16097.36s
                               ETA: 1054928.4s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.984s, learning 0.196s)
               Value function loss: 28698.1691
                    Surrogate loss: -0.0234
             Mean action noise std: 0.78
                       Mean reward: 329.84
               Mean episode length: 41.33
                  Mean reward/step: 9.09
       Mean episode length/episode: 6.81
            Mean episode successes: 0.8657
Mean episode consecutive_successes: 1.4183
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 8.18s
                        Total time: 16105.54s
                               ETA: 1054751.9s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.169s, learning 0.165s)
               Value function loss: 29993.1082
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 534.36
               Mean episode length: 44.25
                  Mean reward/step: 9.82
       Mean episode length/episode: 6.88
            Mean episode successes: 0.8398
Mean episode consecutive_successes: 1.4605
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 8.33s
                        Total time: 16113.87s
                               ETA: 1054585.9s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.989s, learning 0.171s)
               Value function loss: 30064.0486
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 389.90
               Mean episode length: 41.53
                  Mean reward/step: 8.65
       Mean episode length/episode: 6.88
            Mean episode successes: 0.7998
Mean episode consecutive_successes: 1.4692
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 8.16s
                        Total time: 16122.03s
                               ETA: 1054408.5s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.197s, learning 0.166s)
               Value function loss: 25841.9460
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 359.26
               Mean episode length: 40.93
                  Mean reward/step: 7.76
       Mean episode length/episode: 6.80
            Mean episode successes: 0.7412
Mean episode consecutive_successes: 1.4563
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 8.36s
                        Total time: 16130.39s
                               ETA: 1054244.7s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.939s, learning 0.247s)
               Value function loss: 22920.6878
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 496.05
               Mean episode length: 43.46
                  Mean reward/step: 7.53
       Mean episode length/episode: 6.93
            Mean episode successes: 0.7026
Mean episode consecutive_successes: 1.4773
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 8.19s
                        Total time: 16138.58s
                               ETA: 1054069.6s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.317s, learning 0.165s)
               Value function loss: 23851.7936
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 217.12
               Mean episode length: 41.33
                  Mean reward/step: 7.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.6626
Mean episode consecutive_successes: 1.4424
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 8.48s
                        Total time: 16147.06s
                               ETA: 1053914.0s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.161s)
               Value function loss: 24150.0397
                    Surrogate loss: -0.0206
             Mean action noise std: 0.78
                       Mean reward: 279.38
               Mean episode length: 41.14
                  Mean reward/step: 7.20
       Mean episode length/episode: 6.87
            Mean episode successes: 0.6704
Mean episode consecutive_successes: 1.4193
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 8.63s
                        Total time: 16155.69s
                               ETA: 1053768.5s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.826s, learning 0.167s)
               Value function loss: 25309.0234
                    Surrogate loss: -0.0228
             Mean action noise std: 0.78
                       Mean reward: 278.80
               Mean episode length: 39.66
                  Mean reward/step: 7.81
       Mean episode length/episode: 6.88
            Mean episode successes: 0.7261
Mean episode consecutive_successes: 1.3816
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 7.99s
                        Total time: 16163.69s
                               ETA: 1053581.4s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.490s, learning 0.163s)
               Value function loss: 25251.9522
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 430.68
               Mean episode length: 42.35
                  Mean reward/step: 8.20
       Mean episode length/episode: 6.90
            Mean episode successes: 0.7368
Mean episode consecutive_successes: 1.3849
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 8.65s
                        Total time: 16172.34s
                               ETA: 1053437.5s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.118s, learning 0.182s)
               Value function loss: 27809.8879
                    Surrogate loss: -0.0218
             Mean action noise std: 0.78
                       Mean reward: 310.98
               Mean episode length: 43.52
                  Mean reward/step: 7.80
       Mean episode length/episode: 6.90
            Mean episode successes: 0.7109
Mean episode consecutive_successes: 1.3868
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 8.30s
                        Total time: 16180.64s
                               ETA: 1053270.8s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.300s, learning 0.162s)
               Value function loss: 26804.9261
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 288.68
               Mean episode length: 39.95
                  Mean reward/step: 8.13
       Mean episode length/episode: 6.78
            Mean episode successes: 0.7554
Mean episode consecutive_successes: 1.3522
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 8.46s
                        Total time: 16189.10s
                               ETA: 1053114.8s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.420s, learning 0.194s)
               Value function loss: 26903.4191
                    Surrogate loss: -0.0217
             Mean action noise std: 0.78
                       Mean reward: 266.70
               Mean episode length: 40.68
                  Mean reward/step: 8.20
       Mean episode length/episode: 6.77
            Mean episode successes: 0.7725
Mean episode consecutive_successes: 1.3443
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 8.61s
                        Total time: 16197.71s
                               ETA: 1052969.0s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.008s, learning 0.161s)
               Value function loss: 26619.7047
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 354.10
               Mean episode length: 44.00
                  Mean reward/step: 7.84
       Mean episode length/episode: 6.93
            Mean episode successes: 0.7358
Mean episode consecutive_successes: 1.3604
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 8.17s
                        Total time: 16205.88s
                               ETA: 1052794.4s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.366s, learning 0.184s)
               Value function loss: 29661.8773
                    Surrogate loss: -0.0193
             Mean action noise std: 0.78
                       Mean reward: 342.83
               Mean episode length: 42.96
                  Mean reward/step: 9.22
       Mean episode length/episode: 6.97
            Mean episode successes: 0.8301
Mean episode consecutive_successes: 1.3443
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 8.55s
                        Total time: 16214.43s
                               ETA: 1052644.8s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.746s, learning 0.176s)
               Value function loss: 28757.4415
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 442.40
               Mean episode length: 45.38
                  Mean reward/step: 8.39
       Mean episode length/episode: 6.76
            Mean episode successes: 0.7798
Mean episode consecutive_successes: 1.3605
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 7.92s
                        Total time: 16222.35s
                               ETA: 1052454.6s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.976s, learning 0.167s)
               Value function loss: 27040.9590
                    Surrogate loss: -0.0224
             Mean action noise std: 0.78
                       Mean reward: 416.98
               Mean episode length: 41.54
                  Mean reward/step: 8.98
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8140
Mean episode consecutive_successes: 1.3493
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 8.14s
                        Total time: 16230.50s
                               ETA: 1052279.0s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.859s, learning 0.163s)
               Value function loss: 26052.2161
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 311.47
               Mean episode length: 39.94
                  Mean reward/step: 8.23
       Mean episode length/episode: 6.90
            Mean episode successes: 0.7515
Mean episode consecutive_successes: 1.3651
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 8.02s
                        Total time: 16238.52s
                               ETA: 1052095.8s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.165s, learning 0.205s)
               Value function loss: 29229.4991
                    Surrogate loss: -0.0227
             Mean action noise std: 0.77
                       Mean reward: 318.39
               Mean episode length: 43.77
                  Mean reward/step: 8.15
       Mean episode length/episode: 6.90
            Mean episode successes: 0.7905
Mean episode consecutive_successes: 1.3540
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 8.37s
                        Total time: 16246.89s
                               ETA: 1051935.3s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.126s, learning 0.164s)
               Value function loss: 26325.9956
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 214.26
               Mean episode length: 40.43
                  Mean reward/step: 7.78
       Mean episode length/episode: 6.81
            Mean episode successes: 0.7485
Mean episode consecutive_successes: 1.3495
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 8.29s
                        Total time: 16255.18s
                               ETA: 1051769.8s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.160s)
               Value function loss: 27664.0213
                    Surrogate loss: -0.0221
             Mean action noise std: 0.77
                       Mean reward: 343.47
               Mean episode length: 39.42
                  Mean reward/step: 8.65
       Mean episode length/episode: 6.89
            Mean episode successes: 0.7935
Mean episode consecutive_successes: 1.3401
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 8.64s
                        Total time: 16263.82s
                               ETA: 1051627.0s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.161s)
               Value function loss: 30245.1899
                    Surrogate loss: -0.0232
             Mean action noise std: 0.77
                       Mean reward: 458.56
               Mean episode length: 44.09
                  Mean reward/step: 9.09
       Mean episode length/episode: 6.77
            Mean episode successes: 0.6929
Mean episode consecutive_successes: 1.3810
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 8.13s
                        Total time: 16271.94s
                               ETA: 1051451.3s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.387s, learning 0.227s)
               Value function loss: 27576.3484
                    Surrogate loss: -0.0245
             Mean action noise std: 0.77
                       Mean reward: 266.24
               Mean episode length: 39.62
                  Mean reward/step: 7.99
       Mean episode length/episode: 6.85
            Mean episode successes: 0.7114
Mean episode consecutive_successes: 1.3636
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 8.61s
                        Total time: 16280.55s
                               ETA: 1051307.4s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.329s, learning 0.161s)
               Value function loss: 28991.1034
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 195.79
               Mean episode length: 39.00
                  Mean reward/step: 9.12
       Mean episode length/episode: 6.84
            Mean episode successes: 0.7598
Mean episode consecutive_successes: 1.3457
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 8.49s
                        Total time: 16289.04s
                               ETA: 1051155.7s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.930s, learning 0.163s)
               Value function loss: 29825.5154
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 386.65
               Mean episode length: 40.09
                  Mean reward/step: 8.59
       Mean episode length/episode: 6.85
            Mean episode successes: 0.7759
Mean episode consecutive_successes: 1.3583
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 8.09s
                        Total time: 16297.14s
                               ETA: 1050978.6s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.256s, learning 0.178s)
               Value function loss: 26812.0974
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 288.30
               Mean episode length: 39.01
                  Mean reward/step: 8.83
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7959
Mean episode consecutive_successes: 1.3408
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 8.43s
                        Total time: 16305.57s
                               ETA: 1050823.7s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.097s, learning 0.257s)
               Value function loss: 26921.4792
                    Surrogate loss: -0.0229
             Mean action noise std: 0.77
                       Mean reward: 472.88
               Mean episode length: 42.17
                  Mean reward/step: 7.70
       Mean episode length/episode: 6.88
            Mean episode successes: 0.7114
Mean episode consecutive_successes: 1.3738
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 8.35s
                        Total time: 16313.92s
                               ETA: 1050663.7s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.235s, learning 0.209s)
               Value function loss: 26916.6277
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 240.25
               Mean episode length: 42.66
                  Mean reward/step: 7.38
       Mean episode length/episode: 6.81
            Mean episode successes: 0.7227
Mean episode consecutive_successes: 1.3502
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 8.44s
                        Total time: 16322.37s
                               ETA: 1050509.8s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.146s, learning 0.202s)
               Value function loss: 25944.8876
                    Surrogate loss: -0.0223
             Mean action noise std: 0.77
                       Mean reward: 452.61
               Mean episode length: 42.01
                  Mean reward/step: 8.37
       Mean episode length/episode: 6.85
            Mean episode successes: 0.6426
Mean episode consecutive_successes: 1.3867
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 8.35s
                        Total time: 16330.72s
                               ETA: 1050349.9s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.772s, learning 0.220s)
               Value function loss: 23589.4448
                    Surrogate loss: -0.0214
             Mean action noise std: 0.77
                       Mean reward: 397.45
               Mean episode length: 41.41
                  Mean reward/step: 7.36
       Mean episode length/episode: 6.83
            Mean episode successes: 0.6304
Mean episode consecutive_successes: 1.3720
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 7.99s
                        Total time: 16338.71s
                               ETA: 1050167.3s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.009s, learning 0.321s)
               Value function loss: 25120.2946
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 258.79
               Mean episode length: 43.81
                  Mean reward/step: 7.93
       Mean episode length/episode: 6.91
            Mean episode successes: 0.6660
Mean episode consecutive_successes: 1.3479
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 8.33s
                        Total time: 16347.04s
                               ETA: 1050006.7s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.081s, learning 0.169s)
               Value function loss: 30489.1683
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 259.88
               Mean episode length: 41.65
                  Mean reward/step: 9.20
       Mean episode length/episode: 6.84
            Mean episode successes: 0.7637
Mean episode consecutive_successes: 1.3315
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 8.25s
                        Total time: 16355.29s
                               ETA: 1049841.1s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.110s, learning 0.165s)
               Value function loss: 31803.2144
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 419.18
               Mean episode length: 40.71
                  Mean reward/step: 10.15
       Mean episode length/episode: 6.90
            Mean episode successes: 0.8477
Mean episode consecutive_successes: 1.3294
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 8.28s
                        Total time: 16363.56s
                               ETA: 1049677.4s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.155s, learning 0.159s)
               Value function loss: 28677.3127
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 538.64
               Mean episode length: 43.30
                  Mean reward/step: 9.31
       Mean episode length/episode: 6.80
            Mean episode successes: 0.8569
Mean episode consecutive_successes: 1.3490
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 8.31s
                        Total time: 16371.88s
                               ETA: 1049516.3s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.820s, learning 0.165s)
               Value function loss: 29830.7182
                    Surrogate loss: -0.0257
             Mean action noise std: 0.77
                       Mean reward: 430.79
               Mean episode length: 42.55
                  Mean reward/step: 8.41
       Mean episode length/episode: 6.93
            Mean episode successes: 0.7573
Mean episode consecutive_successes: 1.3900
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 7.98s
                        Total time: 16379.86s
                               ETA: 1049334.3s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.421s, learning 0.158s)
               Value function loss: 28562.7705
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 346.64
               Mean episode length: 40.51
                  Mean reward/step: 8.72
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8140
Mean episode consecutive_successes: 1.3727
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 8.58s
                        Total time: 16388.44s
                               ETA: 1049190.7s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.689s, learning 0.167s)
               Value function loss: 29732.5412
                    Surrogate loss: -0.0255
             Mean action noise std: 0.77
                       Mean reward: 672.50
               Mean episode length: 45.08
                  Mean reward/step: 9.50
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8242
Mean episode consecutive_successes: 1.3946
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 7.86s
                        Total time: 16396.30s
                               ETA: 1049000.9s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.058s, learning 0.202s)
               Value function loss: 31404.8671
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 647.90
               Mean episode length: 46.52
                  Mean reward/step: 10.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.8433
Mean episode consecutive_successes: 1.4092
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 8.26s
                        Total time: 16404.56s
                               ETA: 1048837.1s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.219s, learning 0.231s)
               Value function loss: 29536.5757
                    Surrogate loss: -0.0222
             Mean action noise std: 0.77
                       Mean reward: 400.75
               Mean episode length: 42.78
                  Mean reward/step: 9.13
       Mean episode length/episode: 6.88
            Mean episode successes: 0.8301
Mean episode consecutive_successes: 1.4240
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 8.45s
                        Total time: 16413.01s
                               ETA: 1048685.8s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.081s, learning 0.163s)
               Value function loss: 29179.4947
                    Surrogate loss: -0.0212
             Mean action noise std: 0.77
                       Mean reward: 600.48
               Mean episode length: 42.46
                  Mean reward/step: 8.79
       Mean episode length/episode: 6.92
            Mean episode successes: 0.7563
Mean episode consecutive_successes: 1.4482
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 8.24s
                        Total time: 16421.25s
                               ETA: 1048521.5s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.072s, learning 0.171s)
               Value function loss: 32931.4703
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 224.42
               Mean episode length: 41.04
                  Mean reward/step: 10.00
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8496
Mean episode consecutive_successes: 1.4266
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 8.24s
                        Total time: 16429.50s
                               ETA: 1048357.3s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.112s, learning 0.192s)
               Value function loss: 34647.7602
                    Surrogate loss: -0.0210
             Mean action noise std: 0.77
                       Mean reward: 179.65
               Mean episode length: 40.96
                  Mean reward/step: 8.96
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8403
Mean episode consecutive_successes: 1.4290
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 8.30s
                        Total time: 16437.80s
                               ETA: 1048197.2s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.350s, learning 0.186s)
               Value function loss: 30385.5802
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 630.58
               Mean episode length: 47.15
                  Mean reward/step: 8.57
       Mean episode length/episode: 6.83
            Mean episode successes: 0.7227
Mean episode consecutive_successes: 1.4761
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 8.54s
                        Total time: 16446.34s
                               ETA: 1048052.1s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.039s, learning 0.289s)
               Value function loss: 25868.9022
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 342.51
               Mean episode length: 41.43
                  Mean reward/step: 8.83
       Mean episode length/episode: 6.94
            Mean episode successes: 0.6899
Mean episode consecutive_successes: 1.5010
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 8.33s
                        Total time: 16454.66s
                               ETA: 1047893.9s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.580s, learning 0.192s)
               Value function loss: 28751.8265
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 336.26
               Mean episode length: 43.63
                  Mean reward/step: 8.91
       Mean episode length/episode: 6.83
            Mean episode successes: 0.7788
Mean episode consecutive_successes: 1.4502
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 8.77s
                        Total time: 16463.44s
                               ETA: 1047764.2s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.910s, learning 0.183s)
               Value function loss: 27585.7623
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 287.96
               Mean episode length: 42.70
                  Mean reward/step: 8.90
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8379
Mean episode consecutive_successes: 1.4395
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 8.09s
                        Total time: 16471.53s
                               ETA: 1047591.4s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.386s, learning 0.206s)
               Value function loss: 33091.9407
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 355.76
               Mean episode length: 43.34
                  Mean reward/step: 9.79
       Mean episode length/episode: 6.80
            Mean episode successes: 0.7861
Mean episode consecutive_successes: 1.4550
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 8.59s
                        Total time: 16480.12s
                               ETA: 1047450.5s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.244s, learning 0.235s)
               Value function loss: 33244.4659
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 377.95
               Mean episode length: 42.28
                  Mean reward/step: 9.88
       Mean episode length/episode: 6.90
            Mean episode successes: 0.8447
Mean episode consecutive_successes: 1.4520
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 8.48s
                        Total time: 16488.60s
                               ETA: 1047302.6s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.148s, learning 0.169s)
               Value function loss: 35030.7317
                    Surrogate loss: -0.0200
             Mean action noise std: 0.77
                       Mean reward: 402.60
               Mean episode length: 41.56
                  Mean reward/step: 10.45
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8691
Mean episode consecutive_successes: 1.4723
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 8.32s
                        Total time: 16496.92s
                               ETA: 1047144.7s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.025s, learning 0.162s)
               Value function loss: 33210.2552
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 228.48
               Mean episode length: 39.38
                  Mean reward/step: 8.84
       Mean episode length/episode: 6.91
            Mean episode successes: 0.8579
Mean episode consecutive_successes: 1.4657
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 8.19s
                        Total time: 16505.10s
                               ETA: 1046978.7s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.048s, learning 0.182s)
               Value function loss: 30707.2100
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 624.56
               Mean episode length: 45.00
                  Mean reward/step: 8.90
       Mean episode length/episode: 6.92
            Mean episode successes: 0.9165
Mean episode consecutive_successes: 1.4803
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 8.23s
                        Total time: 16513.33s
                               ETA: 1046815.7s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.902s, learning 0.257s)
               Value function loss: 32875.9945
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 528.96
               Mean episode length: 44.21
                  Mean reward/step: 9.26
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8013
Mean episode consecutive_successes: 1.5174
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 8.16s
                        Total time: 16521.49s
                               ETA: 1046648.3s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.961s, learning 0.165s)
               Value function loss: 32703.4667
                    Surrogate loss: -0.0228
             Mean action noise std: 0.77
                       Mean reward: 190.10
               Mean episode length: 42.50
                  Mean reward/step: 9.20
       Mean episode length/episode: 6.83
            Mean episode successes: 0.8735
Mean episode consecutive_successes: 1.4660
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 8.13s
                        Total time: 16529.62s
                               ETA: 1046479.1s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.318s, learning 0.166s)
               Value function loss: 34296.9156
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 344.97
               Mean episode length: 42.19
                  Mean reward/step: 9.31
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8755
Mean episode consecutive_successes: 1.4838
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 8.48s
                        Total time: 16538.10s
                               ETA: 1046332.6s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.032s, learning 0.186s)
               Value function loss: 31273.3720
                    Surrogate loss: -0.0223
             Mean action noise std: 0.77
                       Mean reward: 570.07
               Mean episode length: 45.86
                  Mean reward/step: 8.99
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8164
Mean episode consecutive_successes: 1.5219
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 8.22s
                        Total time: 16546.32s
                               ETA: 1046169.6s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.808s, learning 0.165s)
               Value function loss: 31559.2075
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 389.66
               Mean episode length: 41.07
                  Mean reward/step: 8.87
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8408
Mean episode consecutive_successes: 1.4983
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 7.97s
                        Total time: 16554.29s
                               ETA: 1045991.3s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.177s, learning 0.276s)
               Value function loss: 32807.8310
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 390.54
               Mean episode length: 42.59
                  Mean reward/step: 9.13
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7866
Mean episode consecutive_successes: 1.5093
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 8.45s
                        Total time: 16562.75s
                               ETA: 1045843.5s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.246s, learning 0.179s)
               Value function loss: 29069.6221
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 378.05
               Mean episode length: 42.83
                  Mean reward/step: 9.23
       Mean episode length/episode: 6.92
            Mean episode successes: 0.8472
Mean episode consecutive_successes: 1.4976
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 8.43s
                        Total time: 16571.17s
                               ETA: 1045694.1s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.246s, learning 0.162s)
               Value function loss: 31407.1393
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 368.29
               Mean episode length: 43.82
                  Mean reward/step: 10.71
       Mean episode length/episode: 6.91
            Mean episode successes: 0.8828
Mean episode consecutive_successes: 1.5045
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 8.41s
                        Total time: 16579.58s
                               ETA: 1045543.8s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.862s, learning 0.190s)
               Value function loss: 35464.9711
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 358.68
               Mean episode length: 43.34
                  Mean reward/step: 10.76
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8906
Mean episode consecutive_successes: 1.5233
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 8.05s
                        Total time: 16587.63s
                               ETA: 1045371.3s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.888s, learning 0.194s)
               Value function loss: 33929.3344
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 528.35
               Mean episode length: 43.24
                  Mean reward/step: 10.86
       Mean episode length/episode: 6.92
            Mean episode successes: 0.9673
Mean episode consecutive_successes: 1.5224
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 8.08s
                        Total time: 16595.72s
                               ETA: 1045200.9s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.788s, learning 0.231s)
               Value function loss: 37759.2124
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 456.43
               Mean episode length: 44.38
                  Mean reward/step: 11.61
       Mean episode length/episode: 6.92
            Mean episode successes: 0.9917
Mean episode consecutive_successes: 1.5445
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 9.02s
                        Total time: 16604.73s
                               ETA: 1045089.6s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.406s, learning 0.185s)
               Value function loss: 31854.1654
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 450.01
               Mean episode length: 41.43
                  Mean reward/step: 9.93
       Mean episode length/episode: 6.81
            Mean episode successes: 0.9712
Mean episode consecutive_successes: 1.5400
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 8.59s
                        Total time: 16613.32s
                               ETA: 1044951.6s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.061s, learning 0.161s)
               Value function loss: 27497.0531
                    Surrogate loss: -0.0262
             Mean action noise std: 0.77
                       Mean reward: 369.93
               Mean episode length: 42.53
                  Mean reward/step: 8.54
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8906
Mean episode consecutive_successes: 1.5587
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 8.22s
                        Total time: 16621.55s
                               ETA: 1044790.5s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.161s)
               Value function loss: 30029.1092
                    Surrogate loss: -0.0274
             Mean action noise std: 0.77
                       Mean reward: 391.57
               Mean episode length: 44.25
                  Mean reward/step: 8.55
       Mean episode length/episode: 6.87
            Mean episode successes: 0.8936
Mean episode consecutive_successes: 1.5475
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 8.36s
                        Total time: 16629.91s
                               ETA: 1044638.3s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.953s, learning 0.187s)
               Value function loss: 32515.6165
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 618.83
               Mean episode length: 43.84
                  Mean reward/step: 8.28
       Mean episode length/episode: 6.84
            Mean episode successes: 0.8711
Mean episode consecutive_successes: 1.5546
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 8.14s
                        Total time: 16638.05s
                               ETA: 1044472.5s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.145s, learning 0.214s)
               Value function loss: 33809.2562
                    Surrogate loss: -0.0198
             Mean action noise std: 0.77
                       Mean reward: 453.50
               Mean episode length: 43.36
                  Mean reward/step: 10.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.7881
Mean episode consecutive_successes: 1.5699
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 8.36s
                        Total time: 16646.41s
                               ETA: 1044320.6s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.400s, learning 0.158s)
               Value function loss: 29224.3329
                    Surrogate loss: -0.0233
             Mean action noise std: 0.77
                       Mean reward: 155.09
               Mean episode length: 42.64
                  Mean reward/step: 9.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.8354
Mean episode consecutive_successes: 1.5454
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 8.56s
                        Total time: 16654.96s
                               ETA: 1044181.3s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.901s, learning 0.162s)
               Value function loss: 29571.2231
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 440.75
               Mean episode length: 43.34
                  Mean reward/step: 9.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.8350
Mean episode consecutive_successes: 1.5659
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 8.06s
                        Total time: 16663.03s
                               ETA: 1044011.2s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.230s, learning 0.173s)
               Value function loss: 29527.9964
                    Surrogate loss: -0.0242
             Mean action noise std: 0.77
                       Mean reward: 460.80
               Mean episode length: 39.25
                  Mean reward/step: 9.34
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8442
Mean episode consecutive_successes: 1.5424
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 8.40s
                        Total time: 16671.43s
                               ETA: 1043862.6s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.075s, learning 0.164s)
               Value function loss: 35110.1658
                    Surrogate loss: -0.0220
             Mean action noise std: 0.77
                       Mean reward: 560.50
               Mean episode length: 43.11
                  Mean reward/step: 10.03
       Mean episode length/episode: 6.89
            Mean episode successes: 0.8848
Mean episode consecutive_successes: 1.5424
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 8.24s
                        Total time: 16679.67s
                               ETA: 1043704.0s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.835s, learning 0.167s)
               Value function loss: 33863.1542
                    Surrogate loss: -0.0229
             Mean action noise std: 0.77
                       Mean reward: 496.53
               Mean episode length: 44.82
                  Mean reward/step: 10.38
       Mean episode length/episode: 6.85
            Mean episode successes: 0.9463
Mean episode consecutive_successes: 1.5365
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 8.00s
                        Total time: 16687.67s
                               ETA: 1043530.6s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.018s, learning 0.172s)
               Value function loss: 30942.2350
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 549.04
               Mean episode length: 44.83
                  Mean reward/step: 9.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.8477
Mean episode consecutive_successes: 1.5528
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 8.19s
                        Total time: 16695.86s
                               ETA: 1043369.3s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.751s, learning 0.171s)
               Value function loss: 30556.9683
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 602.97
               Mean episode length: 47.07
                  Mean reward/step: 8.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.8105
Mean episode consecutive_successes: 1.5724
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 7.92s
                        Total time: 16703.78s
                               ETA: 1043191.4s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.203s, learning 0.264s)
               Value function loss: 29237.5093
                    Surrogate loss: -0.0208
             Mean action noise std: 0.77
                       Mean reward: 225.30
               Mean episode length: 42.49
                  Mean reward/step: 8.45
       Mean episode length/episode: 6.82
            Mean episode successes: 0.7764
Mean episode consecutive_successes: 1.5575
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 8.47s
                        Total time: 16712.25s
                               ETA: 1043047.8s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.280s, learning 0.302s)
               Value function loss: 29729.0956
                    Surrogate loss: -0.0224
             Mean action noise std: 0.77
                       Mean reward: 334.11
               Mean episode length: 40.04
                  Mean reward/step: 7.99
       Mean episode length/episode: 6.76
            Mean episode successes: 0.7588
Mean episode consecutive_successes: 1.5496
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 8.58s
                        Total time: 16720.83s
                               ETA: 1042911.5s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.440s, learning 0.181s)
               Value function loss: 29044.4401
                    Surrogate loss: -0.0213
             Mean action noise std: 0.77
                       Mean reward: 400.44
               Mean episode length: 43.22
                  Mean reward/step: 7.91
       Mean episode length/episode: 6.94
            Mean episode successes: 0.6890
Mean episode consecutive_successes: 1.5549
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 8.62s
                        Total time: 16729.45s
                               ETA: 1042777.8s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.016s, learning 0.162s)
               Value function loss: 28020.7330
                    Surrogate loss: -0.0234
             Mean action noise std: 0.77
                       Mean reward: 232.08
               Mean episode length: 41.39
                  Mean reward/step: 8.45
       Mean episode length/episode: 6.92
            Mean episode successes: 0.7134
Mean episode consecutive_successes: 1.5279
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 8.18s
                        Total time: 16737.63s
                               ETA: 1042616.6s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.042s, learning 0.175s)
               Value function loss: 29632.9691
                    Surrogate loss: -0.0233
             Mean action noise std: 0.77
                       Mean reward: 383.25
               Mean episode length: 43.52
                  Mean reward/step: 8.44
       Mean episode length/episode: 6.95
            Mean episode successes: 0.7505
Mean episode consecutive_successes: 1.5216
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 8.22s
                        Total time: 16745.85s
                               ETA: 1042458.1s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.845s, learning 0.159s)
               Value function loss: 28789.8977
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 350.62
               Mean episode length: 42.19
                  Mean reward/step: 7.92
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7773
Mean episode consecutive_successes: 1.4930
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 8.00s
                        Total time: 16753.85s
                               ETA: 1042286.5s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.973s, learning 0.206s)
               Value function loss: 29015.5977
                    Surrogate loss: -0.0212
             Mean action noise std: 0.77
                       Mean reward: 370.67
               Mean episode length: 43.25
                  Mean reward/step: 8.74
       Mean episode length/episode: 6.84
            Mean episode successes: 0.7803
Mean episode consecutive_successes: 1.4788
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 8.18s
                        Total time: 16762.03s
                               ETA: 1042126.0s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.954s, learning 0.201s)
               Value function loss: 28589.7009
                    Surrogate loss: -0.0237
             Mean action noise std: 0.77
                       Mean reward: 255.15
               Mean episode length: 42.07
                  Mean reward/step: 8.41
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7651
Mean episode consecutive_successes: 1.4726
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 8.15s
                        Total time: 16770.19s
                               ETA: 1041964.2s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.416s, learning 0.184s)
               Value function loss: 27804.6228
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 304.77
               Mean episode length: 42.14
                  Mean reward/step: 9.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.8008
Mean episode consecutive_successes: 1.4610
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 8.60s
                        Total time: 16778.79s
                               ETA: 1041830.2s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.951s, learning 0.184s)
               Value function loss: 30895.3242
                    Surrogate loss: -0.0212
             Mean action noise std: 0.77
                       Mean reward: 262.14
               Mean episode length: 42.01
                  Mean reward/step: 9.75
       Mean episode length/episode: 6.87
            Mean episode successes: 0.8467
Mean episode consecutive_successes: 1.4567
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 8.13s
                        Total time: 16786.92s
                               ETA: 1041667.5s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.774s, learning 0.176s)
               Value function loss: 29745.8337
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 272.15
               Mean episode length: 41.17
                  Mean reward/step: 8.07
       Mean episode length/episode: 6.73
            Mean episode successes: 0.8462
Mean episode consecutive_successes: 1.4391
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 7.95s
                        Total time: 16794.87s
                               ETA: 1041493.6s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.965s, learning 0.175s)
               Value function loss: 31510.1931
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 607.73
               Mean episode length: 42.41
                  Mean reward/step: 10.45
       Mean episode length/episode: 6.94
            Mean episode successes: 0.9399
Mean episode consecutive_successes: 1.4577
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 8.14s
                        Total time: 16803.01s
                               ETA: 1041331.6s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.254s, learning 0.168s)
               Value function loss: 31109.6190
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 300.19
               Mean episode length: 42.73
                  Mean reward/step: 9.68
       Mean episode length/episode: 6.95
            Mean episode successes: 0.9043
Mean episode consecutive_successes: 1.4661
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 8.42s
                        Total time: 16811.43s
                               ETA: 1041187.3s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.254s, learning 0.180s)
               Value function loss: 34145.5021
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 320.68
               Mean episode length: 43.22
                  Mean reward/step: 10.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.9663
Mean episode consecutive_successes: 1.4609
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 8.43s
                        Total time: 16819.87s
                               ETA: 1041043.9s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 2096 steps/s (collection: 7.661s, learning 0.155s)
               Value function loss: 33154.8451
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 449.09
               Mean episode length: 40.58
                  Mean reward/step: 10.16
       Mean episode length/episode: 6.86
            Mean episode successes: 1.0078
Mean episode consecutive_successes: 1.4695
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 7.82s
                        Total time: 16827.68s
                               ETA: 1040862.5s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.365s, learning 0.190s)
               Value function loss: 32064.5949
                    Surrogate loss: -0.0248
             Mean action noise std: 0.77
                       Mean reward: 397.56
               Mean episode length: 41.68
                  Mean reward/step: 9.67
       Mean episode length/episode: 6.83
            Mean episode successes: 0.9473
Mean episode consecutive_successes: 1.4845
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 8.56s
                        Total time: 16836.24s
                               ETA: 1040727.0s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.137s, learning 0.186s)
               Value function loss: 34744.5020
                    Surrogate loss: -0.0241
             Mean action noise std: 0.77
                       Mean reward: 461.35
               Mean episode length: 44.63
                  Mean reward/step: 9.85
       Mean episode length/episode: 6.93
            Mean episode successes: 0.9038
Mean episode consecutive_successes: 1.5174
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 8.32s
                        Total time: 16844.56s
                               ETA: 1040577.2s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.141s, learning 0.168s)
               Value function loss: 33149.0755
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 458.20
               Mean episode length: 43.10
                  Mean reward/step: 9.77
       Mean episode length/episode: 6.90
            Mean episode successes: 0.8848
Mean episode consecutive_successes: 1.5361
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 8.31s
                        Total time: 16852.87s
                               ETA: 1040426.8s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.023s, learning 0.158s)
               Value function loss: 34540.6703
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 407.44
               Mean episode length: 41.87
                  Mean reward/step: 9.49
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8418
Mean episode consecutive_successes: 1.5481
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 8.18s
                        Total time: 16861.05s
                               ETA: 1040268.6s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.861s, learning 0.163s)
               Value function loss: 34339.7748
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 509.90
               Mean episode length: 46.19
                  Mean reward/step: 9.44
       Mean episode length/episode: 6.91
            Mean episode successes: 0.8867
Mean episode consecutive_successes: 1.5426
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 8.02s
                        Total time: 16869.07s
                               ETA: 1040101.0s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.171s, learning 0.164s)
               Value function loss: 30528.3952
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 400.18
               Mean episode length: 42.41
                  Mean reward/step: 8.66
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8149
Mean episode consecutive_successes: 1.5512
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 8.34s
                        Total time: 16877.41s
                               ETA: 1039952.8s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.233s, learning 0.161s)
               Value function loss: 32330.1643
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 311.45
               Mean episode length: 40.23
                  Mean reward/step: 8.93
       Mean episode length/episode: 6.95
            Mean episode successes: 0.8589
Mean episode consecutive_successes: 1.5264
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 8.39s
                        Total time: 16885.80s
                               ETA: 1039808.3s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.204s, learning 0.169s)
               Value function loss: 33176.1889
                    Surrogate loss: -0.0208
             Mean action noise std: 0.77
                       Mean reward: 475.19
               Mean episode length: 41.85
                  Mean reward/step: 10.28
       Mean episode length/episode: 6.91
            Mean episode successes: 0.9482
Mean episode consecutive_successes: 1.5263
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 8.37s
                        Total time: 16894.18s
                               ETA: 1039662.8s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.087s, learning 0.162s)
               Value function loss: 33270.6647
                    Surrogate loss: -0.0231
             Mean action noise std: 0.77
                       Mean reward: 244.16
               Mean episode length: 41.26
                  Mean reward/step: 10.15
       Mean episode length/episode: 6.82
            Mean episode successes: 0.9531
Mean episode consecutive_successes: 1.5209
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 8.25s
                        Total time: 16902.43s
                               ETA: 1039509.8s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.160s)
               Value function loss: 35995.4812
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 287.65
               Mean episode length: 42.19
                  Mean reward/step: 9.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.8618
Mean episode consecutive_successes: 1.5583
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 8.18s
                        Total time: 16910.61s
                               ETA: 1039352.8s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.974s, learning 0.163s)
               Value function loss: 34357.3178
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 338.21
               Mean episode length: 42.83
                  Mean reward/step: 9.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.8369
Mean episode consecutive_successes: 1.5551
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 8.14s
                        Total time: 16918.75s
                               ETA: 1039193.3s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.933s, learning 0.166s)
               Value function loss: 38426.1387
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 420.44
               Mean episode length: 42.86
                  Mean reward/step: 9.68
       Mean episode length/episode: 6.90
            Mean episode successes: 0.8867
Mean episode consecutive_successes: 1.5656
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 8.10s
                        Total time: 16926.84s
                               ETA: 1039031.6s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.005s, learning 0.186s)
               Value function loss: 34117.2642
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 383.00
               Mean episode length: 42.98
                  Mean reward/step: 10.70
       Mean episode length/episode: 6.84
            Mean episode successes: 0.9409
Mean episode consecutive_successes: 1.5372
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 8.19s
                        Total time: 16935.04s
                               ETA: 1038875.8s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.316s, learning 0.208s)
               Value function loss: 35722.9998
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 479.40
               Mean episode length: 41.25
                  Mean reward/step: 9.64
       Mean episode length/episode: 6.87
            Mean episode successes: 0.8398
Mean episode consecutive_successes: 1.5853
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 8.52s
                        Total time: 16943.56s
                               ETA: 1038740.5s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.952s, learning 0.161s)
               Value function loss: 35935.8656
                    Surrogate loss: -0.0223
             Mean action noise std: 0.77
                       Mean reward: 324.46
               Mean episode length: 45.00
                  Mean reward/step: 9.59
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8926
Mean episode consecutive_successes: 1.5655
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 8.11s
                        Total time: 16951.67s
                               ETA: 1038580.2s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.354s, learning 0.178s)
               Value function loss: 32403.9253
                    Surrogate loss: -0.0234
             Mean action noise std: 0.77
                       Mean reward: 356.31
               Mean episode length: 44.03
                  Mean reward/step: 9.37
       Mean episode length/episode: 7.03
            Mean episode successes: 0.8770
Mean episode consecutive_successes: 1.5802
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 8.53s
                        Total time: 16960.20s
                               ETA: 1038445.8s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.041s, learning 0.171s)
               Value function loss: 33183.2303
                    Surrogate loss: -0.0198
             Mean action noise std: 0.77
                       Mean reward: 431.70
               Mean episode length: 45.60
                  Mean reward/step: 9.29
       Mean episode length/episode: 6.83
            Mean episode successes: 0.8462
Mean episode consecutive_successes: 1.5723
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 8.21s
                        Total time: 16968.42s
                               ETA: 1038291.9s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.177s, learning 0.161s)
               Value function loss: 35419.5931
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 476.23
               Mean episode length: 43.96
                  Mean reward/step: 9.17
       Mean episode length/episode: 6.82
            Mean episode successes: 0.8506
Mean episode consecutive_successes: 1.5843
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 8.34s
                        Total time: 16976.75s
                               ETA: 1038146.0s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.185s, learning 0.161s)
               Value function loss: 36211.8369
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 340.98
               Mean episode length: 43.74
                  Mean reward/step: 10.29
       Mean episode length/episode: 6.84
            Mean episode successes: 0.7769
Mean episode consecutive_successes: 1.6014
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 8.35s
                        Total time: 16985.10s
                               ETA: 1038000.7s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.184s, learning 0.236s)
               Value function loss: 31842.1881
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 463.02
               Mean episode length: 42.82
                  Mean reward/step: 9.37
       Mean episode length/episode: 6.99
            Mean episode successes: 0.8423
Mean episode consecutive_successes: 1.5815
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 8.42s
                        Total time: 16993.52s
                               ETA: 1037860.0s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.932s, learning 0.164s)
               Value function loss: 34483.9441
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 418.69
               Mean episode length: 43.62
                  Mean reward/step: 10.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.8677
Mean episode consecutive_successes: 1.5814
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 8.10s
                        Total time: 17001.62s
                               ETA: 1037699.8s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.181s, learning 0.167s)
               Value function loss: 35513.8139
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 332.46
               Mean episode length: 41.88
                  Mean reward/step: 10.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.8560
Mean episode consecutive_successes: 1.5915
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 8.35s
                        Total time: 17009.97s
                               ETA: 1037555.2s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.025s, learning 0.198s)
               Value function loss: 35126.5214
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 331.05
               Mean episode length: 43.61
                  Mean reward/step: 9.16
       Mean episode length/episode: 6.86
            Mean episode successes: 0.8638
Mean episode consecutive_successes: 1.5818
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 8.22s
                        Total time: 17018.19s
                               ETA: 1037403.0s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.265s, learning 0.191s)
               Value function loss: 33067.2194
                    Surrogate loss: -0.0222
             Mean action noise std: 0.77
                       Mean reward: 760.44
               Mean episode length: 46.00
                  Mean reward/step: 10.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.8779
Mean episode consecutive_successes: 1.6059
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 8.46s
                        Total time: 17026.64s
                               ETA: 1037265.3s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.224s, learning 0.210s)
               Value function loss: 35907.0147
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 359.96
               Mean episode length: 42.43
                  Mean reward/step: 10.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.8828
Mean episode consecutive_successes: 1.5905
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 8.43s
                        Total time: 17035.08s
                               ETA: 1037126.3s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.164s)
               Value function loss: 37034.3663
                    Surrogate loss: -0.0225
             Mean action noise std: 0.77
                       Mean reward: 586.02
               Mean episode length: 43.35
                  Mean reward/step: 11.83
       Mean episode length/episode: 6.85
            Mean episode successes: 0.9185
Mean episode consecutive_successes: 1.6101
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 8.04s
                        Total time: 17043.12s
                               ETA: 1036963.7s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1583 steps/s (collection: 10.162s, learning 0.186s)
               Value function loss: 38870.9856
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 318.39
               Mean episode length: 39.34
                  Mean reward/step: 11.52
       Mean episode length/episode: 6.86
            Mean episode successes: 0.9277
Mean episode consecutive_successes: 1.6192
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 10.35s
                        Total time: 17053.47s
                               ETA: 1036941.5s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.999s, learning 0.187s)
               Value function loss: 35348.6456
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 311.87
               Mean episode length: 41.08
                  Mean reward/step: 10.65
       Mean episode length/episode: 6.90
            Mean episode successes: 0.9258
Mean episode consecutive_successes: 1.6315
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 16.19s
                        Total time: 17069.65s
                               ETA: 1037274.1s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.863s, learning 0.192s)
               Value function loss: 34553.5681
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 503.00
               Mean episode length: 42.56
                  Mean reward/step: 10.02
       Mean episode length/episode: 6.86
            Mean episode successes: 0.9004
Mean episode consecutive_successes: 1.6464
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 16.06s
                        Total time: 17085.71s
                               ETA: 1037598.3s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.845s, learning 0.192s)
               Value function loss: 36728.0410
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 291.88
               Mean episode length: 40.68
                  Mean reward/step: 10.74
       Mean episode length/episode: 6.90
            Mean episode successes: 0.9419
Mean episode consecutive_successes: 1.6338
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 16.04s
                        Total time: 17101.75s
                               ETA: 1037920.9s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.555s, learning 0.159s)
               Value function loss: 36281.5171
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 548.90
               Mean episode length: 43.31
                  Mean reward/step: 9.75
       Mean episode length/episode: 6.85
            Mean episode successes: 0.9082
Mean episode consecutive_successes: 1.6473
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 15.71s
                        Total time: 17117.46s
                               ETA: 1038223.5s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.670s, learning 0.169s)
               Value function loss: 31952.4816
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 543.55
               Mean episode length: 43.56
                  Mean reward/step: 9.11
       Mean episode length/episode: 6.85
            Mean episode successes: 0.8384
Mean episode consecutive_successes: 1.6670
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 15.84s
                        Total time: 17133.30s
                               ETA: 1038533.4s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.665s, learning 0.190s)
               Value function loss: 34425.5146
                    Surrogate loss: -0.0231
             Mean action noise std: 0.77
                       Mean reward: 405.53
               Mean episode length: 42.86
                  Mean reward/step: 8.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.8354
Mean episode consecutive_successes: 1.6306
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 15.86s
                        Total time: 17149.15s
                               ETA: 1038843.8s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.985s, learning 0.157s)
               Value function loss: 36095.4857
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 359.75
               Mean episode length: 41.71
                  Mean reward/step: 10.66
       Mean episode length/episode: 6.82
            Mean episode successes: 0.8447
Mean episode consecutive_successes: 1.6261
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 16.14s
                        Total time: 17165.30s
                               ETA: 1039171.2s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.239s, learning 0.181s)
               Value function loss: 41670.0108
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 452.77
               Mean episode length: 42.22
                  Mean reward/step: 12.47
       Mean episode length/episode: 6.95
            Mean episode successes: 1.0449
Mean episode consecutive_successes: 1.6025
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 16.42s
                        Total time: 17181.72s
                               ETA: 1039514.9s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.805s, learning 0.189s)
               Value function loss: 51744.7234
                    Surrogate loss: -0.0218
             Mean action noise std: 0.77
                       Mean reward: 370.59
               Mean episode length: 42.58
                  Mean reward/step: 11.77
       Mean episode length/episode: 6.89
            Mean episode successes: 1.0322
Mean episode consecutive_successes: 1.6228
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 15.99s
                        Total time: 17197.71s
                               ETA: 1039832.5s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1040 steps/s (collection: 15.571s, learning 0.171s)
               Value function loss: 47295.6991
                    Surrogate loss: -0.0237
             Mean action noise std: 0.77
                       Mean reward: 645.00
               Mean episode length: 45.68
                  Mean reward/step: 9.68
       Mean episode length/episode: 6.89
            Mean episode successes: 0.9624
Mean episode consecutive_successes: 1.6701
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 15.74s
                        Total time: 17213.45s
                               ETA: 1040134.4s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.038s, learning 0.163s)
               Value function loss: 28493.1793
                    Surrogate loss: -0.0241
             Mean action noise std: 0.77
                       Mean reward: 427.66
               Mean episode length: 41.98
                  Mean reward/step: 9.40
       Mean episode length/episode: 6.82
            Mean episode successes: 0.9414
Mean episode consecutive_successes: 1.6478
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 16.20s
                        Total time: 17229.65s
                               ETA: 1040463.7s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.918s, learning 0.171s)
               Value function loss: 29330.5831
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 514.73
               Mean episode length: 45.51
                  Mean reward/step: 8.60
       Mean episode length/episode: 6.90
            Mean episode successes: 0.7207
Mean episode consecutive_successes: 1.7140
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 16.09s
                        Total time: 17245.74s
                               ETA: 1040785.7s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.009s, learning 0.212s)
               Value function loss: 32823.7478
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 434.02
               Mean episode length: 44.47
                  Mean reward/step: 10.73
       Mean episode length/episode: 6.89
            Mean episode successes: 0.7900
Mean episode consecutive_successes: 1.6952
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 16.22s
                        Total time: 17261.96s
                               ETA: 1041115.3s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.920s, learning 0.187s)
               Value function loss: 35305.3088
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 441.10
               Mean episode length: 43.89
                  Mean reward/step: 10.54
       Mean episode length/episode: 6.97
            Mean episode successes: 0.8267
Mean episode consecutive_successes: 1.7067
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 16.11s
                        Total time: 17278.07s
                               ETA: 1041437.6s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.830s, learning 0.166s)
               Value function loss: 37986.7881
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 287.15
               Mean episode length: 41.22
                  Mean reward/step: 11.86
       Mean episode length/episode: 6.92
            Mean episode successes: 0.9434
Mean episode consecutive_successes: 1.6811
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 16.00s
                        Total time: 17294.06s
                               ETA: 1041752.9s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.261s, learning 0.169s)
               Value function loss: 36997.8983
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 552.60
               Mean episode length: 41.33
                  Mean reward/step: 10.51
       Mean episode length/episode: 6.83
            Mean episode successes: 0.9805
Mean episode consecutive_successes: 1.6807
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 16.43s
                        Total time: 17310.49s
                               ETA: 1042093.8s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.871s, learning 0.162s)
               Value function loss: 41186.0559
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 618.45
               Mean episode length: 46.78
                  Mean reward/step: 11.47
       Mean episode length/episode: 6.91
            Mean episode successes: 0.9766
Mean episode consecutive_successes: 1.7012
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 16.03s
                        Total time: 17326.53s
                               ETA: 1042410.5s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.839s, learning 0.163s)
               Value function loss: 34111.1142
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 507.18
               Mean episode length: 40.74
                  Mean reward/step: 11.02
       Mean episode length/episode: 6.89
            Mean episode successes: 1.0146
Mean episode consecutive_successes: 1.6956
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 16.00s
                        Total time: 17342.53s
                               ETA: 1042724.8s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.123s, learning 0.186s)
               Value function loss: 36431.8073
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 426.93
               Mean episode length: 40.75
                  Mean reward/step: 10.86
       Mean episode length/episode: 6.86
            Mean episode successes: 0.9121
Mean episode consecutive_successes: 1.7430
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 16.31s
                        Total time: 17358.84s
                               ETA: 1043057.2s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.086s, learning 0.159s)
               Value function loss: 33488.5937
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 536.04
               Mean episode length: 42.66
                  Mean reward/step: 9.86
       Mean episode length/episode: 6.82
            Mean episode successes: 0.8477
Mean episode consecutive_successes: 1.7435
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 16.25s
                        Total time: 17375.08s
                               ETA: 1043385.4s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.926s, learning 0.188s)
               Value function loss: 33021.3851
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 300.53
               Mean episode length: 42.96
                  Mean reward/step: 10.71
       Mean episode length/episode: 6.90
            Mean episode successes: 0.8604
Mean episode consecutive_successes: 1.7357
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 16.11s
                        Total time: 17391.20s
                               ETA: 1043705.2s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.689s, learning 0.168s)
               Value function loss: 31217.2142
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 362.87
               Mean episode length: 42.60
                  Mean reward/step: 9.53
       Mean episode length/episode: 6.82
            Mean episode successes: 0.8989
Mean episode consecutive_successes: 1.7140
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 15.86s
                        Total time: 17407.05s
                               ETA: 1044009.3s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.516s, learning 0.195s)
               Value function loss: 32258.6161
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 489.26
               Mean episode length: 44.60
                  Mean reward/step: 9.26
       Mean episode length/episode: 6.86
            Mean episode successes: 0.7383
Mean episode consecutive_successes: 1.7571
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 16.71s
                        Total time: 17423.77s
                               ETA: 1044364.1s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.942s, learning 0.168s)
               Value function loss: 32307.6025
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 224.79
               Mean episode length: 41.95
                  Mean reward/step: 10.15
       Mean episode length/episode: 6.88
            Mean episode successes: 0.8198
Mean episode consecutive_successes: 1.7185
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 16.11s
                        Total time: 17439.87s
                               ETA: 1044682.5s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.715s, learning 0.186s)
               Value function loss: 32630.4336
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 224.72
               Mean episode length: 41.81
                  Mean reward/step: 10.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.8154
Mean episode consecutive_successes: 1.7119
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 16.90s
                        Total time: 17456.78s
                               ETA: 1045047.8s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.758s, learning 0.164s)
               Value function loss: 32062.5148
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 298.79
               Mean episode length: 40.01
                  Mean reward/step: 9.72
       Mean episode length/episode: 6.82
            Mean episode successes: 0.7949
Mean episode consecutive_successes: 1.7054
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 15.92s
                        Total time: 17472.70s
                               ETA: 1045354.1s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.753s, learning 0.213s)
               Value function loss: 31736.9794
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 159.46
               Mean episode length: 41.23
                  Mean reward/step: 9.40
       Mean episode length/episode: 6.94
            Mean episode successes: 0.8135
Mean episode consecutive_successes: 1.6903
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 15.97s
                        Total time: 17488.66s
                               ETA: 1045662.6s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.184s, learning 0.186s)
               Value function loss: 36652.4611
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 486.34
               Mean episode length: 44.31
                  Mean reward/step: 10.27
       Mean episode length/episode: 6.88
            Mean episode successes: 0.8379
Mean episode consecutive_successes: 1.6937
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 16.37s
                        Total time: 17505.03s
                               ETA: 1045994.8s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.868s, learning 0.190s)
               Value function loss: 36221.0135
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 455.14
               Mean episode length: 41.87
                  Mean reward/step: 10.34
       Mean episode length/episode: 6.87
            Mean episode successes: 0.7935
Mean episode consecutive_successes: 1.7089
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 16.06s
                        Total time: 17521.09s
                               ETA: 1046308.1s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.861s, learning 0.165s)
               Value function loss: 38284.6818
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 355.27
               Mean episode length: 42.05
                  Mean reward/step: 11.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.9131
Mean episode consecutive_successes: 1.6748
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 16.03s
                        Total time: 17537.12s
                               ETA: 1046619.0s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.255s, learning 0.164s)
               Value function loss: 38844.5653
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 505.49
               Mean episode length: 42.37
                  Mean reward/step: 11.54
       Mean episode length/episode: 6.95
            Mean episode successes: 0.9365
Mean episode consecutive_successes: 1.6915
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 16.42s
                        Total time: 17553.54s
                               ETA: 1046952.9s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.012s, learning 0.166s)
               Value function loss: 42900.3296
                    Surrogate loss: -0.0214
             Mean action noise std: 0.77
                       Mean reward: 612.30
               Mean episode length: 45.55
                  Mean reward/step: 13.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.9883
Mean episode consecutive_successes: 1.7360
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 16.18s
                        Total time: 17569.71s
                               ETA: 1047272.1s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1040 steps/s (collection: 15.583s, learning 0.163s)
               Value function loss: 47672.6348
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 499.14
               Mean episode length: 44.30
                  Mean reward/step: 14.15
       Mean episode length/episode: 6.88
            Mean episode successes: 1.0962
Mean episode consecutive_successes: 1.7297
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 15.75s
                        Total time: 17585.46s
                               ETA: 1047565.1s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.270s, learning 0.186s)
               Value function loss: 42640.8095
                    Surrogate loss: -0.0214
             Mean action noise std: 0.77
                       Mean reward: 732.71
               Mean episode length: 46.48
                  Mean reward/step: 13.29
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2104
Mean episode consecutive_successes: 1.7414
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 16.46s
                        Total time: 17601.92s
                               ETA: 1047900.0s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.004s, learning 0.190s)
               Value function loss: 43430.5675
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 625.60
               Mean episode length: 47.15
                  Mean reward/step: 12.36
       Mean episode length/episode: 6.88
            Mean episode successes: 1.1299
Mean episode consecutive_successes: 1.8014
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 16.19s
                        Total time: 17618.11s
                               ETA: 1048218.9s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.232s, learning 0.191s)
               Value function loss: 46076.4947
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 517.72
               Mean episode length: 42.66
                  Mean reward/step: 12.71
       Mean episode length/episode: 6.93
            Mean episode successes: 1.1099
Mean episode consecutive_successes: 1.8242
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 16.42s
                        Total time: 17634.53s
                               ETA: 1048551.1s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.212s, learning 0.164s)
               Value function loss: 57905.4047
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 260.32
               Mean episode length: 42.52
                  Mean reward/step: 14.50
       Mean episode length/episode: 7.05
            Mean episode successes: 1.2480
Mean episode consecutive_successes: 1.8261
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 16.38s
                        Total time: 17650.91s
                               ETA: 1048879.9s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.111s, learning 0.221s)
               Value function loss: 50445.2460
                    Surrogate loss: -0.0231
             Mean action noise std: 0.77
                       Mean reward: 534.16
               Mean episode length: 44.90
                  Mean reward/step: 14.55
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2153
Mean episode consecutive_successes: 1.8980
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 11.33s
                        Total time: 17662.24s
                               ETA: 1048908.9s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1992 steps/s (collection: 7.988s, learning 0.233s)
               Value function loss: 55858.4941
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 714.40
               Mean episode length: 45.40
                  Mean reward/step: 13.54
       Mean episode length/episode: 6.92
            Mean episode successes: 1.1943
Mean episode consecutive_successes: 1.9484
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 8.22s
                        Total time: 17670.46s
                               ETA: 1048753.2s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.941s, learning 0.226s)
               Value function loss: 46962.6893
                    Surrogate loss: -0.0243
             Mean action noise std: 0.77
                       Mean reward: 503.74
               Mean episode length: 43.75
                  Mean reward/step: 14.01
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2222
Mean episode consecutive_successes: 1.9708
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 8.17s
                        Total time: 17678.63s
                               ETA: 1048594.4s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.920s, learning 0.195s)
               Value function loss: 46691.0734
                    Surrogate loss: -0.0238
             Mean action noise std: 0.77
                       Mean reward: 345.32
               Mean episode length: 46.21
                  Mean reward/step: 13.66
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2378
Mean episode consecutive_successes: 2.0081
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 8.12s
                        Total time: 17686.75s
                               ETA: 1048432.7s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.264s, learning 0.163s)
               Value function loss: 48311.5626
                    Surrogate loss: -0.0229
             Mean action noise std: 0.77
                       Mean reward: 698.83
               Mean episode length: 44.40
                  Mean reward/step: 14.06
       Mean episode length/episode: 6.88
            Mean episode successes: 1.2026
Mean episode consecutive_successes: 2.0578
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 8.43s
                        Total time: 17695.17s
                               ETA: 1048289.7s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.040s, learning 0.165s)
               Value function loss: 46917.0627
                    Surrogate loss: -0.0238
             Mean action noise std: 0.77
                       Mean reward: 531.45
               Mean episode length: 44.24
                  Mean reward/step: 14.11
       Mean episode length/episode: 6.89
            Mean episode successes: 1.1899
Mean episode consecutive_successes: 2.0885
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 8.21s
                        Total time: 17703.38s
                               ETA: 1048133.8s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.097s, learning 0.161s)
               Value function loss: 44297.9614
                    Surrogate loss: -0.0208
             Mean action noise std: 0.77
                       Mean reward: 656.97
               Mean episode length: 44.93
                  Mean reward/step: 12.91
       Mean episode length/episode: 6.91
            Mean episode successes: 1.2681
Mean episode consecutive_successes: 2.0839
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 8.26s
                        Total time: 17711.64s
                               ETA: 1047981.1s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.128s, learning 0.170s)
               Value function loss: 41879.6267
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 863.42
               Mean episode length: 46.86
                  Mean reward/step: 13.38
       Mean episode length/episode: 6.91
            Mean episode successes: 1.1553
Mean episode consecutive_successes: 2.1430
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 8.30s
                        Total time: 17719.93s
                               ETA: 1047830.9s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.099s, learning 0.216s)
               Value function loss: 41162.7350
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 374.96
               Mean episode length: 45.71
                  Mean reward/step: 13.12
       Mean episode length/episode: 6.89
            Mean episode successes: 1.1836
Mean episode consecutive_successes: 2.1238
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 8.32s
                        Total time: 17728.25s
                               ETA: 1047682.0s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.327s, learning 0.162s)
               Value function loss: 43757.3108
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 525.77
               Mean episode length: 43.66
                  Mean reward/step: 13.05
       Mean episode length/episode: 6.97
            Mean episode successes: 1.1958
Mean episode consecutive_successes: 2.1466
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 8.49s
                        Total time: 17736.74s
                               ETA: 1047543.5s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.323s, learning 0.200s)
               Value function loss: 42783.6136
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 369.07
               Mean episode length: 44.49
                  Mean reward/step: 13.10
       Mean episode length/episode: 6.98
            Mean episode successes: 1.1167
Mean episode consecutive_successes: 2.1922
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 8.52s
                        Total time: 17745.26s
                               ETA: 1047407.2s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.275s, learning 0.159s)
               Value function loss: 44787.6698
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 439.08
               Mean episode length: 44.57
                  Mean reward/step: 12.85
       Mean episode length/episode: 6.86
            Mean episode successes: 1.1162
Mean episode consecutive_successes: 2.1836
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 8.43s
                        Total time: 17753.70s
                               ETA: 1047265.7s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.145s, learning 0.164s)
               Value function loss: 40324.9028
                    Surrogate loss: -0.0232
             Mean action noise std: 0.77
                       Mean reward: 438.29
               Mean episode length: 43.17
                  Mean reward/step: 12.15
       Mean episode length/episode: 6.95
            Mean episode successes: 1.1304
Mean episode consecutive_successes: 2.1837
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 8.31s
                        Total time: 17762.00s
                               ETA: 1047117.0s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.909s, learning 0.198s)
               Value function loss: 45494.5187
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 680.53
               Mean episode length: 46.29
                  Mean reward/step: 13.68
       Mean episode length/episode: 7.00
            Mean episode successes: 1.1689
Mean episode consecutive_successes: 2.2105
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 8.11s
                        Total time: 17770.11s
                               ETA: 1046956.6s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.485s, learning 0.161s)
               Value function loss: 51380.5004
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 839.38
               Mean episode length: 44.98
                  Mean reward/step: 14.53
       Mean episode length/episode: 6.92
            Mean episode successes: 1.1968
Mean episode consecutive_successes: 2.2301
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 8.65s
                        Total time: 17778.76s
                               ETA: 1046828.1s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.080s, learning 0.158s)
               Value function loss: 47667.0331
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 793.92
               Mean episode length: 48.17
                  Mean reward/step: 14.30
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2397
Mean episode consecutive_successes: 2.2319
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 8.24s
                        Total time: 17786.99s
                               ETA: 1046675.8s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.686s, learning 0.168s)
               Value function loss: 42165.2907
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 628.82
               Mean episode length: 44.72
                  Mean reward/step: 12.85
       Mean episode length/episode: 6.91
            Mean episode successes: 1.1431
Mean episode consecutive_successes: 2.2535
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 7.85s
                        Total time: 17794.85s
                               ETA: 1046501.0s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.278s, learning 0.175s)
               Value function loss: 46049.1841
                    Surrogate loss: -0.0238
             Mean action noise std: 0.77
                       Mean reward: 710.02
               Mean episode length: 45.60
                  Mean reward/step: 13.42
       Mean episode length/episode: 6.87
            Mean episode successes: 1.1177
Mean episode consecutive_successes: 2.2698
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 8.45s
                        Total time: 17803.30s
                               ETA: 1046361.7s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.004s, learning 0.232s)
               Value function loss: 44238.0160
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 479.87
               Mean episode length: 45.89
                  Mean reward/step: 13.20
       Mean episode length/episode: 6.99
            Mean episode successes: 1.1436
Mean episode consecutive_successes: 2.2817
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 8.24s
                        Total time: 17811.54s
                               ETA: 1046209.8s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.961s, learning 0.204s)
               Value function loss: 46164.2833
                    Surrogate loss: -0.0261
             Mean action noise std: 0.77
                       Mean reward: 603.29
               Mean episode length: 43.23
                  Mean reward/step: 12.94
       Mean episode length/episode: 6.97
            Mean episode successes: 1.1460
Mean episode consecutive_successes: 2.2902
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 8.17s
                        Total time: 17819.70s
                               ETA: 1046053.8s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.686s, learning 0.166s)
               Value function loss: 49632.2273
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 827.05
               Mean episode length: 44.90
                  Mean reward/step: 12.67
       Mean episode length/episode: 6.90
            Mean episode successes: 1.0479
Mean episode consecutive_successes: 2.3284
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 7.85s
                        Total time: 17827.56s
                               ETA: 1045879.7s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.067s, learning 0.159s)
               Value function loss: 47533.4733
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 664.22
               Mean episode length: 48.28
                  Mean reward/step: 12.19
       Mean episode length/episode: 6.93
            Mean episode successes: 1.0630
Mean episode consecutive_successes: 2.2998
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 8.23s
                        Total time: 17835.78s
                               ETA: 1045727.8s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.859s, learning 0.162s)
               Value function loss: 44317.9751
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 300.44
               Mean episode length: 42.74
                  Mean reward/step: 13.43
       Mean episode length/episode: 7.00
            Mean episode successes: 1.1157
Mean episode consecutive_successes: 2.2828
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 8.02s
                        Total time: 17843.80s
                               ETA: 1045564.0s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.833s, learning 0.169s)
               Value function loss: 50538.1253
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 778.87
               Mean episode length: 48.07
                  Mean reward/step: 14.48
       Mean episode length/episode: 6.97
            Mean episode successes: 1.2021
Mean episode consecutive_successes: 2.2923
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 8.00s
                        Total time: 17851.81s
                               ETA: 1045399.2s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.192s, learning 0.214s)
               Value function loss: 47918.7714
                    Surrogate loss: -0.0238
             Mean action noise std: 0.77
                       Mean reward: 579.98
               Mean episode length: 46.16
                  Mean reward/step: 14.20
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2588
Mean episode consecutive_successes: 2.2735
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 8.41s
                        Total time: 17860.21s
                               ETA: 1045258.2s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.804s, learning 0.187s)
               Value function loss: 49669.4639
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 676.35
               Mean episode length: 44.19
                  Mean reward/step: 13.44
       Mean episode length/episode: 6.88
            Mean episode successes: 1.1133
Mean episode consecutive_successes: 2.3196
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 7.99s
                        Total time: 17868.20s
                               ETA: 1045093.2s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.008s, learning 0.164s)
               Value function loss: 47642.6341
                    Surrogate loss: -0.0213
             Mean action noise std: 0.77
                       Mean reward: 685.23
               Mean episode length: 46.18
                  Mean reward/step: 12.74
       Mean episode length/episode: 6.91
            Mean episode successes: 1.1416
Mean episode consecutive_successes: 2.3125
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 8.17s
                        Total time: 17876.37s
                               ETA: 1044938.9s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.034s, learning 0.166s)
               Value function loss: 43685.0169
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 368.22
               Mean episode length: 43.00
                  Mean reward/step: 13.61
       Mean episode length/episode: 6.90
            Mean episode successes: 1.0938
Mean episode consecutive_successes: 2.3052
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 8.20s
                        Total time: 17884.57s
                               ETA: 1044786.5s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.032s, learning 0.173s)
               Value function loss: 48349.8927
                    Surrogate loss: -0.0201
             Mean action noise std: 0.77
                       Mean reward: 709.57
               Mean episode length: 44.98
                  Mean reward/step: 13.22
       Mean episode length/episode: 6.80
            Mean episode successes: 1.1279
Mean episode consecutive_successes: 2.3108
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 8.21s
                        Total time: 17892.78s
                               ETA: 1044634.5s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.967s, learning 0.157s)
               Value function loss: 51220.0907
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 318.41
               Mean episode length: 43.09
                  Mean reward/step: 13.75
       Mean episode length/episode: 6.97
            Mean episode successes: 1.1431
Mean episode consecutive_successes: 2.3001
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 8.12s
                        Total time: 17900.90s
                               ETA: 1044477.9s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.377s, learning 0.231s)
               Value function loss: 47226.2989
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 400.43
               Mean episode length: 42.46
                  Mean reward/step: 14.10
       Mean episode length/episode: 6.95
            Mean episode successes: 1.2378
Mean episode consecutive_successes: 2.2759
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 8.61s
                        Total time: 17909.51s
                               ETA: 1044349.8s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.162s)
               Value function loss: 51459.3516
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 459.15
               Mean episode length: 40.70
                  Mean reward/step: 15.49
       Mean episode length/episode: 6.94
            Mean episode successes: 1.3247
Mean episode consecutive_successes: 2.2789
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 8.36s
                        Total time: 17917.87s
                               ETA: 1044207.1s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.177s, learning 0.162s)
               Value function loss: 55816.4151
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 553.81
               Mean episode length: 44.59
                  Mean reward/step: 15.95
       Mean episode length/episode: 6.95
            Mean episode successes: 1.3950
Mean episode consecutive_successes: 2.2899
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 8.34s
                        Total time: 17926.21s
                               ETA: 1044063.5s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.824s, learning 0.279s)
               Value function loss: 57635.9401
                    Surrogate loss: -0.0248
             Mean action noise std: 0.77
                       Mean reward: 610.80
               Mean episode length: 46.41
                  Mean reward/step: 14.52
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2954
Mean episode consecutive_successes: 2.3316
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 9.10s
                        Total time: 17935.31s
                               ETA: 1043964.6s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.338s, learning 0.180s)
               Value function loss: 54747.2216
                    Surrogate loss: -0.0234
             Mean action noise std: 0.77
                       Mean reward: 892.73
               Mean episode length: 45.73
                  Mean reward/step: 13.97
       Mean episode length/episode: 6.94
            Mean episode successes: 1.3027
Mean episode consecutive_successes: 2.3646
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 8.52s
                        Total time: 17943.83s
                               ETA: 1043831.7s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.010s, learning 0.159s)
               Value function loss: 56006.6225
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 624.11
               Mean episode length: 44.02
                  Mean reward/step: 13.77
       Mean episode length/episode: 6.92
            Mean episode successes: 1.1216
Mean episode consecutive_successes: 2.4288
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 8.17s
                        Total time: 17952.00s
                               ETA: 1043678.8s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.007s, learning 0.166s)
               Value function loss: 61298.2671
                    Surrogate loss: -0.0218
             Mean action noise std: 0.77
                       Mean reward: 544.82
               Mean episode length: 45.54
                  Mean reward/step: 14.41
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2300
Mean episode consecutive_successes: 2.3848
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 8.17s
                        Total time: 17960.17s
                               ETA: 1043526.2s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.216s, learning 0.187s)
               Value function loss: 48505.7939
                    Surrogate loss: -0.0243
             Mean action noise std: 0.77
                       Mean reward: 743.83
               Mean episode length: 48.29
                  Mean reward/step: 14.74
       Mean episode length/episode: 6.88
            Mean episode successes: 1.2524
Mean episode consecutive_successes: 2.3883
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 8.40s
                        Total time: 17968.57s
                               ETA: 1043387.2s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.138s, learning 0.176s)
               Value function loss: 45618.8494
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 875.14
               Mean episode length: 45.32
                  Mean reward/step: 13.16
       Mean episode length/episode: 6.87
            Mean episode successes: 1.1509
Mean episode consecutive_successes: 2.4296
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 8.31s
                        Total time: 17976.89s
                               ETA: 1043243.1s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.293s, learning 0.195s)
               Value function loss: 49588.1011
                    Surrogate loss: -0.0213
             Mean action noise std: 0.77
                       Mean reward: 690.18
               Mean episode length: 45.43
                  Mean reward/step: 12.98
       Mean episode length/episode: 6.96
            Mean episode successes: 1.1758
Mean episode consecutive_successes: 2.4097
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 8.49s
                        Total time: 17985.38s
                               ETA: 1043109.3s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.974s, learning 0.163s)
               Value function loss: 49878.5340
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 343.17
               Mean episode length: 43.55
                  Mean reward/step: 14.07
       Mean episode length/episode: 6.99
            Mean episode successes: 1.1504
Mean episode consecutive_successes: 2.4227
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 8.14s
                        Total time: 17993.51s
                               ETA: 1042955.3s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.086s, learning 0.189s)
               Value function loss: 45927.3872
                    Surrogate loss: -0.0210
             Mean action noise std: 0.77
                       Mean reward: 310.67
               Mean episode length: 43.04
                  Mean reward/step: 12.56
       Mean episode length/episode: 6.83
            Mean episode successes: 1.1992
Mean episode consecutive_successes: 2.3738
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 8.28s
                        Total time: 18001.79s
                               ETA: 1042809.5s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.134s, learning 0.183s)
               Value function loss: 41686.1040
                    Surrogate loss: -0.0201
             Mean action noise std: 0.77
                       Mean reward: 476.58
               Mean episode length: 45.13
                  Mean reward/step: 12.93
       Mean episode length/episode: 6.90
            Mean episode successes: 1.1318
Mean episode consecutive_successes: 2.3955
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 8.32s
                        Total time: 18010.11s
                               ETA: 1042666.3s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.322s, learning 0.165s)
               Value function loss: 45069.6891
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 859.01
               Mean episode length: 48.21
                  Mean reward/step: 12.49
       Mean episode length/episode: 6.92
            Mean episode successes: 1.0972
Mean episode consecutive_successes: 2.4162
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 8.49s
                        Total time: 18018.59s
                               ETA: 1042533.1s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.841s, learning 0.214s)
               Value function loss: 42226.8991
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 961.13
               Mean episode length: 47.55
                  Mean reward/step: 12.31
       Mean episode length/episode: 6.92
            Mean episode successes: 0.9932
Mean episode consecutive_successes: 2.4329
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 8.06s
                        Total time: 18026.65s
                               ETA: 1042375.0s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.447s, learning 0.165s)
               Value function loss: 47164.8579
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 294.33
               Mean episode length: 41.17
                  Mean reward/step: 12.82
       Mean episode length/episode: 6.95
            Mean episode successes: 1.0767
Mean episode consecutive_successes: 2.3610
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 8.61s
                        Total time: 18035.26s
                               ETA: 1042249.3s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.158s)
               Value function loss: 48769.1262
                    Surrogate loss: -0.0237
             Mean action noise std: 0.77
                       Mean reward: 724.27
               Mean episode length: 44.54
                  Mean reward/step: 13.68
       Mean episode length/episode: 6.94
            Mean episode successes: 1.1636
Mean episode consecutive_successes: 2.3375
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 8.30s
                        Total time: 18043.56s
                               ETA: 1042105.9s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.049s, learning 0.210s)
               Value function loss: 47049.7677
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 814.27
               Mean episode length: 48.54
                  Mean reward/step: 13.33
       Mean episode length/episode: 6.95
            Mean episode successes: 1.0801
Mean episode consecutive_successes: 2.3787
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 8.26s
                        Total time: 18051.82s
                               ETA: 1041960.1s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.402s, learning 0.164s)
               Value function loss: 45576.7393
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 600.99
               Mean episode length: 43.40
                  Mean reward/step: 13.64
       Mean episode length/episode: 7.01
            Mean episode successes: 1.1016
Mean episode consecutive_successes: 2.3884
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 8.57s
                        Total time: 18060.39s
                               ETA: 1041832.1s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.031s, learning 0.170s)
               Value function loss: 49998.5136
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 486.34
               Mean episode length: 44.69
                  Mean reward/step: 13.13
       Mean episode length/episode: 6.96
            Mean episode successes: 1.1431
Mean episode consecutive_successes: 2.3739
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 8.20s
                        Total time: 18068.59s
                               ETA: 1041683.3s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.036s, learning 0.186s)
               Value function loss: 52397.3509
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 657.22
               Mean episode length: 48.65
                  Mean reward/step: 14.02
       Mean episode length/episode: 6.86
            Mean episode successes: 1.1382
Mean episode consecutive_successes: 2.3738
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 8.22s
                        Total time: 18076.81s
                               ETA: 1041535.8s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.912s, learning 0.277s)
               Value function loss: 49595.3102
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 809.01
               Mean episode length: 48.19
                  Mean reward/step: 14.78
       Mean episode length/episode: 6.93
            Mean episode successes: 1.1724
Mean episode consecutive_successes: 2.3823
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 8.19s
                        Total time: 18085.00s
                               ETA: 1041386.7s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.095s, learning 0.202s)
               Value function loss: 44067.4683
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 793.62
               Mean episode length: 47.57
                  Mean reward/step: 13.24
       Mean episode length/episode: 7.00
            Mean episode successes: 1.0537
Mean episode consecutive_successes: 2.4230
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 8.30s
                        Total time: 18093.30s
                               ETA: 1041243.9s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.973s, learning 0.160s)
               Value function loss: 48477.9171
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 302.85
               Mean episode length: 42.37
                  Mean reward/step: 12.77
       Mean episode length/episode: 6.82
            Mean episode successes: 1.0957
Mean episode consecutive_successes: 2.3699
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 8.13s
                        Total time: 18101.43s
                               ETA: 1041091.8s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.190s, learning 0.177s)
               Value function loss: 49052.2771
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 682.98
               Mean episode length: 46.72
                  Mean reward/step: 13.60
       Mean episode length/episode: 6.93
            Mean episode successes: 1.1357
Mean episode consecutive_successes: 2.3808
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 8.37s
                        Total time: 18109.80s
                               ETA: 1040953.3s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.688s, learning 0.166s)
               Value function loss: 51018.3111
                    Surrogate loss: -0.0265
             Mean action noise std: 0.77
                       Mean reward: 539.01
               Mean episode length: 44.73
                  Mean reward/step: 13.64
       Mean episode length/episode: 6.97
            Mean episode successes: 1.0747
Mean episode consecutive_successes: 2.3997
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 7.85s
                        Total time: 18117.65s
                               ETA: 1040785.5s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.090s, learning 0.232s)
               Value function loss: 57081.8835
                    Surrogate loss: -0.0245
             Mean action noise std: 0.77
                       Mean reward: 446.93
               Mean episode length: 41.12
                  Mean reward/step: 13.75
       Mean episode length/episode: 6.97
            Mean episode successes: 0.9912
Mean episode consecutive_successes: 2.4406
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 8.32s
                        Total time: 18125.97s
                               ETA: 1040644.8s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.996s, learning 0.193s)
               Value function loss: 47517.0085
                    Surrogate loss: -0.0231
             Mean action noise std: 0.77
                       Mean reward: 297.17
               Mean episode length: 42.01
                  Mean reward/step: 13.13
       Mean episode length/episode: 6.93
            Mean episode successes: 1.0928
Mean episode consecutive_successes: 2.3703
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 8.19s
                        Total time: 18134.16s
                               ETA: 1040496.6s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.034s, learning 0.166s)
               Value function loss: 50118.4615
                    Surrogate loss: -0.0235
             Mean action noise std: 0.77
                       Mean reward: 531.77
               Mean episode length: 44.37
                  Mean reward/step: 14.07
       Mean episode length/episode: 6.84
            Mean episode successes: 1.2397
Mean episode consecutive_successes: 2.3238
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 8.20s
                        Total time: 18142.36s
                               ETA: 1040349.1s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.178s, learning 0.163s)
               Value function loss: 53654.6660
                    Surrogate loss: -0.0218
             Mean action noise std: 0.77
                       Mean reward: 938.20
               Mean episode length: 47.01
                  Mean reward/step: 13.47
       Mean episode length/episode: 6.92
            Mean episode successes: 1.1655
Mean episode consecutive_successes: 2.3693
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 8.34s
                        Total time: 18150.70s
                               ETA: 1040209.9s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.981s, learning 0.186s)
               Value function loss: 52198.9770
                    Surrogate loss: -0.0219
             Mean action noise std: 0.77
                       Mean reward: 712.53
               Mean episode length: 45.33
                  Mean reward/step: 13.04
       Mean episode length/episode: 6.95
            Mean episode successes: 1.0664
Mean episode consecutive_successes: 2.3927
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 8.17s
                        Total time: 18158.87s
                               ETA: 1040060.9s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.820s, learning 0.167s)
               Value function loss: 56178.1217
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 492.38
               Mean episode length: 45.73
                  Mean reward/step: 14.30
       Mean episode length/episode: 6.98
            Mean episode successes: 1.1260
Mean episode consecutive_successes: 2.3788
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 7.99s
                        Total time: 18166.86s
                               ETA: 1039901.8s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.160s)
               Value function loss: 54445.3867
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 817.68
               Mean episode length: 45.96
                  Mean reward/step: 16.33
       Mean episode length/episode: 6.86
            Mean episode successes: 1.1577
Mean episode consecutive_successes: 2.4176
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 8.40s
                        Total time: 18175.26s
                               ETA: 1039766.5s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.043s, learning 0.172s)
               Value function loss: 51956.9700
                    Surrogate loss: -0.0229
             Mean action noise std: 0.77
                       Mean reward: 767.25
               Mean episode length: 45.16
                  Mean reward/step: 14.68
       Mean episode length/episode: 6.87
            Mean episode successes: 1.1201
Mean episode consecutive_successes: 2.4405
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 8.22s
                        Total time: 18183.47s
                               ETA: 1039620.8s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.923s, learning 0.169s)
               Value function loss: 55830.7175
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 435.58
               Mean episode length: 42.64
                  Mean reward/step: 15.19
       Mean episode length/episode: 6.84
            Mean episode successes: 1.2256
Mean episode consecutive_successes: 2.4035
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 8.09s
                        Total time: 18191.57s
                               ETA: 1039468.2s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.086s, learning 0.201s)
               Value function loss: 55237.6507
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 486.10
               Mean episode length: 44.13
                  Mean reward/step: 15.06
       Mean episode length/episode: 7.00
            Mean episode successes: 1.2651
Mean episode consecutive_successes: 2.4080
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 8.29s
                        Total time: 18199.85s
                               ETA: 1039326.9s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.932s, learning 0.208s)
               Value function loss: 53318.3513
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 661.95
               Mean episode length: 44.79
                  Mean reward/step: 14.51
       Mean episode length/episode: 6.95
            Mean episode successes: 1.2671
Mean episode consecutive_successes: 2.4463
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 8.14s
                        Total time: 18207.99s
                               ETA: 1039177.3s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.269s, learning 0.163s)
               Value function loss: 51941.2209
                    Surrogate loss: -0.0198
             Mean action noise std: 0.77
                       Mean reward: 699.49
               Mean episode length: 44.85
                  Mean reward/step: 14.40
       Mean episode length/episode: 6.86
            Mean episode successes: 1.2295
Mean episode consecutive_successes: 2.4483
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 8.43s
                        Total time: 18216.43s
                               ETA: 1039044.6s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.020s, learning 0.166s)
               Value function loss: 55725.4920
                    Surrogate loss: -0.0230
             Mean action noise std: 0.77
                       Mean reward: 684.64
               Mean episode length: 45.19
                  Mean reward/step: 13.65
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2549
Mean episode consecutive_successes: 2.4272
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 8.19s
                        Total time: 18224.61s
                               ETA: 1038898.0s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.055s, learning 0.177s)
               Value function loss: 57293.2495
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 386.13
               Mean episode length: 42.80
                  Mean reward/step: 14.48
       Mean episode length/episode: 6.93
            Mean episode successes: 1.2241
Mean episode consecutive_successes: 2.4455
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 8.23s
                        Total time: 18232.84s
                               ETA: 1038754.2s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.848s, learning 0.203s)
               Value function loss: 52871.9865
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 567.16
               Mean episode length: 45.83
                  Mean reward/step: 13.31
       Mean episode length/episode: 6.90
            Mean episode successes: 1.1436
Mean episode consecutive_successes: 2.4733
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 8.05s
                        Total time: 18240.89s
                               ETA: 1038600.2s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.996s, learning 0.169s)
               Value function loss: 50764.5263
                    Surrogate loss: -0.0230
             Mean action noise std: 0.77
                       Mean reward: 467.24
               Mean episode length: 45.38
                  Mean reward/step: 14.85
       Mean episode length/episode: 7.00
            Mean episode successes: 1.1719
Mean episode consecutive_successes: 2.4781
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 8.17s
                        Total time: 18249.06s
                               ETA: 1038452.9s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.142s, learning 0.162s)
               Value function loss: 57379.4433
                    Surrogate loss: -0.0227
             Mean action noise std: 0.77
                       Mean reward: 423.84
               Mean episode length: 44.27
                  Mean reward/step: 14.82
       Mean episode length/episode: 6.95
            Mean episode successes: 1.2275
Mean episode consecutive_successes: 2.4641
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 8.30s
                        Total time: 18257.36s
                               ETA: 1038313.6s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.241s, learning 0.180s)
               Value function loss: 58294.6826
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 888.00
               Mean episode length: 46.85
                  Mean reward/step: 15.02
       Mean episode length/episode: 6.93
            Mean episode successes: 1.3086
Mean episode consecutive_successes: 2.4636
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 8.42s
                        Total time: 18265.78s
                               ETA: 1038181.1s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.003s, learning 0.159s)
               Value function loss: 51361.1093
                    Surrogate loss: -0.0235
             Mean action noise std: 0.77
                       Mean reward: 783.19
               Mean episode length: 47.19
                  Mean reward/step: 13.90
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2427
Mean episode consecutive_successes: 2.4747
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 8.16s
                        Total time: 18273.95s
                               ETA: 1038034.1s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.083s, learning 0.186s)
               Value function loss: 51278.9330
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 658.90
               Mean episode length: 43.68
                  Mean reward/step: 13.54
       Mean episode length/episode: 7.00
            Mean episode successes: 1.2588
Mean episode consecutive_successes: 2.4858
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 8.27s
                        Total time: 18282.21s
                               ETA: 1037893.2s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.799s, learning 0.190s)
               Value function loss: 56929.9868
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 786.79
               Mean episode length: 48.82
                  Mean reward/step: 15.62
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2568
Mean episode consecutive_successes: 2.5146
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 7.99s
                        Total time: 18290.20s
                               ETA: 1037736.7s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.369s, learning 0.161s)
               Value function loss: 52164.2724
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 499.31
               Mean episode length: 44.99
                  Mean reward/step: 14.35
       Mean episode length/episode: 6.93
            Mean episode successes: 1.2646
Mean episode consecutive_successes: 2.5064
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 8.53s
                        Total time: 18298.73s
                               ETA: 1037611.1s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.383s, learning 0.159s)
               Value function loss: 50418.9162
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 607.52
               Mean episode length: 45.89
                  Mean reward/step: 15.59
       Mean episode length/episode: 6.96
            Mean episode successes: 1.4087
Mean episode consecutive_successes: 2.4874
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 8.54s
                        Total time: 18307.28s
                               ETA: 1037486.2s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.193s, learning 0.177s)
               Value function loss: 55038.3296
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 788.37
               Mean episode length: 47.61
                  Mean reward/step: 15.12
       Mean episode length/episode: 6.90
            Mean episode successes: 1.3335
Mean episode consecutive_successes: 2.5315
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 8.37s
                        Total time: 18315.65s
                               ETA: 1037351.7s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.921s, learning 0.185s)
               Value function loss: 50128.6824
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 920.53
               Mean episode length: 46.32
                  Mean reward/step: 14.46
       Mean episode length/episode: 7.01
            Mean episode successes: 1.3735
Mean episode consecutive_successes: 2.5545
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 8.11s
                        Total time: 18323.75s
                               ETA: 1037202.4s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.282s, learning 0.157s)
               Value function loss: 53937.1559
                    Surrogate loss: -0.0220
             Mean action noise std: 0.77
                       Mean reward: 704.43
               Mean episode length: 44.68
                  Mean reward/step: 15.26
       Mean episode length/episode: 6.88
            Mean episode successes: 1.2744
Mean episode consecutive_successes: 2.5805
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 8.44s
                        Total time: 18332.19s
                               ETA: 1037072.2s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.202s, learning 0.169s)
               Value function loss: 54646.8163
                    Surrogate loss: -0.0234
             Mean action noise std: 0.77
                       Mean reward: 650.62
               Mean episode length: 46.66
                  Mean reward/step: 14.97
       Mean episode length/episode: 6.91
            Mean episode successes: 1.3120
Mean episode consecutive_successes: 2.5643
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 8.37s
                        Total time: 18340.56s
                               ETA: 1036938.2s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.255s, learning 0.169s)
               Value function loss: 56651.8617
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 831.35
               Mean episode length: 47.76
                  Mean reward/step: 13.95
       Mean episode length/episode: 6.90
            Mean episode successes: 1.1636
Mean episode consecutive_successes: 2.5976
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 8.42s
                        Total time: 18348.99s
                               ETA: 1036807.3s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.027s, learning 0.234s)
               Value function loss: 52488.5600
                    Surrogate loss: -0.0266
             Mean action noise std: 0.77
                       Mean reward: 740.47
               Mean episode length: 46.34
                  Mean reward/step: 13.72
       Mean episode length/episode: 7.00
            Mean episode successes: 1.2559
Mean episode consecutive_successes: 2.5685
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 8.26s
                        Total time: 18357.25s
                               ETA: 1036667.4s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.097s, learning 0.168s)
               Value function loss: 60561.3621
                    Surrogate loss: -0.0254
             Mean action noise std: 0.77
                       Mean reward: 960.41
               Mean episode length: 46.07
                  Mean reward/step: 14.41
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2129
Mean episode consecutive_successes: 2.5936
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 8.27s
                        Total time: 18365.51s
                               ETA: 1036527.9s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.805s, learning 0.172s)
               Value function loss: 55515.6203
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 876.93
               Mean episode length: 44.84
                  Mean reward/step: 15.71
       Mean episode length/episode: 6.91
            Mean episode successes: 1.3037
Mean episode consecutive_successes: 2.5720
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 7.98s
                        Total time: 18373.49s
                               ETA: 1036372.3s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.251s, learning 0.259s)
               Value function loss: 54755.1354
                    Surrogate loss: -0.0225
             Mean action noise std: 0.77
                       Mean reward: 569.68
               Mean episode length: 45.18
                  Mean reward/step: 15.57
       Mean episode length/episode: 6.93
            Mean episode successes: 1.3096
Mean episode consecutive_successes: 2.5674
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 8.51s
                        Total time: 18382.00s
                               ETA: 1036246.9s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.073s, learning 0.215s)
               Value function loss: 59934.2300
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 564.47
               Mean episode length: 44.22
                  Mean reward/step: 16.78
       Mean episode length/episode: 6.95
            Mean episode successes: 1.4575
Mean episode consecutive_successes: 2.5333
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 8.29s
                        Total time: 18390.29s
                               ETA: 1036109.1s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1990 steps/s (collection: 7.992s, learning 0.237s)
               Value function loss: 55283.5273
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 571.02
               Mean episode length: 42.97
                  Mean reward/step: 16.41
       Mean episode length/episode: 6.90
            Mean episode successes: 1.4829
Mean episode consecutive_successes: 2.5440
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 8.23s
                        Total time: 18398.52s
                               ETA: 1035968.2s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.375s, learning 0.165s)
               Value function loss: 54921.0555
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 844.36
               Mean episode length: 48.39
                  Mean reward/step: 15.46
       Mean episode length/episode: 6.80
            Mean episode successes: 1.3916
Mean episode consecutive_successes: 2.5858
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 8.54s
                        Total time: 18407.06s
                               ETA: 1035844.9s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.005s, learning 0.171s)
               Value function loss: 53693.5360
                    Surrogate loss: -0.0218
             Mean action noise std: 0.77
                       Mean reward: 634.86
               Mean episode length: 45.66
                  Mean reward/step: 14.41
       Mean episode length/episode: 6.88
            Mean episode successes: 1.2285
Mean episode consecutive_successes: 2.6267
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 8.18s
                        Total time: 18415.23s
                               ETA: 1035701.2s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.894s, learning 0.167s)
               Value function loss: 49472.9095
                    Surrogate loss: -0.0212
             Mean action noise std: 0.77
                       Mean reward: 396.75
               Mean episode length: 40.20
                  Mean reward/step: 12.34
       Mean episode length/episode: 6.96
            Mean episode successes: 1.1182
Mean episode consecutive_successes: 2.6309
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 8.06s
                        Total time: 18423.29s
                               ETA: 1035551.3s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.097s, learning 0.174s)
               Value function loss: 55574.2063
                    Surrogate loss: -0.0253
             Mean action noise std: 0.77
                       Mean reward: 582.36
               Mean episode length: 45.06
                  Mean reward/step: 12.87
       Mean episode length/episode: 6.94
            Mean episode successes: 1.1230
Mean episode consecutive_successes: 2.6157
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 8.27s
                        Total time: 18431.56s
                               ETA: 1035413.3s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.515s, learning 0.227s)
               Value function loss: 52406.1360
                    Surrogate loss: -0.0261
             Mean action noise std: 0.77
                       Mean reward: 883.50
               Mean episode length: 47.89
                  Mean reward/step: 13.93
       Mean episode length/episode: 6.93
            Mean episode successes: 1.1577
Mean episode consecutive_successes: 2.6049
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 8.74s
                        Total time: 18440.30s
                               ETA: 1035301.9s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.495s, learning 0.167s)
               Value function loss: 48799.5465
                    Surrogate loss: -0.0239
             Mean action noise std: 0.77
                       Mean reward: 494.33
               Mean episode length: 45.19
                  Mean reward/step: 14.45
       Mean episode length/episode: 6.91
            Mean episode successes: 1.2305
Mean episode consecutive_successes: 2.5499
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 8.66s
                        Total time: 18448.97s
                               ETA: 1035186.1s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.342s, learning 0.185s)
               Value function loss: 49256.3270
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 552.84
               Mean episode length: 46.91
                  Mean reward/step: 13.82
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2876
Mean episode consecutive_successes: 2.5139
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 8.53s
                        Total time: 18457.49s
                               ETA: 1035062.9s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.157s, learning 0.164s)
               Value function loss: 54346.5280
                    Surrogate loss: -0.0232
             Mean action noise std: 0.77
                       Mean reward: 643.48
               Mean episode length: 43.16
                  Mean reward/step: 14.55
       Mean episode length/episode: 6.90
            Mean episode successes: 1.3071
Mean episode consecutive_successes: 2.5079
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 8.32s
                        Total time: 18465.81s
                               ETA: 1034928.2s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.930s, learning 0.188s)
               Value function loss: 56984.0058
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 980.77
               Mean episode length: 47.16
                  Mean reward/step: 14.98
       Mean episode length/episode: 6.97
            Mean episode successes: 1.3101
Mean episode consecutive_successes: 2.5398
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 8.12s
                        Total time: 18473.93s
                               ETA: 1034782.4s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.322s, learning 0.164s)
               Value function loss: 61579.2570
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 711.59
               Mean episode length: 44.41
                  Mean reward/step: 17.47
       Mean episode length/episode: 6.89
            Mean episode successes: 1.3916
Mean episode consecutive_successes: 2.5476
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 8.49s
                        Total time: 18482.42s
                               ETA: 1034657.3s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.889s, learning 0.185s)
               Value function loss: 60516.5865
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 667.46
               Mean episode length: 45.10
                  Mean reward/step: 16.43
       Mean episode length/episode: 6.95
            Mean episode successes: 1.4165
Mean episode consecutive_successes: 2.5306
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 8.07s
                        Total time: 18490.49s
                               ETA: 1034509.2s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.157s, learning 0.211s)
               Value function loss: 62690.1497
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 553.23
               Mean episode length: 42.60
                  Mean reward/step: 16.75
       Mean episode length/episode: 7.03
            Mean episode successes: 1.4741
Mean episode consecutive_successes: 2.5851
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 8.37s
                        Total time: 18498.86s
                               ETA: 1034377.8s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.687s, learning 0.225s)
               Value function loss: 58594.5840
                    Surrogate loss: -0.0273
             Mean action noise std: 0.77
                       Mean reward: 973.32
               Mean episode length: 46.55
                  Mean reward/step: 17.13
       Mean episode length/episode: 6.98
            Mean episode successes: 1.5142
Mean episode consecutive_successes: 2.6445
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 7.91s
                        Total time: 18506.77s
                               ETA: 1034221.1s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.077s, learning 0.205s)
               Value function loss: 65207.5579
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 526.75
               Mean episode length: 40.87
                  Mean reward/step: 16.88
       Mean episode length/episode: 6.87
            Mean episode successes: 1.5156
Mean episode consecutive_successes: 2.6389
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 8.28s
                        Total time: 18515.05s
                               ETA: 1034085.2s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.951s, learning 0.207s)
               Value function loss: 68770.6132
                    Surrogate loss: -0.0208
             Mean action noise std: 0.77
                       Mean reward: 825.18
               Mean episode length: 46.39
                  Mean reward/step: 16.50
       Mean episode length/episode: 6.85
            Mean episode successes: 1.4795
Mean episode consecutive_successes: 2.6750
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 8.16s
                        Total time: 18523.21s
                               ETA: 1033942.4s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.307s, learning 0.184s)
               Value function loss: 59475.6102
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 1021.38
               Mean episode length: 47.53
                  Mean reward/step: 17.02
       Mean episode length/episode: 7.02
            Mean episode successes: 1.4590
Mean episode consecutive_successes: 2.7289
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 8.49s
                        Total time: 18531.70s
                               ETA: 1033818.4s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.393s, learning 0.161s)
               Value function loss: 63270.6350
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 1342.13
               Mean episode length: 48.46
                  Mean reward/step: 17.27
       Mean episode length/episode: 6.91
            Mean episode successes: 1.4653
Mean episode consecutive_successes: 2.7703
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 11.55s
                        Total time: 18543.25s
                               ETA: 1033865.4s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.634s, learning 0.162s)
               Value function loss: 62599.5188
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 607.77
               Mean episode length: 41.94
                  Mean reward/step: 15.18
       Mean episode length/episode: 6.87
            Mean episode successes: 1.3882
Mean episode consecutive_successes: 2.7545
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 15.80s
                        Total time: 18559.05s
                               ETA: 1034148.6s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.194s, learning 0.169s)
               Value function loss: 58589.6333
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 672.68
               Mean episode length: 45.79
                  Mean reward/step: 14.36
       Mean episode length/episode: 6.96
            Mean episode successes: 1.3164
Mean episode consecutive_successes: 2.7848
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 16.36s
                        Total time: 18575.41s
                               ETA: 1034463.1s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.796s, learning 0.185s)
               Value function loss: 57548.1712
                    Surrogate loss: -0.0247
             Mean action noise std: 0.77
                       Mean reward: 809.24
               Mean episode length: 43.86
                  Mean reward/step: 13.66
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2939
Mean episode consecutive_successes: 2.7652
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 15.98s
                        Total time: 18591.39s
                               ETA: 1034755.9s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.017s, learning 0.230s)
               Value function loss: 55929.2234
                    Surrogate loss: -0.0271
             Mean action noise std: 0.77
                       Mean reward: 799.29
               Mean episode length: 49.02
                  Mean reward/step: 13.58
       Mean episode length/episode: 6.92
            Mean episode successes: 1.1890
Mean episode consecutive_successes: 2.7819
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 16.25s
                        Total time: 18607.64s
                               ETA: 1035063.2s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.958s, learning 0.211s)
               Value function loss: 50814.8637
                    Surrogate loss: -0.0246
             Mean action noise std: 0.77
                       Mean reward: 610.31
               Mean episode length: 46.32
                  Mean reward/step: 14.13
       Mean episode length/episode: 6.93
            Mean episode successes: 1.1509
Mean episode consecutive_successes: 2.7688
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 16.17s
                        Total time: 18623.81s
                               ETA: 1035365.8s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.054s, learning 0.164s)
               Value function loss: 54688.4008
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 323.69
               Mean episode length: 43.32
                  Mean reward/step: 14.96
       Mean episode length/episode: 6.93
            Mean episode successes: 1.2139
Mean episode consecutive_successes: 2.7211
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 16.22s
                        Total time: 18640.03s
                               ETA: 1035670.8s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.786s, learning 0.165s)
               Value function loss: 59386.7673
                    Surrogate loss: -0.0220
             Mean action noise std: 0.76
                       Mean reward: 755.80
               Mean episode length: 47.06
                  Mean reward/step: 14.93
       Mean episode length/episode: 6.92
            Mean episode successes: 1.2217
Mean episode consecutive_successes: 2.7230
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 15.95s
                        Total time: 18655.98s
                               ETA: 1035960.5s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.905s, learning 0.163s)
               Value function loss: 59784.4999
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 368.71
               Mean episode length: 39.86
                  Mean reward/step: 15.66
       Mean episode length/episode: 6.89
            Mean episode successes: 1.2671
Mean episode consecutive_successes: 2.6927
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 16.07s
                        Total time: 18672.05s
                               ETA: 1036256.4s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1039 steps/s (collection: 15.586s, learning 0.170s)
               Value function loss: 57501.7101
                    Surrogate loss: -0.0230
             Mean action noise std: 0.76
                       Mean reward: 591.86
               Mean episode length: 44.95
                  Mean reward/step: 15.93
       Mean episode length/episode: 6.90
            Mean episode successes: 1.3818
Mean episode consecutive_successes: 2.6743
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 15.76s
                        Total time: 18687.80s
                               ETA: 1036534.7s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.028s, learning 0.189s)
               Value function loss: 61087.9359
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 755.81
               Mean episode length: 46.77
                  Mean reward/step: 15.94
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2920
Mean episode consecutive_successes: 2.7196
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 16.22s
                        Total time: 18704.02s
                               ETA: 1036838.1s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.882s, learning 0.195s)
               Value function loss: 60767.7430
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 539.11
               Mean episode length: 44.29
                  Mean reward/step: 15.71
       Mean episode length/episode: 6.88
            Mean episode successes: 1.4131
Mean episode consecutive_successes: 2.6628
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 16.08s
                        Total time: 18720.10s
                               ETA: 1037133.5s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.011s, learning 0.163s)
               Value function loss: 60457.0102
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 1111.34
               Mean episode length: 46.95
                  Mean reward/step: 15.69
       Mean episode length/episode: 7.03
            Mean episode successes: 1.4443
Mean episode consecutive_successes: 2.7223
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 16.17s
                        Total time: 18736.27s
                               ETA: 1037433.9s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.914s, learning 0.171s)
               Value function loss: 57745.5612
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 879.01
               Mean episode length: 44.00
                  Mean reward/step: 15.10
       Mean episode length/episode: 6.80
            Mean episode successes: 1.3535
Mean episode consecutive_successes: 2.7300
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 16.08s
                        Total time: 18752.36s
                               ETA: 1037729.0s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.249s, learning 0.161s)
               Value function loss: 60825.2867
                    Surrogate loss: -0.0220
             Mean action noise std: 0.76
                       Mean reward: 593.39
               Mean episode length: 43.03
                  Mean reward/step: 15.64
       Mean episode length/episode: 6.90
            Mean episode successes: 1.3267
Mean episode consecutive_successes: 2.7089
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 16.41s
                        Total time: 18768.77s
                               ETA: 1038041.7s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.150s, learning 0.265s)
               Value function loss: 57168.3079
                    Surrogate loss: -0.0227
             Mean action noise std: 0.76
                       Mean reward: 301.81
               Mean episode length: 40.77
                  Mean reward/step: 15.04
       Mean episode length/episode: 6.93
            Mean episode successes: 1.3760
Mean episode consecutive_successes: 2.6673
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 16.42s
                        Total time: 18785.18s
                               ETA: 1038354.3s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.110s, learning 0.164s)
               Value function loss: 58538.0062
                    Surrogate loss: -0.0221
             Mean action noise std: 0.76
                       Mean reward: 825.69
               Mean episode length: 42.68
                  Mean reward/step: 14.94
       Mean episode length/episode: 6.90
            Mean episode successes: 1.3960
Mean episode consecutive_successes: 2.6676
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 16.27s
                        Total time: 18801.46s
                               ETA: 1038658.8s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.864s, learning 0.174s)
               Value function loss: 60826.4384
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 696.52
               Mean episode length: 43.78
                  Mean reward/step: 15.43
       Mean episode length/episode: 6.93
            Mean episode successes: 1.3687
Mean episode consecutive_successes: 2.6836
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 16.04s
                        Total time: 18817.49s
                               ETA: 1038949.9s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.008s, learning 0.164s)
               Value function loss: 58074.2142
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 786.75
               Mean episode length: 44.35
                  Mean reward/step: 14.27
       Mean episode length/episode: 6.90
            Mean episode successes: 1.2993
Mean episode consecutive_successes: 2.6974
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 16.17s
                        Total time: 18833.67s
                               ETA: 1039248.0s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.907s, learning 0.169s)
               Value function loss: 58173.7334
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 589.04
               Mean episode length: 43.39
                  Mean reward/step: 14.13
       Mean episode length/episode: 6.93
            Mean episode successes: 1.1968
Mean episode consecutive_successes: 2.7015
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 16.08s
                        Total time: 18849.74s
                               ETA: 1039540.5s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.915s, learning 0.165s)
               Value function loss: 53545.6276
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 538.14
               Mean episode length: 43.14
                  Mean reward/step: 15.58
       Mean episode length/episode: 7.00
            Mean episode successes: 1.3081
Mean episode consecutive_successes: 2.6862
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 16.08s
                        Total time: 18865.82s
                               ETA: 1039832.8s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.912s, learning 0.190s)
               Value function loss: 59142.7776
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 628.50
               Mean episode length: 43.76
                  Mean reward/step: 16.26
       Mean episode length/episode: 6.93
            Mean episode successes: 1.4131
Mean episode consecutive_successes: 2.6562
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 16.10s
                        Total time: 18881.92s
                               ETA: 1040126.0s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.730s, learning 0.230s)
               Value function loss: 57356.0968
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 543.54
               Mean episode length: 43.61
                  Mean reward/step: 15.77
       Mean episode length/episode: 6.97
            Mean episode successes: 1.4521
Mean episode consecutive_successes: 2.6523
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 15.96s
                        Total time: 18897.88s
                               ETA: 1040411.0s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.635s, learning 0.168s)
               Value function loss: 58546.7660
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 583.18
               Mean episode length: 47.59
                  Mean reward/step: 16.62
       Mean episode length/episode: 6.96
            Mean episode successes: 1.5093
Mean episode consecutive_successes: 2.6741
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 15.80s
                        Total time: 18913.69s
                               ETA: 1040687.1s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.833s, learning 0.164s)
               Value function loss: 63643.4562
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 974.32
               Mean episode length: 48.19
                  Mean reward/step: 16.85
       Mean episode length/episode: 6.95
            Mean episode successes: 1.4595
Mean episode consecutive_successes: 2.7293
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 16.00s
                        Total time: 18929.68s
                               ETA: 1040973.6s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.138s, learning 0.164s)
               Value function loss: 59968.9112
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 507.38
               Mean episode length: 41.24
                  Mean reward/step: 16.89
       Mean episode length/episode: 6.90
            Mean episode successes: 1.4438
Mean episode consecutive_successes: 2.7215
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 16.30s
                        Total time: 18945.98s
                               ETA: 1041276.4s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.641s, learning 0.174s)
               Value function loss: 63184.3411
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 818.52
               Mean episode length: 47.08
                  Mean reward/step: 17.31
       Mean episode length/episode: 6.98
            Mean episode successes: 1.5186
Mean episode consecutive_successes: 2.7345
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 15.82s
                        Total time: 18961.80s
                               ETA: 1041552.1s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.321s, learning 0.165s)
               Value function loss: 62171.1618
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 800.10
               Mean episode length: 45.99
                  Mean reward/step: 16.28
       Mean episode length/episode: 6.93
            Mean episode successes: 1.4448
Mean episode consecutive_successes: 2.7805
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 16.49s
                        Total time: 18978.29s
                               ETA: 1041864.4s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.328s, learning 0.189s)
               Value function loss: 61025.8159
                    Surrogate loss: -0.0209
             Mean action noise std: 0.76
                       Mean reward: 694.98
               Mean episode length: 45.36
                  Mean reward/step: 15.90
       Mean episode length/episode: 6.97
            Mean episode successes: 1.4556
Mean episode consecutive_successes: 2.7875
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 16.52s
                        Total time: 18994.80s
                               ETA: 1042178.0s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.676s, learning 0.194s)
               Value function loss: 62952.2082
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 661.39
               Mean episode length: 44.34
                  Mean reward/step: 17.52
       Mean episode length/episode: 6.92
            Mean episode successes: 1.4946
Mean episode consecutive_successes: 2.7953
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 15.87s
                        Total time: 19010.67s
                               ETA: 1042455.7s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.630s, learning 0.180s)
               Value function loss: 58007.1101
                    Surrogate loss: -0.0225
             Mean action noise std: 0.76
                       Mean reward: 1005.51
               Mean episode length: 46.28
                  Mean reward/step: 16.16
       Mean episode length/episode: 6.87
            Mean episode successes: 1.3657
Mean episode consecutive_successes: 2.8262
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 15.81s
                        Total time: 19026.48s
                               ETA: 1042729.8s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.125s, learning 0.156s)
               Value function loss: 57252.0304
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 581.13
               Mean episode length: 47.56
                  Mean reward/step: 15.42
       Mean episode length/episode: 6.98
            Mean episode successes: 1.4482
Mean episode consecutive_successes: 2.7889
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 16.28s
                        Total time: 19042.76s
                               ETA: 1043029.4s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.029s, learning 0.157s)
               Value function loss: 62713.5990
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 874.78
               Mean episode length: 49.54
                  Mean reward/step: 16.93
       Mean episode length/episode: 6.97
            Mean episode successes: 1.4951
Mean episode consecutive_successes: 2.8035
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 16.19s
                        Total time: 19058.95s
                               ETA: 1043323.5s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.986s, learning 0.182s)
               Value function loss: 65132.1083
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 785.36
               Mean episode length: 45.64
                  Mean reward/step: 17.85
       Mean episode length/episode: 6.93
            Mean episode successes: 1.6108
Mean episode consecutive_successes: 2.7740
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 16.17s
                        Total time: 19075.12s
                               ETA: 1043616.2s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1047 steps/s (collection: 15.471s, learning 0.164s)
               Value function loss: 64822.3214
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 846.85
               Mean episode length: 44.58
                  Mean reward/step: 17.29
       Mean episode length/episode: 6.88
            Mean episode successes: 1.5239
Mean episode consecutive_successes: 2.7929
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 15.63s
                        Total time: 19090.75s
                               ETA: 1043879.4s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.622s, learning 0.160s)
               Value function loss: 62073.3045
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 485.88
               Mean episode length: 43.75
                  Mean reward/step: 15.67
       Mean episode length/episode: 6.88
            Mean episode successes: 1.4546
Mean episode consecutive_successes: 2.8206
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 15.78s
                        Total time: 19106.53s
                               ETA: 1044150.3s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.841s, learning 0.162s)
               Value function loss: 60547.3686
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 510.59
               Mean episode length: 42.86
                  Mean reward/step: 16.10
       Mean episode length/episode: 6.97
            Mean episode successes: 1.4858
Mean episode consecutive_successes: 2.8317
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 16.00s
                        Total time: 19122.54s
                               ETA: 1044433.0s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.238s, learning 0.181s)
               Value function loss: 63989.2040
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 1018.98
               Mean episode length: 47.11
                  Mean reward/step: 17.31
       Mean episode length/episode: 6.96
            Mean episode successes: 1.4502
Mean episode consecutive_successes: 2.8718
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 16.42s
                        Total time: 19138.96s
                               ETA: 1044738.1s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1619 steps/s (collection: 9.935s, learning 0.182s)
               Value function loss: 65724.4932
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 605.84
               Mean episode length: 42.69
                  Mean reward/step: 16.06
       Mean episode length/episode: 6.88
            Mean episode successes: 1.3857
Mean episode consecutive_successes: 2.8672
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 10.12s
                        Total time: 19149.07s
                               ETA: 1044699.0s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.052s, learning 0.279s)
               Value function loss: 66307.5622
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 579.45
               Mean episode length: 45.00
                  Mean reward/step: 16.76
       Mean episode length/episode: 6.87
            Mean episode successes: 1.4282
Mean episode consecutive_successes: 2.8571
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 8.33s
                        Total time: 19157.40s
                               ETA: 1044562.6s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.084s, learning 0.168s)
               Value function loss: 65230.5338
                    Surrogate loss: -0.0236
             Mean action noise std: 0.76
                       Mean reward: 1106.19
               Mean episode length: 47.45
                  Mean reward/step: 17.95
       Mean episode length/episode: 6.97
            Mean episode successes: 1.5518
Mean episode consecutive_successes: 2.8532
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 8.25s
                        Total time: 19165.66s
                               ETA: 1044421.9s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.009s, learning 0.161s)
               Value function loss: 69810.3826
                    Surrogate loss: -0.0228
             Mean action noise std: 0.76
                       Mean reward: 1044.73
               Mean episode length: 48.98
                  Mean reward/step: 16.13
       Mean episode length/episode: 6.98
            Mean episode successes: 1.5200
Mean episode consecutive_successes: 2.8856
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 8.17s
                        Total time: 19173.83s
                               ETA: 1044277.0s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.221s, learning 0.169s)
               Value function loss: 60747.7228
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 710.17
               Mean episode length: 46.19
                  Mean reward/step: 15.32
       Mean episode length/episode: 7.00
            Mean episode successes: 1.4917
Mean episode consecutive_successes: 2.8801
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 8.39s
                        Total time: 19182.22s
                               ETA: 1044144.2s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.960s, learning 0.162s)
               Value function loss: 69355.1828
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 576.65
               Mean episode length: 44.49
                  Mean reward/step: 16.52
       Mean episode length/episode: 6.88
            Mean episode successes: 1.5190
Mean episode consecutive_successes: 2.8687
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 8.12s
                        Total time: 19190.34s
                               ETA: 1043996.9s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.847s, learning 0.162s)
               Value function loss: 60465.7688
                    Surrogate loss: -0.0230
             Mean action noise std: 0.76
                       Mean reward: 597.34
               Mean episode length: 45.17
                  Mean reward/step: 15.13
       Mean episode length/episode: 6.91
            Mean episode successes: 1.4507
Mean episode consecutive_successes: 2.8775
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 8.01s
                        Total time: 19198.35s
                               ETA: 1043843.7s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.027s, learning 0.173s)
               Value function loss: 61594.7791
                    Surrogate loss: -0.0045
             Mean action noise std: 0.76
                       Mean reward: 972.21
               Mean episode length: 49.25
                  Mean reward/step: 15.87
       Mean episode length/episode: 6.95
            Mean episode successes: 1.4883
Mean episode consecutive_successes: 2.8787
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 8.20s
                        Total time: 19206.55s
                               ETA: 1043701.0s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.379s, learning 0.207s)
               Value function loss: 55461.9491
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 910.44
               Mean episode length: 46.21
                  Mean reward/step: 17.08
       Mean episode length/episode: 7.00
            Mean episode successes: 1.5034
Mean episode consecutive_successes: 2.8766
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 8.59s
                        Total time: 19215.13s
                               ETA: 1043579.4s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.120s, learning 0.166s)
               Value function loss: 58897.4466
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 915.36
               Mean episode length: 46.09
                  Mean reward/step: 15.49
       Mean episode length/episode: 6.83
            Mean episode successes: 1.5308
Mean episode consecutive_successes: 2.8488
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 8.29s
                        Total time: 19223.42s
                               ETA: 1043441.7s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.078s, learning 0.169s)
               Value function loss: 64989.3784
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 724.59
               Mean episode length: 44.55
                  Mean reward/step: 15.10
       Mean episode length/episode: 6.96
            Mean episode successes: 1.3672
Mean episode consecutive_successes: 2.8975
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 8.25s
                        Total time: 19231.67s
                               ETA: 1043301.9s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.417s, learning 0.165s)
               Value function loss: 64833.4485
                    Surrogate loss: -0.0230
             Mean action noise std: 0.76
                       Mean reward: 715.65
               Mean episode length: 46.15
                  Mean reward/step: 16.76
       Mean episode length/episode: 7.00
            Mean episode successes: 1.4370
Mean episode consecutive_successes: 2.8846
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 8.58s
                        Total time: 19240.25s
                               ETA: 1043180.5s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.135s, learning 0.170s)
               Value function loss: 65983.3223
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 776.53
               Mean episode length: 44.36
                  Mean reward/step: 16.68
       Mean episode length/episode: 6.94
            Mean episode successes: 1.4878
Mean episode consecutive_successes: 2.8737
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 8.31s
                        Total time: 19248.55s
                               ETA: 1043044.2s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.917s, learning 0.162s)
               Value function loss: 67376.8755
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 632.23
               Mean episode length: 45.39
                  Mean reward/step: 16.86
       Mean episode length/episode: 6.90
            Mean episode successes: 1.4097
Mean episode consecutive_successes: 2.8996
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 8.08s
                        Total time: 19256.63s
                               ETA: 1042895.8s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.239s, learning 0.171s)
               Value function loss: 64751.1709
                    Surrogate loss: -0.0231
             Mean action noise std: 0.76
                       Mean reward: 450.51
               Mean episode length: 46.70
                  Mean reward/step: 16.12
       Mean episode length/episode: 6.91
            Mean episode successes: 1.4844
Mean episode consecutive_successes: 2.8600
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 8.41s
                        Total time: 19265.04s
                               ETA: 1042765.5s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.556s, learning 0.186s)
               Value function loss: 74210.7252
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 387.16
               Mean episode length: 45.54
                  Mean reward/step: 16.74
       Mean episode length/episode: 6.97
            Mean episode successes: 1.5835
Mean episode consecutive_successes: 2.8559
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 8.74s
                        Total time: 19273.78s
                               ETA: 1042653.2s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.381s, learning 0.162s)
               Value function loss: 60603.0051
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 874.37
               Mean episode length: 44.58
                  Mean reward/step: 15.89
       Mean episode length/episode: 6.96
            Mean episode successes: 1.5474
Mean episode consecutive_successes: 2.8904
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 8.54s
                        Total time: 19282.33s
                               ETA: 1042530.3s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.110s, learning 0.184s)
               Value function loss: 60055.9723
                    Surrogate loss: -0.0235
             Mean action noise std: 0.76
                       Mean reward: 741.76
               Mean episode length: 44.36
                  Mean reward/step: 15.74
       Mean episode length/episode: 6.90
            Mean episode successes: 1.4941
Mean episode consecutive_successes: 2.8839
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 8.29s
                        Total time: 19290.62s
                               ETA: 1042394.1s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.261s, learning 0.158s)
               Value function loss: 66075.2640
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 786.06
               Mean episode length: 43.12
                  Mean reward/step: 15.55
       Mean episode length/episode: 6.90
            Mean episode successes: 1.4419
Mean episode consecutive_successes: 2.9087
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 8.42s
                        Total time: 19299.04s
                               ETA: 1042264.8s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.183s, learning 0.204s)
               Value function loss: 58567.3696
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 825.15
               Mean episode length: 46.03
                  Mean reward/step: 16.30
       Mean episode length/episode: 6.93
            Mean episode successes: 1.5791
Mean episode consecutive_successes: 2.8660
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 8.39s
                        Total time: 19307.42s
                               ETA: 1042133.9s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.445s, learning 0.165s)
               Value function loss: 64546.6863
                    Surrogate loss: -0.0213
             Mean action noise std: 0.76
                       Mean reward: 694.27
               Mean episode length: 44.06
                  Mean reward/step: 17.90
       Mean episode length/episode: 6.95
            Mean episode successes: 1.4907
Mean episode consecutive_successes: 2.9144
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 8.61s
                        Total time: 19316.03s
                               ETA: 1042015.1s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.191s, learning 0.162s)
               Value function loss: 71972.7024
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 914.41
               Mean episode length: 44.60
                  Mean reward/step: 17.96
       Mean episode length/episode: 6.99
            Mean episode successes: 1.5972
Mean episode consecutive_successes: 2.9080
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 8.35s
                        Total time: 19324.39s
                               ETA: 1041882.6s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.483s, learning 0.189s)
               Value function loss: 73695.0190
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 851.30
               Mean episode length: 43.35
                  Mean reward/step: 18.49
       Mean episode length/episode: 6.90
            Mean episode successes: 1.6528
Mean episode consecutive_successes: 2.9158
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 8.67s
                        Total time: 19333.06s
                               ETA: 1041767.4s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.182s, learning 0.225s)
               Value function loss: 70807.7968
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 917.38
               Mean episode length: 44.95
                  Mean reward/step: 19.51
       Mean episode length/episode: 6.91
            Mean episode successes: 1.6650
Mean episode consecutive_successes: 2.9535
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 8.41s
                        Total time: 19341.46s
                               ETA: 1041638.1s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.306s, learning 0.194s)
               Value function loss: 65360.2848
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 400.45
               Mean episode length: 42.57
                  Mean reward/step: 16.84
       Mean episode length/episode: 6.96
            Mean episode successes: 1.6284
Mean episode consecutive_successes: 2.9715
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 8.50s
                        Total time: 19349.97s
                               ETA: 1041514.0s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.013s, learning 0.192s)
               Value function loss: 67147.7252
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 931.13
               Mean episode length: 47.49
                  Mean reward/step: 16.78
       Mean episode length/episode: 6.99
            Mean episode successes: 1.7236
Mean episode consecutive_successes: 2.9725
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 8.20s
                        Total time: 19358.17s
                               ETA: 1041374.1s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.208s, learning 0.155s)
               Value function loss: 75804.6826
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 654.48
               Mean episode length: 44.73
                  Mean reward/step: 16.91
       Mean episode length/episode: 6.92
            Mean episode successes: 1.6118
Mean episode consecutive_successes: 2.9918
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 8.36s
                        Total time: 19366.53s
                               ETA: 1041242.8s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.924s, learning 0.156s)
               Value function loss: 73441.4619
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 924.19
               Mean episode length: 47.92
                  Mean reward/step: 17.63
       Mean episode length/episode: 6.97
            Mean episode successes: 1.4932
Mean episode consecutive_successes: 3.0702
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 8.08s
                        Total time: 19374.61s
                               ETA: 1041096.5s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.145s, learning 0.162s)
               Value function loss: 73153.2653
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 916.07
               Mean episode length: 47.75
                  Mean reward/step: 17.48
       Mean episode length/episode: 7.00
            Mean episode successes: 1.4653
Mean episode consecutive_successes: 3.0939
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 8.31s
                        Total time: 19382.92s
                               ETA: 1040962.5s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.158s, learning 0.233s)
               Value function loss: 69606.2208
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 802.18
               Mean episode length: 45.12
                  Mean reward/step: 18.58
       Mean episode length/episode: 7.00
            Mean episode successes: 1.6040
Mean episode consecutive_successes: 3.0861
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 8.39s
                        Total time: 19391.31s
                               ETA: 1040833.1s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.180s, learning 0.165s)
               Value function loss: 80729.1256
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 632.03
               Mean episode length: 45.20
                  Mean reward/step: 19.43
       Mean episode length/episode: 6.94
            Mean episode successes: 1.6948
Mean episode consecutive_successes: 3.0666
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 8.34s
                        Total time: 19399.65s
                               ETA: 1040701.4s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.045s, learning 0.164s)
               Value function loss: 72695.6053
                    Surrogate loss: -0.0293
             Mean action noise std: 0.76
                       Mean reward: 910.02
               Mean episode length: 50.46
                  Mean reward/step: 18.84
       Mean episode length/episode: 6.93
            Mean episode successes: 1.7573
Mean episode consecutive_successes: 3.0601
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 8.21s
                        Total time: 19407.86s
                               ETA: 1040562.5s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.140s, learning 0.158s)
               Value function loss: 72187.8784
                    Surrogate loss: -0.0235
             Mean action noise std: 0.76
                       Mean reward: 825.68
               Mean episode length: 50.56
                  Mean reward/step: 17.38
       Mean episode length/episode: 6.88
            Mean episode successes: 1.7773
Mean episode consecutive_successes: 3.0626
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 8.30s
                        Total time: 19416.16s
                               ETA: 1040428.6s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.046s, learning 0.188s)
               Value function loss: 60496.1821
                    Surrogate loss: -0.0248
             Mean action noise std: 0.76
                       Mean reward: 1041.23
               Mean episode length: 47.63
                  Mean reward/step: 16.14
       Mean episode length/episode: 6.95
            Mean episode successes: 1.6167
Mean episode consecutive_successes: 3.1368
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 8.23s
                        Total time: 19424.40s
                               ETA: 1040291.5s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.372s, learning 0.163s)
               Value function loss: 69322.2491
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 700.59
               Mean episode length: 46.38
                  Mean reward/step: 16.56
       Mean episode length/episode: 6.91
            Mean episode successes: 1.4517
Mean episode consecutive_successes: 3.1463
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 8.53s
                        Total time: 19432.93s
                               ETA: 1040170.5s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.123s, learning 0.170s)
               Value function loss: 54702.7526
                    Surrogate loss: -0.0266
             Mean action noise std: 0.76
                       Mean reward: 719.00
               Mean episode length: 44.15
                  Mean reward/step: 16.27
       Mean episode length/episode: 6.93
            Mean episode successes: 1.5078
Mean episode consecutive_successes: 3.1108
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 8.29s
                        Total time: 19441.23s
                               ETA: 1040036.7s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.181s, learning 0.167s)
               Value function loss: 55027.4123
                    Surrogate loss: -0.0225
             Mean action noise std: 0.76
                       Mean reward: 902.75
               Mean episode length: 46.13
                  Mean reward/step: 15.57
       Mean episode length/episode: 6.92
            Mean episode successes: 1.4771
Mean episode consecutive_successes: 3.1154
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 8.35s
                        Total time: 19449.57s
                               ETA: 1039905.9s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.882s, learning 0.187s)
               Value function loss: 59906.0197
                    Surrogate loss: -0.0239
             Mean action noise std: 0.76
                       Mean reward: 1020.69
               Mean episode length: 46.41
                  Mean reward/step: 15.77
       Mean episode length/episode: 6.95
            Mean episode successes: 1.4038
Mean episode consecutive_successes: 3.1269
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 8.07s
                        Total time: 19457.64s
                               ETA: 1039760.5s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.891s, learning 0.210s)
               Value function loss: 64140.7124
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 494.76
               Mean episode length: 45.48
                  Mean reward/step: 17.34
       Mean episode length/episode: 7.00
            Mean episode successes: 1.4502
Mean episode consecutive_successes: 3.1010
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 8.10s
                        Total time: 19465.74s
                               ETA: 1039616.8s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.238s, learning 0.218s)
               Value function loss: 62662.6453
                    Surrogate loss: -0.0234
             Mean action noise std: 0.76
                       Mean reward: 839.95
               Mean episode length: 45.78
                  Mean reward/step: 16.42
       Mean episode length/episode: 6.97
            Mean episode successes: 1.5405
Mean episode consecutive_successes: 3.0728
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 8.46s
                        Total time: 19474.20s
                               ETA: 1039492.3s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.094s, learning 0.230s)
               Value function loss: 71358.2697
                    Surrogate loss: -0.0243
             Mean action noise std: 0.76
                       Mean reward: 868.71
               Mean episode length: 47.49
                  Mean reward/step: 17.58
       Mean episode length/episode: 6.98
            Mean episode successes: 1.5190
Mean episode consecutive_successes: 3.0912
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 8.32s
                        Total time: 19482.52s
                               ETA: 1039360.9s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.161s)
               Value function loss: 70131.1714
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 785.13
               Mean episode length: 45.84
                  Mean reward/step: 16.66
       Mean episode length/episode: 6.93
            Mean episode successes: 1.4746
Mean episode consecutive_successes: 3.0724
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 8.40s
                        Total time: 19490.92s
                               ETA: 1039233.6s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.297s, learning 0.187s)
               Value function loss: 64259.5151
                    Surrogate loss: -0.0213
             Mean action noise std: 0.76
                       Mean reward: 976.36
               Mean episode length: 47.58
                  Mean reward/step: 15.94
       Mean episode length/episode: 7.02
            Mean episode successes: 1.5171
Mean episode consecutive_successes: 3.0685
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 8.48s
                        Total time: 19499.41s
                               ETA: 1039111.0s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.327s, learning 0.164s)
               Value function loss: 58775.1701
                    Surrogate loss: -0.0209
             Mean action noise std: 0.76
                       Mean reward: 807.93
               Mean episode length: 46.49
                  Mean reward/step: 16.19
       Mean episode length/episode: 6.88
            Mean episode successes: 1.4736
Mean episode consecutive_successes: 3.0541
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 8.49s
                        Total time: 19507.90s
                               ETA: 1038988.8s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.252s, learning 0.176s)
               Value function loss: 61031.0120
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 620.27
               Mean episode length: 45.79
                  Mean reward/step: 15.22
       Mean episode length/episode: 6.93
            Mean episode successes: 1.4590
Mean episode consecutive_successes: 3.0125
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 8.43s
                        Total time: 19516.33s
                               ETA: 1038863.4s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.161s, learning 0.159s)
               Value function loss: 62333.2984
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 1045.94
               Mean episode length: 47.25
                  Mean reward/step: 14.75
       Mean episode length/episode: 6.92
            Mean episode successes: 1.3574
Mean episode consecutive_successes: 3.0366
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 8.32s
                        Total time: 19524.65s
                               ETA: 1038732.4s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.919s, learning 0.176s)
               Value function loss: 59349.0408
                    Surrogate loss: -0.0246
             Mean action noise std: 0.76
                       Mean reward: 770.00
               Mean episode length: 46.00
                  Mean reward/step: 15.61
       Mean episode length/episode: 7.05
            Mean episode successes: 1.3223
Mean episode consecutive_successes: 3.0562
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 8.09s
                        Total time: 19532.74s
                               ETA: 1038589.6s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.189s, learning 0.156s)
               Value function loss: 66332.6540
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 720.28
               Mean episode length: 49.33
                  Mean reward/step: 16.79
       Mean episode length/episode: 6.92
            Mean episode successes: 1.4624
Mean episode consecutive_successes: 3.0015
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 8.34s
                        Total time: 19541.09s
                               ETA: 1038460.1s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.860s, learning 0.202s)
               Value function loss: 63726.7748
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 826.37
               Mean episode length: 47.93
                  Mean reward/step: 18.21
       Mean episode length/episode: 6.99
            Mean episode successes: 1.6055
Mean episode consecutive_successes: 2.9711
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 8.06s
                        Total time: 19549.15s
                               ETA: 1038315.8s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.383s, learning 0.183s)
               Value function loss: 59970.9418
                    Surrogate loss: -0.0239
             Mean action noise std: 0.76
                       Mean reward: 836.94
               Mean episode length: 48.66
                  Mean reward/step: 16.41
       Mean episode length/episode: 6.90
            Mean episode successes: 1.5283
Mean episode consecutive_successes: 2.9846
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 8.57s
                        Total time: 19557.71s
                               ETA: 1038198.3s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.041s, learning 0.172s)
               Value function loss: 61789.2233
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 701.16
               Mean episode length: 47.26
                  Mean reward/step: 16.93
       Mean episode length/episode: 6.94
            Mean episode successes: 1.4546
Mean episode consecutive_successes: 3.0214
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 8.21s
                        Total time: 19565.93s
                               ETA: 1038062.3s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.400s, learning 0.173s)
               Value function loss: 59171.8476
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 644.33
               Mean episode length: 44.64
                  Mean reward/step: 17.02
       Mean episode length/episode: 6.98
            Mean episode successes: 1.5410
Mean episode consecutive_successes: 2.9898
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 8.57s
                        Total time: 19574.50s
                               ETA: 1037945.5s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.357s, learning 0.161s)
               Value function loss: 63374.4905
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 673.97
               Mean episode length: 44.08
                  Mean reward/step: 17.34
       Mean episode length/episode: 7.00
            Mean episode successes: 1.6143
Mean episode consecutive_successes: 2.9851
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 8.52s
                        Total time: 19583.02s
                               ETA: 1037825.9s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.394s, learning 0.163s)
               Value function loss: 65333.0135
                    Surrogate loss: -0.0251
             Mean action noise std: 0.76
                       Mean reward: 935.50
               Mean episode length: 46.02
                  Mean reward/step: 18.51
       Mean episode length/episode: 6.90
            Mean episode successes: 1.6611
Mean episode consecutive_successes: 3.0074
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 8.56s
                        Total time: 19591.58s
                               ETA: 1037708.6s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.039s, learning 0.175s)
               Value function loss: 63706.9162
                    Surrogate loss: -0.0244
             Mean action noise std: 0.76
                       Mean reward: 1268.99
               Mean episode length: 47.41
                  Mean reward/step: 18.63
       Mean episode length/episode: 6.92
            Mean episode successes: 1.6221
Mean episode consecutive_successes: 3.0607
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 8.21s
                        Total time: 19599.79s
                               ETA: 1037573.1s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.408s, learning 0.212s)
               Value function loss: 65899.7657
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 654.04
               Mean episode length: 48.15
                  Mean reward/step: 17.74
       Mean episode length/episode: 6.97
            Mean episode successes: 1.6602
Mean episode consecutive_successes: 3.0467
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 8.62s
                        Total time: 19608.41s
                               ETA: 1037459.2s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.661s, learning 0.228s)
               Value function loss: 63136.1279
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 1003.54
               Mean episode length: 47.27
                  Mean reward/step: 18.64
       Mean episode length/episode: 6.99
            Mean episode successes: 1.6523
Mean episode consecutive_successes: 3.0973
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 7.89s
                        Total time: 19616.30s
                               ETA: 1037306.9s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.442s, learning 0.183s)
               Value function loss: 67625.6786
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 737.50
               Mean episode length: 45.94
                  Mean reward/step: 18.15
       Mean episode length/episode: 6.89
            Mean episode successes: 1.7065
Mean episode consecutive_successes: 3.0828
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 8.62s
                        Total time: 19624.92s
                               ETA: 1037193.5s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.430s, learning 0.168s)
               Value function loss: 67376.5542
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 1190.26
               Mean episode length: 45.91
                  Mean reward/step: 18.88
       Mean episode length/episode: 6.96
            Mean episode successes: 1.6499
Mean episode consecutive_successes: 3.1461
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 8.60s
                        Total time: 19633.52s
                               ETA: 1037078.9s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.076s, learning 0.170s)
               Value function loss: 67558.2871
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 669.02
               Mean episode length: 48.03
                  Mean reward/step: 17.73
       Mean episode length/episode: 7.06
            Mean episode successes: 1.6602
Mean episode consecutive_successes: 3.1610
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 8.25s
                        Total time: 19641.77s
                               ETA: 1036945.8s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.233s, learning 0.204s)
               Value function loss: 68302.1969
                    Surrogate loss: -0.0204
             Mean action noise std: 0.76
                       Mean reward: 795.22
               Mean episode length: 45.87
                  Mean reward/step: 18.28
       Mean episode length/episode: 6.94
            Mean episode successes: 1.6753
Mean episode consecutive_successes: 3.1854
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 8.44s
                        Total time: 19650.20s
                               ETA: 1036822.9s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.984s, learning 0.235s)
               Value function loss: 65404.2540
                    Surrogate loss: 0.0054
             Mean action noise std: 0.76
                       Mean reward: 491.96
               Mean episode length: 44.77
                  Mean reward/step: 18.21
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8555
Mean episode consecutive_successes: 3.1160
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 8.22s
                        Total time: 19658.42s
                               ETA: 1036688.6s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.369s, learning 0.168s)
               Value function loss: 70363.7541
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 1135.27
               Mean episode length: 49.98
                  Mean reward/step: 20.18
       Mean episode length/episode: 6.90
            Mean episode successes: 1.8237
Mean episode consecutive_successes: 3.1875
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 8.54s
                        Total time: 19666.96s
                               ETA: 1036571.3s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.273s, learning 0.191s)
               Value function loss: 71336.2410
                    Surrogate loss: -0.0249
             Mean action noise std: 0.76
                       Mean reward: 843.28
               Mean episode length: 46.50
                  Mean reward/step: 19.08
       Mean episode length/episode: 7.01
            Mean episode successes: 1.7173
Mean episode consecutive_successes: 3.2626
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 8.46s
                        Total time: 19675.42s
                               ETA: 1036450.2s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.921s, learning 0.189s)
               Value function loss: 70746.2891
                    Surrogate loss: -0.0281
             Mean action noise std: 0.76
                       Mean reward: 764.41
               Mean episode length: 48.59
                  Mean reward/step: 18.97
       Mean episode length/episode: 7.03
            Mean episode successes: 1.7910
Mean episode consecutive_successes: 3.2728
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 8.11s
                        Total time: 19683.53s
                               ETA: 1036310.6s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.160s)
               Value function loss: 80143.1252
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 901.63
               Mean episode length: 48.02
                  Mean reward/step: 17.43
       Mean episode length/episode: 7.00
            Mean episode successes: 1.6885
Mean episode consecutive_successes: 3.3145
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 8.31s
                        Total time: 19691.84s
                               ETA: 1036181.4s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.294s, learning 0.261s)
               Value function loss: 75882.2740
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 936.46
               Mean episode length: 48.25
                  Mean reward/step: 16.57
       Mean episode length/episode: 6.94
            Mean episode successes: 1.7129
Mean episode consecutive_successes: 3.2961
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 8.55s
                        Total time: 19700.39s
                               ETA: 1036065.4s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.195s, learning 0.181s)
               Value function loss: 66091.0023
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 719.39
               Mean episode length: 48.45
                  Mean reward/step: 17.05
       Mean episode length/episode: 6.92
            Mean episode successes: 1.5762
Mean episode consecutive_successes: 3.3099
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 8.38s
                        Total time: 19708.77s
                               ETA: 1035940.2s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.958s, learning 0.161s)
               Value function loss: 63395.2504
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 790.24
               Mean episode length: 46.26
                  Mean reward/step: 17.84
       Mean episode length/episode: 7.08
            Mean episode successes: 1.6201
Mean episode consecutive_successes: 3.3268
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 8.12s
                        Total time: 19716.89s
                               ETA: 1035801.6s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.270s, learning 0.165s)
               Value function loss: 66096.1159
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 1027.47
               Mean episode length: 49.27
                  Mean reward/step: 17.37
       Mean episode length/episode: 6.87
            Mean episode successes: 1.5938
Mean episode consecutive_successes: 3.3230
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 8.43s
                        Total time: 19725.32s
                               ETA: 1035679.7s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.294s, learning 0.161s)
               Value function loss: 63365.6181
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 1001.72
               Mean episode length: 48.48
                  Mean reward/step: 16.48
       Mean episode length/episode: 6.94
            Mean episode successes: 1.5996
Mean episode consecutive_successes: 3.3035
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 8.45s
                        Total time: 19733.78s
                               ETA: 1035559.0s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.784s, learning 0.157s)
               Value function loss: 61293.0650
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 755.33
               Mean episode length: 50.76
                  Mean reward/step: 16.99
       Mean episode length/episode: 6.93
            Mean episode successes: 1.4839
Mean episode consecutive_successes: 3.3205
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 7.94s
                        Total time: 19741.72s
                               ETA: 1035411.5s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.164s)
               Value function loss: 62733.3252
                    Surrogate loss: -0.0255
             Mean action noise std: 0.76
                       Mean reward: 822.22
               Mean episode length: 45.61
                  Mean reward/step: 15.71
       Mean episode length/episode: 6.96
            Mean episode successes: 1.4868
Mean episode consecutive_successes: 3.2801
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 8.37s
                        Total time: 19750.09s
                               ETA: 1035286.8s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.147s, learning 0.180s)
               Value function loss: 69083.9100
                    Surrogate loss: -0.0168
             Mean action noise std: 0.76
                       Mean reward: 916.85
               Mean episode length: 48.38
                  Mean reward/step: 16.67
       Mean episode length/episode: 6.97
            Mean episode successes: 1.4873
Mean episode consecutive_successes: 3.2778
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 8.33s
                        Total time: 19758.42s
                               ETA: 1035159.8s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.102s, learning 0.162s)
               Value function loss: 62658.9028
                    Surrogate loss: -0.0245
             Mean action noise std: 0.76
                       Mean reward: 622.28
               Mean episode length: 45.26
                  Mean reward/step: 16.08
       Mean episode length/episode: 7.02
            Mean episode successes: 1.4663
Mean episode consecutive_successes: 3.2659
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 8.26s
                        Total time: 19766.68s
                               ETA: 1035029.5s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.287s, learning 0.209s)
               Value function loss: 74457.3766
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 1070.82
               Mean episode length: 50.29
                  Mean reward/step: 18.78
       Mean episode length/episode: 7.02
            Mean episode successes: 1.6011
Mean episode consecutive_successes: 3.2533
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 8.50s
                        Total time: 19775.18s
                               ETA: 1034911.6s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 2125 steps/s (collection: 7.504s, learning 0.203s)
               Value function loss: 73346.4762
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 1177.74
               Mean episode length: 50.31
                  Mean reward/step: 21.10
       Mean episode length/episode: 7.01
            Mean episode successes: 1.6782
Mean episode consecutive_successes: 3.2851
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 7.71s
                        Total time: 19782.89s
                               ETA: 1034752.6s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.643s, learning 0.175s)
               Value function loss: 76254.5365
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 798.81
               Mean episode length: 48.16
                  Mean reward/step: 20.51
       Mean episode length/episode: 7.01
            Mean episode successes: 1.8560
Mean episode consecutive_successes: 3.2341
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 8.82s
                        Total time: 19791.71s
                               ETA: 1034651.7s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.470s, learning 0.188s)
               Value function loss: 71798.8592
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 777.47
               Mean episode length: 50.10
                  Mean reward/step: 20.22
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9229
Mean episode consecutive_successes: 3.2697
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 8.66s
                        Total time: 19800.36s
                               ETA: 1034542.6s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.565s, learning 0.185s)
               Value function loss: 76093.1740
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 986.69
               Mean episode length: 48.47
                  Mean reward/step: 20.41
       Mean episode length/episode: 6.96
            Mean episode successes: 1.9888
Mean episode consecutive_successes: 3.3070
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 8.75s
                        Total time: 19809.11s
                               ETA: 1034438.4s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.163s)
               Value function loss: 72787.5137
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 907.60
               Mean episode length: 49.78
                  Mean reward/step: 20.75
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0200
Mean episode consecutive_successes: 3.3314
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 8.37s
                        Total time: 19817.49s
                               ETA: 1034314.6s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.044s, learning 0.167s)
               Value function loss: 68712.8235
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 1015.63
               Mean episode length: 47.27
                  Mean reward/step: 20.00
       Mean episode length/episode: 6.92
            Mean episode successes: 1.8989
Mean episode consecutive_successes: 3.3878
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 8.21s
                        Total time: 19825.70s
                               ETA: 1034182.5s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.256s, learning 0.184s)
               Value function loss: 75444.1289
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 1300.92
               Mean episode length: 50.16
                  Mean reward/step: 18.76
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8623
Mean episode consecutive_successes: 3.4610
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 8.44s
                        Total time: 19834.14s
                               ETA: 1034062.5s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.875s, learning 0.171s)
               Value function loss: 72291.5470
                    Surrogate loss: -0.0238
             Mean action noise std: 0.76
                       Mean reward: 930.65
               Mean episode length: 46.74
                  Mean reward/step: 18.21
       Mean episode length/episode: 6.96
            Mean episode successes: 1.6826
Mean episode consecutive_successes: 3.5335
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 8.05s
                        Total time: 19842.18s
                               ETA: 1033922.1s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.856s, learning 0.169s)
               Value function loss: 69748.0854
                    Surrogate loss: -0.0254
             Mean action noise std: 0.76
                       Mean reward: 595.06
               Mean episode length: 46.18
                  Mean reward/step: 18.67
       Mean episode length/episode: 6.92
            Mean episode successes: 1.5996
Mean episode consecutive_successes: 3.5166
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 8.02s
                        Total time: 19850.21s
                               ETA: 1033780.7s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.990s, learning 0.160s)
               Value function loss: 70401.5065
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 1004.90
               Mean episode length: 49.02
                  Mean reward/step: 18.57
       Mean episode length/episode: 6.91
            Mean episode successes: 1.4883
Mean episode consecutive_successes: 3.5728
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 8.15s
                        Total time: 19858.36s
                               ETA: 1033645.9s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.155s, learning 0.170s)
               Value function loss: 66552.6779
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 957.18
               Mean episode length: 48.78
                  Mean reward/step: 18.57
       Mean episode length/episode: 6.96
            Mean episode successes: 1.5190
Mean episode consecutive_successes: 3.5550
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 8.32s
                        Total time: 19866.68s
                               ETA: 1033520.4s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.017s, learning 0.181s)
               Value function loss: 60706.5394
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 811.27
               Mean episode length: 48.22
                  Mean reward/step: 18.87
       Mean episode length/episode: 7.10
            Mean episode successes: 1.6865
Mean episode consecutive_successes: 3.5022
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 8.20s
                        Total time: 19874.88s
                               ETA: 1033388.4s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.303s, learning 0.200s)
               Value function loss: 73697.1562
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 1016.83
               Mean episode length: 48.08
                  Mean reward/step: 16.75
       Mean episode length/episode: 6.98
            Mean episode successes: 1.6265
Mean episode consecutive_successes: 3.5157
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 8.50s
                        Total time: 19883.38s
                               ETA: 1033272.4s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.138s, learning 0.164s)
               Value function loss: 87586.6316
                    Surrogate loss: -0.0188
             Mean action noise std: 0.76
                       Mean reward: 1014.93
               Mean episode length: 49.54
                  Mean reward/step: 18.93
       Mean episode length/episode: 6.97
            Mean episode successes: 1.6855
Mean episode consecutive_successes: 3.4898
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 8.30s
                        Total time: 19891.68s
                               ETA: 1033146.0s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.218s, learning 0.200s)
               Value function loss: 77308.3887
                    Surrogate loss: -0.0232
             Mean action noise std: 0.76
                       Mean reward: 1391.07
               Mean episode length: 51.19
                  Mean reward/step: 18.81
       Mean episode length/episode: 6.98
            Mean episode successes: 1.6963
Mean episode consecutive_successes: 3.5157
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 8.42s
                        Total time: 19900.10s
                               ETA: 1033025.8s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 2107 steps/s (collection: 7.588s, learning 0.186s)
               Value function loss: 81380.7322
                    Surrogate loss: -0.0251
             Mean action noise std: 0.76
                       Mean reward: 916.32
               Mean episode length: 47.51
                  Mean reward/step: 19.50
       Mean episode length/episode: 7.02
            Mean episode successes: 1.7700
Mean episode consecutive_successes: 3.4781
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 7.77s
                        Total time: 19907.87s
                               ETA: 1032872.3s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.362s, learning 0.172s)
               Value function loss: 74961.2939
                    Surrogate loss: 0.0010
             Mean action noise std: 0.76
                       Mean reward: 665.71
               Mean episode length: 46.55
                  Mean reward/step: 18.86
       Mean episode length/episode: 7.08
            Mean episode successes: 1.8828
Mean episode consecutive_successes: 3.4651
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 8.53s
                        Total time: 19916.41s
                               ETA: 1032758.5s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.811s, learning 0.168s)
               Value function loss: 76197.3614
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 799.11
               Mean episode length: 48.64
                  Mean reward/step: 18.88
       Mean episode length/episode: 6.92
            Mean episode successes: 1.8105
Mean episode consecutive_successes: 3.4860
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 7.98s
                        Total time: 19924.39s
                               ETA: 1032615.9s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.028s, learning 0.162s)
               Value function loss: 68906.3447
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 1091.61
               Mean episode length: 48.14
                  Mean reward/step: 18.24
       Mean episode length/episode: 6.96
            Mean episode successes: 1.7070
Mean episode consecutive_successes: 3.5383
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 8.19s
                        Total time: 19932.58s
                               ETA: 1032484.4s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.103s, learning 0.161s)
               Value function loss: 71523.2906
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 976.57
               Mean episode length: 47.88
                  Mean reward/step: 18.55
       Mean episode length/episode: 6.98
            Mean episode successes: 1.6572
Mean episode consecutive_successes: 3.5592
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 8.26s
                        Total time: 19940.84s
                               ETA: 1032356.9s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.128s, learning 0.175s)
               Value function loss: 65014.3440
                    Surrogate loss: -0.0204
             Mean action noise std: 0.76
                       Mean reward: 867.65
               Mean episode length: 45.73
                  Mean reward/step: 17.74
       Mean episode length/episode: 7.01
            Mean episode successes: 1.6079
Mean episode consecutive_successes: 3.5510
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 8.30s
                        Total time: 19949.15s
                               ETA: 1032231.5s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.883s, learning 0.173s)
               Value function loss: 68918.2904
                    Surrogate loss: -0.0263
             Mean action noise std: 0.76
                       Mean reward: 868.65
               Mean episode length: 47.34
                  Mean reward/step: 17.45
       Mean episode length/episode: 6.94
            Mean episode successes: 1.5361
Mean episode consecutive_successes: 3.5291
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 8.06s
                        Total time: 19957.20s
                               ETA: 1032093.5s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.030s, learning 0.161s)
               Value function loss: 63489.0412
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 877.17
               Mean episode length: 48.65
                  Mean reward/step: 17.29
       Mean episode length/episode: 6.98
            Mean episode successes: 1.5454
Mean episode consecutive_successes: 3.5121
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 8.19s
                        Total time: 19965.39s
                               ETA: 1031962.6s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.244s, learning 0.159s)
               Value function loss: 64217.6572
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 967.29
               Mean episode length: 49.06
                  Mean reward/step: 18.60
       Mean episode length/episode: 6.96
            Mean episode successes: 1.5835
Mean episode consecutive_successes: 3.4904
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 8.40s
                        Total time: 19973.80s
                               ETA: 1031842.7s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.176s, learning 0.243s)
               Value function loss: 70056.8816
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 861.77
               Mean episode length: 48.14
                  Mean reward/step: 19.32
       Mean episode length/episode: 6.96
            Mean episode successes: 1.6479
Mean episode consecutive_successes: 3.4744
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 8.42s
                        Total time: 19982.22s
                               ETA: 1031723.8s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.287s, learning 0.279s)
               Value function loss: 68070.8473
                    Surrogate loss: -0.0220
             Mean action noise std: 0.76
                       Mean reward: 645.61
               Mean episode length: 46.38
                  Mean reward/step: 20.00
       Mean episode length/episode: 6.96
            Mean episode successes: 1.7974
Mean episode consecutive_successes: 3.4191
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 8.57s
                        Total time: 19990.78s
                               ETA: 1031612.6s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.037s, learning 0.179s)
               Value function loss: 76686.7531
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 1214.10
               Mean episode length: 47.98
                  Mean reward/step: 19.61
       Mean episode length/episode: 7.03
            Mean episode successes: 1.7964
Mean episode consecutive_successes: 3.4765
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 8.22s
                        Total time: 19999.00s
                               ETA: 1031483.5s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.363s, learning 0.166s)
               Value function loss: 78927.4020
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 1039.78
               Mean episode length: 48.43
                  Mean reward/step: 21.28
       Mean episode length/episode: 6.99
            Mean episode successes: 1.7520
Mean episode consecutive_successes: 3.5082
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 8.53s
                        Total time: 20007.53s
                               ETA: 1031370.7s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.051s, learning 0.163s)
               Value function loss: 71840.7562
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 1132.43
               Mean episode length: 49.01
                  Mean reward/step: 20.56
       Mean episode length/episode: 6.95
            Mean episode successes: 1.8062
Mean episode consecutive_successes: 3.5250
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 8.21s
                        Total time: 20015.74s
                               ETA: 1031241.7s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.184s, learning 0.162s)
               Value function loss: 69614.2765
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 830.68
               Mean episode length: 47.45
                  Mean reward/step: 17.74
       Mean episode length/episode: 7.01
            Mean episode successes: 1.7007
Mean episode consecutive_successes: 3.5462
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 8.35s
                        Total time: 20024.09s
                               ETA: 1031119.6s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.161s)
               Value function loss: 61749.7200
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 1012.13
               Mean episode length: 48.61
                  Mean reward/step: 16.80
       Mean episode length/episode: 7.02
            Mean episode successes: 1.6606
Mean episode consecutive_successes: 3.5599
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 8.47s
                        Total time: 20032.56s
                               ETA: 1031004.1s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.085s, learning 0.165s)
               Value function loss: 64906.0484
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 690.21
               Mean episode length: 46.11
                  Mean reward/step: 17.90
       Mean episode length/episode: 7.06
            Mean episode successes: 1.6313
Mean episode consecutive_successes: 3.5588
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 8.25s
                        Total time: 20040.81s
                               ETA: 1030877.3s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.215s, learning 0.175s)
               Value function loss: 63075.8186
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 828.14
               Mean episode length: 46.23
                  Mean reward/step: 17.29
       Mean episode length/episode: 6.86
            Mean episode successes: 1.5942
Mean episode consecutive_successes: 3.5188
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 8.39s
                        Total time: 20049.20s
                               ETA: 1030757.9s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.232s, learning 0.185s)
               Value function loss: 62325.5546
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 881.40
               Mean episode length: 48.02
                  Mean reward/step: 18.22
       Mean episode length/episode: 6.95
            Mean episode successes: 1.5566
Mean episode consecutive_successes: 3.5349
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 8.42s
                        Total time: 20057.62s
                               ETA: 1030639.9s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.007s, learning 0.209s)
               Value function loss: 64617.1024
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 1024.86
               Mean episode length: 52.75
                  Mean reward/step: 17.13
       Mean episode length/episode: 7.07
            Mean episode successes: 1.5864
Mean episode consecutive_successes: 3.5144
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 8.22s
                        Total time: 20065.83s
                               ETA: 1030511.8s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.231s, learning 0.159s)
               Value function loss: 64328.4130
                    Surrogate loss: -0.0252
             Mean action noise std: 0.76
                       Mean reward: 845.14
               Mean episode length: 45.61
                  Mean reward/step: 17.97
       Mean episode length/episode: 6.99
            Mean episode successes: 1.6797
Mean episode consecutive_successes: 3.4754
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 8.39s
                        Total time: 20074.22s
                               ETA: 1030392.7s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.350s, learning 0.182s)
               Value function loss: 68930.4678
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 827.88
               Mean episode length: 50.13
                  Mean reward/step: 17.94
       Mean episode length/episode: 6.98
            Mean episode successes: 1.5981
Mean episode consecutive_successes: 3.4709
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 8.53s
                        Total time: 20082.75s
                               ETA: 1030281.0s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.452s, learning 0.235s)
               Value function loss: 67237.5153
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 826.86
               Mean episode length: 49.04
                  Mean reward/step: 19.69
       Mean episode length/episode: 7.03
            Mean episode successes: 1.7046
Mean episode consecutive_successes: 3.4547
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 8.69s
                        Total time: 20091.44s
                               ETA: 1030177.3s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.351s, learning 0.168s)
               Value function loss: 70625.5488
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 1291.90
               Mean episode length: 51.67
                  Mean reward/step: 19.56
       Mean episode length/episode: 7.03
            Mean episode successes: 1.7310
Mean episode consecutive_successes: 3.4803
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 8.52s
                        Total time: 20099.96s
                               ETA: 1030065.2s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.762s, learning 0.162s)
               Value function loss: 75523.9457
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 1118.65
               Mean episode length: 46.73
                  Mean reward/step: 20.83
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8379
Mean episode consecutive_successes: 3.4881
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 7.92s
                        Total time: 20107.88s
                               ETA: 1029922.6s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.092s, learning 0.187s)
               Value function loss: 76859.3129
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 1242.55
               Mean episode length: 49.11
                  Mean reward/step: 21.42
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9668
Mean episode consecutive_successes: 3.4740
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 8.28s
                        Total time: 20116.16s
                               ETA: 1029798.4s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.939s, learning 0.162s)
               Value function loss: 73178.5735
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 1382.17
               Mean episode length: 51.97
                  Mean reward/step: 20.93
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8501
Mean episode consecutive_successes: 3.5487
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 8.10s
                        Total time: 20124.26s
                               ETA: 1029665.2s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.416s, learning 0.186s)
               Value function loss: 80036.0674
                    Surrogate loss: -0.0252
             Mean action noise std: 0.76
                       Mean reward: 1040.34
               Mean episode length: 49.72
                  Mean reward/step: 22.16
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8975
Mean episode consecutive_successes: 3.5750
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 8.60s
                        Total time: 20132.86s
                               ETA: 1029557.7s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.147s, learning 0.173s)
               Value function loss: 84861.3971
                    Surrogate loss: -0.0235
             Mean action noise std: 0.76
                       Mean reward: 890.98
               Mean episode length: 47.14
                  Mean reward/step: 21.47
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0024
Mean episode consecutive_successes: 3.5694
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 8.32s
                        Total time: 20141.18s
                               ETA: 1029436.0s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.220s, learning 0.175s)
               Value function loss: 84795.0213
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 907.98
               Mean episode length: 49.60
                  Mean reward/step: 21.55
       Mean episode length/episode: 7.02
            Mean episode successes: 2.0044
Mean episode consecutive_successes: 3.6047
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 8.40s
                        Total time: 20149.58s
                               ETA: 1029318.2s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.182s, learning 0.220s)
               Value function loss: 76843.2236
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 925.66
               Mean episode length: 49.88
                  Mean reward/step: 21.07
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9824
Mean episode consecutive_successes: 3.6409
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 8.40s
                        Total time: 20157.98s
                               ETA: 1029200.9s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.174s, learning 0.167s)
               Value function loss: 83254.7041
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 791.70
               Mean episode length: 47.68
                  Mean reward/step: 21.24
       Mean episode length/episode: 6.95
            Mean episode successes: 1.8931
Mean episode consecutive_successes: 3.6811
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 8.34s
                        Total time: 20166.32s
                               ETA: 1029080.6s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.059s, learning 0.171s)
               Value function loss: 84000.5689
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 1110.42
               Mean episode length: 50.14
                  Mean reward/step: 22.05
       Mean episode length/episode: 7.10
            Mean episode successes: 1.9072
Mean episode consecutive_successes: 3.7674
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 8.23s
                        Total time: 20174.55s
                               ETA: 1028954.6s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.449s, learning 0.170s)
               Value function loss: 84717.1354
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 1012.54
               Mean episode length: 49.32
                  Mean reward/step: 22.49
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9722
Mean episode consecutive_successes: 3.7604
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 8.62s
                        Total time: 20183.17s
                               ETA: 1028848.7s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.297s, learning 0.188s)
               Value function loss: 79111.9812
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 1075.52
               Mean episode length: 49.66
                  Mean reward/step: 21.82
       Mean episode length/episode: 7.01
            Mean episode successes: 2.0166
Mean episode consecutive_successes: 3.7888
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 8.48s
                        Total time: 20191.66s
                               ETA: 1028736.0s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.074s, learning 0.211s)
               Value function loss: 76626.2945
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 1372.77
               Mean episode length: 50.20
                  Mean reward/step: 21.12
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0566
Mean episode consecutive_successes: 3.7976
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 8.28s
                        Total time: 20199.94s
                               ETA: 1028613.3s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.918s, learning 0.261s)
               Value function loss: 81630.5225
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 1417.25
               Mean episode length: 52.60
                  Mean reward/step: 20.72
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9019
Mean episode consecutive_successes: 3.8639
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 8.18s
                        Total time: 20208.12s
                               ETA: 1028485.3s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.423s, learning 0.162s)
               Value function loss: 74642.0463
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 688.09
               Mean episode length: 47.18
                  Mean reward/step: 21.68
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8564
Mean episode consecutive_successes: 3.8710
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 8.59s
                        Total time: 20216.70s
                               ETA: 1028378.1s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.092s, learning 0.166s)
               Value function loss: 79867.9664
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 879.06
               Mean episode length: 48.31
                  Mean reward/step: 22.08
       Mean episode length/episode: 6.96
            Mean episode successes: 1.9609
Mean episode consecutive_successes: 3.8349
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 8.26s
                        Total time: 20224.96s
                               ETA: 1028254.3s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.941s, learning 0.161s)
               Value function loss: 79553.8277
                    Surrogate loss: -0.0267
             Mean action noise std: 0.76
                       Mean reward: 1265.99
               Mean episode length: 50.44
                  Mean reward/step: 21.67
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9678
Mean episode consecutive_successes: 3.8876
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 8.10s
                        Total time: 20233.07s
                               ETA: 1028122.8s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.026s, learning 0.169s)
               Value function loss: 88333.2920
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 1129.25
               Mean episode length: 48.62
                  Mean reward/step: 20.04
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8647
Mean episode consecutive_successes: 3.9145
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 8.19s
                        Total time: 20241.26s
                               ETA: 1027996.1s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.263s, learning 0.177s)
               Value function loss: 85846.6451
                    Surrogate loss: -0.0213
             Mean action noise std: 0.76
                       Mean reward: 861.11
               Mean episode length: 46.78
                  Mean reward/step: 20.39
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8335
Mean episode consecutive_successes: 3.9281
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 8.44s
                        Total time: 20249.70s
                               ETA: 1027881.9s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.564s, learning 0.190s)
               Value function loss: 81692.4490
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 836.60
               Mean episode length: 48.44
                  Mean reward/step: 20.67
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8291
Mean episode consecutive_successes: 3.9245
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 16.75s
                        Total time: 20266.45s
                               ETA: 1028189.7s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.801s, learning 0.162s)
               Value function loss: 79593.6908
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 806.14
               Mean episode length: 47.19
                  Mean reward/step: 20.55
       Mean episode length/episode: 6.92
            Mean episode successes: 1.9019
Mean episode consecutive_successes: 3.8825
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 15.96s
                        Total time: 20282.42s
                               ETA: 1028457.0s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.006s, learning 0.170s)
               Value function loss: 81040.5035
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 1278.77
               Mean episode length: 51.24
                  Mean reward/step: 20.86
       Mean episode length/episode: 6.95
            Mean episode successes: 1.9429
Mean episode consecutive_successes: 3.8940
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 16.18s
                        Total time: 20298.59s
                               ETA: 1028734.8s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.978s, learning 0.167s)
               Value function loss: 82213.1186
                    Surrogate loss: -0.0209
             Mean action noise std: 0.76
                       Mean reward: 1174.61
               Mean episode length: 48.83
                  Mean reward/step: 22.51
       Mean episode length/episode: 7.02
            Mean episode successes: 1.9248
Mean episode consecutive_successes: 3.9262
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 16.14s
                        Total time: 20314.74s
                               ETA: 1029010.8s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.510s, learning 0.158s)
               Value function loss: 83247.2162
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 1494.67
               Mean episode length: 52.47
                  Mean reward/step: 21.03
       Mean episode length/episode: 7.01
            Mean episode successes: 1.9043
Mean episode consecutive_successes: 3.9763
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 15.67s
                        Total time: 20330.41s
                               ETA: 1029262.2s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.928s, learning 0.171s)
               Value function loss: 77637.8410
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 780.67
               Mean episode length: 47.00
                  Mean reward/step: 19.70
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8765
Mean episode consecutive_successes: 3.9485
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 16.10s
                        Total time: 20346.50s
                               ETA: 1029535.2s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.972s, learning 0.171s)
               Value function loss: 79825.5580
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 1247.07
               Mean episode length: 48.76
                  Mean reward/step: 19.80
       Mean episode length/episode: 6.95
            Mean episode successes: 1.9146
Mean episode consecutive_successes: 3.9315
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 16.14s
                        Total time: 20362.65s
                               ETA: 1029810.2s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.861s, learning 0.170s)
               Value function loss: 76400.8202
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 1265.35
               Mean episode length: 49.82
                  Mean reward/step: 20.27
       Mean episode length/episode: 6.98
            Mean episode successes: 1.7637
Mean episode consecutive_successes: 3.9613
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 16.03s
                        Total time: 20378.68s
                               ETA: 1030079.2s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.182s, learning 0.189s)
               Value function loss: 74962.1963
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 855.25
               Mean episode length: 46.29
                  Mean reward/step: 20.15
       Mean episode length/episode: 6.95
            Mean episode successes: 1.7739
Mean episode consecutive_successes: 3.9508
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 16.37s
                        Total time: 20395.05s
                               ETA: 1030365.1s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.124s, learning 0.182s)
               Value function loss: 79866.2930
                    Surrogate loss: -0.0219
             Mean action noise std: 0.75
                       Mean reward: 962.05
               Mean episode length: 48.49
                  Mean reward/step: 19.91
       Mean episode length/episode: 7.00
            Mean episode successes: 1.7817
Mean episode consecutive_successes: 3.9328
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 16.31s
                        Total time: 20411.36s
                               ETA: 1030647.4s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.169s, learning 0.183s)
               Value function loss: 79142.6232
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 945.18
               Mean episode length: 49.14
                  Mean reward/step: 21.33
       Mean episode length/episode: 7.06
            Mean episode successes: 1.8345
Mean episode consecutive_successes: 3.9199
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 16.35s
                        Total time: 20427.71s
                               ETA: 1030931.7s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.043s, learning 0.185s)
               Value function loss: 89136.8268
                    Surrogate loss: -0.0229
             Mean action noise std: 0.75
                       Mean reward: 1334.01
               Mean episode length: 47.41
                  Mean reward/step: 21.12
       Mean episode length/episode: 6.92
            Mean episode successes: 1.8765
Mean episode consecutive_successes: 3.8939
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 16.23s
                        Total time: 20443.94s
                               ETA: 1031209.5s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.037s, learning 0.184s)
               Value function loss: 86035.3996
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 996.72
               Mean episode length: 48.30
                  Mean reward/step: 21.52
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9531
Mean episode consecutive_successes: 3.8842
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 16.22s
                        Total time: 20460.16s
                               ETA: 1031486.5s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.129s, learning 0.167s)
               Value function loss: 86277.4363
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 1307.34
               Mean episode length: 49.09
                  Mean reward/step: 20.73
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0352
Mean episode consecutive_successes: 3.8962
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 16.30s
                        Total time: 20476.45s
                               ETA: 1031767.0s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1059 steps/s (collection: 15.306s, learning 0.157s)
               Value function loss: 93029.0398
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 982.08
               Mean episode length: 48.56
                  Mean reward/step: 20.58
       Mean episode length/episode: 6.94
            Mean episode successes: 1.9912
Mean episode consecutive_successes: 3.8834
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 15.46s
                        Total time: 20491.92s
                               ETA: 1032005.4s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.913s, learning 0.158s)
               Value function loss: 90314.3953
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 1180.50
               Mean episode length: 45.76
                  Mean reward/step: 20.81
       Mean episode length/episode: 6.99
            Mean episode successes: 1.9116
Mean episode consecutive_successes: 3.8997
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 16.07s
                        Total time: 20507.99s
                               ETA: 1032274.0s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.723s, learning 0.168s)
               Value function loss: 80210.3582
                    Surrogate loss: -0.0215
             Mean action noise std: 0.75
                       Mean reward: 890.48
               Mean episode length: 46.68
                  Mean reward/step: 19.73
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9146
Mean episode consecutive_successes: 3.8910
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 15.89s
                        Total time: 20523.88s
                               ETA: 1032533.3s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.757s, learning 0.163s)
               Value function loss: 101532.7291
                    Surrogate loss: -0.0254
             Mean action noise std: 0.75
                       Mean reward: 933.62
               Mean episode length: 47.43
                  Mean reward/step: 18.18
       Mean episode length/episode: 6.91
            Mean episode successes: 1.8813
Mean episode consecutive_successes: 3.8809
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 15.92s
                        Total time: 20539.80s
                               ETA: 1032793.8s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.398s, learning 0.191s)
               Value function loss: 85614.0049
                    Surrogate loss: -0.0243
             Mean action noise std: 0.75
                       Mean reward: 984.56
               Mean episode length: 47.93
                  Mean reward/step: 17.89
       Mean episode length/episode: 7.01
            Mean episode successes: 1.7563
Mean episode consecutive_successes: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 16.59s
                        Total time: 20556.39s
                               ETA: 1033087.6s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.943s, learning 0.230s)
               Value function loss: 86049.4787
                    Surrogate loss: -0.0245
             Mean action noise std: 0.75
                       Mean reward: 924.66
               Mean episode length: 48.77
                  Mean reward/step: 18.41
       Mean episode length/episode: 6.95
            Mean episode successes: 1.6768
Mean episode consecutive_successes: 3.8513
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 16.17s
                        Total time: 20572.56s
                               ETA: 1033360.2s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.913s, learning 0.174s)
               Value function loss: 83616.9662
                    Surrogate loss: -0.0220
             Mean action noise std: 0.75
                       Mean reward: 1288.66
               Mean episode length: 46.43
                  Mean reward/step: 18.29
       Mean episode length/episode: 7.05
            Mean episode successes: 1.7017
Mean episode consecutive_successes: 3.8858
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 16.09s
                        Total time: 20588.65s
                               ETA: 1033628.3s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.955s, learning 0.208s)
               Value function loss: 70406.4908
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: 530.11
               Mean episode length: 45.93
                  Mean reward/step: 17.49
       Mean episode length/episode: 6.90
            Mean episode successes: 1.5640
Mean episode consecutive_successes: 3.8228
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 16.16s
                        Total time: 20604.81s
                               ETA: 1033899.8s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.629s, learning 0.173s)
               Value function loss: 77167.2633
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 857.14
               Mean episode length: 45.18
                  Mean reward/step: 19.22
       Mean episode length/episode: 7.06
            Mean episode successes: 1.7603
Mean episode consecutive_successes: 3.7613
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 15.80s
                        Total time: 20620.62s
                               ETA: 1034152.9s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.966s, learning 0.163s)
               Value function loss: 75050.1314
                    Surrogate loss: -0.0279
             Mean action noise std: 0.75
                       Mean reward: 863.26
               Mean episode length: 46.61
                  Mean reward/step: 19.25
       Mean episode length/episode: 7.02
            Mean episode successes: 1.7080
Mean episode consecutive_successes: 3.7666
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 16.13s
                        Total time: 20636.75s
                               ETA: 1034422.1s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.100s, learning 0.167s)
               Value function loss: 76448.9125
                    Surrogate loss: -0.0240
             Mean action noise std: 0.75
                       Mean reward: 940.03
               Mean episode length: 49.53
                  Mean reward/step: 18.98
       Mean episode length/episode: 6.97
            Mean episode successes: 1.7007
Mean episode consecutive_successes: 3.7702
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 16.27s
                        Total time: 20653.01s
                               ETA: 1034698.0s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.089s, learning 0.181s)
               Value function loss: 74630.2731
                    Surrogate loss: -0.0212
             Mean action noise std: 0.75
                       Mean reward: 934.30
               Mean episode length: 48.62
                  Mean reward/step: 18.81
       Mean episode length/episode: 6.94
            Mean episode successes: 1.6792
Mean episode consecutive_successes: 3.7561
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 16.27s
                        Total time: 20669.28s
                               ETA: 1034973.6s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.339s, learning 0.205s)
               Value function loss: 76499.5139
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 733.05
               Mean episode length: 46.51
                  Mean reward/step: 18.90
       Mean episode length/episode: 7.02
            Mean episode successes: 1.6650
Mean episode consecutive_successes: 3.7442
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 16.54s
                        Total time: 20685.83s
                               ETA: 1035262.7s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.992s, learning 0.158s)
               Value function loss: 81909.3400
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 738.96
               Mean episode length: 47.94
                  Mean reward/step: 20.13
       Mean episode length/episode: 6.91
            Mean episode successes: 1.5928
Mean episode consecutive_successes: 3.7698
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 16.15s
                        Total time: 20701.98s
                               ETA: 1035531.9s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.892s, learning 0.163s)
               Value function loss: 74947.0828
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 857.75
               Mean episode length: 46.04
                  Mean reward/step: 18.99
       Mean episode length/episode: 7.00
            Mean episode successes: 1.6343
Mean episode consecutive_successes: 3.7365
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 16.05s
                        Total time: 20718.03s
                               ETA: 1035795.9s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.095s, learning 0.183s)
               Value function loss: 72546.3520
                    Surrogate loss: -0.0219
             Mean action noise std: 0.75
                       Mean reward: 787.92
               Mean episode length: 45.73
                  Mean reward/step: 19.37
       Mean episode length/episode: 6.98
            Mean episode successes: 1.7241
Mean episode consecutive_successes: 3.6808
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 16.28s
                        Total time: 20734.31s
                               ETA: 1036070.8s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.660s, learning 0.198s)
               Value function loss: 77039.9836
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 861.70
               Mean episode length: 47.55
                  Mean reward/step: 18.95
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8745
Mean episode consecutive_successes: 3.6232
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 16.86s
                        Total time: 20751.17s
                               ETA: 1036374.4s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.032s, learning 0.185s)
               Value function loss: 75954.5596
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 1142.33
               Mean episode length: 49.04
                  Mean reward/step: 18.75
       Mean episode length/episode: 6.99
            Mean episode successes: 1.6768
Mean episode consecutive_successes: 3.6996
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 16.22s
                        Total time: 20767.38s
                               ETA: 1036645.7s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.102s, learning 0.156s)
               Value function loss: 76663.7061
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 697.87
               Mean episode length: 46.54
                  Mean reward/step: 18.35
       Mean episode length/episode: 7.00
            Mean episode successes: 1.6714
Mean episode consecutive_successes: 3.6796
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 16.26s
                        Total time: 20783.64s
                               ETA: 1036918.7s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.643s, learning 0.163s)
               Value function loss: 83070.5754
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 747.42
               Mean episode length: 49.13
                  Mean reward/step: 19.15
       Mean episode length/episode: 6.97
            Mean episode successes: 1.6885
Mean episode consecutive_successes: 3.6673
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 15.81s
                        Total time: 20799.45s
                               ETA: 1037168.9s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.692s, learning 0.166s)
               Value function loss: 88084.2568
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 1176.54
               Mean episode length: 51.23
                  Mean reward/step: 21.61
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8315
Mean episode consecutive_successes: 3.6593
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 16.86s
                        Total time: 20816.31s
                               ETA: 1037471.2s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.181s, learning 0.230s)
               Value function loss: 77990.9592
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 941.93
               Mean episode length: 47.98
                  Mean reward/step: 20.59
       Mean episode length/episode: 7.08
            Mean episode successes: 1.8647
Mean episode consecutive_successes: 3.6643
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 16.41s
                        Total time: 20832.72s
                               ETA: 1037751.0s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.931s, learning 0.167s)
               Value function loss: 87319.9658
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 1064.26
               Mean episode length: 47.90
                  Mean reward/step: 21.42
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8516
Mean episode consecutive_successes: 3.7094
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 16.10s
                        Total time: 20848.82s
                               ETA: 1038014.8s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1234 steps/s (collection: 13.106s, learning 0.161s)
               Value function loss: 83772.0455
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 859.70
               Mean episode length: 49.25
                  Mean reward/step: 20.31
       Mean episode length/episode: 6.98
            Mean episode successes: 1.8662
Mean episode consecutive_successes: 3.6817
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 13.27s
                        Total time: 20862.08s
                               ETA: 1038137.5s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.020s, learning 0.162s)
               Value function loss: 84906.4340
                    Surrogate loss: -0.0241
             Mean action noise std: 0.75
                       Mean reward: 891.23
               Mean episode length: 47.27
                  Mean reward/step: 22.10
       Mean episode length/episode: 7.03
            Mean episode successes: 1.9531
Mean episode consecutive_successes: 3.7102
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 8.18s
                        Total time: 20870.27s
                               ETA: 1038007.2s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.932s, learning 0.160s)
               Value function loss: 90545.9354
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 1009.50
               Mean episode length: 48.32
                  Mean reward/step: 21.84
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9419
Mean episode consecutive_successes: 3.7608
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 8.09s
                        Total time: 20878.36s
                               ETA: 1037872.5s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.980s, learning 0.167s)
               Value function loss: 95073.7311
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 945.02
               Mean episode length: 45.38
                  Mean reward/step: 21.27
       Mean episode length/episode: 6.96
            Mean episode successes: 1.8335
Mean episode consecutive_successes: 3.8006
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 8.15s
                        Total time: 20886.51s
                               ETA: 1037740.7s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.132s, learning 0.177s)
               Value function loss: 78468.4059
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 1238.27
               Mean episode length: 50.37
                  Mean reward/step: 21.33
       Mean episode length/episode: 7.07
            Mean episode successes: 1.8926
Mean episode consecutive_successes: 3.8408
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 8.31s
                        Total time: 20894.82s
                               ETA: 1037617.1s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.808s, learning 0.173s)
               Value function loss: 84478.9501
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 947.36
               Mean episode length: 48.96
                  Mean reward/step: 22.56
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8413
Mean episode consecutive_successes: 3.8637
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 7.98s
                        Total time: 20902.80s
                               ETA: 1037477.2s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.454s, learning 0.168s)
               Value function loss: 84318.0252
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 750.15
               Mean episode length: 45.28
                  Mean reward/step: 22.43
       Mean episode length/episode: 6.92
            Mean episode successes: 1.9771
Mean episode consecutive_successes: 3.8224
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 8.62s
                        Total time: 20911.42s
                               ETA: 1037369.3s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.281s, learning 0.165s)
               Value function loss: 82085.4037
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 878.61
               Mean episode length: 43.19
                  Mean reward/step: 20.54
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8647
Mean episode consecutive_successes: 3.8726
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 8.45s
                        Total time: 20919.86s
                               ETA: 1037252.8s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.235s, learning 0.160s)
               Value function loss: 82716.3572
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 1344.24
               Mean episode length: 48.49
                  Mean reward/step: 22.61
       Mean episode length/episode: 6.95
            Mean episode successes: 1.9604
Mean episode consecutive_successes: 3.8937
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 8.40s
                        Total time: 20928.26s
                               ETA: 1037133.9s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1997 steps/s (collection: 7.995s, learning 0.206s)
               Value function loss: 78402.2986
                    Surrogate loss: -0.0221
             Mean action noise std: 0.75
                       Mean reward: 778.85
               Mean episode length: 47.48
                  Mean reward/step: 21.34
       Mean episode length/episode: 6.96
            Mean episode successes: 1.8828
Mean episode consecutive_successes: 3.8898
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 8.20s
                        Total time: 20936.46s
                               ETA: 1037005.4s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.441s, learning 0.163s)
               Value function loss: 90349.6412
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 1155.33
               Mean episode length: 49.78
                  Mean reward/step: 21.42
       Mean episode length/episode: 7.02
            Mean episode successes: 1.9146
Mean episode consecutive_successes: 3.9221
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 8.60s
                        Total time: 20945.07s
                               ETA: 1036897.1s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.466s, learning 0.192s)
               Value function loss: 76249.5148
                    Surrogate loss: -0.0257
             Mean action noise std: 0.75
                       Mean reward: 1150.10
               Mean episode length: 48.91
                  Mean reward/step: 18.83
       Mean episode length/episode: 7.00
            Mean episode successes: 1.7920
Mean episode consecutive_successes: 3.9358
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 8.66s
                        Total time: 20953.72s
                               ETA: 1036791.5s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.263s, learning 0.160s)
               Value function loss: 87182.7148
                    Surrogate loss: -0.0217
             Mean action noise std: 0.75
                       Mean reward: 869.46
               Mean episode length: 47.92
                  Mean reward/step: 18.56
       Mean episode length/episode: 6.98
            Mean episode successes: 1.7910
Mean episode consecutive_successes: 3.9052
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 8.42s
                        Total time: 20962.15s
                               ETA: 1036674.4s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.091s, learning 0.162s)
               Value function loss: 87149.0455
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 1001.96
               Mean episode length: 47.79
                  Mean reward/step: 21.41
       Mean episode length/episode: 6.98
            Mean episode successes: 1.7832
Mean episode consecutive_successes: 3.9102
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 8.25s
                        Total time: 20970.40s
                               ETA: 1036549.0s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.218s, learning 0.184s)
               Value function loss: 99900.9240
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 1098.47
               Mean episode length: 50.45
                  Mean reward/step: 21.98
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9644
Mean episode consecutive_successes: 3.8902
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 8.40s
                        Total time: 20978.80s
                               ETA: 1036431.0s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.057s, learning 0.165s)
               Value function loss: 99451.3639
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 1518.30
               Mean episode length: 49.98
                  Mean reward/step: 20.28
       Mean episode length/episode: 6.95
            Mean episode successes: 1.8652
Mean episode consecutive_successes: 3.9288
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 8.22s
                        Total time: 20987.02s
                               ETA: 1036304.3s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.225s, learning 0.187s)
               Value function loss: 94632.9559
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 1070.48
               Mean episode length: 49.62
                  Mean reward/step: 18.75
       Mean episode length/episode: 6.95
            Mean episode successes: 1.7861
Mean episode consecutive_successes: 3.9106
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 8.41s
                        Total time: 20995.43s
                               ETA: 1036187.0s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.005s, learning 0.205s)
               Value function loss: 84569.1836
                    Surrogate loss: -0.0243
             Mean action noise std: 0.75
                       Mean reward: 1323.71
               Mean episode length: 51.20
                  Mean reward/step: 19.41
       Mean episode length/episode: 6.96
            Mean episode successes: 1.7520
Mean episode consecutive_successes: 3.9174
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 8.21s
                        Total time: 21003.64s
                               ETA: 1036060.0s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.191s, learning 0.212s)
               Value function loss: 78518.9777
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 1125.19
               Mean episode length: 49.52
                  Mean reward/step: 19.59
       Mean episode length/episode: 6.99
            Mean episode successes: 1.7007
Mean episode consecutive_successes: 3.9210
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 8.40s
                        Total time: 21012.05s
                               ETA: 1035942.5s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.404s, learning 0.165s)
               Value function loss: 77039.5117
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 726.50
               Mean episode length: 47.80
                  Mean reward/step: 20.32
       Mean episode length/episode: 7.06
            Mean episode successes: 1.8525
Mean episode consecutive_successes: 3.8485
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 8.57s
                        Total time: 21020.62s
                               ETA: 1035833.4s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.173s, learning 0.163s)
               Value function loss: 77119.2617
                    Surrogate loss: -0.0231
             Mean action noise std: 0.75
                       Mean reward: 771.23
               Mean episode length: 47.61
                  Mean reward/step: 20.79
       Mean episode length/episode: 6.94
            Mean episode successes: 1.7847
Mean episode consecutive_successes: 3.8451
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 8.34s
                        Total time: 21028.95s
                               ETA: 1035712.9s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.474s, learning 0.162s)
               Value function loss: 72403.5484
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 1079.31
               Mean episode length: 47.67
                  Mean reward/step: 18.71
       Mean episode length/episode: 6.95
            Mean episode successes: 1.7803
Mean episode consecutive_successes: 3.8416
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 8.64s
                        Total time: 21037.59s
                               ETA: 1035607.2s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.199s, learning 0.161s)
               Value function loss: 73881.1713
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 831.20
               Mean episode length: 47.17
                  Mean reward/step: 19.01
       Mean episode length/episode: 6.98
            Mean episode successes: 1.7192
Mean episode consecutive_successes: 3.8439
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 8.36s
                        Total time: 21045.95s
                               ETA: 1035488.1s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.154s, learning 0.185s)
               Value function loss: 78068.8486
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 996.42
               Mean episode length: 47.09
                  Mean reward/step: 18.87
       Mean episode length/episode: 7.08
            Mean episode successes: 1.8188
Mean episode consecutive_successes: 3.8327
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 8.34s
                        Total time: 21054.29s
                               ETA: 1035368.0s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.349s, learning 0.164s)
               Value function loss: 81206.4963
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 968.07
               Mean episode length: 50.68
                  Mean reward/step: 21.14
       Mean episode length/episode: 7.05
            Mean episode successes: 1.7808
Mean episode consecutive_successes: 3.8365
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 8.51s
                        Total time: 21062.80s
                               ETA: 1035256.6s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.508s, learning 0.185s)
               Value function loss: 80769.1213
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 1129.93
               Mean episode length: 52.63
                  Mean reward/step: 21.05
       Mean episode length/episode: 6.90
            Mean episode successes: 1.8384
Mean episode consecutive_successes: 3.8492
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 8.69s
                        Total time: 21071.49s
                               ETA: 1035154.2s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.817s, learning 0.162s)
               Value function loss: 77133.9949
                    Surrogate loss: -0.0221
             Mean action noise std: 0.75
                       Mean reward: 1066.03
               Mean episode length: 50.31
                  Mean reward/step: 19.61
       Mean episode length/episode: 6.96
            Mean episode successes: 1.7305
Mean episode consecutive_successes: 3.8676
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 7.98s
                        Total time: 21079.47s
                               ETA: 1035016.8s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.249s, learning 0.168s)
               Value function loss: 71472.0140
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 902.93
               Mean episode length: 45.62
                  Mean reward/step: 19.32
       Mean episode length/episode: 7.03
            Mean episode successes: 1.7588
Mean episode consecutive_successes: 3.8280
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 8.42s
                        Total time: 21087.89s
                               ETA: 1034901.0s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.973s, learning 0.165s)
               Value function loss: 74679.0541
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 1191.95
               Mean episode length: 47.69
                  Mean reward/step: 21.26
       Mean episode length/episode: 7.05
            Mean episode successes: 1.8887
Mean episode consecutive_successes: 3.8327
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 8.14s
                        Total time: 21096.03s
                               ETA: 1034771.7s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.212s, learning 0.184s)
               Value function loss: 79800.4979
                    Surrogate loss: -0.0247
             Mean action noise std: 0.75
                       Mean reward: 886.30
               Mean episode length: 46.80
                  Mean reward/step: 21.63
       Mean episode length/episode: 6.83
            Mean episode successes: 1.8408
Mean episode consecutive_successes: 3.8189
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 8.40s
                        Total time: 21104.42s
                               ETA: 1034655.1s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.103s, learning 0.165s)
               Value function loss: 81484.9344
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 666.23
               Mean episode length: 47.29
                  Mean reward/step: 21.28
       Mean episode length/episode: 6.95
            Mean episode successes: 1.8960
Mean episode consecutive_successes: 3.7813
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 8.27s
                        Total time: 21112.69s
                               ETA: 1034532.4s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.084s, learning 0.163s)
               Value function loss: 79650.7414
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 1040.78
               Mean episode length: 50.31
                  Mean reward/step: 21.66
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9370
Mean episode consecutive_successes: 3.8164
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 8.25s
                        Total time: 21120.94s
                               ETA: 1034408.7s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.005s, learning 0.204s)
               Value function loss: 83436.8879
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 714.24
               Mean episode length: 43.68
                  Mean reward/step: 21.82
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8999
Mean episode consecutive_successes: 3.8464
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 8.21s
                        Total time: 21129.15s
                               ETA: 1034283.4s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.196s, learning 0.187s)
               Value function loss: 96353.2410
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 1123.66
               Mean episode length: 46.60
                  Mean reward/step: 21.30
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9531
Mean episode consecutive_successes: 3.8504
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 8.38s
                        Total time: 21137.53s
                               ETA: 1034166.6s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.244s, learning 0.191s)
               Value function loss: 90153.1189
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 1281.41
               Mean episode length: 51.85
                  Mean reward/step: 22.71
       Mean episode length/episode: 7.01
            Mean episode successes: 1.9600
Mean episode consecutive_successes: 3.8770
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 8.44s
                        Total time: 21145.97s
                               ETA: 1034052.5s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.070s, learning 0.162s)
               Value function loss: 97205.1963
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 979.69
               Mean episode length: 49.05
                  Mean reward/step: 21.23
       Mean episode length/episode: 6.94
            Mean episode successes: 1.9385
Mean episode consecutive_successes: 3.8776
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 8.23s
                        Total time: 21154.20s
                               ETA: 1033928.6s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.174s, learning 0.162s)
               Value function loss: 85581.1732
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 760.16
               Mean episode length: 44.63
                  Mean reward/step: 21.01
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8413
Mean episode consecutive_successes: 3.9123
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 8.34s
                        Total time: 21162.53s
                               ETA: 1033809.8s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.527s, learning 0.162s)
               Value function loss: 81701.5803
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 934.58
               Mean episode length: 48.02
                  Mean reward/step: 21.65
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8423
Mean episode consecutive_successes: 3.9406
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 8.69s
                        Total time: 21171.22s
                               ETA: 1033708.4s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.495s, learning 0.165s)
               Value function loss: 86620.5096
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 1014.82
               Mean episode length: 47.87
                  Mean reward/step: 22.65
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8950
Mean episode consecutive_successes: 3.9641
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 8.66s
                        Total time: 21179.88s
                               ETA: 1033605.7s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.160s, learning 0.174s)
               Value function loss: 83141.3465
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 937.14
               Mean episode length: 48.56
                  Mean reward/step: 20.73
       Mean episode length/episode: 6.96
            Mean episode successes: 1.8188
Mean episode consecutive_successes: 3.9768
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 8.33s
                        Total time: 21188.22s
                               ETA: 1033487.1s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.903s, learning 0.178s)
               Value function loss: 81810.6021
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 930.96
               Mean episode length: 46.43
                  Mean reward/step: 19.50
       Mean episode length/episode: 6.97
            Mean episode successes: 1.7817
Mean episode consecutive_successes: 3.9534
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 8.08s
                        Total time: 21196.30s
                               ETA: 1033356.4s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.172s)
               Value function loss: 76773.7492
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 1047.60
               Mean episode length: 49.47
                  Mean reward/step: 19.78
       Mean episode length/episode: 6.97
            Mean episode successes: 1.7734
Mean episode consecutive_successes: 3.9703
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 8.59s
                        Total time: 21204.89s
                               ETA: 1033250.5s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.223s, learning 0.165s)
               Value function loss: 86277.1592
                    Surrogate loss: -0.0262
             Mean action noise std: 0.75
                       Mean reward: 1010.03
               Mean episode length: 49.10
                  Mean reward/step: 20.72
       Mean episode length/episode: 6.95
            Mean episode successes: 1.8232
Mean episode consecutive_successes: 3.9256
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 8.39s
                        Total time: 21213.27s
                               ETA: 1033134.9s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.304s, learning 0.269s)
               Value function loss: 93255.2771
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 676.26
               Mean episode length: 47.31
                  Mean reward/step: 21.13
       Mean episode length/episode: 6.93
            Mean episode successes: 1.7803
Mean episode consecutive_successes: 3.9411
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 8.57s
                        Total time: 21221.85s
                               ETA: 1033028.5s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.323s, learning 0.274s)
               Value function loss: 97278.9742
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 1000.47
               Mean episode length: 49.94
                  Mean reward/step: 20.78
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8228
Mean episode consecutive_successes: 3.9577
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 8.60s
                        Total time: 21230.44s
                               ETA: 1032923.3s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.280s, learning 0.202s)
               Value function loss: 94762.1867
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 1099.40
               Mean episode length: 47.43
                  Mean reward/step: 22.95
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9155
Mean episode consecutive_successes: 3.9397
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 8.48s
                        Total time: 21238.93s
                               ETA: 1032812.6s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.021s, learning 0.158s)
               Value function loss: 114493.1119
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 937.86
               Mean episode length: 49.80
                  Mean reward/step: 22.97
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9595
Mean episode consecutive_successes: 3.9313
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.18s
                        Total time: 21247.11s
                               ETA: 1032687.3s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.187s, learning 0.168s)
               Value function loss: 129558.5859
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 1453.85
               Mean episode length: 50.72
                  Mean reward/step: 21.25
       Mean episode length/episode: 7.01
            Mean episode successes: 2.0166
Mean episode consecutive_successes: 3.9722
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 8.36s
                        Total time: 21255.46s
                               ETA: 1032570.7s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.863s, learning 0.168s)
               Value function loss: 81084.5885
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 744.72
               Mean episode length: 48.98
                  Mean reward/step: 20.77
       Mean episode length/episode: 6.91
            Mean episode successes: 1.9570
Mean episode consecutive_successes: 3.9412
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 8.03s
                        Total time: 21263.49s
                               ETA: 1032438.4s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.009s, learning 0.192s)
               Value function loss: 78742.2553
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 1121.38
               Mean episode length: 47.28
                  Mean reward/step: 20.93
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9751
Mean episode consecutive_successes: 3.9471
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 8.20s
                        Total time: 21271.69s
                               ETA: 1032314.6s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.987s, learning 0.164s)
               Value function loss: 82174.8920
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 888.78
               Mean episode length: 47.04
                  Mean reward/step: 19.91
       Mean episode length/episode: 7.01
            Mean episode successes: 1.8848
Mean episode consecutive_successes: 3.9846
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 8.15s
                        Total time: 21279.85s
                               ETA: 1032188.4s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.577s, learning 0.212s)
               Value function loss: 79704.5051
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 875.74
               Mean episode length: 46.59
                  Mean reward/step: 20.92
       Mean episode length/episode: 7.06
            Mean episode successes: 1.9390
Mean episode consecutive_successes: 3.9581
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 8.79s
                        Total time: 21288.63s
                               ETA: 1032093.2s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.369s, learning 0.166s)
               Value function loss: 84885.9830
                    Surrogate loss: -0.0236
             Mean action noise std: 0.75
                       Mean reward: 1032.19
               Mean episode length: 49.05
                  Mean reward/step: 19.79
       Mean episode length/episode: 6.86
            Mean episode successes: 1.8501
Mean episode consecutive_successes: 3.9532
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 8.53s
                        Total time: 21297.17s
                               ETA: 1031985.8s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.412s, learning 0.193s)
               Value function loss: 78298.4051
                    Surrogate loss: -0.0225
             Mean action noise std: 0.75
                       Mean reward: 905.82
               Mean episode length: 46.26
                  Mean reward/step: 19.83
       Mean episode length/episode: 6.94
            Mean episode successes: 1.8926
Mean episode consecutive_successes: 3.9211
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 8.61s
                        Total time: 21305.77s
                               ETA: 1031881.9s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.899s, learning 0.168s)
               Value function loss: 86963.9494
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 1308.36
               Mean episode length: 50.06
                  Mean reward/step: 18.94
       Mean episode length/episode: 6.95
            Mean episode successes: 1.7158
Mean episode consecutive_successes: 3.9681
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 8.07s
                        Total time: 21313.84s
                               ETA: 1031752.1s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.265s, learning 0.203s)
               Value function loss: 75486.3992
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 789.65
               Mean episode length: 44.61
                  Mean reward/step: 19.57
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8301
Mean episode consecutive_successes: 3.8990
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 8.47s
                        Total time: 21322.31s
                               ETA: 1031641.7s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.026s, learning 0.160s)
               Value function loss: 81337.4910
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 800.43
               Mean episode length: 46.14
                  Mean reward/step: 18.98
       Mean episode length/episode: 6.96
            Mean episode successes: 1.8135
Mean episode consecutive_successes: 3.8614
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 8.19s
                        Total time: 21330.49s
                               ETA: 1031517.8s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.310s, learning 0.249s)
               Value function loss: 85523.4121
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 833.51
               Mean episode length: 46.96
                  Mean reward/step: 19.43
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8364
Mean episode consecutive_successes: 3.8747
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 8.56s
                        Total time: 21339.05s
                               ETA: 1031412.1s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.243s, learning 0.163s)
               Value function loss: 89772.8049
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 970.60
               Mean episode length: 49.79
                  Mean reward/step: 21.41
       Mean episode length/episode: 6.89
            Mean episode successes: 1.8608
Mean episode consecutive_successes: 3.8614
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 8.41s
                        Total time: 21347.46s
                               ETA: 1031299.1s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.071s, learning 0.204s)
               Value function loss: 85988.8598
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 833.81
               Mean episode length: 46.60
                  Mean reward/step: 21.54
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0146
Mean episode consecutive_successes: 3.8037
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 8.28s
                        Total time: 21355.73s
                               ETA: 1031179.9s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.127s, learning 0.183s)
               Value function loss: 91134.5934
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 761.88
               Mean episode length: 48.80
                  Mean reward/step: 22.30
       Mean episode length/episode: 7.06
            Mean episode successes: 2.1230
Mean episode consecutive_successes: 3.8089
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 8.31s
                        Total time: 21364.04s
                               ETA: 1031062.4s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.061s, learning 0.161s)
               Value function loss: 83361.3736
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 959.43
               Mean episode length: 47.84
                  Mean reward/step: 20.65
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1851
Mean episode consecutive_successes: 3.8031
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 8.22s
                        Total time: 21372.27s
                               ETA: 1030940.8s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.259s, learning 0.165s)
               Value function loss: 85069.8504
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 1048.63
               Mean episode length: 47.46
                  Mean reward/step: 18.29
       Mean episode length/episode: 6.92
            Mean episode successes: 2.0249
Mean episode consecutive_successes: 3.8469
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 8.42s
                        Total time: 21380.69s
                               ETA: 1030829.1s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.038s, learning 0.160s)
               Value function loss: 87411.8275
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 1148.50
               Mean episode length: 51.06
                  Mean reward/step: 18.38
       Mean episode length/episode: 6.93
            Mean episode successes: 1.8638
Mean episode consecutive_successes: 3.8607
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 8.20s
                        Total time: 21388.89s
                               ETA: 1030706.6s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.922s, learning 0.171s)
               Value function loss: 73999.2439
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 1019.84
               Mean episode length: 52.91
                  Mean reward/step: 17.31
       Mean episode length/episode: 7.08
            Mean episode successes: 1.8623
Mean episode consecutive_successes: 3.8444
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 8.09s
                        Total time: 21396.98s
                               ETA: 1030579.1s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.888s, learning 0.207s)
               Value function loss: 78484.6189
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 736.22
               Mean episode length: 47.41
                  Mean reward/step: 18.84
       Mean episode length/episode: 6.89
            Mean episode successes: 1.7212
Mean episode consecutive_successes: 3.8252
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 8.09s
                        Total time: 21405.07s
                               ETA: 1030451.9s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.143s, learning 0.164s)
               Value function loss: 73386.8982
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 893.82
               Mean episode length: 47.78
                  Mean reward/step: 18.65
       Mean episode length/episode: 6.99
            Mean episode successes: 1.7290
Mean episode consecutive_successes: 3.8279
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 8.31s
                        Total time: 21413.38s
                               ETA: 1030335.0s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.867s, learning 0.191s)
               Value function loss: 75561.4822
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 1213.60
               Mean episode length: 46.98
                  Mean reward/step: 18.47
       Mean episode length/episode: 6.99
            Mean episode successes: 1.6489
Mean episode consecutive_successes: 3.8352
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 8.06s
                        Total time: 21421.44s
                               ETA: 1030206.2s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.189s, learning 0.177s)
               Value function loss: 77849.2734
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 766.93
               Mean episode length: 48.50
                  Mean reward/step: 19.35
       Mean episode length/episode: 7.05
            Mean episode successes: 1.8115
Mean episode consecutive_successes: 3.7580
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 8.37s
                        Total time: 21429.81s
                               ETA: 1030092.3s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.174s, learning 0.187s)
               Value function loss: 82712.4709
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 717.14
               Mean episode length: 45.03
                  Mean reward/step: 20.64
       Mean episode length/episode: 6.92
            Mean episode successes: 1.8770
Mean episode consecutive_successes: 3.7219
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 8.36s
                        Total time: 21438.17s
                               ETA: 1029978.3s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.302s, learning 0.161s)
               Value function loss: 89544.2146
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 953.45
               Mean episode length: 47.09
                  Mean reward/step: 21.23
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9893
Mean episode consecutive_successes: 3.7107
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 8.46s
                        Total time: 21446.63s
                               ETA: 1029869.3s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.145s, learning 0.167s)
               Value function loss: 83244.2723
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 1351.61
               Mean episode length: 51.53
                  Mean reward/step: 21.59
       Mean episode length/episode: 7.02
            Mean episode successes: 1.9639
Mean episode consecutive_successes: 3.7681
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 8.31s
                        Total time: 21454.94s
                               ETA: 1029753.1s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.232s, learning 0.163s)
               Value function loss: 94537.4369
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 1333.83
               Mean episode length: 51.46
                  Mean reward/step: 22.47
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0029
Mean episode consecutive_successes: 3.8370
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 8.39s
                        Total time: 21463.34s
                               ETA: 1029641.0s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.983s, learning 0.165s)
               Value function loss: 105691.2430
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 1030.57
               Mean episode length: 50.17
                  Mean reward/step: 22.67
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0254
Mean episode consecutive_successes: 3.8363
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 8.15s
                        Total time: 21471.48s
                               ETA: 1029517.2s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.130s, learning 0.189s)
               Value function loss: 110801.3625
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 1417.93
               Mean episode length: 53.77
                  Mean reward/step: 20.86
       Mean episode length/episode: 6.95
            Mean episode successes: 2.0015
Mean episode consecutive_successes: 3.8691
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 8.32s
                        Total time: 21479.80s
                               ETA: 1029401.8s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.301s, learning 0.161s)
               Value function loss: 91117.2268
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 1006.84
               Mean episode length: 48.07
                  Mean reward/step: 19.94
       Mean episode length/episode: 6.99
            Mean episode successes: 1.9136
Mean episode consecutive_successes: 3.8852
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 8.46s
                        Total time: 21488.27s
                               ETA: 1029293.2s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.205s, learning 0.170s)
               Value function loss: 77760.3137
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 1185.24
               Mean episode length: 49.12
                  Mean reward/step: 20.44
       Mean episode length/episode: 6.93
            Mean episode successes: 1.8623
Mean episode consecutive_successes: 3.9194
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 8.37s
                        Total time: 21496.64s
                               ETA: 1029180.6s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.476s, learning 0.195s)
               Value function loss: 72482.1389
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 730.68
               Mean episode length: 46.81
                  Mean reward/step: 20.27
       Mean episode length/episode: 6.93
            Mean episode successes: 1.8892
Mean episode consecutive_successes: 3.8807
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 8.67s
                        Total time: 21505.31s
                               ETA: 1029082.2s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.928s, learning 0.256s)
               Value function loss: 79603.8273
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 1171.38
               Mean episode length: 51.53
                  Mean reward/step: 20.23
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9639
Mean episode consecutive_successes: 3.8567
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 8.18s
                        Total time: 21513.50s
                               ETA: 1028960.7s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.162s, learning 0.239s)
               Value function loss: 76408.7398
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 1139.73
               Mean episode length: 48.79
                  Mean reward/step: 19.65
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9268
Mean episode consecutive_successes: 3.8576
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 8.40s
                        Total time: 21521.90s
                               ETA: 1028849.6s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.930s, learning 0.253s)
               Value function loss: 76239.1463
                    Surrogate loss: -0.0225
             Mean action noise std: 0.75
                       Mean reward: 960.88
               Mean episode length: 46.71
                  Mean reward/step: 19.74
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8247
Mean episode consecutive_successes: 3.8907
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 8.18s
                        Total time: 21530.08s
                               ETA: 1028728.3s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.107s, learning 0.157s)
               Value function loss: 87979.7564
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 696.13
               Mean episode length: 47.69
                  Mean reward/step: 21.91
       Mean episode length/episode: 7.07
            Mean episode successes: 1.9185
Mean episode consecutive_successes: 3.8979
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 8.26s
                        Total time: 21538.34s
                               ETA: 1028610.9s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.025s, learning 0.161s)
               Value function loss: 81534.2663
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 918.33
               Mean episode length: 46.75
                  Mean reward/step: 22.01
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0781
Mean episode consecutive_successes: 3.8501
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 8.19s
                        Total time: 21546.53s
                               ETA: 1028489.9s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.377s, learning 0.183s)
               Value function loss: 87397.7193
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 1196.12
               Mean episode length: 51.35
                  Mean reward/step: 21.39
       Mean episode length/episode: 6.94
            Mean episode successes: 1.9976
Mean episode consecutive_successes: 3.9072
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 8.56s
                        Total time: 21555.09s
                               ETA: 1028386.8s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.761s, learning 0.177s)
               Value function loss: 110026.9877
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 784.99
               Mean episode length: 45.17
                  Mean reward/step: 21.60
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0063
Mean episode consecutive_successes: 3.9113
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 7.94s
                        Total time: 21563.03s
                               ETA: 1028254.2s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.578s, learning 0.203s)
               Value function loss: 92321.4283
                    Surrogate loss: -0.0247
             Mean action noise std: 0.75
                       Mean reward: 1111.78
               Mean episode length: 48.20
                  Mean reward/step: 20.22
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8960
Mean episode consecutive_successes: 3.9646
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 8.78s
                        Total time: 21571.81s
                               ETA: 1028161.9s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.066s, learning 0.165s)
               Value function loss: 86817.9746
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 701.44
               Mean episode length: 47.52
                  Mean reward/step: 21.25
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9058
Mean episode consecutive_successes: 3.9284
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 8.23s
                        Total time: 21580.04s
                               ETA: 1028043.4s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.002s, learning 0.161s)
               Value function loss: 78434.6201
                    Surrogate loss: -0.0260
             Mean action noise std: 0.75
                       Mean reward: 717.28
               Mean episode length: 44.56
                  Mean reward/step: 19.43
       Mean episode length/episode: 6.87
            Mean episode successes: 1.8457
Mean episode consecutive_successes: 3.9037
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 8.16s
                        Total time: 21588.21s
                               ETA: 1027921.8s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.886s, learning 0.169s)
               Value function loss: 79757.8549
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 974.19
               Mean episode length: 47.60
                  Mean reward/step: 18.68
       Mean episode length/episode: 7.10
            Mean episode successes: 1.8691
Mean episode consecutive_successes: 3.9006
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 8.06s
                        Total time: 21596.26s
                               ETA: 1027795.3s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.017s, learning 0.163s)
               Value function loss: 88861.8926
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 1053.64
               Mean episode length: 51.48
                  Mean reward/step: 19.51
       Mean episode length/episode: 6.95
            Mean episode successes: 1.8364
Mean episode consecutive_successes: 3.9137
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 8.18s
                        Total time: 21604.44s
                               ETA: 1027674.7s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.187s, learning 0.195s)
               Value function loss: 93149.9748
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 838.43
               Mean episode length: 45.81
                  Mean reward/step: 21.33
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8638
Mean episode consecutive_successes: 3.9017
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 8.38s
                        Total time: 21612.82s
                               ETA: 1027563.8s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.923s, learning 0.184s)
               Value function loss: 85303.0561
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 933.26
               Mean episode length: 46.19
                  Mean reward/step: 20.17
       Mean episode length/episode: 6.98
            Mean episode successes: 1.8574
Mean episode consecutive_successes: 3.8708
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 8.11s
                        Total time: 21620.93s
                               ETA: 1027440.0s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.117s, learning 0.177s)
               Value function loss: 95311.5285
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 915.55
               Mean episode length: 46.19
                  Mean reward/step: 19.54
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8770
Mean episode consecutive_successes: 3.8453
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 8.29s
                        Total time: 21629.22s
                               ETA: 1027325.2s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.936s, learning 0.172s)
               Value function loss: 92565.3211
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 1236.45
               Mean episode length: 51.34
                  Mean reward/step: 18.66
       Mean episode length/episode: 7.04
            Mean episode successes: 1.8706
Mean episode consecutive_successes: 3.8481
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 8.11s
                        Total time: 21637.33s
                               ETA: 1027201.7s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.115s, learning 0.164s)
               Value function loss: 86836.5607
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 1011.43
               Mean episode length: 47.61
                  Mean reward/step: 20.57
       Mean episode length/episode: 6.95
            Mean episode successes: 1.7593
Mean episode consecutive_successes: 3.8989
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 8.28s
                        Total time: 21645.61s
                               ETA: 1027086.3s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.723s, learning 0.168s)
               Value function loss: 86237.8709
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 781.92
               Mean episode length: 48.48
                  Mean reward/step: 20.87
       Mean episode length/episode: 6.94
            Mean episode successes: 1.8555
Mean episode consecutive_successes: 3.8284
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 7.89s
                        Total time: 21653.50s
                               ETA: 1026952.7s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.566s, learning 0.169s)
               Value function loss: 92703.1455
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 839.00
               Mean episode length: 47.36
                  Mean reward/step: 20.52
       Mean episode length/episode: 6.94
            Mean episode successes: 1.9038
Mean episode consecutive_successes: 3.8121
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 8.74s
                        Total time: 21662.24s
                               ETA: 1026859.2s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.981s, learning 0.169s)
               Value function loss: 97403.4596
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 792.60
               Mean episode length: 49.35
                  Mean reward/step: 20.57
       Mean episode length/episode: 7.04
            Mean episode successes: 1.8999
Mean episode consecutive_successes: 3.8231
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 8.15s
                        Total time: 21670.39s
                               ETA: 1026738.1s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.994s, learning 0.160s)
               Value function loss: 110664.6580
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 1219.22
               Mean episode length: 47.38
                  Mean reward/step: 19.55
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9180
Mean episode consecutive_successes: 3.8325
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 8.15s
                        Total time: 21678.54s
                               ETA: 1026617.2s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.837s, learning 0.160s)
               Value function loss: 86850.9850
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 1154.06
               Mean episode length: 51.39
                  Mean reward/step: 20.54
       Mean episode length/episode: 6.86
            Mean episode successes: 1.7798
Mean episode consecutive_successes: 3.8559
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 8.00s
                        Total time: 21686.54s
                               ETA: 1026489.1s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.341s, learning 0.169s)
               Value function loss: 71393.1582
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 1176.27
               Mean episode length: 47.08
                  Mean reward/step: 18.93
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8218
Mean episode consecutive_successes: 3.8272
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 8.51s
                        Total time: 21695.05s
                               ETA: 1026385.3s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.098s, learning 0.191s)
               Value function loss: 75722.3469
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 884.80
               Mean episode length: 48.97
                  Mean reward/step: 19.10
       Mean episode length/episode: 7.01
            Mean episode successes: 1.7656
Mean episode consecutive_successes: 3.8289
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 8.29s
                        Total time: 21703.34s
                               ETA: 1026271.2s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.314s, learning 0.161s)
               Value function loss: 79199.0578
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 736.77
               Mean episode length: 48.20
                  Mean reward/step: 19.32
       Mean episode length/episode: 7.05
            Mean episode successes: 1.7905
Mean episode consecutive_successes: 3.8223
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 8.48s
                        Total time: 21711.81s
                               ETA: 1026166.0s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.686s, learning 0.166s)
               Value function loss: 78490.3368
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 455.64
               Mean episode length: 46.09
                  Mean reward/step: 18.58
       Mean episode length/episode: 6.92
            Mean episode successes: 1.8438
Mean episode consecutive_successes: 3.7422
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 7.85s
                        Total time: 21719.66s
                               ETA: 1026031.4s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.276s, learning 0.164s)
               Value function loss: 79340.3502
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 1033.39
               Mean episode length: 46.33
                  Mean reward/step: 18.55
       Mean episode length/episode: 6.97
            Mean episode successes: 1.7861
Mean episode consecutive_successes: 3.7597
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 8.44s
                        Total time: 21728.10s
                               ETA: 1025924.8s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.283s, learning 0.234s)
               Value function loss: 87576.7984
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 922.23
               Mean episode length: 49.23
                  Mean reward/step: 19.11
       Mean episode length/episode: 6.95
            Mean episode successes: 1.8433
Mean episode consecutive_successes: 3.7194
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 8.52s
                        Total time: 21736.62s
                               ETA: 1025821.8s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.941s, learning 0.184s)
               Value function loss: 81906.6686
                    Surrogate loss: -0.0238
             Mean action noise std: 0.75
                       Mean reward: 684.69
               Mean episode length: 45.13
                  Mean reward/step: 19.44
       Mean episode length/episode: 7.07
            Mean episode successes: 1.8872
Mean episode consecutive_successes: 3.7153
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 8.12s
                        Total time: 21744.75s
                               ETA: 1025700.5s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.278s, learning 0.192s)
               Value function loss: 113636.2787
                    Surrogate loss: -0.0220
             Mean action noise std: 0.75
                       Mean reward: 1024.71
               Mean episode length: 49.22
                  Mean reward/step: 19.82
       Mean episode length/episode: 6.96
            Mean episode successes: 1.8916
Mean episode consecutive_successes: 3.7251
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 8.47s
                        Total time: 21753.22s
                               ETA: 1025595.5s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.184s, learning 0.166s)
               Value function loss: 92453.1791
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 770.29
               Mean episode length: 49.46
                  Mean reward/step: 21.22
       Mean episode length/episode: 6.89
            Mean episode successes: 1.8862
Mean episode consecutive_successes: 3.7207
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 8.35s
                        Total time: 21761.57s
                               ETA: 1025485.0s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.624s, learning 0.163s)
               Value function loss: 74362.7272
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 815.23
               Mean episode length: 45.85
                  Mean reward/step: 21.17
       Mean episode length/episode: 6.91
            Mean episode successes: 1.9131
Mean episode consecutive_successes: 3.6978
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 8.79s
                        Total time: 21770.35s
                               ETA: 1025395.2s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.453s, learning 0.166s)
               Value function loss: 68457.7087
                    Surrogate loss: -0.0249
             Mean action noise std: 0.75
                       Mean reward: 830.66
               Mean episode length: 46.88
                  Mean reward/step: 19.37
       Mean episode length/episode: 7.06
            Mean episode successes: 2.0298
Mean episode consecutive_successes: 3.6698
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 8.62s
                        Total time: 21778.97s
                               ETA: 1025297.5s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.978s, learning 0.180s)
               Value function loss: 74787.0664
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 989.08
               Mean episode length: 48.00
                  Mean reward/step: 19.34
       Mean episode length/episode: 6.93
            Mean episode successes: 2.0220
Mean episode consecutive_successes: 3.6698
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 8.16s
                        Total time: 21787.13s
                               ETA: 1025178.2s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.221s, learning 0.170s)
               Value function loss: 77570.8898
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 822.67
               Mean episode length: 45.68
                  Mean reward/step: 19.78
       Mean episode length/episode: 6.90
            Mean episode successes: 1.8989
Mean episode consecutive_successes: 3.7001
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 8.39s
                        Total time: 21795.52s
                               ETA: 1025070.0s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.233s, learning 0.265s)
               Value function loss: 78048.0400
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 741.47
               Mean episode length: 43.47
                  Mean reward/step: 21.15
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9126
Mean episode consecutive_successes: 3.7182
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 8.50s
                        Total time: 21804.02s
                               ETA: 1024966.9s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.347s, learning 0.269s)
               Value function loss: 81976.3578
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 927.95
               Mean episode length: 50.20
                  Mean reward/step: 19.70
       Mean episode length/episode: 7.06
            Mean episode successes: 1.9517
Mean episode consecutive_successes: 3.7337
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 8.62s
                        Total time: 21812.64s
                               ETA: 1024869.4s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.135s, learning 0.166s)
               Value function loss: 74766.0869
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 894.37
               Mean episode length: 48.68
                  Mean reward/step: 18.53
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9287
Mean episode consecutive_successes: 3.7224
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 8.30s
                        Total time: 21820.94s
                               ETA: 1024757.3s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.953s, learning 0.156s)
               Value function loss: 74148.7707
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 910.46
               Mean episode length: 46.16
                  Mean reward/step: 18.95
       Mean episode length/episode: 7.01
            Mean episode successes: 1.8350
Mean episode consecutive_successes: 3.7613
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 8.11s
                        Total time: 21829.05s
                               ETA: 1024636.2s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.976s, learning 0.160s)
               Value function loss: 73292.5088
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 858.11
               Mean episode length: 50.18
                  Mean reward/step: 19.91
       Mean episode length/episode: 6.90
            Mean episode successes: 1.7847
Mean episode consecutive_successes: 3.7419
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 8.14s
                        Total time: 21837.18s
                               ETA: 1024516.5s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.809s, learning 0.170s)
               Value function loss: 73929.5785
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 1037.20
               Mean episode length: 49.19
                  Mean reward/step: 19.61
       Mean episode length/episode: 6.99
            Mean episode successes: 1.7676
Mean episode consecutive_successes: 3.7442
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 7.98s
                        Total time: 21845.16s
                               ETA: 1024389.5s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.295s, learning 0.208s)
               Value function loss: 77874.2127
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 893.09
               Mean episode length: 45.54
                  Mean reward/step: 16.55
       Mean episode length/episode: 6.99
            Mean episode successes: 1.7915
Mean episode consecutive_successes: 3.6959
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 8.50s
                        Total time: 21853.66s
                               ETA: 1024287.2s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.032s, learning 0.188s)
               Value function loss: 69085.9018
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 529.34
               Mean episode length: 44.47
                  Mean reward/step: 18.09
       Mean episode length/episode: 7.01
            Mean episode successes: 1.9023
Mean episode consecutive_successes: 3.6097
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 8.22s
                        Total time: 21861.89s
                               ETA: 1024171.8s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.384s, learning 0.164s)
               Value function loss: 71214.0472
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 806.68
               Mean episode length: 48.15
                  Mean reward/step: 17.94
       Mean episode length/episode: 6.87
            Mean episode successes: 1.6914
Mean episode consecutive_successes: 3.6545
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 8.55s
                        Total time: 21870.43s
                               ETA: 1024071.8s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.182s, learning 0.167s)
               Value function loss: 77642.2924
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 847.81
               Mean episode length: 45.67
                  Mean reward/step: 19.41
       Mean episode length/episode: 6.98
            Mean episode successes: 1.6509
Mean episode consecutive_successes: 3.6639
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 8.35s
                        Total time: 21878.78s
                               ETA: 1023962.6s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.953s, learning 0.205s)
               Value function loss: 79646.6313
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 1124.17
               Mean episode length: 47.01
                  Mean reward/step: 21.00
       Mean episode length/episode: 7.03
            Mean episode successes: 1.7856
Mean episode consecutive_successes: 3.6601
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 8.16s
                        Total time: 21886.94s
                               ETA: 1023844.5s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.051s, learning 0.189s)
               Value function loss: 75492.1844
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 1043.22
               Mean episode length: 49.65
                  Mean reward/step: 21.33
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8750
Mean episode consecutive_successes: 3.6473
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 8.24s
                        Total time: 21895.18s
                               ETA: 1023730.4s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.138s, learning 0.223s)
               Value function loss: 85352.3238
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 704.44
               Mean episode length: 44.48
                  Mean reward/step: 21.73
       Mean episode length/episode: 6.95
            Mean episode successes: 1.9985
Mean episode consecutive_successes: 3.6085
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 8.36s
                        Total time: 21903.54s
                               ETA: 1023622.0s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.171s, learning 0.160s)
               Value function loss: 88410.3910
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 886.73
               Mean episode length: 48.25
                  Mean reward/step: 21.73
       Mean episode length/episode: 7.07
            Mean episode successes: 2.0176
Mean episode consecutive_successes: 3.6779
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 8.33s
                        Total time: 21911.87s
                               ETA: 1023512.4s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.357s, learning 0.189s)
               Value function loss: 90049.0641
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 900.05
               Mean episode length: 45.46
                  Mean reward/step: 21.27
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0825
Mean episode consecutive_successes: 3.6711
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 8.55s
                        Total time: 21920.42s
                               ETA: 1023412.8s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.540s, learning 0.159s)
               Value function loss: 83889.1025
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 738.47
               Mean episode length: 46.49
                  Mean reward/step: 22.00
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0786
Mean episode consecutive_successes: 3.6800
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 8.70s
                        Total time: 21929.12s
                               ETA: 1023320.5s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.859s, learning 0.251s)
               Value function loss: 84035.4754
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 901.89
               Mean episode length: 48.34
                  Mean reward/step: 22.38
       Mean episode length/episode: 6.94
            Mean episode successes: 2.0146
Mean episode consecutive_successes: 3.7059
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 8.11s
                        Total time: 21937.23s
                               ETA: 1023200.8s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.179s, learning 0.231s)
               Value function loss: 93579.6957
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 1385.43
               Mean episode length: 49.47
                  Mean reward/step: 20.93
       Mean episode length/episode: 6.99
            Mean episode successes: 1.9800
Mean episode consecutive_successes: 3.7788
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 8.41s
                        Total time: 21945.64s
                               ETA: 1023095.2s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.255s, learning 0.160s)
               Value function loss: 103392.9920
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 846.71
               Mean episode length: 47.74
                  Mean reward/step: 19.71
       Mean episode length/episode: 6.94
            Mean episode successes: 1.8555
Mean episode consecutive_successes: 3.8009
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 8.41s
                        Total time: 21954.05s
                               ETA: 1022989.9s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.247s, learning 0.185s)
               Value function loss: 99273.3842
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 963.61
               Mean episode length: 46.85
                  Mean reward/step: 20.48
       Mean episode length/episode: 6.96
            Mean episode successes: 1.7798
Mean episode consecutive_successes: 3.8407
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 8.43s
                        Total time: 21962.49s
                               ETA: 1022885.5s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.975s, learning 0.167s)
               Value function loss: 106106.8703
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 811.63
               Mean episode length: 47.82
                  Mean reward/step: 20.83
       Mean episode length/episode: 7.13
            Mean episode successes: 1.8906
Mean episode consecutive_successes: 3.8218
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 8.14s
                        Total time: 21970.63s
                               ETA: 1022767.7s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.176s, learning 0.163s)
               Value function loss: 111579.1568
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 1195.47
               Mean episode length: 49.43
                  Mean reward/step: 22.62
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0044
Mean episode consecutive_successes: 3.8216
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 8.34s
                        Total time: 21978.97s
                               ETA: 1022659.2s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.382s, learning 0.194s)
               Value function loss: 126106.0426
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 756.87
               Mean episode length: 47.93
                  Mean reward/step: 23.97
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1348
Mean episode consecutive_successes: 3.7981
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 8.58s
                        Total time: 21987.54s
                               ETA: 1022561.8s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.160s, learning 0.163s)
               Value function loss: 106570.0086
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 982.42
               Mean episode length: 49.47
                  Mean reward/step: 23.14
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2031
Mean episode consecutive_successes: 3.8257
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 8.32s
                        Total time: 21995.87s
                               ETA: 1022452.7s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.258s, learning 0.173s)
               Value function loss: 116636.9584
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 1226.09
               Mean episode length: 50.90
                  Mean reward/step: 23.66
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1621
Mean episode consecutive_successes: 3.9166
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 16.43s
                        Total time: 22012.30s
                               ETA: 1022720.4s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.190s, learning 0.162s)
               Value function loss: 135237.0406
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 1397.77
               Mean episode length: 49.37
                  Mean reward/step: 22.57
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0322
Mean episode consecutive_successes: 4.0261
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 16.35s
                        Total time: 22028.65s
                               ETA: 1022984.1s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.773s, learning 0.159s)
               Value function loss: 114142.3461
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 1177.41
               Mean episode length: 49.14
                  Mean reward/step: 22.11
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0786
Mean episode consecutive_successes: 4.0120
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 15.93s
                        Total time: 22044.58s
                               ETA: 1023228.2s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.409s, learning 0.171s)
               Value function loss: 110179.7592
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 1211.98
               Mean episode length: 48.25
                  Mean reward/step: 22.85
       Mean episode length/episode: 7.02
            Mean episode successes: 2.0771
Mean episode consecutive_successes: 4.0622
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 16.58s
                        Total time: 22061.16s
                               ETA: 1023502.0s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.199s, learning 0.163s)
               Value function loss: 94209.1609
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 1030.37
               Mean episode length: 49.71
                  Mean reward/step: 22.01
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9600
Mean episode consecutive_successes: 4.1095
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 16.36s
                        Total time: 22077.52s
                               ETA: 1023765.4s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.123s, learning 0.174s)
               Value function loss: 117934.2363
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 1319.60
               Mean episode length: 52.04
                  Mean reward/step: 21.33
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8696
Mean episode consecutive_successes: 4.1560
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 16.30s
                        Total time: 22093.82s
                               ETA: 1024025.5s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.917s, learning 0.159s)
               Value function loss: 106775.1367
                    Surrogate loss: -0.0229
             Mean action noise std: 0.75
                       Mean reward: 1019.67
               Mean episode length: 49.02
                  Mean reward/step: 23.20
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9551
Mean episode consecutive_successes: 4.1235
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 16.08s
                        Total time: 22109.90s
                               ETA: 1024275.2s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.677s, learning 0.195s)
               Value function loss: 100801.3707
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 765.57
               Mean episode length: 46.47
                  Mean reward/step: 24.24
       Mean episode length/episode: 7.12
            Mean episode successes: 2.1851
Mean episode consecutive_successes: 4.1175
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 15.87s
                        Total time: 22125.77s
                               ETA: 1024515.2s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.124s, learning 0.167s)
               Value function loss: 109939.3721
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 1082.96
               Mean episode length: 49.92
                  Mean reward/step: 25.08
       Mean episode length/episode: 6.92
            Mean episode successes: 2.2134
Mean episode consecutive_successes: 4.1207
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 16.29s
                        Total time: 22142.06s
                               ETA: 1024774.3s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.213s, learning 0.170s)
               Value function loss: 105339.1652
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 1122.42
               Mean episode length: 48.57
                  Mean reward/step: 25.16
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2563
Mean episode consecutive_successes: 4.1838
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 16.38s
                        Total time: 22158.44s
                               ETA: 1025037.4s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.639s, learning 0.162s)
               Value function loss: 107064.2193
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 989.92
               Mean episode length: 48.88
                  Mean reward/step: 26.80
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3101
Mean episode consecutive_successes: 4.2114
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 15.80s
                        Total time: 22174.24s
                               ETA: 1025273.3s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.038s, learning 0.166s)
               Value function loss: 99839.7908
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 1125.36
               Mean episode length: 48.92
                  Mean reward/step: 24.41
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2271
Mean episode consecutive_successes: 4.3030
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 16.20s
                        Total time: 22190.45s
                               ETA: 1025527.6s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.779s, learning 0.161s)
               Value function loss: 88579.6523
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 1127.08
               Mean episode length: 48.92
                  Mean reward/step: 23.55
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2036
Mean episode consecutive_successes: 4.3308
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 15.94s
                        Total time: 22206.39s
                               ETA: 1025769.5s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.976s, learning 0.158s)
               Value function loss: 93349.2574
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 1122.78
               Mean episode length: 49.85
                  Mean reward/step: 22.37
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9927
Mean episode consecutive_successes: 4.3964
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 16.13s
                        Total time: 22222.52s
                               ETA: 1026020.1s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.075s, learning 0.185s)
               Value function loss: 88653.8398
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 766.46
               Mean episode length: 43.55
                  Mean reward/step: 21.97
       Mean episode length/episode: 6.94
            Mean episode successes: 1.8789
Mean episode consecutive_successes: 4.4241
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 16.26s
                        Total time: 22238.78s
                               ETA: 1026276.3s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.257s, learning 0.241s)
               Value function loss: 94421.9809
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 1143.13
               Mean episode length: 49.96
                  Mean reward/step: 23.14
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0356
Mean episode consecutive_successes: 4.4073
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 16.50s
                        Total time: 22255.28s
                               ETA: 1026543.2s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.029s, learning 0.167s)
               Value function loss: 107476.9377
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 1219.85
               Mean episode length: 48.51
                  Mean reward/step: 24.18
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1108
Mean episode consecutive_successes: 4.3852
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 16.20s
                        Total time: 22271.48s
                               ETA: 1026795.8s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.539s, learning 0.161s)
               Value function loss: 105903.4330
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 1734.61
               Mean episode length: 51.24
                  Mean reward/step: 25.54
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1123
Mean episode consecutive_successes: 4.4703
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 15.70s
                        Total time: 22287.18s
                               ETA: 1027025.4s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.733s, learning 0.189s)
               Value function loss: 114337.9109
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 1232.66
               Mean episode length: 48.93
                  Mean reward/step: 26.03
       Mean episode length/episode: 7.01
            Mean episode successes: 2.1665
Mean episode consecutive_successes: 4.4573
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 15.92s
                        Total time: 22303.10s
                               ETA: 1027265.0s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.706s, learning 0.205s)
               Value function loss: 109156.9688
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 1408.09
               Mean episode length: 49.41
                  Mean reward/step: 26.67
       Mean episode length/episode: 7.10
            Mean episode successes: 2.2109
Mean episode consecutive_successes: 4.5223
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 15.91s
                        Total time: 22319.01s
                               ETA: 1027503.8s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.057s, learning 0.173s)
               Value function loss: 129166.6211
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 1090.30
               Mean episode length: 49.27
                  Mean reward/step: 26.54
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3438
Mean episode consecutive_successes: 4.5300
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 16.23s
                        Total time: 22335.24s
                               ETA: 1027757.1s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.128s, learning 0.173s)
               Value function loss: 141808.9227
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 1248.78
               Mean episode length: 50.94
                  Mean reward/step: 26.12
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3057
Mean episode consecutive_successes: 4.5683
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 16.30s
                        Total time: 22351.54s
                               ETA: 1028013.3s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.970s, learning 0.190s)
               Value function loss: 139227.5410
                    Surrogate loss: -0.0255
             Mean action noise std: 0.75
                       Mean reward: 1501.97
               Mean episode length: 51.54
                  Mean reward/step: 25.27
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2354
Mean episode consecutive_successes: 4.6416
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 16.16s
                        Total time: 22367.70s
                               ETA: 1028262.8s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.161s, learning 0.169s)
               Value function loss: 133819.9215
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 1018.02
               Mean episode length: 49.67
                  Mean reward/step: 25.01
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2329
Mean episode consecutive_successes: 4.6818
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 16.33s
                        Total time: 22384.03s
                               ETA: 1028519.9s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.100s, learning 0.164s)
               Value function loss: 117535.4508
                    Surrogate loss: -0.0246
             Mean action noise std: 0.75
                       Mean reward: 1095.28
               Mean episode length: 49.45
                  Mean reward/step: 22.29
       Mean episode length/episode: 6.92
            Mean episode successes: 2.0552
Mean episode consecutive_successes: 4.7060
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 16.26s
                        Total time: 22400.29s
                               ETA: 1028773.7s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.887s, learning 0.163s)
               Value function loss: 95948.2021
                    Surrogate loss: -0.0235
             Mean action noise std: 0.75
                       Mean reward: 1167.22
               Mean episode length: 48.58
                  Mean reward/step: 24.00
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9810
Mean episode consecutive_successes: 4.7127
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 16.05s
                        Total time: 22416.34s
                               ETA: 1029017.4s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.716s, learning 0.173s)
               Value function loss: 87284.1887
                    Surrogate loss: -0.0228
             Mean action noise std: 0.75
                       Mean reward: 975.54
               Mean episode length: 45.90
                  Mean reward/step: 22.27
       Mean episode length/episode: 7.01
            Mean episode successes: 2.0093
Mean episode consecutive_successes: 4.6693
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 15.89s
                        Total time: 22432.23s
                               ETA: 1029253.5s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.217s, learning 0.194s)
               Value function loss: 87630.6297
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 942.58
               Mean episode length: 45.55
                  Mean reward/step: 20.84
       Mean episode length/episode: 6.90
            Mean episode successes: 1.9146
Mean episode consecutive_successes: 4.6122
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 16.41s
                        Total time: 22448.64s
                               ETA: 1029513.3s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.249s, learning 0.181s)
               Value function loss: 94427.1666
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 1428.77
               Mean episode length: 50.76
                  Mean reward/step: 21.71
       Mean episode length/episode: 7.05
            Mean episode successes: 1.9146
Mean episode consecutive_successes: 4.6116
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 16.43s
                        Total time: 22465.07s
                               ETA: 1029773.7s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.514s, learning 0.211s)
               Value function loss: 113303.4283
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 888.64
               Mean episode length: 47.18
                  Mean reward/step: 23.59
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0747
Mean episode consecutive_successes: 4.5450
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 16.72s
                        Total time: 22481.80s
                               ETA: 1030047.4s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.360s, learning 0.160s)
               Value function loss: 102331.7885
                    Surrogate loss: -0.0206
             Mean action noise std: 0.74
                       Mean reward: 1319.17
               Mean episode length: 47.43
                  Mean reward/step: 25.51
       Mean episode length/episode: 6.96
            Mean episode successes: 2.0732
Mean episode consecutive_successes: 4.5605
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 16.52s
                        Total time: 22498.32s
                               ETA: 1030311.4s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.873s, learning 0.158s)
               Value function loss: 103123.8182
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 1390.33
               Mean episode length: 49.62
                  Mean reward/step: 23.63
       Mean episode length/episode: 6.98
            Mean episode successes: 2.1123
Mean episode consecutive_successes: 4.5598
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 16.03s
                        Total time: 22514.35s
                               ETA: 1030552.8s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.244s, learning 0.162s)
               Value function loss: 105205.1820
                    Surrogate loss: -0.0248
             Mean action noise std: 0.74
                       Mean reward: 1418.58
               Mean episode length: 50.48
                  Mean reward/step: 23.76
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2080
Mean episode consecutive_successes: 4.5521
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 16.41s
                        Total time: 22530.76s
                               ETA: 1030811.0s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.038s, learning 0.166s)
               Value function loss: 107264.2645
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 1036.49
               Mean episode length: 51.35
                  Mean reward/step: 24.64
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1069
Mean episode consecutive_successes: 4.5862
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 16.20s
                        Total time: 22546.96s
                               ETA: 1031059.8s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.311s, learning 0.160s)
               Value function loss: 96345.5301
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 1289.88
               Mean episode length: 48.73
                  Mean reward/step: 26.04
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2793
Mean episode consecutive_successes: 4.6011
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 16.47s
                        Total time: 22563.43s
                               ETA: 1031320.6s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.181s, learning 0.165s)
               Value function loss: 92330.8785
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 969.76
               Mean episode length: 48.72
                  Mean reward/step: 27.88
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4326
Mean episode consecutive_successes: 4.5881
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 16.35s
                        Total time: 22579.78s
                               ETA: 1031575.3s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.846s, learning 0.207s)
               Value function loss: 111953.5922
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 1534.58
               Mean episode length: 52.60
                  Mean reward/step: 29.56
       Mean episode length/episode: 6.95
            Mean episode successes: 2.6001
Mean episode consecutive_successes: 4.6174
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 16.05s
                        Total time: 22595.83s
                               ETA: 1031816.5s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1173 steps/s (collection: 13.777s, learning 0.187s)
               Value function loss: 104817.8400
                    Surrogate loss: -0.0019
             Mean action noise std: 0.74
                       Mean reward: 1082.56
               Mean episode length: 49.31
                  Mean reward/step: 25.95
       Mean episode length/episode: 7.01
            Mean episode successes: 2.5347
Mean episode consecutive_successes: 4.6531
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 13.96s
                        Total time: 22609.79s
                               ETA: 1031962.1s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.322s, learning 0.169s)
               Value function loss: 91174.5150
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 1444.07
               Mean episode length: 50.75
                  Mean reward/step: 25.02
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3940
Mean episode consecutive_successes: 4.7370
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 8.49s
                        Total time: 22618.29s
                               ETA: 1031857.8s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.225s, learning 0.166s)
               Value function loss: 89955.8939
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 1448.57
               Mean episode length: 50.50
                  Mean reward/step: 23.33
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1436
Mean episode consecutive_successes: 4.8490
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 8.39s
                        Total time: 22626.68s
                               ETA: 1031749.1s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.167s)
               Value function loss: 110961.3031
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 890.24
               Mean episode length: 45.57
                  Mean reward/step: 24.21
       Mean episode length/episode: 7.02
            Mean episode successes: 2.1562
Mean episode consecutive_successes: 4.8203
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 8.46s
                        Total time: 22635.14s
                               ETA: 1031643.7s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.399s, learning 0.299s)
               Value function loss: 92661.7850
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 1204.29
               Mean episode length: 47.44
                  Mean reward/step: 22.76
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2524
Mean episode consecutive_successes: 4.7657
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 8.70s
                        Total time: 22643.84s
                               ETA: 1031549.1s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.143s, learning 0.206s)
               Value function loss: 125885.9156
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 1302.25
               Mean episode length: 48.53
                  Mean reward/step: 25.61
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3154
Mean episode consecutive_successes: 4.7551
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 8.35s
                        Total time: 22652.19s
                               ETA: 1031438.7s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.387s, learning 0.167s)
               Value function loss: 105988.1080
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 1065.42
               Mean episode length: 49.58
                  Mean reward/step: 26.30
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2598
Mean episode consecutive_successes: 4.7708
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 8.55s
                        Total time: 22660.74s
                               ETA: 1031337.7s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.278s, learning 0.189s)
               Value function loss: 99701.1490
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 1528.72
               Mean episode length: 50.40
                  Mean reward/step: 26.19
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2417
Mean episode consecutive_successes: 4.8680
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 8.47s
                        Total time: 22669.21s
                               ETA: 1031232.9s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.162s, learning 0.159s)
               Value function loss: 85552.8027
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 1024.69
               Mean episode length: 48.37
                  Mean reward/step: 25.30
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3242
Mean episode consecutive_successes: 4.8262
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 8.32s
                        Total time: 22677.53s
                               ETA: 1031121.5s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.163s)
               Value function loss: 94643.6486
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 1401.33
               Mean episode length: 51.23
                  Mean reward/step: 25.35
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3657
Mean episode consecutive_successes: 4.8500
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 8.43s
                        Total time: 22685.96s
                               ETA: 1031015.4s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.222s, learning 0.166s)
               Value function loss: 86462.4398
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 1291.31
               Mean episode length: 50.73
                  Mean reward/step: 23.43
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2798
Mean episode consecutive_successes: 4.8710
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 8.39s
                        Total time: 22694.35s
                               ETA: 1030907.2s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.345s, learning 0.184s)
               Value function loss: 82529.6631
                    Surrogate loss: -0.0259
             Mean action noise std: 0.74
                       Mean reward: 1337.80
               Mean episode length: 50.03
                  Mean reward/step: 25.08
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2749
Mean episode consecutive_successes: 4.8767
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 8.53s
                        Total time: 22702.88s
                               ETA: 1030805.6s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.486s, learning 0.167s)
               Value function loss: 90277.3826
                    Surrogate loss: -0.0242
             Mean action noise std: 0.74
                       Mean reward: 1494.14
               Mean episode length: 51.44
                  Mean reward/step: 24.93
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2061
Mean episode consecutive_successes: 4.8952
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 8.65s
                        Total time: 22711.53s
                               ETA: 1030709.7s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.260s, learning 0.178s)
               Value function loss: 95911.1875
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 1414.38
               Mean episode length: 51.56
                  Mean reward/step: 25.39
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1646
Mean episode consecutive_successes: 4.9032
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 8.44s
                        Total time: 22719.97s
                               ETA: 1030604.1s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.311s, learning 0.202s)
               Value function loss: 93684.9334
                    Surrogate loss: -0.0210
             Mean action noise std: 0.74
                       Mean reward: 1305.88
               Mean episode length: 50.98
                  Mean reward/step: 26.50
       Mean episode length/episode: 7.11
            Mean episode successes: 2.2832
Mean episode consecutive_successes: 4.8987
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 8.51s
                        Total time: 22728.48s
                               ETA: 1030501.9s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.155s, learning 0.206s)
               Value function loss: 105537.3660
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 1040.72
               Mean episode length: 47.04
                  Mean reward/step: 25.66
       Mean episode length/episode: 7.02
            Mean episode successes: 2.2339
Mean episode consecutive_successes: 4.9362
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 8.36s
                        Total time: 22736.85s
                               ETA: 1030393.0s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.111s, learning 0.207s)
               Value function loss: 106300.6678
                    Surrogate loss: -0.0205
             Mean action noise std: 0.74
                       Mean reward: 1194.82
               Mean episode length: 49.24
                  Mean reward/step: 22.96
       Mean episode length/episode: 6.93
            Mean episode successes: 2.0308
Mean episode consecutive_successes: 4.9588
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 8.32s
                        Total time: 22745.16s
                               ETA: 1030282.2s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.235s, learning 0.269s)
               Value function loss: 84742.7635
                    Surrogate loss: -0.0238
             Mean action noise std: 0.74
                       Mean reward: 1406.81
               Mean episode length: 51.62
                  Mean reward/step: 25.05
       Mean episode length/episode: 7.06
            Mean episode successes: 2.0854
Mean episode consecutive_successes: 4.9611
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 8.50s
                        Total time: 22753.67s
                               ETA: 1030179.9s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.270s, learning 0.272s)
               Value function loss: 91547.1260
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 1123.77
               Mean episode length: 50.63
                  Mean reward/step: 25.75
       Mean episode length/episode: 7.08
            Mean episode successes: 2.2114
Mean episode consecutive_successes: 4.9273
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 8.54s
                        Total time: 22762.21s
                               ETA: 1030079.5s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.088s, learning 0.161s)
               Value function loss: 93086.4455
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 1145.74
               Mean episode length: 49.83
                  Mean reward/step: 26.29
       Mean episode length/episode: 6.95
            Mean episode successes: 2.2451
Mean episode consecutive_successes: 4.8961
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 8.25s
                        Total time: 22770.46s
                               ETA: 1029965.9s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.279s, learning 0.161s)
               Value function loss: 90439.5600
                    Surrogate loss: -0.0219
             Mean action noise std: 0.74
                       Mean reward: 1017.68
               Mean episode length: 49.46
                  Mean reward/step: 26.16
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3867
Mean episode consecutive_successes: 4.8976
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 8.44s
                        Total time: 22778.90s
                               ETA: 1029861.0s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.412s, learning 0.168s)
               Value function loss: 94688.6111
                    Surrogate loss: -0.0215
             Mean action noise std: 0.74
                       Mean reward: 1348.33
               Mean episode length: 51.21
                  Mean reward/step: 26.28
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3530
Mean episode consecutive_successes: 4.9105
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 8.58s
                        Total time: 22787.48s
                               ETA: 1029762.5s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.113s, learning 0.179s)
               Value function loss: 106488.8893
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 1468.63
               Mean episode length: 50.98
                  Mean reward/step: 25.25
       Mean episode length/episode: 7.02
            Mean episode successes: 2.2422
Mean episode consecutive_successes: 4.9912
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 8.29s
                        Total time: 22795.77s
                               ETA: 1029651.1s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.233s, learning 0.161s)
               Value function loss: 98906.5221
                    Surrogate loss: -0.0232
             Mean action noise std: 0.74
                       Mean reward: 942.01
               Mean episode length: 44.80
                  Mean reward/step: 24.62
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1929
Mean episode consecutive_successes: 4.9470
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 8.39s
                        Total time: 22804.17s
                               ETA: 1029544.4s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.089s, learning 0.229s)
               Value function loss: 99641.6084
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 1496.04
               Mean episode length: 50.79
                  Mean reward/step: 24.12
       Mean episode length/episode: 7.09
            Mean episode successes: 2.2666
Mean episode consecutive_successes: 4.9319
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 8.32s
                        Total time: 22812.48s
                               ETA: 1029434.4s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.163s)
               Value function loss: 100241.9660
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 1435.06
               Mean episode length: 49.41
                  Mean reward/step: 25.96
       Mean episode length/episode: 6.94
            Mean episode successes: 2.2168
Mean episode consecutive_successes: 4.9364
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 8.37s
                        Total time: 22820.85s
                               ETA: 1029326.7s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.207s, learning 0.167s)
               Value function loss: 119221.7363
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 1277.58
               Mean episode length: 48.83
                  Mean reward/step: 26.46
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2207
Mean episode consecutive_successes: 4.9437
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 8.37s
                        Total time: 22829.23s
                               ETA: 1029219.4s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.312s, learning 0.216s)
               Value function loss: 131513.8213
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 1676.79
               Mean episode length: 52.24
                  Mean reward/step: 26.65
       Mean episode length/episode: 7.11
            Mean episode successes: 2.3105
Mean episode consecutive_successes: 4.9459
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 8.53s
                        Total time: 22837.75s
                               ETA: 1029119.1s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.310s, learning 0.165s)
               Value function loss: 112271.7871
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 1908.86
               Mean episode length: 51.18
                  Mean reward/step: 26.24
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2617
Mean episode consecutive_successes: 5.0113
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 8.48s
                        Total time: 22846.23s
                               ETA: 1029016.5s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.296s, learning 0.172s)
               Value function loss: 91968.6148
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 1280.54
               Mean episode length: 50.07
                  Mean reward/step: 25.18
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3418
Mean episode consecutive_successes: 4.9833
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 8.47s
                        Total time: 22854.70s
                               ETA: 1028913.7s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.496s, learning 0.213s)
               Value function loss: 104479.4967
                    Surrogate loss: -0.0229
             Mean action noise std: 0.74
                       Mean reward: 1211.55
               Mean episode length: 51.58
                  Mean reward/step: 24.18
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2949
Mean episode consecutive_successes: 4.9755
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 8.71s
                        Total time: 22863.41s
                               ETA: 1028821.7s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.564s, learning 0.189s)
               Value function loss: 110345.4025
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 1239.74
               Mean episode length: 52.26
                  Mean reward/step: 25.54
       Mean episode length/episode: 7.07
            Mean episode successes: 2.3506
Mean episode consecutive_successes: 4.9707
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 8.75s
                        Total time: 22872.16s
                               ETA: 1028731.9s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.436s, learning 0.261s)
               Value function loss: 100158.0533
                    Surrogate loss: -0.0260
             Mean action noise std: 0.74
                       Mean reward: 949.78
               Mean episode length: 49.71
                  Mean reward/step: 24.96
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3022
Mean episode consecutive_successes: 4.9891
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 8.70s
                        Total time: 22880.86s
                               ETA: 1028639.6s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.224s, learning 0.170s)
               Value function loss: 105610.7832
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 1103.85
               Mean episode length: 51.68
                  Mean reward/step: 25.66
       Mean episode length/episode: 6.94
            Mean episode successes: 2.2656
Mean episode consecutive_successes: 4.9864
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 8.39s
                        Total time: 22889.25s
                               ETA: 1028533.8s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.459s, learning 0.165s)
               Value function loss: 102167.3922
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 1206.49
               Mean episode length: 51.16
                  Mean reward/step: 24.99
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2114
Mean episode consecutive_successes: 5.0251
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 8.62s
                        Total time: 22897.88s
                               ETA: 1028438.4s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.233s, learning 0.168s)
               Value function loss: 92052.8127
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 860.82
               Mean episode length: 46.90
                  Mean reward/step: 24.38
       Mean episode length/episode: 7.02
            Mean episode successes: 2.1953
Mean episode consecutive_successes: 4.9992
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 8.40s
                        Total time: 22906.28s
                               ETA: 1028333.1s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.135s, learning 0.188s)
               Value function loss: 92258.3023
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 1093.03
               Mean episode length: 50.23
                  Mean reward/step: 24.24
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2437
Mean episode consecutive_successes: 4.9697
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 8.32s
                        Total time: 22914.60s
                               ETA: 1028224.3s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.354s, learning 0.165s)
               Value function loss: 90974.7596
                    Surrogate loss: -0.0270
             Mean action noise std: 0.74
                       Mean reward: 1120.01
               Mean episode length: 48.78
                  Mean reward/step: 24.03
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2646
Mean episode consecutive_successes: 4.9423
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 8.52s
                        Total time: 22923.12s
                               ETA: 1028124.5s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.353s, learning 0.160s)
               Value function loss: 96420.2555
                    Surrogate loss: -0.0221
             Mean action noise std: 0.74
                       Mean reward: 1626.96
               Mean episode length: 51.72
                  Mean reward/step: 24.24
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3276
Mean episode consecutive_successes: 4.9278
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 8.51s
                        Total time: 22931.63s
                               ETA: 1028024.4s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.185s, learning 0.170s)
               Value function loss: 125785.7922
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 932.86
               Mean episode length: 49.85
                  Mean reward/step: 24.74
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2578
Mean episode consecutive_successes: 4.9089
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 8.36s
                        Total time: 22939.99s
                               ETA: 1027917.3s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.163s)
               Value function loss: 89654.9381
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 1357.19
               Mean episode length: 48.87
                  Mean reward/step: 23.96
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0767
Mean episode consecutive_successes: 4.9688
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 8.48s
                        Total time: 22948.46s
                               ETA: 1027815.8s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1997 steps/s (collection: 7.983s, learning 0.219s)
               Value function loss: 92317.2434
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 1761.35
               Mean episode length: 51.00
                  Mean reward/step: 23.55
       Mean episode length/episode: 7.10
            Mean episode successes: 2.1260
Mean episode consecutive_successes: 4.9766
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 8.20s
                        Total time: 22956.66s
                               ETA: 1027702.1s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.100s, learning 0.253s)
               Value function loss: 85254.5725
                    Surrogate loss: -0.0236
             Mean action noise std: 0.74
                       Mean reward: 899.36
               Mean episode length: 48.76
                  Mean reward/step: 23.42
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1450
Mean episode consecutive_successes: 4.9203
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 8.35s
                        Total time: 22965.02s
                               ETA: 1027595.2s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.153s, learning 0.165s)
               Value function loss: 89180.5334
                    Surrogate loss: -0.0234
             Mean action noise std: 0.74
                       Mean reward: 1054.42
               Mean episode length: 48.26
                  Mean reward/step: 24.50
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2090
Mean episode consecutive_successes: 4.8749
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 8.32s
                        Total time: 22973.34s
                               ETA: 1027486.9s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.428s, learning 0.162s)
               Value function loss: 100889.4947
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 1199.88
               Mean episode length: 48.64
                  Mean reward/step: 25.46
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3721
Mean episode consecutive_successes: 4.7945
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 8.59s
                        Total time: 22981.92s
                               ETA: 1027390.8s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.257s, learning 0.161s)
               Value function loss: 108581.4770
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 1265.74
               Mean episode length: 50.89
                  Mean reward/step: 26.87
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3081
Mean episode consecutive_successes: 4.8653
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 8.42s
                        Total time: 22990.34s
                               ETA: 1027287.1s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.288s, learning 0.182s)
               Value function loss: 106442.5932
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 1262.05
               Mean episode length: 47.79
                  Mean reward/step: 27.06
       Mean episode length/episode: 6.96
            Mean episode successes: 2.3618
Mean episode consecutive_successes: 4.8439
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 8.47s
                        Total time: 22998.81s
                               ETA: 1027185.8s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.050s, learning 0.168s)
               Value function loss: 98771.9969
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 1204.91
               Mean episode length: 48.54
                  Mean reward/step: 25.56
       Mean episode length/episode: 6.99
            Mean episode successes: 2.4170
Mean episode consecutive_successes: 4.8212
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 8.22s
                        Total time: 23007.03s
                               ETA: 1027073.4s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.498s, learning 0.182s)
               Value function loss: 114746.8098
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 1260.09
               Mean episode length: 48.98
                  Mean reward/step: 26.02
       Mean episode length/episode: 6.99
            Mean episode successes: 2.3325
Mean episode consecutive_successes: 4.8661
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 8.68s
                        Total time: 23015.71s
                               ETA: 1026981.6s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.165s)
               Value function loss: 108559.6545
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 1497.75
               Mean episode length: 48.86
                  Mean reward/step: 24.57
       Mean episode length/episode: 6.97
            Mean episode successes: 2.2002
Mean episode consecutive_successes: 4.9153
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 8.64s
                        Total time: 23024.35s
                               ETA: 1026888.2s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.392s, learning 0.163s)
               Value function loss: 112297.8836
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 1163.07
               Mean episode length: 45.35
                  Mean reward/step: 24.62
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1929
Mean episode consecutive_successes: 4.8894
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 8.55s
                        Total time: 23032.91s
                               ETA: 1026791.0s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.837s, learning 0.161s)
               Value function loss: 108292.0064
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 1392.16
               Mean episode length: 48.10
                  Mean reward/step: 24.48
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1104
Mean episode consecutive_successes: 4.9123
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 8.00s
                        Total time: 23040.90s
                               ETA: 1026669.1s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.167s)
               Value function loss: 114550.5537
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 1041.16
               Mean episode length: 46.37
                  Mean reward/step: 26.40
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1855
Mean episode consecutive_successes: 4.8837
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 8.46s
                        Total time: 23049.37s
                               ETA: 1026568.0s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.098s, learning 0.195s)
               Value function loss: 108798.1531
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 1287.00
               Mean episode length: 48.55
                  Mean reward/step: 26.30
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3647
Mean episode consecutive_successes: 4.8626
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 8.29s
                        Total time: 23057.66s
                               ETA: 1026459.4s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.205s, learning 0.186s)
               Value function loss: 112076.4516
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 1288.71
               Mean episode length: 47.13
                  Mean reward/step: 26.16
       Mean episode length/episode: 6.97
            Mean episode successes: 2.2773
Mean episode consecutive_successes: 4.8788
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 8.39s
                        Total time: 23066.05s
                               ETA: 1026355.3s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.418s, learning 0.181s)
               Value function loss: 119507.4857
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 1084.22
               Mean episode length: 47.94
                  Mean reward/step: 28.13
       Mean episode length/episode: 7.11
            Mean episode successes: 2.4126
Mean episode consecutive_successes: 4.8992
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 8.60s
                        Total time: 23074.65s
                               ETA: 1026260.5s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.285s, learning 0.162s)
               Value function loss: 163725.0318
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 1289.91
               Mean episode length: 49.32
                  Mean reward/step: 25.60
       Mean episode length/episode: 6.94
            Mean episode successes: 2.4224
Mean episode consecutive_successes: 4.8747
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 8.45s
                        Total time: 23083.10s
                               ETA: 1026159.0s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.047s, learning 0.168s)
               Value function loss: 108751.7691
                    Surrogate loss: -0.0238
             Mean action noise std: 0.74
                       Mean reward: 1088.27
               Mean episode length: 50.33
                  Mean reward/step: 24.76
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3403
Mean episode consecutive_successes: 4.9092
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 8.21s
                        Total time: 23091.31s
                               ETA: 1026047.3s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.020s, learning 0.227s)
               Value function loss: 114730.4010
                    Surrogate loss: -0.0199
             Mean action noise std: 0.74
                       Mean reward: 792.66
               Mean episode length: 45.58
                  Mean reward/step: 23.27
       Mean episode length/episode: 6.94
            Mean episode successes: 2.1514
Mean episode consecutive_successes: 4.9224
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 8.25s
                        Total time: 23099.56s
                               ETA: 1025937.2s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.198s, learning 0.172s)
               Value function loss: 126552.1809
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 1770.11
               Mean episode length: 52.22
                  Mean reward/step: 23.58
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0737
Mean episode consecutive_successes: 4.9859
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 8.37s
                        Total time: 23107.93s
                               ETA: 1025832.6s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.108s, learning 0.162s)
               Value function loss: 110286.5049
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 959.77
               Mean episode length: 48.46
                  Mean reward/step: 24.04
       Mean episode length/episode: 6.96
            Mean episode successes: 1.9487
Mean episode consecutive_successes: 4.9569
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 8.27s
                        Total time: 23116.20s
                               ETA: 1025723.6s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.335s, learning 0.186s)
               Value function loss: 104775.2779
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 961.20
               Mean episode length: 50.45
                  Mean reward/step: 23.66
       Mean episode length/episode: 7.02
            Mean episode successes: 2.0918
Mean episode consecutive_successes: 4.8644
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 8.52s
                        Total time: 23124.72s
                               ETA: 1025625.9s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.201s, learning 0.169s)
               Value function loss: 111468.6100
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 1067.86
               Mean episode length: 49.49
                  Mean reward/step: 25.14
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0537
Mean episode consecutive_successes: 4.8721
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 8.37s
                        Total time: 23133.09s
                               ETA: 1025521.5s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.125s, learning 0.183s)
               Value function loss: 106189.5395
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 1531.28
               Mean episode length: 50.11
                  Mean reward/step: 25.47
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1318
Mean episode consecutive_successes: 4.8278
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 8.31s
                        Total time: 23141.40s
                               ETA: 1025414.5s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.386s, learning 0.216s)
               Value function loss: 113547.7299
                    Surrogate loss: -0.0210
             Mean action noise std: 0.74
                       Mean reward: 889.70
               Mean episode length: 44.64
                  Mean reward/step: 25.36
       Mean episode length/episode: 6.96
            Mean episode successes: 2.1929
Mean episode consecutive_successes: 4.7782
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 8.60s
                        Total time: 23150.00s
                               ETA: 1025320.6s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.328s, learning 0.209s)
               Value function loss: 143076.0992
                    Surrogate loss: -0.0250
             Mean action noise std: 0.74
                       Mean reward: 1331.26
               Mean episode length: 50.20
                  Mean reward/step: 24.95
       Mean episode length/episode: 6.95
            Mean episode successes: 2.3237
Mean episode consecutive_successes: 4.7311
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 8.54s
                        Total time: 23158.54s
                               ETA: 1025224.0s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.158s, learning 0.226s)
               Value function loss: 143381.4848
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 1150.70
               Mean episode length: 50.37
                  Mean reward/step: 26.95
       Mean episode length/episode: 6.98
            Mean episode successes: 2.4268
Mean episode consecutive_successes: 4.7208
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 8.38s
                        Total time: 23166.92s
                               ETA: 1025120.6s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.245s, learning 0.163s)
               Value function loss: 113008.2947
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 1095.39
               Mean episode length: 50.06
                  Mean reward/step: 25.67
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3804
Mean episode consecutive_successes: 4.7273
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 8.41s
                        Total time: 23175.33s
                               ETA: 1025018.3s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.221s, learning 0.161s)
               Value function loss: 107500.6963
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 854.98
               Mean episode length: 44.90
                  Mean reward/step: 25.09
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1914
Mean episode consecutive_successes: 4.7931
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 8.38s
                        Total time: 23183.71s
                               ETA: 1024915.0s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.089s, learning 0.164s)
               Value function loss: 114105.8914
                    Surrogate loss: -0.0206
             Mean action noise std: 0.74
                       Mean reward: 1179.08
               Mean episode length: 47.53
                  Mean reward/step: 25.54
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2046
Mean episode consecutive_successes: 4.8199
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 8.25s
                        Total time: 23191.96s
                               ETA: 1024806.0s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.227s, learning 0.205s)
               Value function loss: 136869.6773
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 1154.57
               Mean episode length: 48.17
                  Mean reward/step: 23.63
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2754
Mean episode consecutive_successes: 4.7607
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 8.43s
                        Total time: 23200.40s
                               ETA: 1024705.1s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.921s, learning 0.165s)
               Value function loss: 119133.7332
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 1216.71
               Mean episode length: 47.48
                  Mean reward/step: 25.54
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3364
Mean episode consecutive_successes: 4.7556
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 8.09s
                        Total time: 23208.48s
                               ETA: 1024589.0s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.083s, learning 0.198s)
               Value function loss: 116712.0654
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 1508.46
               Mean episode length: 50.38
                  Mean reward/step: 26.51
       Mean episode length/episode: 6.91
            Mean episode successes: 2.2969
Mean episode consecutive_successes: 4.7729
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 8.28s
                        Total time: 23216.76s
                               ETA: 1024481.6s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.981s, learning 0.206s)
               Value function loss: 108351.7871
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 1298.49
               Mean episode length: 50.22
                  Mean reward/step: 23.68
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3882
Mean episode consecutive_successes: 4.7432
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 8.19s
                        Total time: 23224.95s
                               ETA: 1024370.1s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.482s, learning 0.158s)
               Value function loss: 104015.5549
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 1162.66
               Mean episode length: 49.36
                  Mean reward/step: 23.00
       Mean episode length/episode: 6.95
            Mean episode successes: 2.1792
Mean episode consecutive_successes: 4.7937
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 8.64s
                        Total time: 23233.59s
                               ETA: 1024278.7s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.520s, learning 0.170s)
               Value function loss: 94992.2887
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 951.65
               Mean episode length: 47.29
                  Mean reward/step: 22.31
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1470
Mean episode consecutive_successes: 4.7746
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 8.69s
                        Total time: 23242.28s
                               ETA: 1024189.5s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.429s, learning 0.159s)
               Value function loss: 99657.5242
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 1011.82
               Mean episode length: 47.81
                  Mean reward/step: 23.40
       Mean episode length/episode: 6.90
            Mean episode successes: 2.1494
Mean episode consecutive_successes: 4.7239
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 8.59s
                        Total time: 23250.87s
                               ETA: 1024096.0s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.076s, learning 0.190s)
               Value function loss: 109437.8926
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 1172.80
               Mean episode length: 49.35
                  Mean reward/step: 23.58
       Mean episode length/episode: 7.07
            Mean episode successes: 2.1875
Mean episode consecutive_successes: 4.7311
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 8.27s
                        Total time: 23259.13s
                               ETA: 1023988.4s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.145s, learning 0.159s)
               Value function loss: 141752.3480
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 1279.68
               Mean episode length: 52.06
                  Mean reward/step: 22.99
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1260
Mean episode consecutive_successes: 4.7154
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 8.30s
                        Total time: 23267.44s
                               ETA: 1023882.4s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.488s, learning 0.215s)
               Value function loss: 97406.0105
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 1020.62
               Mean episode length: 45.73
                  Mean reward/step: 22.61
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1807
Mean episode consecutive_successes: 4.6874
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 8.70s
                        Total time: 23276.14s
                               ETA: 1023794.2s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.272s, learning 0.166s)
               Value function loss: 129082.9816
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 1012.82
               Mean episode length: 49.17
                  Mean reward/step: 23.71
       Mean episode length/episode: 6.93
            Mean episode successes: 2.1538
Mean episode consecutive_successes: 4.6796
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 8.44s
                        Total time: 23284.58s
                               ETA: 1023694.3s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.144s, learning 0.164s)
               Value function loss: 111377.6094
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 1348.97
               Mean episode length: 50.78
                  Mean reward/step: 22.75
       Mean episode length/episode: 6.88
            Mean episode successes: 2.0840
Mean episode consecutive_successes: 4.6756
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 8.31s
                        Total time: 23292.89s
                               ETA: 1023588.9s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.100s, learning 0.173s)
               Value function loss: 106476.1914
                    Surrogate loss: -0.0231
             Mean action noise std: 0.74
                       Mean reward: 1060.56
               Mean episode length: 45.79
                  Mean reward/step: 21.92
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9819
Mean episode consecutive_successes: 4.6510
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 8.27s
                        Total time: 23301.16s
                               ETA: 1023482.0s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.123s, learning 0.182s)
               Value function loss: 123597.4672
                    Surrogate loss: -0.0251
             Mean action noise std: 0.74
                       Mean reward: 1233.91
               Mean episode length: 51.08
                  Mean reward/step: 22.55
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1060
Mean episode consecutive_successes: 4.5719
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 8.31s
                        Total time: 23309.47s
                               ETA: 1023376.6s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.948s, learning 0.164s)
               Value function loss: 135267.0461
                    Surrogate loss: -0.0223
             Mean action noise std: 0.74
                       Mean reward: 926.93
               Mean episode length: 48.03
                  Mean reward/step: 22.16
       Mean episode length/episode: 6.93
            Mean episode successes: 2.0454
Mean episode consecutive_successes: 4.5548
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 8.11s
                        Total time: 23317.58s
                               ETA: 1023262.8s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.182s, learning 0.236s)
               Value function loss: 88590.2488
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 1214.02
               Mean episode length: 47.13
                  Mean reward/step: 22.84
       Mean episode length/episode: 6.87
            Mean episode successes: 1.9849
Mean episode consecutive_successes: 4.5159
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 8.42s
                        Total time: 23326.00s
                               ETA: 1023162.6s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.351s, learning 0.161s)
               Value function loss: 81088.2867
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 862.82
               Mean episode length: 45.27
                  Mean reward/step: 21.87
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9399
Mean episode consecutive_successes: 4.5084
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 8.51s
                        Total time: 23334.51s
                               ETA: 1023066.5s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.330s, learning 0.162s)
               Value function loss: 81546.7012
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 979.08
               Mean episode length: 47.36
                  Mean reward/step: 20.92
       Mean episode length/episode: 6.90
            Mean episode successes: 2.0366
Mean episode consecutive_successes: 4.4215
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 8.49s
                        Total time: 23343.00s
                               ETA: 1022969.6s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.380s, learning 0.232s)
               Value function loss: 83833.1377
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 1163.11
               Mean episode length: 46.04
                  Mean reward/step: 20.46
       Mean episode length/episode: 7.02
            Mean episode successes: 2.1294
Mean episode consecutive_successes: 4.3375
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 8.61s
                        Total time: 23351.61s
                               ETA: 1022878.1s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.115s, learning 0.161s)
               Value function loss: 73288.1872
                    Surrogate loss: -0.0223
             Mean action noise std: 0.74
                       Mean reward: 1099.76
               Mean episode length: 48.67
                  Mean reward/step: 20.11
       Mean episode length/episode: 6.90
            Mean episode successes: 2.0703
Mean episode consecutive_successes: 4.3176
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 8.28s
                        Total time: 23359.89s
                               ETA: 1022771.9s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.040s, learning 0.202s)
               Value function loss: 78189.3281
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 927.36
               Mean episode length: 49.05
                  Mean reward/step: 19.51
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9653
Mean episode consecutive_successes: 4.3076
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 8.24s
                        Total time: 23368.13s
                               ETA: 1022664.3s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.012s, learning 0.157s)
               Value function loss: 73040.8432
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 635.11
               Mean episode length: 45.79
                  Mean reward/step: 18.55
       Mean episode length/episode: 6.98
            Mean episode successes: 1.8853
Mean episode consecutive_successes: 4.2543
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 8.17s
                        Total time: 23376.30s
                               ETA: 1022553.6s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.264s, learning 0.249s)
               Value function loss: 68441.3466
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 1094.43
               Mean episode length: 48.41
                  Mean reward/step: 21.23
       Mean episode length/episode: 6.96
            Mean episode successes: 2.0366
Mean episode consecutive_successes: 4.1840
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 8.51s
                        Total time: 23384.81s
                               ETA: 1022458.1s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.275s, learning 0.257s)
               Value function loss: 84246.9758
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 1020.37
               Mean episode length: 45.69
                  Mean reward/step: 23.02
       Mean episode length/episode: 7.01
            Mean episode successes: 2.1182
Mean episode consecutive_successes: 4.1615
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 8.53s
                        Total time: 23393.35s
                               ETA: 1022363.4s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.379s, learning 0.251s)
               Value function loss: 88514.4539
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 1089.41
               Mean episode length: 48.26
                  Mean reward/step: 22.44
       Mean episode length/episode: 6.94
            Mean episode successes: 2.1548
Mean episode consecutive_successes: 4.1638
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 8.63s
                        Total time: 23401.98s
                               ETA: 1022273.2s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.220s, learning 0.171s)
               Value function loss: 84408.6213
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 1278.61
               Mean episode length: 46.83
                  Mean reward/step: 23.13
       Mean episode length/episode: 6.98
            Mean episode successes: 2.1904
Mean episode consecutive_successes: 4.1511
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 8.39s
                        Total time: 23410.37s
                               ETA: 1022172.5s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.395s, learning 0.166s)
               Value function loss: 84265.7539
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 1164.25
               Mean episode length: 47.74
                  Mean reward/step: 22.25
       Mean episode length/episode: 7.02
            Mean episode successes: 2.1748
Mean episode consecutive_successes: 4.1585
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 8.56s
                        Total time: 23418.93s
                               ETA: 1022079.4s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.010s, learning 0.192s)
               Value function loss: 106842.1584
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 1145.72
               Mean episode length: 46.28
                  Mean reward/step: 23.07
       Mean episode length/episode: 6.94
            Mean episode successes: 2.1978
Mean episode consecutive_successes: 4.1492
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 8.20s
                        Total time: 23427.13s
                               ETA: 1021970.7s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.417s, learning 0.189s)
               Value function loss: 82756.5980
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 1036.80
               Mean episode length: 47.70
                  Mean reward/step: 22.96
       Mean episode length/episode: 6.92
            Mean episode successes: 2.2129
Mean episode consecutive_successes: 4.1600
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 8.61s
                        Total time: 23435.74s
                               ETA: 1021879.7s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.410s, learning 0.189s)
               Value function loss: 89001.9270
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 1053.05
               Mean episode length: 49.92
                  Mean reward/step: 23.04
       Mean episode length/episode: 7.01
            Mean episode successes: 2.1538
Mean episode consecutive_successes: 4.1861
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 8.60s
                        Total time: 23444.34s
                               ETA: 1021788.4s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.269s, learning 0.180s)
               Value function loss: 105846.4133
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 1020.00
               Mean episode length: 48.92
                  Mean reward/step: 22.74
       Mean episode length/episode: 7.08
            Mean episode successes: 2.1343
Mean episode consecutive_successes: 4.2593
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 8.45s
                        Total time: 23452.79s
                               ETA: 1021690.7s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.979s, learning 0.166s)
               Value function loss: 117163.6617
                    Surrogate loss: 0.0314
             Mean action noise std: 0.74
                       Mean reward: 597.13
               Mean episode length: 44.90
                  Mean reward/step: 24.25
       Mean episode length/episode: 6.98
            Mean episode successes: 2.2197
Mean episode consecutive_successes: 4.2235
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 8.15s
                        Total time: 23460.93s
                               ETA: 1021579.8s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.624s, learning 0.171s)
               Value function loss: 96587.3668
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 1166.81
               Mean episode length: 47.57
                  Mean reward/step: 26.91
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3369
Mean episode consecutive_successes: 4.2621
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 8.79s
                        Total time: 23469.72s
                               ETA: 1021497.3s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.024s, learning 0.166s)
               Value function loss: 98437.1098
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 1233.67
               Mean episode length: 50.83
                  Mean reward/step: 27.48
       Mean episode length/episode: 7.02
            Mean episode successes: 2.5464
Mean episode consecutive_successes: 4.2491
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 8.19s
                        Total time: 23477.92s
                               ETA: 1021388.6s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.972s, learning 0.190s)
               Value function loss: 130117.3730
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 1553.48
               Mean episode length: 51.00
                  Mean reward/step: 27.52
       Mean episode length/episode: 7.06
            Mean episode successes: 2.5352
Mean episode consecutive_successes: 4.3542
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 8.16s
                        Total time: 23486.08s
                               ETA: 1021278.7s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.164s)
               Value function loss: 203853.9000
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 1213.99
               Mean episode length: 51.29
                  Mean reward/step: 27.19
       Mean episode length/episode: 6.97
            Mean episode successes: 2.5796
Mean episode consecutive_successes: 4.4001
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 8.46s
                        Total time: 23494.54s
                               ETA: 1021181.8s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.150s, learning 0.164s)
               Value function loss: 142505.7109
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 1292.29
               Mean episode length: 48.60
                  Mean reward/step: 26.03
       Mean episode length/episode: 6.96
            Mean episode successes: 2.5020
Mean episode consecutive_successes: 4.4476
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 8.31s
                        Total time: 23502.85s
                               ETA: 1021078.7s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.180s, learning 0.190s)
               Value function loss: 99768.0809
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 839.40
               Mean episode length: 47.75
                  Mean reward/step: 21.81
       Mean episode length/episode: 6.90
            Mean episode successes: 2.2808
Mean episode consecutive_successes: 4.4917
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 8.37s
                        Total time: 23511.22s
                               ETA: 1020978.1s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.162s)
               Value function loss: 90205.7273
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 1396.97
               Mean episode length: 47.82
                  Mean reward/step: 21.35
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2573
Mean episode consecutive_successes: 4.5142
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 8.36s
                        Total time: 23519.58s
                               ETA: 1020877.3s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.841s, learning 0.162s)
               Value function loss: 87510.5957
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 940.97
               Mean episode length: 46.38
                  Mean reward/step: 23.16
       Mean episode length/episode: 7.07
            Mean episode successes: 2.1528
Mean episode consecutive_successes: 4.5279
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 8.00s
                        Total time: 23527.59s
                               ETA: 1020761.0s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.034s, learning 0.159s)
               Value function loss: 96614.8354
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 1082.41
               Mean episode length: 49.04
                  Mean reward/step: 24.80
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2803
Mean episode consecutive_successes: 4.4999
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 8.19s
                        Total time: 23535.78s
                               ETA: 1020653.0s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.696s, learning 0.163s)
               Value function loss: 98682.5658
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 1297.28
               Mean episode length: 49.71
                  Mean reward/step: 25.46
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3706
Mean episode consecutive_successes: 4.5023
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 7.86s
                        Total time: 23543.64s
                               ETA: 1020530.6s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.283s, learning 0.160s)
               Value function loss: 114916.7244
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 1078.45
               Mean episode length: 46.97
                  Mean reward/step: 25.78
       Mean episode length/episode: 6.96
            Mean episode successes: 2.5332
Mean episode consecutive_successes: 4.4528
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 8.44s
                        Total time: 23552.08s
                               ETA: 1020433.6s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 2111 steps/s (collection: 7.599s, learning 0.160s)
               Value function loss: 123913.5250
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 1234.75
               Mean episode length: 49.07
                  Mean reward/step: 25.77
       Mean episode length/episode: 7.00
            Mean episode successes: 2.5020
Mean episode consecutive_successes: 4.4939
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 7.76s
                        Total time: 23559.84s
                               ETA: 1020307.1s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.597s, learning 0.176s)
               Value function loss: 86893.8732
                    Surrogate loss: -0.0235
             Mean action noise std: 0.74
                       Mean reward: 1587.74
               Mean episode length: 49.10
                  Mean reward/step: 24.83
       Mean episode length/episode: 7.06
            Mean episode successes: 2.5273
Mean episode consecutive_successes: 4.5424
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 8.77s
                        Total time: 23568.61s
                               ETA: 1020224.6s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.341s, learning 0.166s)
               Value function loss: 86584.1182
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 1416.64
               Mean episode length: 47.31
                  Mean reward/step: 26.04
       Mean episode length/episode: 6.98
            Mean episode successes: 2.5063
Mean episode consecutive_successes: 4.6052
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 8.51s
                        Total time: 23577.12s
                               ETA: 1020130.6s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.084s, learning 0.178s)
               Value function loss: 95237.8424
                    Surrogate loss: -0.0220
             Mean action noise std: 0.74
                       Mean reward: 999.21
               Mean episode length: 48.23
                  Mean reward/step: 25.25
       Mean episode length/episode: 7.04
            Mean episode successes: 2.5161
Mean episode consecutive_successes: 4.5878
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 8.26s
                        Total time: 23585.38s
                               ETA: 1020026.0s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.255s, learning 0.164s)
               Value function loss: 84184.7958
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: 1140.28
               Mean episode length: 48.74
                  Mean reward/step: 24.95
       Mean episode length/episode: 6.93
            Mean episode successes: 2.3794
Mean episode consecutive_successes: 4.6251
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 8.42s
                        Total time: 23593.80s
                               ETA: 1019928.4s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.136s, learning 0.183s)
               Value function loss: 85832.6170
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 1425.23
               Mean episode length: 49.32
                  Mean reward/step: 25.79
       Mean episode length/episode: 6.98
            Mean episode successes: 2.5464
Mean episode consecutive_successes: 4.6103
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 8.32s
                        Total time: 23602.12s
                               ETA: 1019826.6s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.012s, learning 0.168s)
               Value function loss: 95301.6600
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 1202.95
               Mean episode length: 49.96
                  Mean reward/step: 26.62
       Mean episode length/episode: 7.08
            Mean episode successes: 2.5771
Mean episode consecutive_successes: 4.6521
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 8.18s
                        Total time: 23610.30s
                               ETA: 1019718.8s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.001s, learning 0.159s)
               Value function loss: 102261.3566
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 1335.58
               Mean episode length: 50.07
                  Mean reward/step: 26.94
       Mean episode length/episode: 6.99
            Mean episode successes: 2.5493
Mean episode consecutive_successes: 4.7257
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 8.16s
                        Total time: 23618.46s
                               ETA: 1019610.2s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.022s, learning 0.163s)
               Value function loss: 98824.4240
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 1019.83
               Mean episode length: 45.05
                  Mean reward/step: 26.03
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4741
Mean episode consecutive_successes: 4.7730
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 8.18s
                        Total time: 23626.65s
                               ETA: 1019502.8s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.139s, learning 0.164s)
               Value function loss: 97132.9047
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 961.23
               Mean episode length: 48.12
                  Mean reward/step: 28.67
       Mean episode length/episode: 7.09
            Mean episode successes: 2.5156
Mean episode consecutive_successes: 4.8454
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 8.30s
                        Total time: 23634.95s
                               ETA: 1019400.6s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.293s, learning 0.249s)
               Value function loss: 110340.6811
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 1545.81
               Mean episode length: 50.24
                  Mean reward/step: 29.04
       Mean episode length/episode: 7.06
            Mean episode successes: 2.5415
Mean episode consecutive_successes: 4.9418
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 8.54s
                        Total time: 23643.49s
                               ETA: 1019308.7s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.078s, learning 0.287s)
               Value function loss: 113222.9637
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 1466.37
               Mean episode length: 50.75
                  Mean reward/step: 28.20
       Mean episode length/episode: 7.04
            Mean episode successes: 2.6943
Mean episode consecutive_successes: 4.9516
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 8.37s
                        Total time: 23651.86s
                               ETA: 1019209.4s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.338s, learning 0.167s)
               Value function loss: 118322.0117
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 1141.91
               Mean episode length: 48.21
                  Mean reward/step: 29.40
       Mean episode length/episode: 7.03
            Mean episode successes: 2.8257
Mean episode consecutive_successes: 4.9113
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 8.50s
                        Total time: 23660.36s
                               ETA: 1019116.1s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.024s, learning 0.162s)
               Value function loss: 122003.5465
                    Surrogate loss: -0.0229
             Mean action noise std: 0.74
                       Mean reward: 1701.75
               Mean episode length: 52.00
                  Mean reward/step: 27.78
       Mean episode length/episode: 6.95
            Mean episode successes: 2.5913
Mean episode consecutive_successes: 5.0435
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 8.19s
                        Total time: 23668.55s
                               ETA: 1019009.1s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.412s, learning 0.164s)
               Value function loss: 158615.0199
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 1334.60
               Mean episode length: 48.58
                  Mean reward/step: 25.42
       Mean episode length/episode: 6.99
            Mean episode successes: 2.5107
Mean episode consecutive_successes: 5.0692
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 8.58s
                        Total time: 23677.12s
                               ETA: 1018919.1s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.240s, learning 0.162s)
               Value function loss: 96603.2338
                    Surrogate loss: -0.0236
             Mean action noise std: 0.74
                       Mean reward: 1169.26
               Mean episode length: 47.70
                  Mean reward/step: 24.95
       Mean episode length/episode: 7.08
            Mean episode successes: 2.4141
Mean episode consecutive_successes: 5.0701
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 8.40s
                        Total time: 23685.53s
                               ETA: 1018821.6s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.882s, learning 0.168s)
               Value function loss: 103573.3453
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 1509.60
               Mean episode length: 48.53
                  Mean reward/step: 25.08
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3950
Mean episode consecutive_successes: 5.1131
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 8.05s
                        Total time: 23693.57s
                               ETA: 1018709.1s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.643s, learning 0.189s)
               Value function loss: 94205.2225
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 1387.95
               Mean episode length: 49.93
                  Mean reward/step: 24.61
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1758
Mean episode consecutive_successes: 5.2124
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 8.83s
                        Total time: 23702.41s
                               ETA: 1018630.2s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.062s, learning 0.166s)
               Value function loss: 100564.8574
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 1196.81
               Mean episode length: 48.40
                  Mean reward/step: 26.16
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2207
Mean episode consecutive_successes: 5.1675
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 8.23s
                        Total time: 23710.63s
                               ETA: 1018525.5s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.033s, learning 0.163s)
               Value function loss: 95018.4275
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 1100.62
               Mean episode length: 50.81
                  Mean reward/step: 26.81
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3022
Mean episode consecutive_successes: 5.1599
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 8.20s
                        Total time: 23718.83s
                               ETA: 1018419.5s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.162s)
               Value function loss: 97450.1723
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 1179.20
               Mean episode length: 47.63
                  Mean reward/step: 25.94
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2505
Mean episode consecutive_successes: 5.1912
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 8.13s
                        Total time: 23726.96s
                               ETA: 1018310.5s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.235s, learning 0.188s)
               Value function loss: 109718.3646
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 918.30
               Mean episode length: 50.55
                  Mean reward/step: 29.83
       Mean episode length/episode: 6.95
            Mean episode successes: 2.4551
Mean episode consecutive_successes: 5.1183
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 8.42s
                        Total time: 23735.38s
                               ETA: 1018214.4s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.795s, learning 0.168s)
               Value function loss: 103201.9096
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 1084.68
               Mean episode length: 48.84
                  Mean reward/step: 30.82
       Mean episode length/episode: 6.99
            Mean episode successes: 2.6904
Mean episode consecutive_successes: 5.0936
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 7.96s
                        Total time: 23743.34s
                               ETA: 1018098.7s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.222s, learning 0.161s)
               Value function loss: 108538.0193
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 1771.62
               Mean episode length: 48.00
                  Mean reward/step: 29.26
       Mean episode length/episode: 6.98
            Mean episode successes: 2.5371
Mean episode consecutive_successes: 5.2045
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 8.38s
                        Total time: 23751.73s
                               ETA: 1018001.0s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.577s, learning 0.166s)
               Value function loss: 110679.0023
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 1251.42
               Mean episode length: 52.04
                  Mean reward/step: 28.25
       Mean episode length/episode: 6.98
            Mean episode successes: 2.4702
Mean episode consecutive_successes: 5.2412
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 8.74s
                        Total time: 23760.47s
                               ETA: 1017918.9s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.467s, learning 0.159s)
               Value function loss: 109020.0184
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 1334.53
               Mean episode length: 48.20
                  Mean reward/step: 27.67
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4009
Mean episode consecutive_successes: 5.2745
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 8.63s
                        Total time: 23769.09s
                               ETA: 1017831.8s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.061s, learning 0.208s)
               Value function loss: 106559.9195
                    Surrogate loss: 0.0044
             Mean action noise std: 0.74
                       Mean reward: 1262.33
               Mean episode length: 48.66
                  Mean reward/step: 28.91
       Mean episode length/episode: 6.97
            Mean episode successes: 2.4932
Mean episode consecutive_successes: 5.2624
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 8.27s
                        Total time: 23777.36s
                               ETA: 1017729.5s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.347s, learning 0.157s)
               Value function loss: 110919.3225
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 1065.83
               Mean episode length: 50.36
                  Mean reward/step: 26.38
       Mean episode length/episode: 6.98
            Mean episode successes: 2.4307
Mean episode consecutive_successes: 5.2574
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 8.50s
                        Total time: 23785.87s
                               ETA: 1017637.3s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.078s, learning 0.164s)
               Value function loss: 105952.6674
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 1300.06
               Mean episode length: 49.79
                  Mean reward/step: 27.38
       Mean episode length/episode: 7.10
            Mean episode successes: 2.5303
Mean episode consecutive_successes: 5.2768
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 8.24s
                        Total time: 23794.11s
                               ETA: 1017534.0s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1048 steps/s (collection: 15.463s, learning 0.168s)
               Value function loss: 106532.4424
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 1487.07
               Mean episode length: 52.62
                  Mean reward/step: 26.67
       Mean episode length/episode: 7.00
            Mean episode successes: 2.4282
Mean episode consecutive_successes: 5.2885
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 15.63s
                        Total time: 23809.74s
                               ETA: 1017746.6s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.681s, learning 0.188s)
               Value function loss: 121874.1801
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 1582.41
               Mean episode length: 49.10
                  Mean reward/step: 27.86
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3813
Mean episode consecutive_successes: 5.3639
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 15.87s
                        Total time: 23825.61s
                               ETA: 1017969.2s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.030s, learning 0.170s)
               Value function loss: 130792.6533
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 1429.24
               Mean episode length: 51.88
                  Mean reward/step: 27.84
       Mean episode length/episode: 7.05
            Mean episode successes: 2.4155
Mean episode consecutive_successes: 5.3592
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 16.20s
                        Total time: 23841.81s
                               ETA: 1018205.7s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.618s, learning 0.215s)
               Value function loss: 114654.1885
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 1150.58
               Mean episode length: 50.09
                  Mean reward/step: 26.79
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3428
Mean episode consecutive_successes: 5.3683
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 16.83s
                        Total time: 23858.64s
                               ETA: 1018469.0s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.641s, learning 0.156s)
               Value function loss: 115147.9273
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 1700.38
               Mean episode length: 53.76
                  Mean reward/step: 25.58
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3618
Mean episode consecutive_successes: 5.3717
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 15.80s
                        Total time: 23874.44s
                               ETA: 1018687.9s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.125s, learning 0.160s)
               Value function loss: 115373.4338
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 1228.13
               Mean episode length: 50.89
                  Mean reward/step: 24.15
       Mean episode length/episode: 7.02
            Mean episode successes: 2.2769
Mean episode consecutive_successes: 5.3324
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 16.29s
                        Total time: 23890.72s
                               ETA: 1018927.4s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.513s, learning 0.183s)
               Value function loss: 109523.8166
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 1366.93
               Mean episode length: 48.62
                  Mean reward/step: 24.34
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1719
Mean episode consecutive_successes: 5.3915
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 15.70s
                        Total time: 23906.42s
                               ETA: 1019141.5s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.033s, learning 0.169s)
               Value function loss: 115224.1373
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 1257.14
               Mean episode length: 49.33
                  Mean reward/step: 25.26
       Mean episode length/episode: 7.01
            Mean episode successes: 2.1074
Mean episode consecutive_successes: 5.3614
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 16.20s
                        Total time: 23922.62s
                               ETA: 1019377.0s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.548s, learning 0.223s)
               Value function loss: 118970.7744
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 1301.60
               Mean episode length: 47.86
                  Mean reward/step: 26.99
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2061
Mean episode consecutive_successes: 5.3228
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 16.77s
                        Total time: 23939.39s
                               ETA: 1019636.6s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.313s, learning 0.162s)
               Value function loss: 146010.7488
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 1384.59
               Mean episode length: 52.25
                  Mean reward/step: 26.69
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3306
Mean episode consecutive_successes: 5.2692
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 16.48s
                        Total time: 23955.87s
                               ETA: 1019883.2s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.387s, learning 0.159s)
               Value function loss: 160621.6445
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: 1305.67
               Mean episode length: 54.12
                  Mean reward/step: 26.52
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3970
Mean episode consecutive_successes: 5.2378
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 16.55s
                        Total time: 23972.41s
                               ETA: 1020132.7s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.194s, learning 0.163s)
               Value function loss: 125636.6596
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 1227.11
               Mean episode length: 52.34
                  Mean reward/step: 27.11
       Mean episode length/episode: 7.02
            Mean episode successes: 2.2803
Mean episode consecutive_successes: 5.2820
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 16.36s
                        Total time: 23988.77s
                               ETA: 1020373.9s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.811s, learning 0.163s)
               Value function loss: 111948.7588
                    Surrogate loss: -0.0219
             Mean action noise std: 0.74
                       Mean reward: 1659.28
               Mean episode length: 51.86
                  Mean reward/step: 27.48
       Mean episode length/episode: 7.04
            Mean episode successes: 2.3364
Mean episode consecutive_successes: 5.3042
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 15.97s
                        Total time: 24004.75s
                               ETA: 1020598.6s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.396s, learning 0.168s)
               Value function loss: 110684.1656
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 1282.54
               Mean episode length: 49.07
                  Mean reward/step: 27.64
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2598
Mean episode consecutive_successes: 5.3463
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 16.56s
                        Total time: 24021.31s
                               ETA: 1020848.2s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.917s, learning 0.207s)
               Value function loss: 117171.3211
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 1388.20
               Mean episode length: 50.21
                  Mean reward/step: 26.43
       Mean episode length/episode: 7.05
            Mean episode successes: 2.3481
Mean episode consecutive_successes: 5.3239
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 16.12s
                        Total time: 24037.43s
                               ETA: 1021078.8s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.646s, learning 0.169s)
               Value function loss: 149663.8840
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 820.04
               Mean episode length: 49.70
                  Mean reward/step: 27.58
       Mean episode length/episode: 6.93
            Mean episode successes: 2.2163
Mean episode consecutive_successes: 5.3329
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 15.82s
                        Total time: 24053.25s
                               ETA: 1021296.1s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.127s, learning 0.187s)
               Value function loss: 138474.6129
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 1848.93
               Mean episode length: 50.61
                  Mean reward/step: 28.86
       Mean episode length/episode: 7.08
            Mean episode successes: 2.3838
Mean episode consecutive_successes: 5.3096
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 16.31s
                        Total time: 24069.56s
                               ETA: 1021534.4s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.625s, learning 0.173s)
               Value function loss: 154972.0199
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 1314.30
               Mean episode length: 50.67
                  Mean reward/step: 27.83
       Mean episode length/episode: 7.04
            Mean episode successes: 2.3667
Mean episode consecutive_successes: 5.3300
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 16.80s
                        Total time: 24086.36s
                               ETA: 1021793.0s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.828s, learning 0.166s)
               Value function loss: 122768.9613
                    Surrogate loss: 0.0372
             Mean action noise std: 0.74
                       Mean reward: 1187.97
               Mean episode length: 49.39
                  Mean reward/step: 27.82
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3501
Mean episode consecutive_successes: 5.3631
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 15.99s
                        Total time: 24102.35s
                               ETA: 1022017.2s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.918s, learning 0.233s)
               Value function loss: 109924.7521
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 1152.55
               Mean episode length: 49.44
                  Mean reward/step: 28.15
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4121
Mean episode consecutive_successes: 5.3495
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 16.15s
                        Total time: 24118.50s
                               ETA: 1022247.9s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.553s, learning 0.162s)
               Value function loss: 112964.7711
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 1017.46
               Mean episode length: 49.72
                  Mean reward/step: 27.94
       Mean episode length/episode: 6.94
            Mean episode successes: 2.4087
Mean episode consecutive_successes: 5.3019
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 15.71s
                        Total time: 24134.22s
                               ETA: 1022459.9s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.286s, learning 0.162s)
               Value function loss: 110029.9115
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 1418.24
               Mean episode length: 50.36
                  Mean reward/step: 25.22
       Mean episode length/episode: 7.05
            Mean episode successes: 2.4951
Mean episode consecutive_successes: 5.2762
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 16.45s
                        Total time: 24150.67s
                               ETA: 1022702.8s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.401s, learning 0.159s)
               Value function loss: 109409.9693
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 1444.55
               Mean episode length: 48.52
                  Mean reward/step: 23.79
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2725
Mean episode consecutive_successes: 5.3137
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 16.56s
                        Total time: 24167.23s
                               ETA: 1022950.2s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.329s, learning 0.158s)
               Value function loss: 111057.1002
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 1503.14
               Mean episode length: 49.78
                  Mean reward/step: 23.09
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1943
Mean episode consecutive_successes: 5.3024
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 16.49s
                        Total time: 24183.72s
                               ETA: 1023194.3s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.809s, learning 0.167s)
               Value function loss: 105678.9111
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 1056.01
               Mean episode length: 46.87
                  Mean reward/step: 22.01
       Mean episode length/episode: 6.89
            Mean episode successes: 1.8501
Mean episode consecutive_successes: 5.2983
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 15.98s
                        Total time: 24199.69s
                               ETA: 1023416.5s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.010s, learning 0.162s)
               Value function loss: 103594.8486
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 1117.28
               Mean episode length: 48.82
                  Mean reward/step: 21.30
       Mean episode length/episode: 7.02
            Mean episode successes: 1.7549
Mean episode consecutive_successes: 5.2742
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 16.17s
                        Total time: 24215.86s
                               ETA: 1023646.8s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.152s, learning 0.165s)
               Value function loss: 100081.6133
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 605.53
               Mean episode length: 46.62
                  Mean reward/step: 23.04
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9561
Mean episode consecutive_successes: 5.1171
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 16.32s
                        Total time: 24232.18s
                               ETA: 1023883.0s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.014s, learning 0.185s)
               Value function loss: 107295.9213
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 1021.25
               Mean episode length: 46.93
                  Mean reward/step: 23.97
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0078
Mean episode consecutive_successes: 5.0649
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 16.20s
                        Total time: 24248.38s
                               ETA: 1024114.0s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.132s, learning 0.161s)
               Value function loss: 115255.5816
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 880.42
               Mean episode length: 46.26
                  Mean reward/step: 25.95
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1572
Mean episode consecutive_successes: 4.9875
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 16.29s
                        Total time: 24264.67s
                               ETA: 1024348.7s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.722s, learning 0.191s)
               Value function loss: 161006.1969
                    Surrogate loss: 0.0238
             Mean action noise std: 0.73
                       Mean reward: 988.23
               Mean episode length: 49.92
                  Mean reward/step: 25.99
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0718
Mean episode consecutive_successes: 4.9934
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 15.91s
                        Total time: 24280.58s
                               ETA: 1024567.2s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.400s, learning 0.167s)
               Value function loss: 132255.0645
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 1346.06
               Mean episode length: 51.16
                  Mean reward/step: 26.30
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1230
Mean episode consecutive_successes: 5.0137
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 16.57s
                        Total time: 24297.15s
                               ETA: 1024813.1s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.080s, learning 0.156s)
               Value function loss: 118160.0869
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 1302.23
               Mean episode length: 48.16
                  Mean reward/step: 28.39
       Mean episode length/episode: 7.07
            Mean episode successes: 2.3740
Mean episode consecutive_successes: 4.9702
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 16.24s
                        Total time: 24313.39s
                               ETA: 1025044.9s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.140s, learning 0.170s)
               Value function loss: 132587.5973
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 1350.24
               Mean episode length: 49.10
                  Mean reward/step: 28.77
       Mean episode length/episode: 6.96
            Mean episode successes: 2.3398
Mean episode consecutive_successes: 5.0112
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 16.31s
                        Total time: 24329.70s
                               ETA: 1025279.5s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.821s, learning 0.172s)
               Value function loss: 115109.3639
                    Surrogate loss: 0.0058
             Mean action noise std: 0.73
                       Mean reward: 1840.24
               Mean episode length: 53.42
                  Mean reward/step: 26.39
       Mean episode length/episode: 6.95
            Mean episode successes: 2.2266
Mean episode consecutive_successes: 5.0709
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 15.99s
                        Total time: 24345.69s
                               ETA: 1025500.5s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.447s, learning 0.187s)
               Value function loss: 106436.5389
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 1023.85
               Mean episode length: 47.23
                  Mean reward/step: 25.88
       Mean episode length/episode: 6.90
            Mean episode successes: 2.0186
Mean episode consecutive_successes: 5.0946
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 16.63s
                        Total time: 24362.32s
                               ETA: 1025748.4s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.650s, learning 0.158s)
               Value function loss: 110118.0547
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 1201.59
               Mean episode length: 47.89
                  Mean reward/step: 27.05
       Mean episode length/episode: 7.10
            Mean episode successes: 2.2363
Mean episode consecutive_successes: 5.0520
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 15.81s
                        Total time: 24378.13s
                               ETA: 1025961.2s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.971s, learning 0.161s)
               Value function loss: 117367.8193
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 1370.01
               Mean episode length: 52.95
                  Mean reward/step: 28.86
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3540
Mean episode consecutive_successes: 5.0113
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 16.13s
                        Total time: 24394.26s
                               ETA: 1026187.5s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1120 steps/s (collection: 14.393s, learning 0.233s)
               Value function loss: 123041.3135
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 1504.60
               Mean episode length: 48.39
                  Mean reward/step: 28.21
       Mean episode length/episode: 6.98
            Mean episode successes: 2.4438
Mean episode consecutive_successes: 5.0360
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 14.63s
                        Total time: 24408.89s
                               ETA: 1026350.2s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.438s, learning 0.163s)
               Value function loss: 118083.2836
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 1519.96
               Mean episode length: 49.39
                  Mean reward/step: 28.25
       Mean episode length/episode: 6.90
            Mean episode successes: 2.3174
Mean episode consecutive_successes: 5.0648
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 8.60s
                        Total time: 24417.49s
                               ETA: 1026259.6s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.494s, learning 0.161s)
               Value function loss: 122634.7762
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 1603.66
               Mean episode length: 50.48
                  Mean reward/step: 28.75
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3408
Mean episode consecutive_successes: 5.1114
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 8.65s
                        Total time: 24426.15s
                               ETA: 1026171.3s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.177s, learning 0.155s)
               Value function loss: 132662.2738
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 1404.48
               Mean episode length: 48.26
                  Mean reward/step: 28.68
       Mean episode length/episode: 7.10
            Mean episode successes: 2.3774
Mean episode consecutive_successes: 5.1503
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 8.33s
                        Total time: 24434.48s
                               ETA: 1026069.5s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.556s, learning 0.163s)
               Value function loss: 125628.4740
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 1078.80
               Mean episode length: 47.23
                  Mean reward/step: 28.85
       Mean episode length/episode: 6.96
            Mean episode successes: 2.3384
Mean episode consecutive_successes: 5.1646
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 8.72s
                        Total time: 24443.20s
                               ETA: 1025984.0s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.201s, learning 0.179s)
               Value function loss: 129755.6740
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 1111.71
               Mean episode length: 48.19
                  Mean reward/step: 28.94
       Mean episode length/episode: 7.05
            Mean episode successes: 2.5513
Mean episode consecutive_successes: 5.1466
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 8.38s
                        Total time: 24451.58s
                               ETA: 1025884.4s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.268s, learning 0.176s)
               Value function loss: 134166.8824
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 1711.42
               Mean episode length: 51.20
                  Mean reward/step: 28.65
       Mean episode length/episode: 7.00
            Mean episode successes: 2.4077
Mean episode consecutive_successes: 5.2584
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 8.44s
                        Total time: 24460.02s
                               ETA: 1025787.6s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.245s, learning 0.232s)
               Value function loss: 98397.8805
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 1230.20
               Mean episode length: 49.69
                  Mean reward/step: 28.16
       Mean episode length/episode: 6.99
            Mean episode successes: 2.3330
Mean episode consecutive_successes: 5.2847
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 8.48s
                        Total time: 24468.50s
                               ETA: 1025692.2s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.107s, learning 0.174s)
               Value function loss: 100971.3238
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 1366.08
               Mean episode length: 50.71
                  Mean reward/step: 27.46
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3901
Mean episode consecutive_successes: 5.2674
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 8.28s
                        Total time: 24476.78s
                               ETA: 1025588.6s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.329s, learning 0.184s)
               Value function loss: 98318.1125
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 1725.36
               Mean episode length: 49.09
                  Mean reward/step: 27.32
       Mean episode length/episode: 6.97
            Mean episode successes: 2.2744
Mean episode consecutive_successes: 5.2964
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 8.51s
                        Total time: 24485.29s
                               ETA: 1025494.8s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.173s)
               Value function loss: 99286.1553
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 1934.07
               Mean episode length: 51.44
                  Mean reward/step: 26.51
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1875
Mean episode consecutive_successes: 5.3776
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 8.58s
                        Total time: 24493.88s
                               ETA: 1025404.2s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.925s, learning 0.171s)
               Value function loss: 106048.0152
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 1368.35
               Mean episode length: 50.84
                  Mean reward/step: 26.52
       Mean episode length/episode: 7.04
            Mean episode successes: 2.3379
Mean episode consecutive_successes: 5.3027
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 8.10s
                        Total time: 24501.97s
                               ETA: 1025293.1s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.087s, learning 0.160s)
               Value function loss: 98380.7063
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 797.42
               Mean episode length: 45.58
                  Mean reward/step: 25.14
       Mean episode length/episode: 6.93
            Mean episode successes: 2.2563
Mean episode consecutive_successes: 5.2537
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 8.25s
                        Total time: 24510.22s
                               ETA: 1025188.5s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.458s, learning 0.194s)
               Value function loss: 97945.6508
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 1433.28
               Mean episode length: 50.12
                  Mean reward/step: 23.32
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0078
Mean episode consecutive_successes: 5.3267
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 8.65s
                        Total time: 24518.87s
                               ETA: 1025100.9s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.769s, learning 0.189s)
               Value function loss: 93654.6615
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 1192.39
               Mean episode length: 49.39
                  Mean reward/step: 23.40
       Mean episode length/episode: 7.14
            Mean episode successes: 2.0615
Mean episode consecutive_successes: 5.2665
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 8.96s
                        Total time: 24527.83s
                               ETA: 1025026.1s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.246s, learning 0.162s)
               Value function loss: 119762.2061
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 1504.96
               Mean episode length: 49.02
                  Mean reward/step: 26.15
       Mean episode length/episode: 6.89
            Mean episode successes: 2.0620
Mean episode consecutive_successes: 5.2473
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 8.41s
                        Total time: 24536.24s
                               ETA: 1024928.4s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.274s, learning 0.158s)
               Value function loss: 109785.1131
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 1598.70
               Mean episode length: 50.95
                  Mean reward/step: 24.78
       Mean episode length/episode: 7.03
            Mean episode successes: 1.9937
Mean episode consecutive_successes: 5.2415
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 8.43s
                        Total time: 24544.67s
                               ETA: 1024831.8s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.111s, learning 0.171s)
               Value function loss: 105248.5227
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 1421.25
               Mean episode length: 50.74
                  Mean reward/step: 25.82
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1382
Mean episode consecutive_successes: 5.1867
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 8.28s
                        Total time: 24552.95s
                               ETA: 1024728.9s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.164s)
               Value function loss: 123609.6883
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 1350.26
               Mean episode length: 48.90
                  Mean reward/step: 26.14
       Mean episode length/episode: 6.95
            Mean episode successes: 2.1011
Mean episode consecutive_successes: 5.1277
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 8.46s
                        Total time: 24561.41s
                               ETA: 1024633.8s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.192s, learning 0.165s)
               Value function loss: 128450.2543
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 1308.90
               Mean episode length: 51.96
                  Mean reward/step: 26.67
       Mean episode length/episode: 6.96
            Mean episode successes: 2.1963
Mean episode consecutive_successes: 5.1076
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 8.36s
                        Total time: 24569.77s
                               ETA: 1024534.3s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.471s, learning 0.185s)
               Value function loss: 107031.6865
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 1473.78
               Mean episode length: 51.23
                  Mean reward/step: 26.24
       Mean episode length/episode: 7.02
            Mean episode successes: 2.2622
Mean episode consecutive_successes: 5.0893
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 8.66s
                        Total time: 24578.43s
                               ETA: 1024447.3s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.249s, learning 0.214s)
               Value function loss: 108109.8037
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 1482.84
               Mean episode length: 49.41
                  Mean reward/step: 25.27
       Mean episode length/episode: 6.95
            Mean episode successes: 2.0703
Mean episode consecutive_successes: 5.1138
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 8.46s
                        Total time: 24586.89s
                               ETA: 1024352.4s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.328s, learning 0.163s)
               Value function loss: 102869.2416
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 1789.52
               Mean episode length: 51.95
                  Mean reward/step: 25.83
       Mean episode length/episode: 6.93
            Mean episode successes: 2.0894
Mean episode consecutive_successes: 5.1038
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 8.49s
                        Total time: 24595.38s
                               ETA: 1024258.6s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.987s, learning 0.233s)
               Value function loss: 89632.3736
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 1311.85
               Mean episode length: 51.52
                  Mean reward/step: 23.86
       Mean episode length/episode: 6.96
            Mean episode successes: 2.0684
Mean episode consecutive_successes: 5.0396
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 8.22s
                        Total time: 24603.60s
                               ETA: 1024153.8s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.216s, learning 0.163s)
               Value function loss: 91257.5732
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 1309.58
               Mean episode length: 48.73
                  Mean reward/step: 25.17
       Mean episode length/episode: 7.07
            Mean episode successes: 2.1494
Mean episode consecutive_successes: 5.0444
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 8.38s
                        Total time: 24611.98s
                               ETA: 1024055.5s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.130s, learning 0.165s)
               Value function loss: 90678.2197
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 1433.58
               Mean episode length: 50.44
                  Mean reward/step: 25.02
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9658
Mean episode consecutive_successes: 5.0784
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 8.30s
                        Total time: 24620.28s
                               ETA: 1023953.9s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 2073 steps/s (collection: 7.733s, learning 0.168s)
               Value function loss: 97277.5291
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 1042.85
               Mean episode length: 50.06
                  Mean reward/step: 24.17
       Mean episode length/episode: 6.96
            Mean episode successes: 2.0088
Mean episode consecutive_successes: 5.0153
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 7.90s
                        Total time: 24628.18s
                               ETA: 1023836.0s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.017s, learning 0.170s)
               Value function loss: 100458.3602
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 1121.57
               Mean episode length: 51.40
                  Mean reward/step: 24.51
       Mean episode length/episode: 7.07
            Mean episode successes: 2.0620
Mean episode consecutive_successes: 4.9775
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 8.19s
                        Total time: 24636.36s
                               ETA: 1023730.1s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.438s, learning 0.230s)
               Value function loss: 102862.2982
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 1337.18
               Mean episode length: 47.89
                  Mean reward/step: 24.37
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9795
Mean episode consecutive_successes: 4.9930
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 8.67s
                        Total time: 24645.03s
                               ETA: 1023644.2s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.169s, learning 0.158s)
               Value function loss: 93996.9811
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 1259.68
               Mean episode length: 49.16
                  Mean reward/step: 24.77
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0269
Mean episode consecutive_successes: 4.9726
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 8.33s
                        Total time: 24653.36s
                               ETA: 1023544.2s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.119s, learning 0.164s)
               Value function loss: 97478.2773
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 1093.87
               Mean episode length: 48.05
                  Mean reward/step: 25.29
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1763
Mean episode consecutive_successes: 4.8959
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 8.28s
                        Total time: 24661.64s
                               ETA: 1023442.5s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.095s, learning 0.249s)
               Value function loss: 99503.4410
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 1074.88
               Mean episode length: 49.21
                  Mean reward/step: 24.96
       Mean episode length/episode: 7.02
            Mean episode successes: 2.0713
Mean episode consecutive_successes: 4.9239
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 8.34s
                        Total time: 24669.99s
                               ETA: 1023343.3s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.334s, learning 0.160s)
               Value function loss: 108625.8391
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 1306.22
               Mean episode length: 50.79
                  Mean reward/step: 25.09
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1479
Mean episode consecutive_successes: 4.9202
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 8.49s
                        Total time: 24678.48s
                               ETA: 1023250.5s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.416s, learning 0.182s)
               Value function loss: 107722.5717
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 967.10
               Mean episode length: 48.60
                  Mean reward/step: 26.18
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0566
Mean episode consecutive_successes: 4.9391
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 8.60s
                        Total time: 24687.08s
                               ETA: 1023162.1s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.108s, learning 0.189s)
               Value function loss: 105969.9746
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 1210.40
               Mean episode length: 49.76
                  Mean reward/step: 27.06
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2085
Mean episode consecutive_successes: 4.9098
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 8.30s
                        Total time: 24695.38s
                               ETA: 1023061.2s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.077s, learning 0.155s)
               Value function loss: 106021.9100
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 1724.73
               Mean episode length: 51.71
                  Mean reward/step: 27.37
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2280
Mean episode consecutive_successes: 4.9650
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 8.23s
                        Total time: 24703.61s
                               ETA: 1022957.7s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.854s, learning 0.165s)
               Value function loss: 102793.8918
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 1096.84
               Mean episode length: 48.32
                  Mean reward/step: 26.62
       Mean episode length/episode: 6.94
            Mean episode successes: 2.1274
Mean episode consecutive_successes: 4.9563
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 8.02s
                        Total time: 24711.63s
                               ETA: 1022845.5s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.403s, learning 0.201s)
               Value function loss: 116050.7102
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 1270.00
               Mean episode length: 49.34
                  Mean reward/step: 24.48
       Mean episode length/episode: 7.01
            Mean episode successes: 2.0015
Mean episode consecutive_successes: 4.9936
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 8.60s
                        Total time: 24720.23s
                               ETA: 1022757.6s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.183s, learning 0.186s)
               Value function loss: 164554.8355
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 1494.30
               Mean episode length: 52.20
                  Mean reward/step: 24.86
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9951
Mean episode consecutive_successes: 4.9818
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 8.37s
                        Total time: 24728.60s
                               ETA: 1022660.1s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.301s, learning 0.162s)
               Value function loss: 116739.0041
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 1460.88
               Mean episode length: 49.74
                  Mean reward/step: 24.24
       Mean episode length/episode: 7.11
            Mean episode successes: 1.9829
Mean episode consecutive_successes: 5.0249
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 8.46s
                        Total time: 24737.06s
                               ETA: 1022566.5s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.164s)
               Value function loss: 85585.6686
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 1197.69
               Mean episode length: 49.24
                  Mean reward/step: 22.96
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9053
Mean episode consecutive_successes: 5.0013
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 8.17s
                        Total time: 24745.23s
                               ETA: 1022460.9s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.005s, learning 0.166s)
               Value function loss: 90599.4895
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 1401.85
               Mean episode length: 52.21
                  Mean reward/step: 26.13
       Mean episode length/episode: 7.07
            Mean episode successes: 1.9507
Mean episode consecutive_successes: 5.0118
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 8.17s
                        Total time: 24753.40s
                               ETA: 1022355.4s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.141s, learning 0.188s)
               Value function loss: 90681.3730
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 1137.91
               Mean episode length: 49.79
                  Mean reward/step: 25.13
       Mean episode length/episode: 6.98
            Mean episode successes: 2.0381
Mean episode consecutive_successes: 4.9628
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 8.33s
                        Total time: 24761.73s
                               ETA: 1022256.4s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.429s, learning 0.164s)
               Value function loss: 110740.1723
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 1316.78
               Mean episode length: 51.47
                  Mean reward/step: 26.32
       Mean episode length/episode: 7.09
            Mean episode successes: 2.1919
Mean episode consecutive_successes: 4.9327
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 8.59s
                        Total time: 24770.32s
                               ETA: 1022168.5s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.191s, learning 0.186s)
               Value function loss: 111645.9750
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 1038.05
               Mean episode length: 49.86
                  Mean reward/step: 27.08
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2461
Mean episode consecutive_successes: 4.9413
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 8.38s
                        Total time: 24778.70s
                               ETA: 1022071.7s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.543s, learning 0.191s)
               Value function loss: 104867.3775
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 1065.56
               Mean episode length: 50.61
                  Mean reward/step: 26.77
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1978
Mean episode consecutive_successes: 4.9571
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 8.73s
                        Total time: 24787.44s
                               ETA: 1021989.7s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.521s, learning 0.164s)
               Value function loss: 108470.8787
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 1521.72
               Mean episode length: 51.54
                  Mean reward/step: 25.47
       Mean episode length/episode: 7.02
            Mean episode successes: 2.1426
Mean episode consecutive_successes: 5.0184
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 8.69s
                        Total time: 24796.12s
                               ETA: 1021905.8s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.018s, learning 0.266s)
               Value function loss: 101669.9877
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 1555.05
               Mean episode length: 53.04
                  Mean reward/step: 26.81
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1021
Mean episode consecutive_successes: 5.0423
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 8.28s
                        Total time: 24804.40s
                               ETA: 1021805.4s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.237s, learning 0.235s)
               Value function loss: 93729.8061
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 1196.74
               Mean episode length: 48.00
                  Mean reward/step: 25.05
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1807
Mean episode consecutive_successes: 5.0058
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 8.47s
                        Total time: 24812.88s
                               ETA: 1021712.8s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.214s, learning 0.175s)
               Value function loss: 93035.8125
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 1054.26
               Mean episode length: 47.83
                  Mean reward/step: 25.05
       Mean episode length/episode: 6.95
            Mean episode successes: 2.1919
Mean episode consecutive_successes: 4.9445
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 8.39s
                        Total time: 24821.26s
                               ETA: 1021616.9s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.452s, learning 0.158s)
               Value function loss: 92707.8578
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 1315.17
               Mean episode length: 49.53
                  Mean reward/step: 25.41
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1348
Mean episode consecutive_successes: 5.0110
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 8.61s
                        Total time: 24829.88s
                               ETA: 1021530.2s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.181s, learning 0.224s)
               Value function loss: 100476.2002
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 1599.61
               Mean episode length: 52.27
                  Mean reward/step: 25.29
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1099
Mean episode consecutive_successes: 5.0150
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 8.40s
                        Total time: 24838.28s
                               ETA: 1021435.0s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.103s, learning 0.167s)
               Value function loss: 96266.6773
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 867.55
               Mean episode length: 49.13
                  Mean reward/step: 23.57
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0630
Mean episode consecutive_successes: 4.9770
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 8.27s
                        Total time: 24846.55s
                               ETA: 1021334.4s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.357s, learning 0.255s)
               Value function loss: 98079.4742
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 1487.20
               Mean episode length: 48.42
                  Mean reward/step: 24.91
       Mean episode length/episode: 7.01
            Mean episode successes: 2.1040
Mean episode consecutive_successes: 4.9990
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 8.61s
                        Total time: 24855.16s
                               ETA: 1021248.0s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.324s, learning 0.163s)
               Value function loss: 102751.7891
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 1548.77
               Mean episode length: 50.69
                  Mean reward/step: 26.31
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0908
Mean episode consecutive_successes: 4.9907
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 8.49s
                        Total time: 24863.65s
                               ETA: 1021156.5s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.016s, learning 0.164s)
               Value function loss: 107728.3205
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 1064.01
               Mean episode length: 47.82
                  Mean reward/step: 26.54
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0957
Mean episode consecutive_successes: 4.9790
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 8.18s
                        Total time: 24871.83s
                               ETA: 1021052.5s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.421s, learning 0.183s)
               Value function loss: 130804.8512
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 1561.44
               Mean episode length: 51.41
                  Mean reward/step: 27.83
       Mean episode length/episode: 7.10
            Mean episode successes: 2.2930
Mean episode consecutive_successes: 4.9755
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 8.60s
                        Total time: 24880.43s
                               ETA: 1020965.9s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.188s, learning 0.188s)
               Value function loss: 126760.1824
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 1126.11
               Mean episode length: 47.26
                  Mean reward/step: 25.98
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1558
Mean episode consecutive_successes: 5.0087
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 8.38s
                        Total time: 24888.81s
                               ETA: 1020870.0s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.538s, learning 0.164s)
               Value function loss: 133626.3684
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 855.31
               Mean episode length: 50.18
                  Mean reward/step: 25.90
       Mean episode length/episode: 6.96
            Mean episode successes: 2.1392
Mean episode consecutive_successes: 4.9827
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 8.70s
                        Total time: 24897.51s
                               ETA: 1020787.6s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.987s, learning 0.188s)
               Value function loss: 104236.2873
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 1474.09
               Mean episode length: 51.84
                  Mean reward/step: 24.69
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0112
Mean episode consecutive_successes: 5.0110
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 8.17s
                        Total time: 24905.69s
                               ETA: 1020683.6s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.791s, learning 0.235s)
               Value function loss: 95425.1918
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 1316.89
               Mean episode length: 48.46
                  Mean reward/step: 22.38
       Mean episode length/episode: 6.94
            Mean episode successes: 1.8218
Mean episode consecutive_successes: 5.0227
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 9.03s
                        Total time: 24914.71s
                               ETA: 1020614.6s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.311s, learning 0.160s)
               Value function loss: 97130.9648
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 956.08
               Mean episode length: 47.11
                  Mean reward/step: 22.42
       Mean episode length/episode: 7.04
            Mean episode successes: 1.7358
Mean episode consecutive_successes: 4.9956
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 8.47s
                        Total time: 24923.18s
                               ETA: 1020522.8s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.260s, learning 0.165s)
               Value function loss: 96078.7205
                    Surrogate loss: 0.0124
             Mean action noise std: 0.73
                       Mean reward: 1157.08
               Mean episode length: 48.34
                  Mean reward/step: 23.17
       Mean episode length/episode: 7.04
            Mean episode successes: 1.8125
Mean episode consecutive_successes: 4.9741
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 8.42s
                        Total time: 24931.61s
                               ETA: 1020429.3s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.144s, learning 0.161s)
               Value function loss: 91780.9162
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 978.84
               Mean episode length: 47.74
                  Mean reward/step: 24.21
       Mean episode length/episode: 6.99
            Mean episode successes: 1.9287
Mean episode consecutive_successes: 4.8981
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 8.30s
                        Total time: 24939.91s
                               ETA: 1020330.9s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.435s, learning 0.207s)
               Value function loss: 91789.2506
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 1290.20
               Mean episode length: 49.47
                  Mean reward/step: 24.68
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9263
Mean episode consecutive_successes: 4.9100
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 8.64s
                        Total time: 24948.55s
                               ETA: 1020246.4s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.420s, learning 0.164s)
               Value function loss: 87112.4693
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 1388.66
               Mean episode length: 51.30
                  Mean reward/step: 24.04
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0640
Mean episode consecutive_successes: 4.8542
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 8.58s
                        Total time: 24957.14s
                               ETA: 1020159.6s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.403s, learning 0.186s)
               Value function loss: 94754.9316
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 1434.50
               Mean episode length: 51.74
                  Mean reward/step: 24.67
       Mean episode length/episode: 7.07
            Mean episode successes: 1.9448
Mean episode consecutive_successes: 4.9327
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 8.59s
                        Total time: 24965.73s
                               ETA: 1020073.1s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.612s, learning 0.166s)
               Value function loss: 106715.5346
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 1007.35
               Mean episode length: 49.16
                  Mean reward/step: 24.36
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8042
Mean episode consecutive_successes: 4.9680
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 8.78s
                        Total time: 24974.51s
                               ETA: 1019994.4s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.314s, learning 0.176s)
               Value function loss: 105645.6918
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 890.80
               Mean episode length: 51.15
                  Mean reward/step: 24.62
       Mean episode length/episode: 6.94
            Mean episode successes: 1.9219
Mean episode consecutive_successes: 4.8792
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 8.49s
                        Total time: 24983.00s
                               ETA: 1019903.9s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.600s, learning 0.165s)
               Value function loss: 107812.2377
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 1254.35
               Mean episode length: 51.57
                  Mean reward/step: 24.43
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9873
Mean episode consecutive_successes: 4.8397
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 8.77s
                        Total time: 24991.76s
                               ETA: 1019824.7s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.280s, learning 0.162s)
               Value function loss: 121794.2937
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 941.12
               Mean episode length: 46.64
                  Mean reward/step: 25.73
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9917
Mean episode consecutive_successes: 4.8203
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 8.44s
                        Total time: 25000.20s
                               ETA: 1019732.5s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.382s, learning 0.228s)
               Value function loss: 113056.9811
                    Surrogate loss: -0.0196
             Mean action noise std: 0.73
                       Mean reward: 1157.65
               Mean episode length: 49.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 7.02
            Mean episode successes: 2.0264
Mean episode consecutive_successes: 4.8191
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 8.61s
                        Total time: 25008.81s
                               ETA: 1019647.1s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.258s, learning 0.169s)
               Value function loss: 120642.3510
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 1284.37
               Mean episode length: 51.84
                  Mean reward/step: 25.96
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1133
Mean episode consecutive_successes: 4.8421
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 8.43s
                        Total time: 25017.24s
                               ETA: 1019554.3s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.323s, learning 0.171s)
               Value function loss: 109344.7775
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 1392.35
               Mean episode length: 53.03
                  Mean reward/step: 25.19
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9946
Mean episode consecutive_successes: 4.8944
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 8.49s
                        Total time: 25025.73s
                               ETA: 1019464.4s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.151s, learning 0.185s)
               Value function loss: 116132.2654
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 1280.96
               Mean episode length: 51.05
                  Mean reward/step: 25.54
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0449
Mean episode consecutive_successes: 4.8871
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 8.34s
                        Total time: 25034.07s
                               ETA: 1019368.1s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.150s, learning 0.163s)
               Value function loss: 108310.4434
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 1121.67
               Mean episode length: 48.06
                  Mean reward/step: 25.73
       Mean episode length/episode: 7.06
            Mean episode successes: 2.1089
Mean episode consecutive_successes: 4.8674
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 8.31s
                        Total time: 25042.38s
                               ETA: 1019270.9s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.940s, learning 0.165s)
               Value function loss: 111823.7352
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 1095.41
               Mean episode length: 50.19
                  Mean reward/step: 27.25
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2812
Mean episode consecutive_successes: 4.8360
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 8.10s
                        Total time: 25050.49s
                               ETA: 1019165.3s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.990s, learning 0.162s)
               Value function loss: 113552.4402
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 1511.04
               Mean episode length: 50.77
                  Mean reward/step: 26.89
       Mean episode length/episode: 6.98
            Mean episode successes: 2.1401
Mean episode consecutive_successes: 4.9378
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 8.15s
                        Total time: 25058.64s
                               ETA: 1019061.7s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.075s, learning 0.163s)
               Value function loss: 110711.0807
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 1000.78
               Mean episode length: 50.12
                  Mean reward/step: 24.62
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1772
Mean episode consecutive_successes: 4.8971
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 8.24s
                        Total time: 25066.88s
                               ETA: 1018961.7s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.381s, learning 0.165s)
               Value function loss: 126201.6676
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 1254.29
               Mean episode length: 47.44
                  Mean reward/step: 25.34
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1094
Mean episode consecutive_successes: 4.9465
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 8.55s
                        Total time: 25075.42s
                               ETA: 1018874.3s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.071s, learning 0.233s)
               Value function loss: 115168.7586
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 1469.72
               Mean episode length: 52.63
                  Mean reward/step: 22.07
       Mean episode length/episode: 7.06
            Mean episode successes: 2.0044
Mean episode consecutive_successes: 4.9645
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 8.30s
                        Total time: 25083.73s
                               ETA: 1018777.1s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.305s, learning 0.158s)
               Value function loss: 100594.9516
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 942.13
               Mean episode length: 48.83
                  Mean reward/step: 21.97
       Mean episode length/episode: 6.95
            Mean episode successes: 1.9424
Mean episode consecutive_successes: 4.9041
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 8.46s
                        Total time: 25092.19s
                               ETA: 1018686.5s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.163s)
               Value function loss: 102030.2613
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 1333.26
               Mean episode length: 50.05
                  Mean reward/step: 22.34
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9399
Mean episode consecutive_successes: 4.8899
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 8.36s
                        Total time: 25100.55s
                               ETA: 1018591.9s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.933s, learning 0.193s)
               Value function loss: 84475.6307
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 883.19
               Mean episode length: 47.61
                  Mean reward/step: 23.15
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0381
Mean episode consecutive_successes: 4.7983
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 8.13s
                        Total time: 25108.68s
                               ETA: 1018487.7s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.400s, learning 0.165s)
               Value function loss: 92045.7145
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 920.69
               Mean episode length: 46.25
                  Mean reward/step: 21.70
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9702
Mean episode consecutive_successes: 4.7741
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 8.56s
                        Total time: 25117.24s
                               ETA: 1018401.5s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.049s, learning 0.164s)
               Value function loss: 93013.9277
                    Surrogate loss: 0.0119
             Mean action noise std: 0.73
                       Mean reward: 1375.70
               Mean episode length: 50.51
                  Mean reward/step: 22.30
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9116
Mean episode consecutive_successes: 4.8252
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 8.21s
                        Total time: 25125.46s
                               ETA: 1018300.9s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.197s, learning 0.165s)
               Value function loss: 91398.0828
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 1081.82
               Mean episode length: 48.63
                  Mean reward/step: 22.45
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8501
Mean episode consecutive_successes: 4.7859
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 8.36s
                        Total time: 25133.82s
                               ETA: 1018206.6s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.742s, learning 0.166s)
               Value function loss: 97384.2275
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 979.42
               Mean episode length: 48.80
                  Mean reward/step: 23.24
       Mean episode length/episode: 7.02
            Mean episode successes: 1.9019
Mean episode consecutive_successes: 4.7714
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 7.91s
                        Total time: 25141.73s
                               ETA: 1018093.9s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.974s, learning 0.162s)
               Value function loss: 100896.8857
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 1207.39
               Mean episode length: 49.39
                  Mean reward/step: 26.09
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0923
Mean episode consecutive_successes: 4.7220
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 8.14s
                        Total time: 25149.86s
                               ETA: 1017990.5s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.162s)
               Value function loss: 96566.6152
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 962.13
               Mean episode length: 49.28
                  Mean reward/step: 25.25
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1982
Mean episode consecutive_successes: 4.6802
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 8.29s
                        Total time: 25158.16s
                               ETA: 1017893.5s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.428s, learning 0.161s)
               Value function loss: 100274.4021
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 1400.10
               Mean episode length: 52.96
                  Mean reward/step: 23.88
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2822
Mean episode consecutive_successes: 4.6733
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 8.59s
                        Total time: 25166.74s
                               ETA: 1017808.6s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.431s, learning 0.163s)
               Value function loss: 105716.8246
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 845.74
               Mean episode length: 46.55
                  Mean reward/step: 23.25
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1460
Mean episode consecutive_successes: 4.7314
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 8.59s
                        Total time: 25175.34s
                               ETA: 1017724.0s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.189s, learning 0.161s)
               Value function loss: 115670.1406
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 805.21
               Mean episode length: 49.69
                  Mean reward/step: 21.51
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9409
Mean episode consecutive_successes: 4.7396
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 8.35s
                        Total time: 25183.69s
                               ETA: 1017629.6s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.414s, learning 0.176s)
               Value function loss: 90066.0090
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 1108.61
               Mean episode length: 50.91
                  Mean reward/step: 20.99
       Mean episode length/episode: 6.93
            Mean episode successes: 1.9683
Mean episode consecutive_successes: 4.6693
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 8.59s
                        Total time: 25192.28s
                               ETA: 1017544.9s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.371s, learning 0.165s)
               Value function loss: 87576.4922
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 1167.12
               Mean episode length: 48.57
                  Mean reward/step: 21.41
       Mean episode length/episode: 6.99
            Mean episode successes: 1.9238
Mean episode consecutive_successes: 4.6467
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 8.54s
                        Total time: 25200.81s
                               ETA: 1017458.1s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.356s, learning 0.172s)
               Value function loss: 101469.8320
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 1156.36
               Mean episode length: 50.53
                  Mean reward/step: 21.21
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8286
Mean episode consecutive_successes: 4.6463
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 8.53s
                        Total time: 25209.34s
                               ETA: 1017371.1s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.317s, learning 0.182s)
               Value function loss: 103493.1051
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 968.91
               Mean episode length: 47.83
                  Mean reward/step: 21.65
       Mean episode length/episode: 7.08
            Mean episode successes: 1.9395
Mean episode consecutive_successes: 4.5768
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 8.50s
                        Total time: 25217.84s
                               ETA: 1017282.9s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.362s, learning 0.175s)
               Value function loss: 98684.0824
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 847.86
               Mean episode length: 45.97
                  Mean reward/step: 23.28
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9912
Mean episode consecutive_successes: 4.5458
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 8.54s
                        Total time: 25226.38s
                               ETA: 1017196.4s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.064s, learning 0.168s)
               Value function loss: 127952.9230
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 1307.10
               Mean episode length: 52.44
                  Mean reward/step: 23.40
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9976
Mean episode consecutive_successes: 4.5581
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 8.23s
                        Total time: 25234.61s
                               ETA: 1017097.6s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.058s, learning 0.164s)
               Value function loss: 98605.4875
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 1143.11
               Mean episode length: 50.56
                  Mean reward/step: 21.76
       Mean episode length/episode: 7.05
            Mean episode successes: 1.9150
Mean episode consecutive_successes: 4.5812
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 8.22s
                        Total time: 25242.83s
                               ETA: 1016998.5s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.165s)
               Value function loss: 82184.1611
                    Surrogate loss: -0.0243
             Mean action noise std: 0.73
                       Mean reward: 1185.43
               Mean episode length: 49.83
                  Mean reward/step: 22.20
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0156
Mean episode consecutive_successes: 4.5267
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 8.38s
                        Total time: 25251.21s
                               ETA: 1016905.9s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.073s, learning 0.191s)
               Value function loss: 92369.7262
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 1030.38
               Mean episode length: 50.00
                  Mean reward/step: 23.19
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9087
Mean episode consecutive_successes: 4.5765
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 8.26s
                        Total time: 25259.48s
                               ETA: 1016808.7s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.615s, learning 0.189s)
               Value function loss: 86743.7104
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 1152.44
               Mean episode length: 49.16
                  Mean reward/step: 22.94
       Mean episode length/episode: 7.03
            Mean episode successes: 1.9028
Mean episode consecutive_successes: 4.5585
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 8.80s
                        Total time: 25268.28s
                               ETA: 1016733.2s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.175s, learning 0.177s)
               Value function loss: 85085.5602
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 1415.40
               Mean episode length: 50.02
                  Mean reward/step: 23.03
       Mean episode length/episode: 6.99
            Mean episode successes: 1.9028
Mean episode consecutive_successes: 4.5546
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 8.35s
                        Total time: 25276.63s
                               ETA: 1016639.6s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.989s, learning 0.164s)
               Value function loss: 90288.6674
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 985.19
               Mean episode length: 50.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 7.04
            Mean episode successes: 1.8428
Mean episode consecutive_successes: 4.5473
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 8.15s
                        Total time: 25284.79s
                               ETA: 1016538.1s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.144s, learning 0.174s)
               Value function loss: 85505.9223
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 837.51
               Mean episode length: 49.20
                  Mean reward/step: 24.71
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9590
Mean episode consecutive_successes: 4.5071
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 8.32s
                        Total time: 25293.11s
                               ETA: 1016443.3s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.454s, learning 0.183s)
               Value function loss: 92882.6539
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 1296.90
               Mean episode length: 52.03
                  Mean reward/step: 24.92
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0830
Mean episode consecutive_successes: 4.5162
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 8.64s
                        Total time: 25301.74s
                               ETA: 1016361.3s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.386s, learning 0.166s)
               Value function loss: 86957.1607
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 1154.60
               Mean episode length: 48.78
                  Mean reward/step: 23.47
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0181
Mean episode consecutive_successes: 4.5484
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 8.55s
                        Total time: 25310.30s
                               ETA: 1016276.0s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.129s, learning 0.169s)
               Value function loss: 86327.8623
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 1413.35
               Mean episode length: 50.86
                  Mean reward/step: 23.11
       Mean episode length/episode: 7.10
            Mean episode successes: 2.0371
Mean episode consecutive_successes: 4.5598
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 8.30s
                        Total time: 25318.59s
                               ETA: 1016180.6s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.227s, learning 0.181s)
               Value function loss: 87824.5492
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 1702.63
               Mean episode length: 53.09
                  Mean reward/step: 24.70
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0356
Mean episode consecutive_successes: 4.6244
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 8.41s
                        Total time: 25327.00s
                               ETA: 1016089.7s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.891s, learning 0.162s)
               Value function loss: 102352.5018
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 1478.91
               Mean episode length: 51.61
                  Mean reward/step: 24.25
       Mean episode length/episode: 6.98
            Mean episode successes: 1.8857
Mean episode consecutive_successes: 4.6769
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 8.05s
                        Total time: 25335.05s
                               ETA: 1015984.6s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.060s, learning 0.172s)
               Value function loss: 88365.9000
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 1292.74
               Mean episode length: 49.86
                  Mean reward/step: 24.53
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0146
Mean episode consecutive_successes: 4.6309
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 8.23s
                        Total time: 25343.29s
                               ETA: 1015886.8s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.324s, learning 0.207s)
               Value function loss: 91612.3625
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 1024.99
               Mean episode length: 48.90
                  Mean reward/step: 23.75
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0352
Mean episode consecutive_successes: 4.5926
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 8.53s
                        Total time: 25351.82s
                               ETA: 1015801.0s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.388s, learning 0.163s)
               Value function loss: 89067.2977
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 1156.85
               Mean episode length: 48.96
                  Mean reward/step: 22.64
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8345
Mean episode consecutive_successes: 4.6916
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 8.55s
                        Total time: 25360.37s
                               ETA: 1015716.1s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.161s)
               Value function loss: 89937.0393
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 1100.32
               Mean episode length: 49.85
                  Mean reward/step: 21.59
       Mean episode length/episode: 7.13
            Mean episode successes: 1.8081
Mean episode consecutive_successes: 4.6925
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 8.27s
                        Total time: 25368.64s
                               ETA: 1015620.1s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.249s, learning 0.228s)
               Value function loss: 97933.5346
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 1075.80
               Mean episode length: 51.03
                  Mean reward/step: 21.80
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8101
Mean episode consecutive_successes: 4.6481
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 8.48s
                        Total time: 25377.12s
                               ETA: 1015532.4s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.409s, learning 0.163s)
               Value function loss: 121134.6322
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 1168.42
               Mean episode length: 50.56
                  Mean reward/step: 23.67
       Mean episode length/episode: 6.95
            Mean episode successes: 1.7095
Mean episode consecutive_successes: 4.6362
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 8.57s
                        Total time: 25385.69s
                               ETA: 1015448.5s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.301s, learning 0.162s)
               Value function loss: 94693.1582
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 999.88
               Mean episode length: 49.19
                  Mean reward/step: 23.00
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8032
Mean episode consecutive_successes: 4.5764
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 8.46s
                        Total time: 25394.16s
                               ETA: 1015360.3s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.249s, learning 0.171s)
               Value function loss: 94297.9275
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 740.16
               Mean episode length: 46.20
                  Mean reward/step: 23.07
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8462
Mean episode consecutive_successes: 4.5279
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 8.42s
                        Total time: 25402.58s
                               ETA: 1015270.5s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.366s, learning 0.163s)
               Value function loss: 110974.7543
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 943.88
               Mean episode length: 47.72
                  Mean reward/step: 22.77
       Mean episode length/episode: 7.08
            Mean episode successes: 2.0063
Mean episode consecutive_successes: 4.4788
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 8.53s
                        Total time: 25411.10s
                               ETA: 1015185.1s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.140s, learning 0.181s)
               Value function loss: 91903.1857
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 1160.33
               Mean episode length: 49.88
                  Mean reward/step: 23.34
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9785
Mean episode consecutive_successes: 4.4765
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 8.32s
                        Total time: 25419.43s
                               ETA: 1015091.4s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.302s, learning 0.167s)
               Value function loss: 96581.0643
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 1342.15
               Mean episode length: 48.96
                  Mean reward/step: 23.47
       Mean episode length/episode: 7.03
            Mean episode successes: 1.9175
Mean episode consecutive_successes: 4.5442
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 8.47s
                        Total time: 25427.90s
                               ETA: 1015003.8s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.980s, learning 0.165s)
               Value function loss: 95364.8105
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 1279.49
               Mean episode length: 48.85
                  Mean reward/step: 24.45
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8804
Mean episode consecutive_successes: 4.5799
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 8.15s
                        Total time: 25436.04s
                               ETA: 1014903.2s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.091s, learning 0.175s)
               Value function loss: 99299.6016
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 1014.14
               Mean episode length: 48.34
                  Mean reward/step: 25.83
       Mean episode length/episode: 7.10
            Mean episode successes: 2.0659
Mean episode consecutive_successes: 4.5580
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 8.27s
                        Total time: 25444.31s
                               ETA: 1014807.5s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.014s, learning 0.184s)
               Value function loss: 106262.1012
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 1209.10
               Mean episode length: 47.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 7.06
            Mean episode successes: 2.1118
Mean episode consecutive_successes: 4.5952
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 8.20s
                        Total time: 25452.50s
                               ETA: 1014709.3s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.093s, learning 0.224s)
               Value function loss: 101573.9178
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 1356.32
               Mean episode length: 51.23
                  Mean reward/step: 24.83
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1870
Mean episode consecutive_successes: 4.6064
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 8.32s
                        Total time: 25460.82s
                               ETA: 1014615.8s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.139s, learning 0.158s)
               Value function loss: 98105.4775
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 1237.71
               Mean episode length: 50.17
                  Mean reward/step: 24.72
       Mean episode length/episode: 7.08
            Mean episode successes: 2.0728
Mean episode consecutive_successes: 4.6737
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 8.30s
                        Total time: 25469.12s
                               ETA: 1014521.6s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.282s, learning 0.159s)
               Value function loss: 99950.1240
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 1181.47
               Mean episode length: 52.40
                  Mean reward/step: 25.86
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2520
Mean episode consecutive_successes: 4.6393
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 8.44s
                        Total time: 25477.56s
                               ETA: 1014433.3s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.345s, learning 0.160s)
               Value function loss: 112352.3178
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 1735.41
               Mean episode length: 53.52
                  Mean reward/step: 25.54
       Mean episode length/episode: 7.01
            Mean episode successes: 2.1440
Mean episode consecutive_successes: 4.7355
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 8.51s
                        Total time: 25486.07s
                               ETA: 1014347.5s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.162s, learning 0.201s)
               Value function loss: 132975.9471
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 1476.66
               Mean episode length: 51.90
                  Mean reward/step: 25.04
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0913
Mean episode consecutive_successes: 4.7238
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 8.36s
                        Total time: 25494.43s
                               ETA: 1014256.1s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.099s, learning 0.183s)
               Value function loss: 107996.4648
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 1285.08
               Mean episode length: 49.24
                  Mean reward/step: 24.47
       Mean episode length/episode: 7.07
            Mean episode successes: 2.0112
Mean episode consecutive_successes: 4.7833
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 8.28s
                        Total time: 25502.71s
                               ETA: 1014161.6s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.249s, learning 0.159s)
               Value function loss: 119277.8273
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 1068.97
               Mean episode length: 52.41
                  Mean reward/step: 25.24
       Mean episode length/episode: 7.01
            Mean episode successes: 2.0879
Mean episode consecutive_successes: 4.7567
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 8.41s
                        Total time: 25511.12s
                               ETA: 1014072.2s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1994 steps/s (collection: 7.974s, learning 0.240s)
               Value function loss: 121111.5373
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 1149.67
               Mean episode length: 49.48
                  Mean reward/step: 24.03
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0688
Mean episode consecutive_successes: 4.7708
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 8.21s
                        Total time: 25519.33s
                               ETA: 1013975.1s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.404s, learning 0.184s)
               Value function loss: 104257.1287
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 1295.68
               Mean episode length: 46.78
                  Mean reward/step: 25.71
       Mean episode length/episode: 7.06
            Mean episode successes: 2.1362
Mean episode consecutive_successes: 4.7862
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 8.59s
                        Total time: 25527.92s
                               ETA: 1013892.9s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.954s, learning 0.202s)
               Value function loss: 103444.4957
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 1708.17
               Mean episode length: 54.08
                  Mean reward/step: 25.77
       Mean episode length/episode: 6.98
            Mean episode successes: 2.0010
Mean episode consecutive_successes: 4.8808
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 8.16s
                        Total time: 25536.08s
                               ETA: 1013793.7s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.223s, learning 0.169s)
               Value function loss: 94969.9768
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 1323.27
               Mean episode length: 49.86
                  Mean reward/step: 23.43
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0596
Mean episode consecutive_successes: 4.8171
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 8.39s
                        Total time: 25544.47s
                               ETA: 1013703.9s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.032s, learning 0.163s)
               Value function loss: 101862.1955
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 1398.45
               Mean episode length: 50.98
                  Mean reward/step: 23.81
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0713
Mean episode consecutive_successes: 4.8261
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 8.19s
                        Total time: 25552.66s
                               ETA: 1013606.3s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.973s, learning 0.182s)
               Value function loss: 102004.1227
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 964.39
               Mean episode length: 48.60
                  Mean reward/step: 23.34
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0537
Mean episode consecutive_successes: 4.7862
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 8.16s
                        Total time: 25560.82s
                               ETA: 1013507.3s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.174s, learning 0.159s)
               Value function loss: 95141.5254
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 1420.03
               Mean episode length: 52.69
                  Mean reward/step: 23.61
       Mean episode length/episode: 7.01
            Mean episode successes: 1.9565
Mean episode consecutive_successes: 4.8120
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 8.33s
                        Total time: 25569.15s
                               ETA: 1013415.3s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.432s, learning 0.166s)
               Value function loss: 82096.9512
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 1101.62
               Mean episode length: 48.08
                  Mean reward/step: 21.39
       Mean episode length/episode: 6.96
            Mean episode successes: 1.8315
Mean episode consecutive_successes: 4.8043
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 8.60s
                        Total time: 25577.75s
                               ETA: 1013333.9s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.910s, learning 0.158s)
               Value function loss: 80077.8676
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 983.81
               Mean episode length: 48.04
                  Mean reward/step: 20.99
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9604
Mean episode consecutive_successes: 4.6993
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 8.07s
                        Total time: 25585.82s
                               ETA: 1013231.6s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.169s)
               Value function loss: 78742.2268
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 1116.33
               Mean episode length: 47.85
                  Mean reward/step: 20.59
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8228
Mean episode consecutive_successes: 4.7032
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 8.62s
                        Total time: 25594.44s
                               ETA: 1013151.2s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.468s, learning 0.273s)
               Value function loss: 83641.6223
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 1171.95
               Mean episode length: 48.70
                  Mean reward/step: 20.17
       Mean episode length/episode: 7.04
            Mean episode successes: 1.7651
Mean episode consecutive_successes: 4.6958
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 8.74s
                        Total time: 25603.18s
                               ETA: 1013075.7s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.255s, learning 0.164s)
               Value function loss: 100375.9752
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 1144.64
               Mean episode length: 53.13
                  Mean reward/step: 22.67
       Mean episode length/episode: 6.97
            Mean episode successes: 1.7368
Mean episode consecutive_successes: 4.6910
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 8.42s
                        Total time: 25611.60s
                               ETA: 1012987.5s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.205s, learning 0.168s)
               Value function loss: 101487.9771
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 1154.70
               Mean episode length: 52.59
                  Mean reward/step: 24.81
       Mean episode length/episode: 7.10
            Mean episode successes: 1.9077
Mean episode consecutive_successes: 4.6589
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 8.37s
                        Total time: 25619.97s
                               ETA: 1012897.5s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.163s)
               Value function loss: 139473.0359
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 1306.68
               Mean episode length: 47.91
                  Mean reward/step: 23.53
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9443
Mean episode consecutive_successes: 4.6308
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 8.29s
                        Total time: 25628.26s
                               ETA: 1012804.5s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.070s, learning 0.167s)
               Value function loss: 118851.5477
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 838.24
               Mean episode length: 51.43
                  Mean reward/step: 24.16
       Mean episode length/episode: 6.96
            Mean episode successes: 1.9658
Mean episode consecutive_successes: 4.6075
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 8.24s
                        Total time: 25636.50s
                               ETA: 1012709.3s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.048s, learning 0.159s)
               Value function loss: 101579.9805
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 1224.48
               Mean episode length: 52.33
                  Mean reward/step: 26.80
       Mean episode length/episode: 7.06
            Mean episode successes: 2.1362
Mean episode consecutive_successes: 4.6037
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 8.21s
                        Total time: 25644.71s
                               ETA: 1012613.0s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.510s, learning 0.190s)
               Value function loss: 104316.8912
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 1343.71
               Mean episode length: 51.01
                  Mean reward/step: 27.53
       Mean episode length/episode: 6.98
            Mean episode successes: 2.1201
Mean episode consecutive_successes: 4.6581
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 8.70s
                        Total time: 25653.41s
                               ETA: 1012536.2s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.419s, learning 0.162s)
               Value function loss: 98277.5488
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 1522.18
               Mean episode length: 52.63
                  Mean reward/step: 25.03
       Mean episode length/episode: 6.98
            Mean episode successes: 2.1060
Mean episode consecutive_successes: 4.6925
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 12.58s
                        Total time: 25665.99s
                               ETA: 1012612.6s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.879s, learning 0.186s)
               Value function loss: 105865.9547
                    Surrogate loss: 0.0082
             Mean action noise std: 0.73
                       Mean reward: 1196.14
               Mean episode length: 47.24
                  Mean reward/step: 26.36
       Mean episode length/episode: 7.07
            Mean episode successes: 2.1572
Mean episode consecutive_successes: 4.7106
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 16.07s
                        Total time: 25682.06s
                               ETA: 1012826.4s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.086s, learning 0.165s)
               Value function loss: 99839.2711
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 1289.86
               Mean episode length: 48.79
                  Mean reward/step: 28.20
       Mean episode length/episode: 7.08
            Mean episode successes: 2.3228
Mean episode consecutive_successes: 4.7331
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 16.25s
                        Total time: 25698.31s
                               ETA: 1013047.2s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.000s, learning 0.213s)
               Value function loss: 106673.5742
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 1274.00
               Mean episode length: 52.37
                  Mean reward/step: 27.82
       Mean episode length/episode: 6.99
            Mean episode successes: 2.3594
Mean episode consecutive_successes: 4.7807
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 16.21s
                        Total time: 25714.52s
                               ETA: 1013266.4s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.052s, learning 0.235s)
               Value function loss: 94971.4268
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 1208.68
               Mean episode length: 51.21
                  Mean reward/step: 25.17
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2441
Mean episode consecutive_successes: 4.8574
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 16.29s
                        Total time: 25730.81s
                               ETA: 1013488.3s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.694s, learning 0.160s)
               Value function loss: 94766.6607
                    Surrogate loss: 0.0063
             Mean action noise std: 0.73
                       Mean reward: 1383.83
               Mean episode length: 51.26
                  Mean reward/step: 25.31
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2056
Mean episode consecutive_successes: 4.8819
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 15.85s
                        Total time: 25746.66s
                               ETA: 1013693.0s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.157s, learning 0.227s)
               Value function loss: 94097.6088
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 1659.91
               Mean episode length: 52.67
                  Mean reward/step: 22.44
       Mean episode length/episode: 7.02
            Mean episode successes: 2.0420
Mean episode consecutive_successes: 4.9648
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 16.38s
                        Total time: 25763.05s
                               ETA: 1013918.3s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.750s, learning 0.163s)
               Value function loss: 87041.2395
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 1225.48
               Mean episode length: 49.59
                  Mean reward/step: 22.09
       Mean episode length/episode: 6.92
            Mean episode successes: 1.9810
Mean episode consecutive_successes: 4.8951
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 15.91s
                        Total time: 25778.96s
                               ETA: 1014124.9s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.233s, learning 0.169s)
               Value function loss: 110627.7656
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 1299.15
               Mean episode length: 51.34
                  Mean reward/step: 26.42
       Mean episode length/episode: 7.08
            Mean episode successes: 2.0879
Mean episode consecutive_successes: 4.8965
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 16.40s
                        Total time: 25795.36s
                               ETA: 1014350.6s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.233s, learning 0.159s)
               Value function loss: 112776.5656
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 1053.02
               Mean episode length: 50.15
                  Mean reward/step: 23.82
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0347
Mean episode consecutive_successes: 4.8984
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 16.39s
                        Total time: 25811.75s
                               ETA: 1014575.7s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.077s, learning 0.166s)
               Value function loss: 104919.9621
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 1062.45
               Mean episode length: 48.63
                  Mean reward/step: 23.10
       Mean episode length/episode: 7.07
            Mean episode successes: 2.0723
Mean episode consecutive_successes: 4.8612
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 16.24s
                        Total time: 25828.00s
                               ETA: 1014794.7s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.013s, learning 0.169s)
               Value function loss: 113710.9164
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 1196.01
               Mean episode length: 50.41
                  Mean reward/step: 23.40
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0801
Mean episode consecutive_successes: 4.8659
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 16.18s
                        Total time: 25844.18s
                               ETA: 1015011.1s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.340s, learning 0.160s)
               Value function loss: 90137.2875
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 1371.17
               Mean episode length: 51.43
                  Mean reward/step: 25.45
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1265
Mean episode consecutive_successes: 4.8672
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 16.50s
                        Total time: 25860.68s
                               ETA: 1015239.8s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.674s, learning 0.159s)
               Value function loss: 85121.8336
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 1179.11
               Mean episode length: 51.59
                  Mean reward/step: 24.07
       Mean episode length/episode: 6.96
            Mean episode successes: 2.1138
Mean episode consecutive_successes: 4.8345
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 15.83s
                        Total time: 25876.51s
                               ETA: 1015442.2s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.020s, learning 0.233s)
               Value function loss: 96294.1574
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 1312.83
               Mean episode length: 53.64
                  Mean reward/step: 24.89
       Mean episode length/episode: 6.96
            Mean episode successes: 2.1084
Mean episode consecutive_successes: 4.8256
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 16.25s
                        Total time: 25892.76s
                               ETA: 1015660.8s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.072s, learning 0.165s)
               Value function loss: 88928.9973
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 1622.55
               Mean episode length: 52.97
                  Mean reward/step: 25.71
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2188
Mean episode consecutive_successes: 4.8295
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 16.24s
                        Total time: 25909.00s
                               ETA: 1015878.7s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1011 steps/s (collection: 15.994s, learning 0.211s)
               Value function loss: 96339.3242
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 1399.86
               Mean episode length: 49.36
                  Mean reward/step: 24.68
       Mean episode length/episode: 6.98
            Mean episode successes: 2.1226
Mean episode consecutive_successes: 4.8828
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 16.21s
                        Total time: 25925.21s
                               ETA: 1016095.1s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.288s, learning 0.181s)
               Value function loss: 97157.7658
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 1272.61
               Mean episode length: 49.90
                  Mean reward/step: 23.47
       Mean episode length/episode: 6.93
            Mean episode successes: 1.9634
Mean episode consecutive_successes: 4.8930
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 16.47s
                        Total time: 25941.67s
                               ETA: 1016321.6s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.106s, learning 0.159s)
               Value function loss: 89636.1764
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 981.50
               Mean episode length: 47.56
                  Mean reward/step: 22.67
       Mean episode length/episode: 7.01
            Mean episode successes: 1.8970
Mean episode consecutive_successes: 4.8508
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 16.27s
                        Total time: 25957.94s
                               ETA: 1016540.0s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.937s, learning 0.160s)
               Value function loss: 101268.2711
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 1353.35
               Mean episode length: 50.31
                  Mean reward/step: 23.90
       Mean episode length/episode: 7.07
            Mean episode successes: 1.9556
Mean episode consecutive_successes: 4.8230
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 16.10s
                        Total time: 25974.04s
                               ETA: 1016751.6s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.919s, learning 0.173s)
               Value function loss: 106621.8170
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 1179.48
               Mean episode length: 47.93
                  Mean reward/step: 25.22
       Mean episode length/episode: 6.95
            Mean episode successes: 2.0557
Mean episode consecutive_successes: 4.7976
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 16.09s
                        Total time: 25990.13s
                               ETA: 1016962.8s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.923s, learning 0.174s)
               Value function loss: 105308.5045
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 1171.30
               Mean episode length: 51.18
                  Mean reward/step: 23.70
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0234
Mean episode consecutive_successes: 4.7999
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 16.10s
                        Total time: 26006.23s
                               ETA: 1017174.1s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.255s, learning 0.161s)
               Value function loss: 122029.6816
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 964.37
               Mean episode length: 48.28
                  Mean reward/step: 26.33
       Mean episode length/episode: 7.10
            Mean episode successes: 2.2910
Mean episode consecutive_successes: 4.7272
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 16.42s
                        Total time: 26022.64s
                               ETA: 1017397.6s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.032s, learning 0.210s)
               Value function loss: 107051.3883
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 1127.77
               Mean episode length: 50.08
                  Mean reward/step: 26.40
       Mean episode length/episode: 7.04
            Mean episode successes: 2.3208
Mean episode consecutive_successes: 4.7569
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 16.24s
                        Total time: 26038.88s
                               ETA: 1017614.2s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.306s, learning 0.211s)
               Value function loss: 133081.8090
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 1291.59
               Mean episode length: 51.33
                  Mean reward/step: 26.58
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2388
Mean episode consecutive_successes: 4.8604
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 16.52s
                        Total time: 26055.40s
                               ETA: 1017841.3s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.888s, learning 0.165s)
               Value function loss: 185695.7293
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 1100.03
               Mean episode length: 49.21
                  Mean reward/step: 29.16
       Mean episode length/episode: 7.04
            Mean episode successes: 2.5166
Mean episode consecutive_successes: 4.8049
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 16.05s
                        Total time: 26071.45s
                               ETA: 1018050.0s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.937s, learning 0.163s)
               Value function loss: 131325.3105
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 1709.95
               Mean episode length: 56.93
                  Mean reward/step: 27.89
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3970
Mean episode consecutive_successes: 4.9282
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 16.10s
                        Total time: 26087.55s
                               ETA: 1018260.4s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.324s, learning 0.167s)
               Value function loss: 94308.1596
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 1272.05
               Mean episode length: 49.09
                  Mean reward/step: 27.18
       Mean episode length/episode: 6.96
            Mean episode successes: 2.3394
Mean episode consecutive_successes: 4.9549
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 16.49s
                        Total time: 26104.04s
                               ETA: 1018485.9s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.329s, learning 0.218s)
               Value function loss: 89996.3988
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 1636.65
               Mean episode length: 55.32
                  Mean reward/step: 25.78
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2935
Mean episode consecutive_successes: 4.9930
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 16.55s
                        Total time: 26120.59s
                               ETA: 1018713.4s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.309s, learning 0.226s)
               Value function loss: 93953.3736
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 1627.82
               Mean episode length: 53.39
                  Mean reward/step: 25.81
       Mean episode length/episode: 7.04
            Mean episode successes: 2.4551
Mean episode consecutive_successes: 4.9493
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 16.53s
                        Total time: 26137.12s
                               ETA: 1018940.2s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.080s, learning 0.211s)
               Value function loss: 95343.0035
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 1421.05
               Mean episode length: 50.68
                  Mean reward/step: 23.97
       Mean episode length/episode: 7.09
            Mean episode successes: 2.3843
Mean episode consecutive_successes: 5.0047
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 16.29s
                        Total time: 26153.42s
                               ETA: 1019157.4s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.148s, learning 0.158s)
               Value function loss: 98839.5951
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 894.78
               Mean episode length: 48.93
                  Mean reward/step: 23.75
       Mean episode length/episode: 6.95
            Mean episode successes: 2.1245
Mean episode consecutive_successes: 5.0058
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 16.31s
                        Total time: 26169.72s
                               ETA: 1019374.9s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.674s, learning 0.167s)
               Value function loss: 92531.2168
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 1576.76
               Mean episode length: 52.46
                  Mean reward/step: 22.42
       Mean episode length/episode: 7.08
            Mean episode successes: 2.1387
Mean episode consecutive_successes: 5.0182
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 16.84s
                        Total time: 26186.56s
                               ETA: 1019613.1s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.207s, learning 0.160s)
               Value function loss: 98039.2342
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 735.94
               Mean episode length: 47.47
                  Mean reward/step: 23.84
       Mean episode length/episode: 6.95
            Mean episode successes: 2.0820
Mean episode consecutive_successes: 4.9527
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 16.37s
                        Total time: 26202.93s
                               ETA: 1019832.7s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.257s, learning 0.182s)
               Value function loss: 143865.2793
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 1223.86
               Mean episode length: 47.78
                  Mean reward/step: 24.39
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1040
Mean episode consecutive_successes: 4.9578
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 16.44s
                        Total time: 26219.37s
                               ETA: 1020054.8s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.016s, learning 0.187s)
               Value function loss: 119376.7482
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 1332.60
               Mean episode length: 50.04
                  Mean reward/step: 25.93
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2114
Mean episode consecutive_successes: 4.9147
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 16.20s
                        Total time: 26235.57s
                               ETA: 1020267.6s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.474s, learning 0.163s)
               Value function loss: 100507.2875
                    Surrogate loss: 0.0084
             Mean action noise std: 0.73
                       Mean reward: 1462.63
               Mean episode length: 52.76
                  Mean reward/step: 25.84
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2681
Mean episode consecutive_successes: 4.9220
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 16.64s
                        Total time: 26252.21s
                               ETA: 1020497.0s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.188s, learning 0.171s)
               Value function loss: 96025.6111
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 1190.71
               Mean episode length: 49.93
                  Mean reward/step: 26.98
       Mean episode length/episode: 7.09
            Mean episode successes: 2.4199
Mean episode consecutive_successes: 4.9060
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 16.36s
                        Total time: 26268.57s
                               ETA: 1020715.5s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.324s, learning 0.168s)
               Value function loss: 103386.3186
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 1278.69
               Mean episode length: 50.65
                  Mean reward/step: 26.25
       Mean episode length/episode: 7.07
            Mean episode successes: 2.3750
Mean episode consecutive_successes: 4.9669
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 9.49s
                        Total time: 26278.06s
                               ETA: 1020667.0s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.060s, learning 0.167s)
               Value function loss: 96030.0314
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 990.87
               Mean episode length: 47.14
                  Mean reward/step: 25.39
       Mean episode length/episode: 7.07
            Mean episode successes: 2.4351
Mean episode consecutive_successes: 4.9447
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 8.23s
                        Total time: 26286.29s
                               ETA: 1020569.5s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.638s, learning 0.160s)
               Value function loss: 112327.2852
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 1802.57
               Mean episode length: 53.04
                  Mean reward/step: 24.98
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3872
Mean episode consecutive_successes: 4.9893
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 8.80s
                        Total time: 26295.08s
                               ETA: 1020494.2s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.143s, learning 0.162s)
               Value function loss: 85992.1012
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 886.41
               Mean episode length: 48.09
                  Mean reward/step: 24.83
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2959
Mean episode consecutive_successes: 4.9707
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 8.30s
                        Total time: 26303.39s
                               ETA: 1020399.8s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.240s, learning 0.182s)
               Value function loss: 89357.9961
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 1532.62
               Mean episode length: 53.36
                  Mean reward/step: 25.15
       Mean episode length/episode: 6.97
            Mean episode successes: 2.2593
Mean episode consecutive_successes: 4.9963
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 8.42s
                        Total time: 26311.81s
                               ETA: 1020310.1s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.190s, learning 0.165s)
               Value function loss: 96714.8402
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 1364.62
               Mean episode length: 52.80
                  Mean reward/step: 24.87
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2524
Mean episode consecutive_successes: 4.9942
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 8.36s
                        Total time: 26320.17s
                               ETA: 1020217.8s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.964s, learning 0.159s)
               Value function loss: 108859.6219
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 1036.16
               Mean episode length: 47.70
                  Mean reward/step: 23.12
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3057
Mean episode consecutive_successes: 4.9418
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 8.12s
                        Total time: 26328.29s
                               ETA: 1020116.5s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.431s, learning 0.243s)
               Value function loss: 134697.5254
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 1326.40
               Mean episode length: 50.99
                  Mean reward/step: 22.39
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0767
Mean episode consecutive_successes: 4.9952
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 8.67s
                        Total time: 26336.96s
                               ETA: 1020036.7s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.335s, learning 0.168s)
               Value function loss: 102401.1934
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 1283.50
               Mean episode length: 51.14
                  Mean reward/step: 22.18
       Mean episode length/episode: 7.05
            Mean episode successes: 1.9590
Mean episode consecutive_successes: 5.0119
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 8.50s
                        Total time: 26345.47s
                               ETA: 1019950.4s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.305s, learning 0.169s)
               Value function loss: 80731.7279
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 778.22
               Mean episode length: 51.37
                  Mean reward/step: 22.00
       Mean episode length/episode: 7.09
            Mean episode successes: 1.9531
Mean episode consecutive_successes: 4.9542
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 8.47s
                        Total time: 26353.94s
                               ETA: 1019862.9s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.366s, learning 0.230s)
               Value function loss: 87618.8707
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 904.10
               Mean episode length: 48.08
                  Mean reward/step: 22.39
       Mean episode length/episode: 7.06
            Mean episode successes: 2.1079
Mean episode consecutive_successes: 4.8720
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 8.60s
                        Total time: 26362.54s
                               ETA: 1019780.3s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.163s)
               Value function loss: 92436.0830
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 1017.98
               Mean episode length: 49.79
                  Mean reward/step: 22.52
       Mean episode length/episode: 6.95
            Mean episode successes: 1.9697
Mean episode consecutive_successes: 4.8836
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 8.43s
                        Total time: 26370.97s
                               ETA: 1019691.3s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.162s)
               Value function loss: 81459.2721
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 787.70
               Mean episode length: 49.90
                  Mean reward/step: 23.82
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0444
Mean episode consecutive_successes: 4.8163
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 8.34s
                        Total time: 26379.30s
                               ETA: 1019598.7s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.283s, learning 0.179s)
               Value function loss: 87933.1377
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 1057.07
               Mean episode length: 52.45
                  Mean reward/step: 23.52
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0396
Mean episode consecutive_successes: 4.8093
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 8.46s
                        Total time: 26387.76s
                               ETA: 1019511.1s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.485s, learning 0.299s)
               Value function loss: 98775.6525
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 631.22
               Mean episode length: 47.36
                  Mean reward/step: 23.29
       Mean episode length/episode: 7.07
            Mean episode successes: 2.0786
Mean episode consecutive_successes: 4.7777
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 8.78s
                        Total time: 26396.55s
                               ETA: 1019435.9s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.469s, learning 0.165s)
               Value function loss: 107980.6975
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 868.67
               Mean episode length: 50.94
                  Mean reward/step: 22.54
       Mean episode length/episode: 7.08
            Mean episode successes: 2.1895
Mean episode consecutive_successes: 4.7394
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 8.63s
                        Total time: 26405.18s
                               ETA: 1019355.1s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.907s, learning 0.162s)
               Value function loss: 153291.7695
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 1261.59
               Mean episode length: 51.50
                  Mean reward/step: 24.70
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2393
Mean episode consecutive_successes: 4.7296
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 8.07s
                        Total time: 26413.25s
                               ETA: 1019252.4s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.704s, learning 0.194s)
               Value function loss: 97105.2240
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 881.40
               Mean episode length: 48.30
                  Mean reward/step: 24.88
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2969
Mean episode consecutive_successes: 4.6972
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 8.90s
                        Total time: 26422.15s
                               ETA: 1019181.9s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.407s, learning 0.162s)
               Value function loss: 88700.5273
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 1297.51
               Mean episode length: 49.12
                  Mean reward/step: 24.39
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1895
Mean episode consecutive_successes: 4.7741
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 8.57s
                        Total time: 26430.72s
                               ETA: 1019098.6s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.429s, learning 0.159s)
               Value function loss: 91763.0162
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 1011.56
               Mean episode length: 51.79
                  Mean reward/step: 23.61
       Mean episode length/episode: 7.15
            Mean episode successes: 2.3145
Mean episode consecutive_successes: 4.7475
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 8.59s
                        Total time: 26439.31s
                               ETA: 1019016.2s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.270s, learning 0.166s)
               Value function loss: 96980.7611
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 1217.63
               Mean episode length: 54.00
                  Mean reward/step: 24.11
       Mean episode length/episode: 6.91
            Mean episode successes: 2.1729
Mean episode consecutive_successes: 4.7774
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 8.44s
                        Total time: 26447.74s
                               ETA: 1018928.0s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.297s, learning 0.235s)
               Value function loss: 84431.3377
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 1076.16
               Mean episode length: 47.43
                  Mean reward/step: 22.24
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1895
Mean episode consecutive_successes: 4.7289
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 8.53s
                        Total time: 26456.27s
                               ETA: 1018843.6s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.941s, learning 0.162s)
               Value function loss: 94261.4576
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 1455.38
               Mean episode length: 53.42
                  Mean reward/step: 24.72
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0996
Mean episode consecutive_successes: 4.7826
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 8.10s
                        Total time: 26464.38s
                               ETA: 1018742.7s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.027s, learning 0.283s)
               Value function loss: 84868.1867
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 760.89
               Mean episode length: 46.86
                  Mean reward/step: 23.25
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0664
Mean episode consecutive_successes: 4.7866
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 8.31s
                        Total time: 26472.69s
                               ETA: 1018649.8s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.154s, learning 0.164s)
               Value function loss: 94538.4166
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 953.77
               Mean episode length: 51.25
                  Mean reward/step: 23.69
       Mean episode length/episode: 7.08
            Mean episode successes: 2.1050
Mean episode consecutive_successes: 4.7830
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 8.32s
                        Total time: 26481.01s
                               ETA: 1018557.3s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.187s, learning 0.163s)
               Value function loss: 97748.6838
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 1019.96
               Mean episode length: 49.13
                  Mean reward/step: 25.14
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1958
Mean episode consecutive_successes: 4.7247
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 8.35s
                        Total time: 26489.36s
                               ETA: 1018466.1s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.930s, learning 0.232s)
               Value function loss: 107293.2152
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 892.72
               Mean episode length: 49.49
                  Mean reward/step: 23.54
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2295
Mean episode consecutive_successes: 4.7070
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 8.16s
                        Total time: 26497.52s
                               ETA: 1018367.7s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.125s, learning 0.254s)
               Value function loss: 92882.5873
                    Surrogate loss: 0.0059
             Mean action noise std: 0.73
                       Mean reward: 870.19
               Mean episode length: 49.62
                  Mean reward/step: 21.71
       Mean episode length/episode: 7.02
            Mean episode successes: 2.1699
Mean episode consecutive_successes: 4.6942
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 8.38s
                        Total time: 26505.90s
                               ETA: 1018277.8s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.358s, learning 0.192s)
               Value function loss: 89265.0922
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 753.95
               Mean episode length: 48.33
                  Mean reward/step: 23.39
       Mean episode length/episode: 7.08
            Mean episode successes: 2.1782
Mean episode consecutive_successes: 4.6935
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 8.55s
                        Total time: 26514.45s
                               ETA: 1018194.5s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.039s, learning 0.261s)
               Value function loss: 92758.7063
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 1339.26
               Mean episode length: 52.12
                  Mean reward/step: 25.22
       Mean episode length/episode: 7.05
            Mean episode successes: 2.3047
Mean episode consecutive_successes: 4.7019
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 8.30s
                        Total time: 26522.75s
                               ETA: 1018101.6s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.165s)
               Value function loss: 104130.4432
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 1334.35
               Mean episode length: 52.57
                  Mean reward/step: 25.13
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2705
Mean episode consecutive_successes: 4.7133
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 8.29s
                        Total time: 26531.04s
                               ETA: 1018008.4s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.021s, learning 0.167s)
               Value function loss: 131381.1053
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 1342.59
               Mean episode length: 50.02
                  Mean reward/step: 23.22
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1953
Mean episode consecutive_successes: 4.7349
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 8.19s
                        Total time: 26539.22s
                               ETA: 1017911.4s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.131s, learning 0.206s)
               Value function loss: 100483.3727
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 1403.18
               Mean episode length: 50.52
                  Mean reward/step: 24.59
       Mean episode length/episode: 7.07
            Mean episode successes: 2.1807
Mean episode consecutive_successes: 4.7751
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 8.34s
                        Total time: 26547.56s
                               ETA: 1017820.1s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.602s, learning 0.165s)
               Value function loss: 83868.8240
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 875.85
               Mean episode length: 50.86
                  Mean reward/step: 24.73
       Mean episode length/episode: 6.95
            Mean episode successes: 2.1621
Mean episode consecutive_successes: 4.7654
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 8.77s
                        Total time: 26556.33s
                               ETA: 1017745.4s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.220s, learning 0.188s)
               Value function loss: 84847.7977
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 1304.26
               Mean episode length: 51.71
                  Mean reward/step: 24.37
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0620
Mean episode consecutive_successes: 4.8326
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 8.41s
                        Total time: 26564.74s
                               ETA: 1017657.0s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.227s, learning 0.169s)
               Value function loss: 83148.9064
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 1032.61
               Mean episode length: 49.55
                  Mean reward/step: 24.94
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1753
Mean episode consecutive_successes: 4.7845
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 8.40s
                        Total time: 26573.13s
                               ETA: 1017568.2s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.350s, learning 0.249s)
               Value function loss: 88579.5453
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 1108.84
               Mean episode length: 50.93
                  Mean reward/step: 24.85
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3042
Mean episode consecutive_successes: 4.7146
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 8.60s
                        Total time: 26581.73s
                               ETA: 1017487.3s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.104s, learning 0.188s)
               Value function loss: 84929.0779
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 1355.17
               Mean episode length: 49.38
                  Mean reward/step: 24.40
       Mean episode length/episode: 7.09
            Mean episode successes: 2.3008
Mean episode consecutive_successes: 4.7590
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 8.29s
                        Total time: 26590.02s
                               ETA: 1017394.6s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.905s, learning 0.185s)
               Value function loss: 90847.4119
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 1381.85
               Mean episode length: 52.36
                  Mean reward/step: 23.09
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2646
Mean episode consecutive_successes: 4.7463
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 8.09s
                        Total time: 26598.11s
                               ETA: 1017294.3s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.269s, learning 0.192s)
               Value function loss: 85979.1076
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 1606.11
               Mean episode length: 50.91
                  Mean reward/step: 22.68
       Mean episode length/episode: 7.06
            Mean episode successes: 2.1904
Mean episode consecutive_successes: 4.7829
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 8.46s
                        Total time: 26606.57s
                               ETA: 1017208.3s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.233s, learning 0.163s)
               Value function loss: 93269.2137
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 1189.62
               Mean episode length: 48.90
                  Mean reward/step: 23.09
       Mean episode length/episode: 6.97
            Mean episode successes: 2.0425
Mean episode consecutive_successes: 4.7741
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 8.40s
                        Total time: 26614.97s
                               ETA: 1017119.8s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.160s, learning 0.189s)
               Value function loss: 105113.4533
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 1410.08
               Mean episode length: 49.51
                  Mean reward/step: 23.25
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0747
Mean episode consecutive_successes: 4.7630
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 8.35s
                        Total time: 26623.32s
                               ETA: 1017029.6s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.881s, learning 0.175s)
               Value function loss: 110272.9635
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 1063.23
               Mean episode length: 50.26
                  Mean reward/step: 22.13
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9912
Mean episode consecutive_successes: 4.7414
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 8.06s
                        Total time: 26631.38s
                               ETA: 1016928.2s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.318s, learning 0.159s)
               Value function loss: 93430.1654
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 1076.01
               Mean episode length: 51.17
                  Mean reward/step: 21.50
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9951
Mean episode consecutive_successes: 4.7068
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 8.48s
                        Total time: 26639.85s
                               ETA: 1016843.1s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.161s)
               Value function loss: 99976.5938
                    Surrogate loss: -0.0213
             Mean action noise std: 0.73
                       Mean reward: 1114.20
               Mean episode length: 48.07
                  Mean reward/step: 23.03
       Mean episode length/episode: 7.06
            Mean episode successes: 2.0679
Mean episode consecutive_successes: 4.6760
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 8.31s
                        Total time: 26648.16s
                               ETA: 1016751.4s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.280s, learning 0.230s)
               Value function loss: 138467.4949
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 1000.15
               Mean episode length: 49.74
                  Mean reward/step: 23.40
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1558
Mean episode consecutive_successes: 4.6235
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 8.51s
                        Total time: 26656.67s
                               ETA: 1016667.6s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.291s, learning 0.161s)
               Value function loss: 99933.6090
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 844.62
               Mean episode length: 49.04
                  Mean reward/step: 22.16
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1538
Mean episode consecutive_successes: 4.5867
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 8.45s
                        Total time: 26665.12s
                               ETA: 1016581.7s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.434s, learning 0.164s)
               Value function loss: 97196.2645
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 1323.91
               Mean episode length: 51.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0845
Mean episode consecutive_successes: 4.6238
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 8.60s
                        Total time: 26673.72s
                               ETA: 1016501.3s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.912s, learning 0.187s)
               Value function loss: 91813.5434
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 810.47
               Mean episode length: 46.45
                  Mean reward/step: 21.47
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0234
Mean episode consecutive_successes: 4.6379
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 8.10s
                        Total time: 26681.82s
                               ETA: 1016402.0s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.169s)
               Value function loss: 90982.7000
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 1083.16
               Mean episode length: 50.69
                  Mean reward/step: 22.46
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1543
Mean episode consecutive_successes: 4.5678
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 8.45s
                        Total time: 26690.27s
                               ETA: 1016316.3s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.985s, learning 0.171s)
               Value function loss: 96760.1883
                    Surrogate loss: -0.0211
             Mean action noise std: 0.73
                       Mean reward: 1354.19
               Mean episode length: 51.65
                  Mean reward/step: 23.72
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1450
Mean episode consecutive_successes: 4.5658
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 8.16s
                        Total time: 26698.43s
                               ETA: 1016219.3s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.315s, learning 0.173s)
               Value function loss: 102015.8996
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 1398.97
               Mean episode length: 50.99
                  Mean reward/step: 24.40
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2393
Mean episode consecutive_successes: 4.5284
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 8.49s
                        Total time: 26706.92s
                               ETA: 1016135.1s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.132s, learning 0.164s)
               Value function loss: 98005.0650
                    Surrogate loss: -0.0203
             Mean action noise std: 0.73
                       Mean reward: 1177.46
               Mean episode length: 53.41
                  Mean reward/step: 22.90
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1924
Mean episode consecutive_successes: 4.5272
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 8.30s
                        Total time: 26715.21s
                               ETA: 1016043.5s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.332s, learning 0.232s)
               Value function loss: 106808.5818
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 1239.11
               Mean episode length: 47.46
                  Mean reward/step: 23.06
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1260
Mean episode consecutive_successes: 4.5651
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 8.56s
                        Total time: 26723.78s
                               ETA: 1015962.3s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.435s, learning 0.164s)
               Value function loss: 136410.3232
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 946.24
               Mean episode length: 51.08
                  Mean reward/step: 24.99
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2432
Mean episode consecutive_successes: 4.5503
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 8.60s
                        Total time: 26732.37s
                               ETA: 1015882.4s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.059s, learning 0.166s)
               Value function loss: 110317.0160
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 1588.82
               Mean episode length: 51.40
                  Mean reward/step: 24.19
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2358
Mean episode consecutive_successes: 4.6127
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 8.22s
                        Total time: 26740.60s
                               ETA: 1015788.3s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.298s, learning 0.173s)
               Value function loss: 94850.2344
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 1375.54
               Mean episode length: 49.97
                  Mean reward/step: 23.43
       Mean episode length/episode: 6.95
            Mean episode successes: 2.2710
Mean episode consecutive_successes: 4.5616
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 8.47s
                        Total time: 26749.07s
                               ETA: 1015703.7s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.228s, learning 0.280s)
               Value function loss: 74283.7719
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 1428.50
               Mean episode length: 51.00
                  Mean reward/step: 22.19
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2153
Mean episode consecutive_successes: 4.5832
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 8.51s
                        Total time: 26757.58s
                               ETA: 1015620.5s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.118s, learning 0.203s)
               Value function loss: 77015.0484
                    Surrogate loss: -0.0240
             Mean action noise std: 0.73
                       Mean reward: 1097.24
               Mean episode length: 49.03
                  Mean reward/step: 21.30
       Mean episode length/episode: 6.94
            Mean episode successes: 2.0415
Mean episode consecutive_successes: 4.6165
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 8.32s
                        Total time: 26765.90s
                               ETA: 1015530.3s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.173s, learning 0.169s)
               Value function loss: 78039.0404
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 1249.18
               Mean episode length: 48.21
                  Mean reward/step: 23.07
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0459
Mean episode consecutive_successes: 4.6055
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 8.34s
                        Total time: 26774.24s
                               ETA: 1015441.0s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.573s, learning 0.182s)
               Value function loss: 76495.9910
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 1190.72
               Mean episode length: 49.87
                  Mean reward/step: 22.94
       Mean episode length/episode: 7.09
            Mean episode successes: 2.0337
Mean episode consecutive_successes: 4.5937
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 8.75s
                        Total time: 26783.00s
                               ETA: 1015367.4s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.203s, learning 0.233s)
               Value function loss: 78426.3348
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 1080.73
               Mean episode length: 51.14
                  Mean reward/step: 21.11
       Mean episode length/episode: 7.01
            Mean episode successes: 2.0469
Mean episode consecutive_successes: 4.5699
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 8.44s
                        Total time: 26791.43s
                               ETA: 1015281.7s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1977 steps/s (collection: 7.995s, learning 0.290s)
               Value function loss: 88173.3779
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 1297.45
               Mean episode length: 49.46
                  Mean reward/step: 23.16
       Mean episode length/episode: 6.99
            Mean episode successes: 2.1216
Mean episode consecutive_successes: 4.5375
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 8.29s
                        Total time: 26799.72s
                               ETA: 1015190.4s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.166s)
               Value function loss: 94629.9559
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 1363.35
               Mean episode length: 50.43
                  Mean reward/step: 22.56
       Mean episode length/episode: 7.11
            Mean episode successes: 2.1792
Mean episode consecutive_successes: 4.5350
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 8.53s
                        Total time: 26808.25s
                               ETA: 1015108.5s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.219s, learning 0.200s)
               Value function loss: 85922.8033
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 1268.32
               Mean episode length: 51.13
                  Mean reward/step: 22.17
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0337
Mean episode consecutive_successes: 4.5741
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 8.42s
                        Total time: 26816.67s
                               ETA: 1015022.4s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.002s, learning 0.177s)
               Value function loss: 93868.7293
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 1276.84
               Mean episode length: 52.03
                  Mean reward/step: 21.54
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9980
Mean episode consecutive_successes: 4.5746
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 8.18s
                        Total time: 26824.85s
                               ETA: 1014927.2s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.150s, learning 0.168s)
               Value function loss: 111311.8662
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 1118.86
               Mean episode length: 47.32
                  Mean reward/step: 22.28
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9727
Mean episode consecutive_successes: 4.5409
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 8.32s
                        Total time: 26833.17s
                               ETA: 1014837.4s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.464s, learning 0.172s)
               Value function loss: 88786.5193
                    Surrogate loss: -0.0014
             Mean action noise std: 0.73
                       Mean reward: 1065.06
               Mean episode length: 49.87
                  Mean reward/step: 21.71
       Mean episode length/episode: 7.02
            Mean episode successes: 1.9883
Mean episode consecutive_successes: 4.5377
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 8.64s
                        Total time: 26841.80s
                               ETA: 1014759.7s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.216s, learning 0.161s)
               Value function loss: 81244.9631
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 778.41
               Mean episode length: 51.74
                  Mean reward/step: 19.90
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8901
Mean episode consecutive_successes: 4.5113
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 8.38s
                        Total time: 26850.18s
                               ETA: 1014672.2s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.359s, learning 0.161s)
               Value function loss: 76153.9002
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 1043.00
               Mean episode length: 46.47
                  Mean reward/step: 21.00
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9277
Mean episode consecutive_successes: 4.5079
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 8.52s
                        Total time: 26858.70s
                               ETA: 1014590.2s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.153s, learning 0.170s)
               Value function loss: 81000.8088
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 1150.66
               Mean episode length: 51.17
                  Mean reward/step: 20.29
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8691
Mean episode consecutive_successes: 4.4994
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 8.32s
                        Total time: 26867.02s
                               ETA: 1014500.9s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.391s, learning 0.242s)
               Value function loss: 88067.0348
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 1508.86
               Mean episode length: 51.43
                  Mean reward/step: 21.22
       Mean episode length/episode: 7.04
            Mean episode successes: 1.8862
Mean episode consecutive_successes: 4.5051
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 8.63s
                        Total time: 26875.66s
                               ETA: 1014423.2s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.342s, learning 0.191s)
               Value function loss: 76975.3062
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 1197.84
               Mean episode length: 49.87
                  Mean reward/step: 21.02
       Mean episode length/episode: 6.98
            Mean episode successes: 1.8218
Mean episode consecutive_successes: 4.4846
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 8.53s
                        Total time: 26884.19s
                               ETA: 1014341.9s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.178s, learning 0.188s)
               Value function loss: 75264.8684
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 599.31
               Mean episode length: 48.87
                  Mean reward/step: 21.33
       Mean episode length/episode: 7.09
            Mean episode successes: 2.0146
Mean episode consecutive_successes: 4.3747
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 8.37s
                        Total time: 26892.55s
                               ETA: 1014254.3s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.246s, learning 0.192s)
               Value function loss: 91288.5967
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 977.75
               Mean episode length: 54.22
                  Mean reward/step: 21.04
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0234
Mean episode consecutive_successes: 4.3667
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 8.44s
                        Total time: 26900.99s
                               ETA: 1014169.5s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.163s)
               Value function loss: 119209.7582
                    Surrogate loss: -0.0195
             Mean action noise std: 0.73
                       Mean reward: 914.59
               Mean episode length: 49.21
                  Mean reward/step: 22.02
       Mean episode length/episode: 6.92
            Mean episode successes: 1.9683
Mean episode consecutive_successes: 4.3394
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 8.66s
                        Total time: 26909.65s
                               ETA: 1014093.0s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.163s)
               Value function loss: 109754.7094
                    Surrogate loss: -0.0218
             Mean action noise std: 0.73
                       Mean reward: 1112.65
               Mean episode length: 49.93
                  Mean reward/step: 21.89
       Mean episode length/episode: 7.00
            Mean episode successes: 1.9131
Mean episode consecutive_successes: 4.3950
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 8.43s
                        Total time: 26918.08s
                               ETA: 1014008.0s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.363s, learning 0.158s)
               Value function loss: 82328.4881
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 1348.62
               Mean episode length: 51.01
                  Mean reward/step: 21.65
       Mean episode length/episode: 7.06
            Mean episode successes: 1.8765
Mean episode consecutive_successes: 4.4123
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 8.52s
                        Total time: 26926.60s
                               ETA: 1013926.5s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.747s, learning 0.165s)
               Value function loss: 80843.7111
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 1136.73
               Mean episode length: 48.91
                  Mean reward/step: 22.17
       Mean episode length/episode: 7.00
            Mean episode successes: 1.8403
Mean episode consecutive_successes: 4.4118
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 7.91s
                        Total time: 26934.51s
                               ETA: 1013822.1s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.138s, learning 0.178s)
               Value function loss: 83226.6738
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 1249.17
               Mean episode length: 52.27
                  Mean reward/step: 23.46
       Mean episode length/episode: 7.05
            Mean episode successes: 1.9688
Mean episode consecutive_successes: 4.3862
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 8.32s
                        Total time: 26942.83s
                               ETA: 1013733.0s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1987 steps/s (collection: 7.943s, learning 0.300s)
               Value function loss: 79509.9051
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 1138.36
               Mean episode length: 51.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 7.10
            Mean episode successes: 2.1304
Mean episode consecutive_successes: 4.3561
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 8.24s
                        Total time: 26951.07s
                               ETA: 1013641.2s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.507s, learning 0.162s)
               Value function loss: 90169.5518
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 1694.86
               Mean episode length: 52.86
                  Mean reward/step: 24.65
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2559
Mean episode consecutive_successes: 4.3957
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 8.67s
                        Total time: 26959.74s
                               ETA: 1013565.5s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.089s, learning 0.163s)
               Value function loss: 122291.7762
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 1047.45
               Mean episode length: 49.95
                  Mean reward/step: 25.31
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3862
Mean episode consecutive_successes: 4.3425
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 8.25s
                        Total time: 26967.99s
                               ETA: 1013474.2s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.106s, learning 0.162s)
               Value function loss: 116564.6912
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 1438.31
               Mean episode length: 53.54
                  Mean reward/step: 24.40
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3057
Mean episode consecutive_successes: 4.4013
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 8.27s
                        Total time: 26976.26s
                               ETA: 1013383.6s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.201s, learning 0.184s)
               Value function loss: 93153.9789
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 1480.08
               Mean episode length: 53.12
                  Mean reward/step: 25.22
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1528
Mean episode consecutive_successes: 4.5467
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 8.39s
                        Total time: 26984.65s
                               ETA: 1013297.4s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.145s, learning 0.206s)
               Value function loss: 90047.4795
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 1119.77
               Mean episode length: 48.69
                  Mean reward/step: 24.75
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2041
Mean episode consecutive_successes: 4.5430
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 8.35s
                        Total time: 26993.00s
                               ETA: 1013210.0s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.014s, learning 0.204s)
               Value function loss: 92643.4918
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 1533.32
               Mean episode length: 50.61
                  Mean reward/step: 25.59
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1636
Mean episode consecutive_successes: 4.6230
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 8.22s
                        Total time: 27001.22s
                               ETA: 1013117.7s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.057s, learning 0.164s)
               Value function loss: 92798.7957
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 1366.65
               Mean episode length: 51.30
                  Mean reward/step: 24.90
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1685
Mean episode consecutive_successes: 4.6567
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 8.22s
                        Total time: 27009.44s
                               ETA: 1013025.5s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.324s, learning 0.168s)
               Value function loss: 90161.4867
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 1147.56
               Mean episode length: 48.99
                  Mean reward/step: 25.58
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1821
Mean episode consecutive_successes: 4.6829
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 8.49s
                        Total time: 27017.93s
                               ETA: 1012943.6s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.278s, learning 0.167s)
               Value function loss: 94757.5477
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 1942.35
               Mean episode length: 52.62
                  Mean reward/step: 26.13
       Mean episode length/episode: 7.09
            Mean episode successes: 2.2695
Mean episode consecutive_successes: 4.7607
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 8.45s
                        Total time: 27026.37s
                               ETA: 1012859.9s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.078s, learning 0.231s)
               Value function loss: 94627.6857
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 1423.58
               Mean episode length: 50.45
                  Mean reward/step: 25.52
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2520
Mean episode consecutive_successes: 4.7714
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 8.31s
                        Total time: 27034.68s
                               ETA: 1012771.2s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.135s, learning 0.172s)
               Value function loss: 92795.6441
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 1612.72
               Mean episode length: 53.46
                  Mean reward/step: 24.02
       Mean episode length/episode: 7.09
            Mean episode successes: 2.1250
Mean episode consecutive_successes: 4.8456
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 8.31s
                        Total time: 27042.99s
                               ETA: 1012682.6s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.293s, learning 0.186s)
               Value function loss: 96199.4066
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 1136.16
               Mean episode length: 50.91
                  Mean reward/step: 22.82
       Mean episode length/episode: 7.03
            Mean episode successes: 1.9473
Mean episode consecutive_successes: 4.8964
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 8.48s
                        Total time: 27051.47s
                               ETA: 1012600.4s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.768s, learning 0.162s)
               Value function loss: 97583.3652
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 1509.07
               Mean episode length: 51.68
                  Mean reward/step: 22.13
       Mean episode length/episode: 7.00
            Mean episode successes: 1.7173
Mean episode consecutive_successes: 4.9455
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 7.93s
                        Total time: 27059.40s
                               ETA: 1012497.7s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.404s, learning 0.160s)
               Value function loss: 103529.4758
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 1381.62
               Mean episode length: 51.71
                  Mean reward/step: 22.24
       Mean episode length/episode: 7.00
            Mean episode successes: 1.6855
Mean episode consecutive_successes: 4.9334
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 8.56s
                        Total time: 27067.96s
                               ETA: 1012418.8s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.032s, learning 0.165s)
               Value function loss: 94255.6633
                    Surrogate loss: 0.0431
             Mean action noise std: 0.72
                       Mean reward: 1004.57
               Mean episode length: 48.60
                  Mean reward/step: 22.34
       Mean episode length/episode: 7.13
            Mean episode successes: 1.7705
Mean episode consecutive_successes: 4.8838
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 8.20s
                        Total time: 27076.16s
                               ETA: 1012326.2s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.332s, learning 0.167s)
               Value function loss: 87398.6744
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 1236.40
               Mean episode length: 51.45
                  Mean reward/step: 23.73
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8374
Mean episode consecutive_successes: 4.8530
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 8.50s
                        Total time: 27084.66s
                               ETA: 1012245.0s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.300s, learning 0.209s)
               Value function loss: 85310.5889
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 1010.58
               Mean episode length: 50.94
                  Mean reward/step: 24.86
       Mean episode length/episode: 7.06
            Mean episode successes: 1.9448
Mean episode consecutive_successes: 4.8002
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 8.51s
                        Total time: 27093.17s
                               ETA: 1012164.3s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.204s, learning 0.184s)
               Value function loss: 85818.8217
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 939.79
               Mean episode length: 48.96
                  Mean reward/step: 21.26
       Mean episode length/episode: 7.10
            Mean episode successes: 1.9424
Mean episode consecutive_successes: 4.7773
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 8.39s
                        Total time: 27101.56s
                               ETA: 1012079.0s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.076s, learning 0.164s)
               Value function loss: 88856.4941
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 973.65
               Mean episode length: 51.07
                  Mean reward/step: 22.31
       Mean episode length/episode: 6.97
            Mean episode successes: 1.8862
Mean episode consecutive_successes: 4.7641
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 8.24s
                        Total time: 27109.80s
                               ETA: 1011988.3s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.208s, learning 0.160s)
               Value function loss: 89450.8332
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 1309.09
               Mean episode length: 51.78
                  Mean reward/step: 22.66
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8403
Mean episode consecutive_successes: 4.7853
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 8.37s
                        Total time: 27118.17s
                               ETA: 1011902.4s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.879s, learning 0.184s)
               Value function loss: 81708.3598
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 1120.11
               Mean episode length: 53.41
                  Mean reward/step: 22.87
       Mean episode length/episode: 7.03
            Mean episode successes: 1.8843
Mean episode consecutive_successes: 4.7320
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 8.06s
                        Total time: 27126.23s
                               ETA: 1011805.3s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1986 steps/s (collection: 7.997s, learning 0.253s)
               Value function loss: 96258.0957
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 1039.45
               Mean episode length: 52.94
                  Mean reward/step: 21.88
       Mean episode length/episode: 7.02
            Mean episode successes: 1.7656
Mean episode consecutive_successes: 4.7606
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 8.25s
                        Total time: 27134.48s
                               ETA: 1011715.1s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.571s, learning 0.167s)
               Value function loss: 93310.1805
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 1077.47
               Mean episode length: 53.74
                  Mean reward/step: 21.71
       Mean episode length/episode: 7.12
            Mean episode successes: 1.8687
Mean episode consecutive_successes: 4.6916
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 8.74s
                        Total time: 27143.22s
                               ETA: 1011643.2s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.880s, learning 0.165s)
               Value function loss: 98007.9809
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 997.89
               Mean episode length: 50.02
                  Mean reward/step: 23.90
       Mean episode length/episode: 7.10
            Mean episode successes: 1.9180
Mean episode consecutive_successes: 4.6942
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 8.04s
                        Total time: 27151.26s
                               ETA: 1011545.5s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.168s, learning 0.161s)
               Value function loss: 115270.6076
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 782.10
               Mean episode length: 48.54
                  Mean reward/step: 25.22
       Mean episode length/episode: 7.08
            Mean episode successes: 2.0854
Mean episode consecutive_successes: 4.6515
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 8.33s
                        Total time: 27159.59s
                               ETA: 1011458.5s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.765s, learning 0.164s)
               Value function loss: 128763.5430
                    Surrogate loss: -0.0204
             Mean action noise std: 0.72
                       Mean reward: 1336.15
               Mean episode length: 51.52
                  Mean reward/step: 28.03
       Mean episode length/episode: 7.09
            Mean episode successes: 2.2739
Mean episode consecutive_successes: 4.6755
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 7.93s
                        Total time: 27167.52s
                               ETA: 1011356.7s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.198s, learning 0.171s)
               Value function loss: 138305.6748
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 1319.39
               Mean episode length: 52.82
                  Mean reward/step: 29.63
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3760
Mean episode consecutive_successes: 4.7242
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 8.37s
                        Total time: 27175.89s
                               ETA: 1011271.3s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.144s, learning 0.193s)
               Value function loss: 114967.4098
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 1519.48
               Mean episode length: 52.49
                  Mean reward/step: 29.45
       Mean episode length/episode: 7.10
            Mean episode successes: 2.5581
Mean episode consecutive_successes: 4.7957
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 8.34s
                        Total time: 27184.23s
                               ETA: 1011184.7s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.213s, learning 0.294s)
               Value function loss: 95367.7928
                    Surrogate loss: -0.0200
             Mean action noise std: 0.72
                       Mean reward: 1130.78
               Mean episode length: 50.76
                  Mean reward/step: 29.76
       Mean episode length/episode: 7.02
            Mean episode successes: 2.6943
Mean episode consecutive_successes: 4.8003
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 8.51s
                        Total time: 27192.73s
                               ETA: 1011104.6s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.068s, learning 0.159s)
               Value function loss: 113886.3062
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 1384.43
               Mean episode length: 52.00
                  Mean reward/step: 29.59
       Mean episode length/episode: 6.97
            Mean episode successes: 2.5117
Mean episode consecutive_successes: 4.9537
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 8.23s
                        Total time: 27200.96s
                               ETA: 1011014.0s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.246s, learning 0.160s)
               Value function loss: 103591.4428
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: 1462.86
               Mean episode length: 53.22
                  Mean reward/step: 27.89
       Mean episode length/episode: 7.07
            Mean episode successes: 2.5166
Mean episode consecutive_successes: 5.0116
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 8.41s
                        Total time: 27209.37s
                               ETA: 1010930.2s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.017s, learning 0.171s)
               Value function loss: 92459.6811
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 1740.46
               Mean episode length: 53.86
                  Mean reward/step: 26.72
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3691
Mean episode consecutive_successes: 5.1386
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 8.19s
                        Total time: 27217.55s
                               ETA: 1010838.4s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.235s, learning 0.190s)
               Value function loss: 91575.0215
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 1243.63
               Mean episode length: 51.06
                  Mean reward/step: 24.71
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3169
Mean episode consecutive_successes: 5.1269
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 8.43s
                        Total time: 27225.98s
                               ETA: 1010755.4s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.304s, learning 0.157s)
               Value function loss: 96950.5535
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 1225.80
               Mean episode length: 50.37
                  Mean reward/step: 24.57
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2471
Mean episode consecutive_successes: 5.1272
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 8.46s
                        Total time: 27234.44s
                               ETA: 1010673.8s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.056s, learning 0.160s)
               Value function loss: 93900.3637
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 1497.48
               Mean episode length: 52.95
                  Mean reward/step: 24.63
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2168
Mean episode consecutive_successes: 5.1565
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 8.22s
                        Total time: 27242.66s
                               ETA: 1010583.2s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.331s, learning 0.167s)
               Value function loss: 90141.9807
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 1069.58
               Mean episode length: 49.36
                  Mean reward/step: 23.97
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1055
Mean episode consecutive_successes: 5.1557
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 8.50s
                        Total time: 27251.15s
                               ETA: 1010503.1s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.983s, learning 0.163s)
               Value function loss: 102276.8604
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 1134.33
               Mean episode length: 51.96
                  Mean reward/step: 26.59
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3672
Mean episode consecutive_successes: 5.0782
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 8.15s
                        Total time: 27259.30s
                               ETA: 1010410.0s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.093s, learning 0.165s)
               Value function loss: 141678.7344
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 1364.88
               Mean episode length: 53.32
                  Mean reward/step: 26.22
       Mean episode length/episode: 7.07
            Mean episode successes: 2.4712
Mean episode consecutive_successes: 5.0403
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 8.26s
                        Total time: 27267.56s
                               ETA: 1010321.1s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.252s, learning 0.291s)
               Value function loss: 135466.1535
                    Surrogate loss: 0.0029
             Mean action noise std: 0.72
                       Mean reward: 1325.70
               Mean episode length: 54.07
                  Mean reward/step: 24.87
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3008
Mean episode consecutive_successes: 5.1039
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 8.54s
                        Total time: 27276.10s
                               ETA: 1010242.9s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.151s, learning 0.166s)
               Value function loss: 87446.0123
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 1494.57
               Mean episode length: 52.59
                  Mean reward/step: 25.17
       Mean episode length/episode: 7.13
            Mean episode successes: 2.3418
Mean episode consecutive_successes: 5.1253
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 8.32s
                        Total time: 27284.42s
                               ETA: 1010156.3s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.733s, learning 0.165s)
               Value function loss: 98086.1332
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: 1377.94
               Mean episode length: 50.06
                  Mean reward/step: 26.15
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3896
Mean episode consecutive_successes: 5.1241
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 7.90s
                        Total time: 27292.32s
                               ETA: 1010054.3s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.711s, learning 0.161s)
               Value function loss: 90865.3932
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 1316.87
               Mean episode length: 52.23
                  Mean reward/step: 26.49
       Mean episode length/episode: 6.99
            Mean episode successes: 2.3242
Mean episode consecutive_successes: 5.1639
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 8.87s
                        Total time: 27301.19s
                               ETA: 1009988.3s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.971s, learning 0.211s)
               Value function loss: 101987.9930
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 1232.61
               Mean episode length: 53.13
                  Mean reward/step: 27.41
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2842
Mean episode consecutive_successes: 5.1801
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 8.18s
                        Total time: 27309.37s
                               ETA: 1009896.9s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.131s, learning 0.188s)
               Value function loss: 138041.7711
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 1033.11
               Mean episode length: 46.55
                  Mean reward/step: 26.91
       Mean episode length/episode: 7.05
            Mean episode successes: 2.3745
Mean episode consecutive_successes: 5.1429
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 8.32s
                        Total time: 27317.69s
                               ETA: 1009810.7s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.196s, learning 0.198s)
               Value function loss: 99777.7863
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 1470.42
               Mean episode length: 49.86
                  Mean reward/step: 25.45
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3706
Mean episode consecutive_successes: 5.2005
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 8.39s
                        Total time: 27326.08s
                               ETA: 1009727.2s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.195s, learning 0.172s)
               Value function loss: 105011.8514
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 1231.85
               Mean episode length: 51.74
                  Mean reward/step: 25.39
       Mean episode length/episode: 6.98
            Mean episode successes: 2.1655
Mean episode consecutive_successes: 5.2375
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 8.37s
                        Total time: 27334.45s
                               ETA: 1009642.8s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.108s, learning 0.160s)
               Value function loss: 106184.5344
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 1375.08
               Mean episode length: 53.45
                  Mean reward/step: 25.82
       Mean episode length/episode: 7.11
            Mean episode successes: 2.2852
Mean episode consecutive_successes: 5.2000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 8.27s
                        Total time: 27342.72s
                               ETA: 1009554.9s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.159s, learning 0.267s)
               Value function loss: 115630.0297
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 1509.53
               Mean episode length: 52.57
                  Mean reward/step: 26.31
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2021
Mean episode consecutive_successes: 5.2637
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 8.43s
                        Total time: 27351.14s
                               ETA: 1009472.8s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.250s, learning 0.197s)
               Value function loss: 148679.6582
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 905.77
               Mean episode length: 46.46
                  Mean reward/step: 27.92
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3032
Mean episode consecutive_successes: 5.1893
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 8.45s
                        Total time: 27359.59s
                               ETA: 1009391.6s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.907s, learning 0.174s)
               Value function loss: 104862.1109
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 1575.29
               Mean episode length: 54.11
                  Mean reward/step: 28.64
       Mean episode length/episode: 7.12
            Mean episode successes: 2.5181
Mean episode consecutive_successes: 5.2049
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 8.08s
                        Total time: 27367.67s
                               ETA: 1009296.9s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.868s, learning 0.163s)
               Value function loss: 110084.2340
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 1431.76
               Mean episode length: 51.94
                  Mean reward/step: 28.00
       Mean episode length/episode: 7.04
            Mean episode successes: 2.4272
Mean episode consecutive_successes: 5.2536
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 8.03s
                        Total time: 27375.70s
                               ETA: 1009200.4s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.978s, learning 0.160s)
               Value function loss: 106491.3240
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 1960.27
               Mean episode length: 53.19
                  Mean reward/step: 28.66
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4565
Mean episode consecutive_successes: 5.2835
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 8.14s
                        Total time: 27383.84s
                               ETA: 1009108.0s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.887s, learning 0.159s)
               Value function loss: 113472.9873
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 1298.14
               Mean episode length: 50.56
                  Mean reward/step: 28.70
       Mean episode length/episode: 6.99
            Mean episode successes: 2.4731
Mean episode consecutive_successes: 5.3194
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 8.05s
                        Total time: 27391.89s
                               ETA: 1009012.2s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.799s, learning 0.166s)
               Value function loss: 106815.8758
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 1310.00
               Mean episode length: 49.63
                  Mean reward/step: 27.61
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4883
Mean episode consecutive_successes: 5.2882
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 7.97s
                        Total time: 27399.85s
                               ETA: 1008913.5s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.018s, learning 0.159s)
               Value function loss: 114551.8439
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 1515.88
               Mean episode length: 50.88
                  Mean reward/step: 25.06
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2988
Mean episode consecutive_successes: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 8.18s
                        Total time: 27408.03s
                               ETA: 1008822.7s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.123s, learning 0.191s)
               Value function loss: 110505.1881
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 1493.87
               Mean episode length: 50.86
                  Mean reward/step: 22.97
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1348
Mean episode consecutive_successes: 5.3799
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 8.31s
                        Total time: 27416.34s
                               ETA: 1008737.0s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.107s, learning 0.202s)
               Value function loss: 93543.6922
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 1127.49
               Mean episode length: 49.77
                  Mean reward/step: 24.73
       Mean episode length/episode: 7.08
            Mean episode successes: 2.2192
Mean episode consecutive_successes: 5.2806
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 8.31s
                        Total time: 27424.65s
                               ETA: 1008651.1s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.514s, learning 0.162s)
               Value function loss: 99750.2275
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 1284.53
               Mean episode length: 52.70
                  Mean reward/step: 25.69
       Mean episode length/episode: 7.07
            Mean episode successes: 2.1147
Mean episode consecutive_successes: 5.3243
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 8.68s
                        Total time: 27433.33s
                               ETA: 1008578.8s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.324s, learning 0.157s)
               Value function loss: 105963.1350
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 746.12
               Mean episode length: 47.88
                  Mean reward/step: 26.44
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2808
Mean episode consecutive_successes: 5.2417
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 16.48s
                        Total time: 27449.81s
                               ETA: 1008793.4s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.921s, learning 0.208s)
               Value function loss: 120767.7090
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 1442.75
               Mean episode length: 49.88
                  Mean reward/step: 28.81
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3921
Mean episode consecutive_successes: 5.2791
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 16.13s
                        Total time: 27465.94s
                               ETA: 1008994.9s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.079s, learning 0.162s)
               Value function loss: 151133.8873
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: 975.17
               Mean episode length: 50.19
                  Mean reward/step: 27.94
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4648
Mean episode consecutive_successes: 5.2303
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 16.24s
                        Total time: 27482.18s
                               ETA: 1009200.3s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.196s, learning 0.159s)
               Value function loss: 117542.5811
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 1304.73
               Mean episode length: 52.71
                  Mean reward/step: 27.42
       Mean episode length/episode: 7.06
            Mean episode successes: 2.4761
Mean episode consecutive_successes: 5.2380
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 16.35s
                        Total time: 27498.53s
                               ETA: 1009409.8s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.088s, learning 0.183s)
               Value function loss: 89769.3512
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 1116.32
               Mean episode length: 51.81
                  Mean reward/step: 26.95
       Mean episode length/episode: 7.07
            Mean episode successes: 2.5137
Mean episode consecutive_successes: 5.2729
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 16.27s
                        Total time: 27514.80s
                               ETA: 1009616.0s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.366s, learning 0.218s)
               Value function loss: 103603.7590
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 1429.84
               Mean episode length: 49.51
                  Mean reward/step: 25.93
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3877
Mean episode consecutive_successes: 5.3331
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 16.58s
                        Total time: 27531.39s
                               ETA: 1009833.5s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.992s, learning 0.159s)
               Value function loss: 100850.6135
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 1229.06
               Mean episode length: 51.50
                  Mean reward/step: 26.20
       Mean episode length/episode: 6.92
            Mean episode successes: 2.4111
Mean episode consecutive_successes: 5.2719
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 16.15s
                        Total time: 27547.54s
                               ETA: 1010034.9s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.040s, learning 0.167s)
               Value function loss: 97394.3979
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 900.11
               Mean episode length: 50.22
                  Mean reward/step: 27.34
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3828
Mean episode consecutive_successes: 5.2852
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 16.21s
                        Total time: 27563.75s
                               ETA: 1010238.3s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.142s, learning 0.159s)
               Value function loss: 97972.0658
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 1220.96
               Mean episode length: 51.31
                  Mean reward/step: 26.33
       Mean episode length/episode: 7.07
            Mean episode successes: 2.4053
Mean episode consecutive_successes: 5.3183
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 16.30s
                        Total time: 27580.05s
                               ETA: 1010444.9s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.893s, learning 0.160s)
               Value function loss: 109366.8936
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 934.11
               Mean episode length: 47.86
                  Mean reward/step: 25.03
       Mean episode length/episode: 7.07
            Mean episode successes: 2.2793
Mean episode consecutive_successes: 5.3620
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 16.05s
                        Total time: 27596.10s
                               ETA: 1010642.2s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.168s, learning 0.162s)
               Value function loss: 126633.6717
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 934.31
               Mean episode length: 47.99
                  Mean reward/step: 27.40
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3984
Mean episode consecutive_successes: 5.3067
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 16.33s
                        Total time: 27612.43s
                               ETA: 1010849.6s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.137s, learning 0.160s)
               Value function loss: 104501.4357
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 1593.12
               Mean episode length: 50.52
                  Mean reward/step: 28.60
       Mean episode length/episode: 7.06
            Mean episode successes: 2.4312
Mean episode consecutive_successes: 5.3848
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 16.30s
                        Total time: 27628.73s
                               ETA: 1011055.6s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.293s, learning 0.155s)
               Value function loss: 89518.8912
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 1333.42
               Mean episode length: 50.49
                  Mean reward/step: 26.96
       Mean episode length/episode: 7.01
            Mean episode successes: 2.4487
Mean episode consecutive_successes: 5.3576
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 16.45s
                        Total time: 27645.17s
                               ETA: 1011266.9s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.773s, learning 0.161s)
               Value function loss: 95600.4453
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 1329.92
               Mean episode length: 53.18
                  Mean reward/step: 26.18
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4121
Mean episode consecutive_successes: 5.3166
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 15.93s
                        Total time: 27661.11s
                               ETA: 1011459.3s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.307s, learning 0.167s)
               Value function loss: 90800.3891
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 1469.44
               Mean episode length: 52.14
                  Mean reward/step: 27.77
       Mean episode length/episode: 7.08
            Mean episode successes: 2.4736
Mean episode consecutive_successes: 5.3629
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 16.47s
                        Total time: 27677.58s
                               ETA: 1011671.2s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.397s, learning 0.160s)
               Value function loss: 95075.3625
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: 1331.03
               Mean episode length: 50.60
                  Mean reward/step: 27.07
       Mean episode length/episode: 6.99
            Mean episode successes: 2.4028
Mean episode consecutive_successes: 5.3785
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 16.56s
                        Total time: 27694.14s
                               ETA: 1011886.1s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.561s, learning 0.233s)
               Value function loss: 102070.0348
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 1255.54
               Mean episode length: 50.18
                  Mean reward/step: 26.62
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3213
Mean episode consecutive_successes: 5.3938
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 15.79s
                        Total time: 27709.93s
                               ETA: 1012072.8s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.980s, learning 0.172s)
               Value function loss: 94679.8975
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 1252.87
               Mean episode length: 49.57
                  Mean reward/step: 26.91
       Mean episode length/episode: 7.14
            Mean episode successes: 2.3706
Mean episode consecutive_successes: 5.4124
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 16.15s
                        Total time: 27726.09s
                               ETA: 1012272.5s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.179s, learning 0.180s)
               Value function loss: 105128.8451
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 1310.52
               Mean episode length: 49.95
                  Mean reward/step: 28.56
       Mean episode length/episode: 7.04
            Mean episode successes: 2.4707
Mean episode consecutive_successes: 5.3753
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 16.36s
                        Total time: 27742.44s
                               ETA: 1012479.6s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.752s, learning 0.163s)
               Value function loss: 113499.7797
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 1583.82
               Mean episode length: 51.29
                  Mean reward/step: 29.31
       Mean episode length/episode: 6.97
            Mean episode successes: 2.4487
Mean episode consecutive_successes: 5.3586
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 15.91s
                        Total time: 27758.36s
                               ETA: 1012670.3s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.164s, learning 0.184s)
               Value function loss: 198492.9617
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 1255.00
               Mean episode length: 53.42
                  Mean reward/step: 29.12
       Mean episode length/episode: 7.04
            Mean episode successes: 2.5815
Mean episode consecutive_successes: 5.3557
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 16.35s
                        Total time: 27774.71s
                               ETA: 1012876.7s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.266s, learning 0.185s)
               Value function loss: 118169.2250
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 1685.85
               Mean episode length: 50.19
                  Mean reward/step: 30.16
       Mean episode length/episode: 7.04
            Mean episode successes: 2.5776
Mean episode consecutive_successes: 5.4120
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 16.45s
                        Total time: 27791.16s
                               ETA: 1013086.6s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.906s, learning 0.166s)
               Value function loss: 97819.3928
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 1061.21
               Mean episode length: 47.26
                  Mean reward/step: 28.28
       Mean episode length/episode: 7.05
            Mean episode successes: 2.5894
Mean episode consecutive_successes: 5.4127
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 16.07s
                        Total time: 27807.23s
                               ETA: 1013282.6s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.824s, learning 0.185s)
               Value function loss: 110492.8844
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 1028.99
               Mean episode length: 47.93
                  Mean reward/step: 28.59
       Mean episode length/episode: 7.02
            Mean episode successes: 2.7017
Mean episode consecutive_successes: 5.3856
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 16.01s
                        Total time: 27823.24s
                               ETA: 1013476.1s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.294s, learning 0.169s)
               Value function loss: 109685.0586
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 1560.16
               Mean episode length: 52.85
                  Mean reward/step: 29.22
       Mean episode length/episode: 7.04
            Mean episode successes: 2.6216
Mean episode consecutive_successes: 5.4810
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 16.46s
                        Total time: 27839.70s
                               ETA: 1013685.9s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.519s, learning 0.158s)
               Value function loss: 115339.0572
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 1720.26
               Mean episode length: 49.82
                  Mean reward/step: 29.63
       Mean episode length/episode: 7.03
            Mean episode successes: 2.5371
Mean episode consecutive_successes: 5.5424
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 15.68s
                        Total time: 27855.38s
                               ETA: 1013867.0s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.706s, learning 0.261s)
               Value function loss: 132692.3203
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 1335.53
               Mean episode length: 46.39
                  Mean reward/step: 32.07
       Mean episode length/episode: 7.05
            Mean episode successes: 2.5850
Mean episode consecutive_successes: 5.5943
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 15.97s
                        Total time: 27871.34s
                               ETA: 1014058.5s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.016s, learning 0.234s)
               Value function loss: 154957.5676
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 1636.56
               Mean episode length: 51.59
                  Mean reward/step: 30.09
       Mean episode length/episode: 7.03
            Mean episode successes: 2.7690
Mean episode consecutive_successes: 5.5490
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 16.25s
                        Total time: 27887.59s
                               ETA: 1014260.1s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.286s, learning 0.259s)
               Value function loss: 106167.0191
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 1509.99
               Mean episode length: 49.30
                  Mean reward/step: 28.50
       Mean episode length/episode: 7.02
            Mean episode successes: 2.6924
Mean episode consecutive_successes: 5.5874
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 16.55s
                        Total time: 27904.14s
                               ETA: 1014472.4s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.218s, learning 0.187s)
               Value function loss: 96804.4662
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 1588.19
               Mean episode length: 50.51
                  Mean reward/step: 28.04
       Mean episode length/episode: 6.97
            Mean episode successes: 2.5464
Mean episode consecutive_successes: 5.6292
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 16.41s
                        Total time: 27920.55s
                               ETA: 1014679.3s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.064s, learning 0.156s)
               Value function loss: 105756.6035
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 1671.96
               Mean episode length: 48.28
                  Mean reward/step: 28.31
       Mean episode length/episode: 6.99
            Mean episode successes: 2.3574
Mean episode consecutive_successes: 5.6657
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 16.22s
                        Total time: 27936.77s
                               ETA: 1014879.4s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.658s, learning 0.173s)
               Value function loss: 116806.4203
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 984.22
               Mean episode length: 48.25
                  Mean reward/step: 27.04
       Mean episode length/episode: 7.08
            Mean episode successes: 2.4595
Mean episode consecutive_successes: 5.6105
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 15.83s
                        Total time: 27952.60s
                               ETA: 1015065.2s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.048s, learning 0.159s)
               Value function loss: 90225.7680
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 2016.37
               Mean episode length: 51.17
                  Mean reward/step: 26.04
       Mean episode length/episode: 7.06
            Mean episode successes: 2.5518
Mean episode consecutive_successes: 5.6214
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 16.21s
                        Total time: 27968.80s
                               ETA: 1015264.4s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.266s, learning 0.186s)
               Value function loss: 85629.2234
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 1511.58
               Mean episode length: 51.91
                  Mean reward/step: 26.19
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3569
Mean episode consecutive_successes: 5.6252
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 16.45s
                        Total time: 27985.26s
                               ETA: 1015472.4s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.925s, learning 0.238s)
               Value function loss: 88655.3439
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 1235.95
               Mean episode length: 50.83
                  Mean reward/step: 26.75
       Mean episode length/episode: 7.01
            Mean episode successes: 2.4424
Mean episode consecutive_successes: 5.5770
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 16.16s
                        Total time: 28001.42s
                               ETA: 1015669.8s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.655s, learning 0.168s)
               Value function loss: 88676.1057
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 1009.88
               Mean episode length: 49.18
                  Mean reward/step: 27.02
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3750
Mean episode consecutive_successes: 5.5714
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 15.82s
                        Total time: 28017.24s
                               ETA: 1015854.6s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.145s, learning 0.176s)
               Value function loss: 95985.1137
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: 1375.82
               Mean episode length: 50.56
                  Mean reward/step: 28.35
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3804
Mean episode consecutive_successes: 5.5727
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 16.32s
                        Total time: 28033.56s
                               ETA: 1016057.4s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1213 steps/s (collection: 13.329s, learning 0.170s)
               Value function loss: 96630.4621
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 1261.14
               Mean episode length: 47.22
                  Mean reward/step: 26.47
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3999
Mean episode consecutive_successes: 5.5438
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 13.50s
                        Total time: 28047.06s
                               ETA: 1016157.8s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.072s, learning 0.159s)
               Value function loss: 98833.7686
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 1182.88
               Mean episode length: 49.46
                  Mean reward/step: 25.80
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3188
Mean episode consecutive_successes: 5.5792
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 8.23s
                        Total time: 28055.29s
                               ETA: 1016067.3s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.421s, learning 0.161s)
               Value function loss: 107100.0977
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 1068.99
               Mean episode length: 51.91
                  Mean reward/step: 25.05
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1953
Mean episode consecutive_successes: 5.5875
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 8.58s
                        Total time: 28063.87s
                               ETA: 1015989.5s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.060s, learning 0.205s)
               Value function loss: 122226.1521
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 1202.73
               Mean episode length: 49.97
                  Mean reward/step: 24.51
       Mean episode length/episode: 6.97
            Mean episode successes: 2.1133
Mean episode consecutive_successes: 5.5383
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 8.27s
                        Total time: 28072.14s
                               ETA: 1015900.4s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.117s, learning 0.163s)
               Value function loss: 120108.6316
                    Surrogate loss: 0.0252
             Mean action noise std: 0.72
                       Mean reward: 1762.47
               Mean episode length: 52.92
                  Mean reward/step: 25.61
       Mean episode length/episode: 7.12
            Mean episode successes: 2.0830
Mean episode consecutive_successes: 5.6025
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 8.28s
                        Total time: 28080.42s
                               ETA: 1015811.8s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.049s, learning 0.180s)
               Value function loss: 101920.4295
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 1114.83
               Mean episode length: 49.05
                  Mean reward/step: 26.46
       Mean episode length/episode: 7.08
            Mean episode successes: 2.2100
Mean episode consecutive_successes: 5.5183
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 8.23s
                        Total time: 28088.65s
                               ETA: 1015721.5s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.091s, learning 0.187s)
               Value function loss: 104472.4225
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 1743.52
               Mean episode length: 50.94
                  Mean reward/step: 28.13
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2871
Mean episode consecutive_successes: 5.4776
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 8.28s
                        Total time: 28096.93s
                               ETA: 1015633.0s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.287s, learning 0.185s)
               Value function loss: 108371.9408
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 1041.79
               Mean episode length: 49.08
                  Mean reward/step: 28.14
       Mean episode length/episode: 7.09
            Mean episode successes: 2.4614
Mean episode consecutive_successes: 5.4202
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 8.47s
                        Total time: 28105.40s
                               ETA: 1015551.5s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.017s, learning 0.161s)
               Value function loss: 132178.7982
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 1606.87
               Mean episode length: 51.94
                  Mean reward/step: 30.69
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4766
Mean episode consecutive_successes: 5.4767
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 8.18s
                        Total time: 28113.58s
                               ETA: 1015459.5s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.420s, learning 0.191s)
               Value function loss: 127528.9525
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 1592.01
               Mean episode length: 51.51
                  Mean reward/step: 30.49
       Mean episode length/episode: 7.04
            Mean episode successes: 2.5884
Mean episode consecutive_successes: 5.4858
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 8.61s
                        Total time: 28122.19s
                               ETA: 1015383.2s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.244s, learning 0.189s)
               Value function loss: 131443.7789
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: 1576.84
               Mean episode length: 51.87
                  Mean reward/step: 29.82
       Mean episode length/episode: 7.03
            Mean episode successes: 2.7026
Mean episode consecutive_successes: 5.4933
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 8.43s
                        Total time: 28130.62s
                               ETA: 1015300.5s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.515s, learning 0.158s)
               Value function loss: 111129.3779
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 1722.75
               Mean episode length: 53.49
                  Mean reward/step: 28.25
       Mean episode length/episode: 7.00
            Mean episode successes: 2.5991
Mean episode consecutive_successes: 5.5617
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 8.67s
                        Total time: 28139.29s
                               ETA: 1015226.5s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.010s, learning 0.191s)
               Value function loss: 94954.3377
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 1472.21
               Mean episode length: 52.44
                  Mean reward/step: 26.61
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3550
Mean episode consecutive_successes: 5.6083
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 8.20s
                        Total time: 28147.49s
                               ETA: 1015135.5s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.310s, learning 0.175s)
               Value function loss: 86045.6172
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 1611.48
               Mean episode length: 51.72
                  Mean reward/step: 26.27
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3428
Mean episode consecutive_successes: 5.5995
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 8.48s
                        Total time: 28155.98s
                               ETA: 1015054.8s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.064s, learning 0.274s)
               Value function loss: 95975.3320
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 943.07
               Mean episode length: 50.42
                  Mean reward/step: 25.85
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3184
Mean episode consecutive_successes: 5.5296
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 8.34s
                        Total time: 28164.32s
                               ETA: 1014968.9s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.220s, learning 0.266s)
               Value function loss: 119235.7623
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 1577.45
               Mean episode length: 49.16
                  Mean reward/step: 28.48
       Mean episode length/episode: 7.08
            Mean episode successes: 2.3848
Mean episode consecutive_successes: 5.5392
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 8.49s
                        Total time: 28172.80s
                               ETA: 1014888.4s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.161s)
               Value function loss: 117545.1072
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: 1397.19
               Mean episode length: 48.53
                  Mean reward/step: 29.04
       Mean episode length/episode: 6.95
            Mean episode successes: 2.4771
Mean episode consecutive_successes: 5.5260
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 8.18s
                        Total time: 28180.98s
                               ETA: 1014797.0s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.088s, learning 0.229s)
               Value function loss: 93688.1600
                    Surrogate loss: -0.0155
             Mean action noise std: 0.72
                       Mean reward: 1382.69
               Mean episode length: 49.65
                  Mean reward/step: 29.32
       Mean episode length/episode: 6.96
            Mean episode successes: 2.6885
Mean episode consecutive_successes: 5.4364
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 8.32s
                        Total time: 28189.30s
                               ETA: 1014710.6s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.274s, learning 0.246s)
               Value function loss: 111681.1875
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 1150.47
               Mean episode length: 49.88
                  Mean reward/step: 31.23
       Mean episode length/episode: 7.10
            Mean episode successes: 2.7476
Mean episode consecutive_successes: 5.4818
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 8.52s
                        Total time: 28197.82s
                               ETA: 1014631.4s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.988s, learning 0.164s)
               Value function loss: 115302.5447
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 1339.18
               Mean episode length: 47.47
                  Mean reward/step: 29.75
       Mean episode length/episode: 6.92
            Mean episode successes: 2.5234
Mean episode consecutive_successes: 5.5510
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 8.15s
                        Total time: 28205.97s
                               ETA: 1014539.1s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.899s, learning 0.205s)
               Value function loss: 124654.0172
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 1560.32
               Mean episode length: 49.65
                  Mean reward/step: 30.37
       Mean episode length/episode: 7.15
            Mean episode successes: 2.7080
Mean episode consecutive_successes: 5.5640
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 8.10s
                        Total time: 28214.08s
                               ETA: 1014445.2s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.548s, learning 0.157s)
               Value function loss: 202578.9602
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 1565.84
               Mean episode length: 50.67
                  Mean reward/step: 30.75
       Mean episode length/episode: 6.99
            Mean episode successes: 2.5503
Mean episode consecutive_successes: 5.6653
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 8.71s
                        Total time: 28222.78s
                               ETA: 1014372.9s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.995s, learning 0.168s)
               Value function loss: 149290.6340
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 1728.71
               Mean episode length: 51.16
                  Mean reward/step: 30.35
       Mean episode length/episode: 7.00
            Mean episode successes: 2.5923
Mean episode consecutive_successes: 5.6519
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 8.16s
                        Total time: 28230.94s
                               ETA: 1014281.1s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.491s, learning 0.171s)
               Value function loss: 99931.6967
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 1081.89
               Mean episode length: 48.19
                  Mean reward/step: 30.16
       Mean episode length/episode: 7.09
            Mean episode successes: 2.7134
Mean episode consecutive_successes: 5.6590
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 8.66s
                        Total time: 28239.61s
                               ETA: 1014207.4s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.079s, learning 0.170s)
               Value function loss: 95730.5303
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 1014.55
               Mean episode length: 48.95
                  Mean reward/step: 29.37
       Mean episode length/episode: 7.00
            Mean episode successes: 2.6230
Mean episode consecutive_successes: 5.6733
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 8.25s
                        Total time: 28247.86s
                               ETA: 1014118.9s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.372s, learning 0.213s)
               Value function loss: 96512.7221
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 1446.90
               Mean episode length: 52.33
                  Mean reward/step: 28.65
       Mean episode length/episode: 6.94
            Mean episode successes: 2.5068
Mean episode consecutive_successes: 5.7067
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 8.58s
                        Total time: 28256.44s
                               ETA: 1014042.4s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.044s, learning 0.193s)
               Value function loss: 95939.2420
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 1837.37
               Mean episode length: 48.46
                  Mean reward/step: 32.07
       Mean episode length/episode: 7.01
            Mean episode successes: 2.7148
Mean episode consecutive_successes: 5.6851
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 8.24s
                        Total time: 28264.68s
                               ETA: 1013953.6s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.252s, learning 0.157s)
               Value function loss: 97341.7855
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 1428.60
               Mean episode length: 51.42
                  Mean reward/step: 32.37
       Mean episode length/episode: 7.04
            Mean episode successes: 2.8159
Mean episode consecutive_successes: 5.6798
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 8.41s
                        Total time: 28273.09s
                               ETA: 1013871.0s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.215s, learning 0.186s)
               Value function loss: 103049.8734
                    Surrogate loss: -0.0045
             Mean action noise std: 0.72
                       Mean reward: 2152.99
               Mean episode length: 53.69
                  Mean reward/step: 30.37
       Mean episode length/episode: 6.95
            Mean episode successes: 2.6978
Mean episode consecutive_successes: 5.7498
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 8.40s
                        Total time: 28281.49s
                               ETA: 1013788.2s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.167s, learning 0.168s)
               Value function loss: 104394.1264
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 1487.96
               Mean episode length: 50.40
                  Mean reward/step: 29.47
       Mean episode length/episode: 7.03
            Mean episode successes: 2.7266
Mean episode consecutive_successes: 5.7142
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 8.33s
                        Total time: 28289.82s
                               ETA: 1013703.0s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.054s, learning 0.164s)
               Value function loss: 111777.8781
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 1886.70
               Mean episode length: 51.27
                  Mean reward/step: 30.21
       Mean episode length/episode: 6.95
            Mean episode successes: 2.4932
Mean episode consecutive_successes: 5.8297
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 8.22s
                        Total time: 28298.04s
                               ETA: 1013613.7s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.347s, learning 0.158s)
               Value function loss: 100284.9529
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 963.18
               Mean episode length: 45.98
                  Mean reward/step: 27.58
       Mean episode length/episode: 7.10
            Mean episode successes: 2.4487
Mean episode consecutive_successes: 5.8334
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 8.51s
                        Total time: 28306.55s
                               ETA: 1013534.8s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.176s, learning 0.165s)
               Value function loss: 100296.2875
                    Surrogate loss: 0.0028
             Mean action noise std: 0.72
                       Mean reward: 1910.76
               Mean episode length: 50.16
                  Mean reward/step: 28.87
       Mean episode length/episode: 7.00
            Mean episode successes: 2.5562
Mean episode consecutive_successes: 5.8254
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 8.34s
                        Total time: 28314.89s
                               ETA: 1013450.0s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.886s, learning 0.170s)
               Value function loss: 108740.9643
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 1451.22
               Mean episode length: 50.28
                  Mean reward/step: 29.33
       Mean episode length/episode: 6.99
            Mean episode successes: 2.4941
Mean episode consecutive_successes: 5.8012
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 8.06s
                        Total time: 28322.94s
                               ETA: 1013355.1s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.016s, learning 0.191s)
               Value function loss: 111289.3584
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 1504.54
               Mean episode length: 48.27
                  Mean reward/step: 30.77
       Mean episode length/episode: 7.06
            Mean episode successes: 2.7109
Mean episode consecutive_successes: 5.7421
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 8.21s
                        Total time: 28331.15s
                               ETA: 1013265.6s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.163s, learning 0.159s)
               Value function loss: 119121.9600
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 1488.61
               Mean episode length: 50.58
                  Mean reward/step: 30.81
       Mean episode length/episode: 7.01
            Mean episode successes: 2.6240
Mean episode consecutive_successes: 5.8329
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 8.32s
                        Total time: 28339.47s
                               ETA: 1013180.4s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.297s, learning 0.216s)
               Value function loss: 124213.5596
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 1317.22
               Mean episode length: 48.59
                  Mean reward/step: 29.86
       Mean episode length/episode: 7.03
            Mean episode successes: 2.5513
Mean episode consecutive_successes: 5.8639
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 8.51s
                        Total time: 28347.98s
                               ETA: 1013102.0s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.205s, learning 0.191s)
               Value function loss: 111568.8543
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 1616.39
               Mean episode length: 51.38
                  Mean reward/step: 27.94
       Mean episode length/episode: 6.95
            Mean episode successes: 2.5083
Mean episode consecutive_successes: 5.8426
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 8.40s
                        Total time: 28356.38s
                               ETA: 1013019.5s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.896s, learning 0.215s)
               Value function loss: 159173.6295
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 1365.16
               Mean episode length: 49.41
                  Mean reward/step: 29.76
       Mean episode length/episode: 6.98
            Mean episode successes: 2.4756
Mean episode consecutive_successes: 5.8239
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 8.11s
                        Total time: 28364.49s
                               ETA: 1012926.8s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.042s, learning 0.208s)
               Value function loss: 113445.0637
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: 1913.66
               Mean episode length: 50.44
                  Mean reward/step: 29.60
       Mean episode length/episode: 6.96
            Mean episode successes: 2.4106
Mean episode consecutive_successes: 5.8707
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 8.25s
                        Total time: 28372.74s
                               ETA: 1012839.2s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.224s, learning 0.194s)
               Value function loss: 88238.1910
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 1172.24
               Mean episode length: 49.10
                  Mean reward/step: 27.52
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3022
Mean episode consecutive_successes: 5.8740
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 8.42s
                        Total time: 28381.16s
                               ETA: 1012757.6s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.392s, learning 0.172s)
               Value function loss: 86963.3652
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 1792.78
               Mean episode length: 53.34
                  Mean reward/step: 28.41
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3794
Mean episode consecutive_successes: 5.8809
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 8.56s
                        Total time: 28389.72s
                               ETA: 1012681.3s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.958s, learning 0.166s)
               Value function loss: 90535.5484
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 1851.76
               Mean episode length: 51.89
                  Mean reward/step: 28.29
       Mean episode length/episode: 7.04
            Mean episode successes: 2.4111
Mean episode consecutive_successes: 5.8270
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 8.12s
                        Total time: 28397.85s
                               ETA: 1012589.4s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.021s, learning 0.161s)
               Value function loss: 99378.2518
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 988.72
               Mean episode length: 47.59
                  Mean reward/step: 30.62
       Mean episode length/episode: 7.02
            Mean episode successes: 2.5522
Mean episode consecutive_successes: 5.7682
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 8.18s
                        Total time: 28406.03s
                               ETA: 1012499.5s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.223s, learning 0.163s)
               Value function loss: 108938.8086
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 1699.82
               Mean episode length: 53.03
                  Mean reward/step: 30.26
       Mean episode length/episode: 7.04
            Mean episode successes: 2.6567
Mean episode consecutive_successes: 5.7941
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 8.39s
                        Total time: 28414.42s
                               ETA: 1012417.1s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.173s, learning 0.293s)
               Value function loss: 112950.5641
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 1412.27
               Mean episode length: 48.57
                  Mean reward/step: 29.23
       Mean episode length/episode: 6.93
            Mean episode successes: 2.5811
Mean episode consecutive_successes: 5.7652
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 8.47s
                        Total time: 28422.88s
                               ETA: 1012337.5s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.383s, learning 0.242s)
               Value function loss: 128790.6400
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 1217.52
               Mean episode length: 49.63
                  Mean reward/step: 29.38
       Mean episode length/episode: 7.10
            Mean episode successes: 2.5420
Mean episode consecutive_successes: 5.8148
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 8.63s
                        Total time: 28431.51s
                               ETA: 1012263.6s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.247s, learning 0.254s)
               Value function loss: 124153.0176
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 1889.53
               Mean episode length: 52.20
                  Mean reward/step: 27.43
       Mean episode length/episode: 6.96
            Mean episode successes: 2.3813
Mean episode consecutive_successes: 5.8628
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 8.50s
                        Total time: 28440.01s
                               ETA: 1012185.4s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.245s, learning 0.197s)
               Value function loss: 113122.8773
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 1533.69
               Mean episode length: 51.08
                  Mean reward/step: 25.08
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1436
Mean episode consecutive_successes: 5.9045
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 8.44s
                        Total time: 28448.45s
                               ETA: 1012105.1s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.104s, learning 0.165s)
               Value function loss: 97825.7672
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 1474.95
               Mean episode length: 49.02
                  Mean reward/step: 26.01
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1621
Mean episode consecutive_successes: 5.8558
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 8.27s
                        Total time: 28456.72s
                               ETA: 1012018.7s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.156s, learning 0.200s)
               Value function loss: 98860.0818
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: 1050.69
               Mean episode length: 46.43
                  Mean reward/step: 25.75
       Mean episode length/episode: 6.91
            Mean episode successes: 2.1494
Mean episode consecutive_successes: 5.7673
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 8.36s
                        Total time: 28465.07s
                               ETA: 1011935.5s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.134s, learning 0.162s)
               Value function loss: 103230.6443
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: 1240.90
               Mean episode length: 50.72
                  Mean reward/step: 28.36
       Mean episode length/episode: 7.05
            Mean episode successes: 2.2100
Mean episode consecutive_successes: 5.7363
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 8.30s
                        Total time: 28473.37s
                               ETA: 1011850.2s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.161s)
               Value function loss: 95024.7420
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: 1654.67
               Mean episode length: 52.33
                  Mean reward/step: 30.05
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3184
Mean episode consecutive_successes: 5.7445
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 8.34s
                        Total time: 28481.71s
                               ETA: 1011766.4s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.023s, learning 0.161s)
               Value function loss: 107671.3795
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 1506.33
               Mean episode length: 52.05
                  Mean reward/step: 29.10
       Mean episode length/episode: 7.09
            Mean episode successes: 2.3994
Mean episode consecutive_successes: 5.7408
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 8.18s
                        Total time: 28489.89s
                               ETA: 1011677.2s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.960s, learning 0.224s)
               Value function loss: 119000.5760
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 1853.01
               Mean episode length: 53.78
                  Mean reward/step: 28.21
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2739
Mean episode consecutive_successes: 5.7738
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 8.18s
                        Total time: 28498.08s
                               ETA: 1011588.1s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.553s, learning 0.181s)
               Value function loss: 110882.9416
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 1284.34
               Mean episode length: 48.49
                  Mean reward/step: 26.92
       Mean episode length/episode: 7.10
            Mean episode successes: 2.3604
Mean episode consecutive_successes: 5.7089
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 8.73s
                        Total time: 28506.81s
                               ETA: 1011518.5s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.932s, learning 0.166s)
               Value function loss: 122648.1043
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 1501.97
               Mean episode length: 53.18
                  Mean reward/step: 27.18
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3501
Mean episode consecutive_successes: 5.7149
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 8.10s
                        Total time: 28514.91s
                               ETA: 1011426.5s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.959s, learning 0.186s)
               Value function loss: 104420.2248
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 1219.12
               Mean episode length: 51.71
                  Mean reward/step: 29.08
       Mean episode length/episode: 7.13
            Mean episode successes: 2.4658
Mean episode consecutive_successes: 5.6932
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 8.15s
                        Total time: 28523.05s
                               ETA: 1011336.1s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.065s, learning 0.271s)
               Value function loss: 96000.7051
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 1311.06
               Mean episode length: 50.75
                  Mean reward/step: 31.60
       Mean episode length/episode: 7.02
            Mean episode successes: 2.5332
Mean episode consecutive_successes: 5.6888
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 8.34s
                        Total time: 28531.39s
                               ETA: 1011252.6s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.164s, learning 0.209s)
               Value function loss: 88270.0119
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 1721.91
               Mean episode length: 51.97
                  Mean reward/step: 29.75
       Mean episode length/episode: 6.96
            Mean episode successes: 2.4038
Mean episode consecutive_successes: 5.7608
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 8.37s
                        Total time: 28539.76s
                               ETA: 1011170.5s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.144s, learning 0.188s)
               Value function loss: 90081.0676
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 1437.36
               Mean episode length: 52.91
                  Mean reward/step: 27.25
       Mean episode length/episode: 7.10
            Mean episode successes: 2.3550
Mean episode consecutive_successes: 5.7714
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 8.33s
                        Total time: 28548.09s
                               ETA: 1011087.0s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.064s, learning 0.187s)
               Value function loss: 95415.8184
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 1788.48
               Mean episode length: 54.00
                  Mean reward/step: 26.93
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1929
Mean episode consecutive_successes: 5.8478
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 8.25s
                        Total time: 28556.34s
                               ETA: 1011000.6s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.528s, learning 0.161s)
               Value function loss: 89765.1775
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 1169.99
               Mean episode length: 49.36
                  Mean reward/step: 27.73
       Mean episode length/episode: 7.01
            Mean episode successes: 2.0742
Mean episode consecutive_successes: 5.8031
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 8.69s
                        Total time: 28565.03s
                               ETA: 1010929.8s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.335s, learning 0.166s)
               Value function loss: 102223.4170
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 1213.98
               Mean episode length: 51.60
                  Mean reward/step: 28.02
       Mean episode length/episode: 6.97
            Mean episode successes: 2.2549
Mean episode consecutive_successes: 5.6979
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 8.50s
                        Total time: 28573.53s
                               ETA: 1010852.4s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.496s, learning 0.185s)
               Value function loss: 92998.7072
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: 1085.88
               Mean episode length: 50.12
                  Mean reward/step: 29.61
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3101
Mean episode consecutive_successes: 5.6678
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 8.68s
                        Total time: 28582.22s
                               ETA: 1010781.5s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.357s, learning 0.167s)
               Value function loss: 93322.0248
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 1062.58
               Mean episode length: 49.92
                  Mean reward/step: 30.32
       Mean episode length/episode: 7.09
            Mean episode successes: 2.5981
Mean episode consecutive_successes: 5.5996
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 8.52s
                        Total time: 28590.74s
                               ETA: 1010705.0s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.352s, learning 0.198s)
               Value function loss: 106937.4764
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 1526.07
               Mean episode length: 50.43
                  Mean reward/step: 33.41
       Mean episode length/episode: 7.07
            Mean episode successes: 2.7500
Mean episode consecutive_successes: 5.6222
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 8.55s
                        Total time: 28599.29s
                               ETA: 1010629.5s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.033s, learning 0.164s)
               Value function loss: 107637.8865
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 1528.13
               Mean episode length: 49.50
                  Mean reward/step: 33.19
       Mean episode length/episode: 6.88
            Mean episode successes: 2.6089
Mean episode consecutive_successes: 5.7031
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 8.20s
                        Total time: 28607.49s
                               ETA: 1010541.5s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.710s, learning 0.165s)
               Value function loss: 108402.3734
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 2138.80
               Mean episode length: 54.86
                  Mean reward/step: 30.28
       Mean episode length/episode: 7.05
            Mean episode successes: 2.5732
Mean episode consecutive_successes: 5.7894
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 8.88s
                        Total time: 28616.36s
                               ETA: 1010477.6s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.258s, learning 0.167s)
               Value function loss: 99192.1488
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 1726.73
               Mean episode length: 51.65
                  Mean reward/step: 26.97
       Mean episode length/episode: 7.08
            Mean episode successes: 2.4775
Mean episode consecutive_successes: 5.8027
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 8.43s
                        Total time: 28624.79s
                               ETA: 1010397.9s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.110s, learning 0.159s)
               Value function loss: 133391.6105
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 1936.85
               Mean episode length: 51.67
                  Mean reward/step: 28.03
       Mean episode length/episode: 7.05
            Mean episode successes: 2.4263
Mean episode consecutive_successes: 5.8179
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 8.27s
                        Total time: 28633.06s
                               ETA: 1010312.6s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.333s, learning 0.228s)
               Value function loss: 149041.2750
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 1433.49
               Mean episode length: 50.21
                  Mean reward/step: 29.61
       Mean episode length/episode: 7.05
            Mean episode successes: 2.4355
Mean episode consecutive_successes: 5.8393
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 8.56s
                        Total time: 28641.62s
                               ETA: 1010237.7s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.195s, learning 0.167s)
               Value function loss: 110945.0168
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 1627.84
               Mean episode length: 50.19
                  Mean reward/step: 30.18
       Mean episode length/episode: 6.92
            Mean episode successes: 2.3643
Mean episode consecutive_successes: 5.8172
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 8.36s
                        Total time: 28649.98s
                               ETA: 1010155.9s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.225s, learning 0.155s)
               Value function loss: 115576.9316
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 1271.49
               Mean episode length: 51.50
                  Mean reward/step: 29.11
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3198
Mean episode consecutive_successes: 5.8046
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 8.38s
                        Total time: 28658.36s
                               ETA: 1010074.7s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.832s, learning 0.163s)
               Value function loss: 103920.8438
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: 1446.88
               Mean episode length: 51.67
                  Mean reward/step: 28.78
       Mean episode length/episode: 7.15
            Mean episode successes: 2.5005
Mean episode consecutive_successes: 5.8101
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 8.00s
                        Total time: 28666.35s
                               ETA: 1009980.0s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.247s, learning 0.158s)
               Value function loss: 105537.7307
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 1407.72
               Mean episode length: 49.82
                  Mean reward/step: 29.07
       Mean episode length/episode: 7.01
            Mean episode successes: 2.4604
Mean episode consecutive_successes: 5.8229
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 8.41s
                        Total time: 28674.76s
                               ETA: 1009899.9s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.347s, learning 0.161s)
               Value function loss: 118646.0441
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 1537.08
               Mean episode length: 53.28
                  Mean reward/step: 29.88
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4814
Mean episode consecutive_successes: 5.7789
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 8.51s
                        Total time: 28683.27s
                               ETA: 1009823.4s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.397s, learning 0.167s)
               Value function loss: 126303.2410
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 1189.35
               Mean episode length: 48.12
                  Mean reward/step: 29.61
       Mean episode length/episode: 7.00
            Mean episode successes: 2.4038
Mean episode consecutive_successes: 5.7733
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 8.56s
                        Total time: 28691.83s
                               ETA: 1009748.9s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.360s, learning 0.212s)
               Value function loss: 128211.7156
                    Surrogate loss: 0.0040
             Mean action noise std: 0.72
                       Mean reward: 1382.57
               Mean episode length: 49.72
                  Mean reward/step: 31.31
       Mean episode length/episode: 7.13
            Mean episode successes: 2.5659
Mean episode consecutive_successes: 5.7814
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 8.57s
                        Total time: 28700.40s
                               ETA: 1009674.7s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.253s, learning 0.250s)
               Value function loss: 113989.5441
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 1269.25
               Mean episode length: 48.22
                  Mean reward/step: 31.17
       Mean episode length/episode: 7.02
            Mean episode successes: 2.6206
Mean episode consecutive_successes: 5.7929
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 8.50s
                        Total time: 28708.91s
                               ETA: 1009598.2s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.113s, learning 0.191s)
               Value function loss: 127653.0184
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 1395.35
               Mean episode length: 49.89
                  Mean reward/step: 29.99
       Mean episode length/episode: 6.95
            Mean episode successes: 2.6362
Mean episode consecutive_successes: 5.7587
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 8.30s
                        Total time: 28717.21s
                               ETA: 1009514.8s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.306s, learning 0.160s)
               Value function loss: 124396.4531
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 1701.88
               Mean episode length: 52.61
                  Mean reward/step: 30.45
       Mean episode length/episode: 7.01
            Mean episode successes: 2.5630
Mean episode consecutive_successes: 5.8131
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 8.47s
                        Total time: 28725.68s
                               ETA: 1009437.0s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.950s, learning 0.163s)
               Value function loss: 154517.4742
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 1569.71
               Mean episode length: 52.26
                  Mean reward/step: 31.01
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4834
Mean episode consecutive_successes: 5.8804
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 8.11s
                        Total time: 28733.79s
                               ETA: 1009347.0s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.327s, learning 0.184s)
               Value function loss: 124479.1377
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 1382.59
               Mean episode length: 49.34
                  Mean reward/step: 31.23
       Mean episode length/episode: 7.08
            Mean episode successes: 2.6089
Mean episode consecutive_successes: 5.8666
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 8.51s
                        Total time: 28742.30s
                               ETA: 1009270.9s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.460s, learning 0.172s)
               Value function loss: 103319.7449
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 1896.63
               Mean episode length: 51.17
                  Mean reward/step: 33.62
       Mean episode length/episode: 7.02
            Mean episode successes: 2.7788
Mean episode consecutive_successes: 5.8881
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 8.63s
                        Total time: 28750.93s
                               ETA: 1009199.2s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.409s, learning 0.280s)
               Value function loss: 112870.5242
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 1635.86
               Mean episode length: 50.27
                  Mean reward/step: 33.34
       Mean episode length/episode: 7.02
            Mean episode successes: 2.7886
Mean episode consecutive_successes: 5.9232
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 8.69s
                        Total time: 28759.62s
                               ETA: 1009129.5s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.540s, learning 0.182s)
               Value function loss: 103662.7805
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 1817.24
               Mean episode length: 47.86
                  Mean reward/step: 30.68
       Mean episode length/episode: 6.99
            Mean episode successes: 2.6411
Mean episode consecutive_successes: 5.9764
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 8.72s
                        Total time: 28768.34s
                               ETA: 1009061.0s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.200s, learning 0.188s)
               Value function loss: 116783.8740
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 1370.06
               Mean episode length: 49.36
                  Mean reward/step: 29.60
       Mean episode length/episode: 7.08
            Mean episode successes: 2.6182
Mean episode consecutive_successes: 6.0104
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 8.39s
                        Total time: 28776.73s
                               ETA: 1008980.8s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.598s, learning 0.166s)
               Value function loss: 113362.0879
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: 1773.40
               Mean episode length: 50.32
                  Mean reward/step: 30.24
       Mean episode length/episode: 6.97
            Mean episode successes: 2.4438
Mean episode consecutive_successes: 6.0602
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 8.76s
                        Total time: 28785.49s
                               ETA: 1008913.9s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.309s, learning 0.168s)
               Value function loss: 127224.1092
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 1802.10
               Mean episode length: 52.24
                  Mean reward/step: 30.10
       Mean episode length/episode: 7.07
            Mean episode successes: 2.3574
Mean episode consecutive_successes: 6.1299
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 8.48s
                        Total time: 28793.97s
                               ETA: 1008837.0s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.198s, learning 0.160s)
               Value function loss: 118467.8699
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 1555.71
               Mean episode length: 49.97
                  Mean reward/step: 31.87
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4150
Mean episode consecutive_successes: 6.1067
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 8.36s
                        Total time: 28802.33s
                               ETA: 1008755.9s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.421s, learning 0.234s)
               Value function loss: 123844.5594
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 1752.80
               Mean episode length: 53.29
                  Mean reward/step: 31.45
       Mean episode length/episode: 7.04
            Mean episode successes: 2.4907
Mean episode consecutive_successes: 6.0999
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 8.66s
                        Total time: 28810.98s
                               ETA: 1008685.3s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.534s, learning 0.161s)
               Value function loss: 152581.2957
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: 1272.97
               Mean episode length: 50.04
                  Mean reward/step: 31.68
       Mean episode length/episode: 7.07
            Mean episode successes: 2.5269
Mean episode consecutive_successes: 6.0863
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 8.70s
                        Total time: 28819.68s
                               ETA: 1008616.1s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.119s, learning 0.165s)
               Value function loss: 139230.6715
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 1373.85
               Mean episode length: 50.78
                  Mean reward/step: 33.05
       Mean episode length/episode: 7.05
            Mean episode successes: 2.7578
Mean episode consecutive_successes: 6.0421
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 8.28s
                        Total time: 28827.96s
                               ETA: 1008532.7s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.506s, learning 0.160s)
               Value function loss: 122746.0850
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 1915.37
               Mean episode length: 53.39
                  Mean reward/step: 33.20
       Mean episode length/episode: 7.00
            Mean episode successes: 2.6772
Mean episode consecutive_successes: 6.0862
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 8.67s
                        Total time: 28836.63s
                               ETA: 1008462.6s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.006s, learning 0.212s)
               Value function loss: 104245.6377
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 1797.41
               Mean episode length: 52.51
                  Mean reward/step: 32.70
       Mean episode length/episode: 7.01
            Mean episode successes: 2.5957
Mean episode consecutive_successes: 6.1767
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 8.22s
                        Total time: 28844.85s
                               ETA: 1008376.8s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.498s, learning 0.183s)
               Value function loss: 97146.7631
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 1632.02
               Mean episode length: 48.23
                  Mean reward/step: 33.69
       Mean episode length/episode: 7.06
            Mean episode successes: 2.7461
Mean episode consecutive_successes: 6.1581
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 8.68s
                        Total time: 28853.53s
                               ETA: 1008307.4s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.424s, learning 0.171s)
               Value function loss: 116225.3639
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 1391.60
               Mean episode length: 47.78
                  Mean reward/step: 35.02
       Mean episode length/episode: 7.00
            Mean episode successes: 2.8320
Mean episode consecutive_successes: 6.1741
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 8.60s
                        Total time: 28862.12s
                               ETA: 1008235.0s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.504s, learning 0.283s)
               Value function loss: 112035.5568
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 1327.55
               Mean episode length: 49.68
                  Mean reward/step: 31.57
       Mean episode length/episode: 7.04
            Mean episode successes: 2.8926
Mean episode consecutive_successes: 6.1558
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 8.79s
                        Total time: 28870.91s
                               ETA: 1008169.3s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.220s, learning 0.214s)
               Value function loss: 113507.4170
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 1096.41
               Mean episode length: 47.61
                  Mean reward/step: 31.11
       Mean episode length/episode: 7.01
            Mean episode successes: 2.7866
Mean episode consecutive_successes: 6.1931
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 8.43s
                        Total time: 28879.34s
                               ETA: 1008091.3s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.284s, learning 0.162s)
               Value function loss: 115435.8275
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 1538.06
               Mean episode length: 53.92
                  Mean reward/step: 29.62
       Mean episode length/episode: 7.08
            Mean episode successes: 2.6768
Mean episode consecutive_successes: 6.2690
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 8.45s
                        Total time: 28887.79s
                               ETA: 1008013.8s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.286s, learning 0.227s)
               Value function loss: 100774.4434
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 1825.75
               Mean episode length: 50.53
                  Mean reward/step: 28.56
       Mean episode length/episode: 7.04
            Mean episode successes: 2.4087
Mean episode consecutive_successes: 6.3440
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 8.51s
                        Total time: 28896.30s
                               ETA: 1007938.7s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.837s, learning 0.184s)
               Value function loss: 102806.6203
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 1927.08
               Mean episode length: 52.73
                  Mean reward/step: 29.78
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4321
Mean episode consecutive_successes: 6.3118
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 9.02s
                        Total time: 28905.33s
                               ETA: 1007881.4s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.378s, learning 0.158s)
               Value function loss: 113805.1859
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 1856.35
               Mean episode length: 51.29
                  Mean reward/step: 29.99
       Mean episode length/episode: 6.98
            Mean episode successes: 2.4199
Mean episode consecutive_successes: 6.2904
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 8.54s
                        Total time: 28913.86s
                               ETA: 1007807.2s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.367s, learning 0.274s)
               Value function loss: 104756.1543
                    Surrogate loss: -0.0064
             Mean action noise std: 0.72
                       Mean reward: 1516.75
               Mean episode length: 52.30
                  Mean reward/step: 29.34
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4175
Mean episode consecutive_successes: 6.2293
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 8.64s
                        Total time: 28922.50s
                               ETA: 1007736.7s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.484s, learning 0.168s)
               Value function loss: 106456.2730
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 1536.53
               Mean episode length: 51.74
                  Mean reward/step: 29.52
       Mean episode length/episode: 7.07
            Mean episode successes: 2.5557
Mean episode consecutive_successes: 6.1730
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 8.65s
                        Total time: 28931.15s
                               ETA: 1007666.6s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.096s, learning 0.167s)
               Value function loss: 112482.1936
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: 1686.19
               Mean episode length: 50.65
                  Mean reward/step: 29.39
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4448
Mean episode consecutive_successes: 6.1829
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 8.26s
                        Total time: 28939.42s
                               ETA: 1007583.1s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.272s, learning 0.162s)
               Value function loss: 111808.3586
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 1589.23
               Mean episode length: 51.94
                  Mean reward/step: 30.44
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4780
Mean episode consecutive_successes: 6.1634
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 8.43s
                        Total time: 28947.85s
                               ETA: 1007505.5s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.055s, learning 0.175s)
               Value function loss: 147683.8758
                    Surrogate loss: 0.0064
             Mean action noise std: 0.72
                       Mean reward: 1224.34
               Mean episode length: 52.02
                  Mean reward/step: 29.62
       Mean episode length/episode: 7.07
            Mean episode successes: 2.3887
Mean episode consecutive_successes: 6.2180
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 8.23s
                        Total time: 28956.08s
                               ETA: 1007420.9s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.385s, learning 0.162s)
               Value function loss: 109845.3605
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: 1183.24
               Mean episode length: 46.86
                  Mean reward/step: 28.98
       Mean episode length/episode: 7.10
            Mean episode successes: 2.4307
Mean episode consecutive_successes: 6.1800
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 8.55s
                        Total time: 28964.63s
                               ETA: 1007347.3s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.047s, learning 0.163s)
               Value function loss: 101782.5977
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 1397.42
               Mean episode length: 49.06
                  Mean reward/step: 30.00
       Mean episode length/episode: 7.01
            Mean episode successes: 2.5146
Mean episode consecutive_successes: 6.1457
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 8.21s
                        Total time: 28972.84s
                               ETA: 1007262.1s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.279s, learning 0.161s)
               Value function loss: 103321.3617
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 1249.25
               Mean episode length: 47.86
                  Mean reward/step: 31.77
       Mean episode length/episode: 7.02
            Mean episode successes: 2.5957
Mean episode consecutive_successes: 6.1172
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 8.44s
                        Total time: 28981.28s
                               ETA: 1007185.0s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.217s, learning 0.220s)
               Value function loss: 116556.1061
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 2129.74
               Mean episode length: 56.49
                  Mean reward/step: 33.52
       Mean episode length/episode: 7.01
            Mean episode successes: 2.6201
Mean episode consecutive_successes: 6.1300
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 8.44s
                        Total time: 28989.72s
                               ETA: 1007107.8s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.297s, learning 0.164s)
               Value function loss: 120714.5930
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 1935.61
               Mean episode length: 53.96
                  Mean reward/step: 32.06
       Mean episode length/episode: 7.01
            Mean episode successes: 2.5400
Mean episode consecutive_successes: 6.2056
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 8.46s
                        Total time: 28998.18s
                               ETA: 1007031.4s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.458s, learning 0.159s)
               Value function loss: 116512.4158
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 1523.18
               Mean episode length: 49.91
                  Mean reward/step: 31.50
       Mean episode length/episode: 7.11
            Mean episode successes: 2.6528
Mean episode consecutive_successes: 6.1986
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 8.62s
                        Total time: 29006.80s
                               ETA: 1006960.6s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.217s, learning 0.163s)
               Value function loss: 125676.7477
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 1551.63
               Mean episode length: 47.63
                  Mean reward/step: 31.97
       Mean episode length/episode: 6.99
            Mean episode successes: 2.7949
Mean episode consecutive_successes: 6.1395
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 8.38s
                        Total time: 29015.18s
                               ETA: 1006881.5s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.369s, learning 0.205s)
               Value function loss: 138107.7766
                    Surrogate loss: 0.0152
             Mean action noise std: 0.72
                       Mean reward: 1844.18
               Mean episode length: 51.70
                  Mean reward/step: 32.16
       Mean episode length/episode: 7.06
            Mean episode successes: 2.7456
Mean episode consecutive_successes: 6.2473
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 8.57s
                        Total time: 29023.75s
                               ETA: 1006809.2s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.305s, learning 0.159s)
               Value function loss: 129104.7996
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 1635.79
               Mean episode length: 50.68
                  Mean reward/step: 31.01
       Mean episode length/episode: 7.05
            Mean episode successes: 2.7617
Mean episode consecutive_successes: 6.2129
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 8.46s
                        Total time: 29032.21s
                               ETA: 1006733.2s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.161s)
               Value function loss: 131134.1408
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 1891.64
               Mean episode length: 51.80
                  Mean reward/step: 30.83
       Mean episode length/episode: 7.02
            Mean episode successes: 2.6343
Mean episode consecutive_successes: 6.3062
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 8.64s
                        Total time: 29040.85s
                               ETA: 1006663.2s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.101s, learning 0.161s)
               Value function loss: 238204.2313
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 1672.33
               Mean episode length: 56.22
                  Mean reward/step: 31.46
       Mean episode length/episode: 7.05
            Mean episode successes: 2.5073
Mean episode consecutive_successes: 6.3327
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 8.26s
                        Total time: 29049.11s
                               ETA: 1006580.3s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.382s, learning 0.185s)
               Value function loss: 169876.4527
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: 1466.83
               Mean episode length: 48.38
                  Mean reward/step: 30.40
       Mean episode length/episode: 6.98
            Mean episode successes: 2.3945
Mean episode consecutive_successes: 6.3590
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 8.57s
                        Total time: 29057.68s
                               ETA: 1006507.9s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.166s)
               Value function loss: 107316.1314
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 974.60
               Mean episode length: 49.00
                  Mean reward/step: 29.02
       Mean episode length/episode: 7.10
            Mean episode successes: 2.4922
Mean episode consecutive_successes: 6.2727
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 8.29s
                        Total time: 29065.97s
                               ETA: 1006426.0s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.164s)
               Value function loss: 109044.6965
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: 1310.02
               Mean episode length: 49.40
                  Mean reward/step: 27.77
       Mean episode length/episode: 6.98
            Mean episode successes: 2.5259
Mean episode consecutive_successes: 6.1940
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 8.46s
                        Total time: 29074.43s
                               ETA: 1006350.1s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.003s, learning 0.170s)
               Value function loss: 125090.7914
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 1443.11
               Mean episode length: 50.50
                  Mean reward/step: 27.08
       Mean episode length/episode: 7.02
            Mean episode successes: 2.2617
Mean episode consecutive_successes: 6.2575
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 8.17s
                        Total time: 29082.60s
                               ETA: 1006264.2s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.164s)
               Value function loss: 126480.5244
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 1447.11
               Mean episode length: 52.02
                  Mean reward/step: 27.11
       Mean episode length/episode: 7.02
            Mean episode successes: 2.2080
Mean episode consecutive_successes: 6.2063
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 8.47s
                        Total time: 29091.07s
                               ETA: 1006188.7s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.213s, learning 0.164s)
               Value function loss: 116436.7590
                    Surrogate loss: 0.0152
             Mean action noise std: 0.72
                       Mean reward: 1528.57
               Mean episode length: 51.08
                  Mean reward/step: 29.38
       Mean episode length/episode: 7.07
            Mean episode successes: 2.3218
Mean episode consecutive_successes: 6.1458
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 8.38s
                        Total time: 29099.45s
                               ETA: 1006110.0s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.317s, learning 0.270s)
               Value function loss: 115242.9367
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 1128.55
               Mean episode length: 46.48
                  Mean reward/step: 29.87
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2515
Mean episode consecutive_successes: 6.1125
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 8.59s
                        Total time: 29108.03s
                               ETA: 1006038.7s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.854s, learning 0.164s)
               Value function loss: 116931.8740
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 1644.34
               Mean episode length: 51.32
                  Mean reward/step: 28.81
       Mean episode length/episode: 7.05
            Mean episode successes: 2.3115
Mean episode consecutive_successes: 6.0870
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 8.02s
                        Total time: 29116.05s
                               ETA: 1005947.7s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.338s, learning 0.234s)
               Value function loss: 120615.0234
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 1685.61
               Mean episode length: 50.36
                  Mean reward/step: 28.72
       Mean episode length/episode: 6.97
            Mean episode successes: 2.4570
Mean episode consecutive_successes: 6.0187
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 8.57s
                        Total time: 29124.62s
                               ETA: 1005875.9s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.293s, learning 0.163s)
               Value function loss: 126480.0426
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: 1675.62
               Mean episode length: 49.68
                  Mean reward/step: 30.41
       Mean episode length/episode: 7.02
            Mean episode successes: 2.4302
Mean episode consecutive_successes: 6.0232
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 8.46s
                        Total time: 29133.08s
                               ETA: 1005800.2s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.474s, learning 0.161s)
               Value function loss: 116646.7254
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 1284.66
               Mean episode length: 49.09
                  Mean reward/step: 28.84
       Mean episode length/episode: 6.93
            Mean episode successes: 2.3804
Mean episode consecutive_successes: 5.9625
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 8.64s
                        Total time: 29141.71s
                               ETA: 1005730.7s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.139s, learning 0.212s)
               Value function loss: 179825.9246
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 1617.24
               Mean episode length: 52.30
                  Mean reward/step: 29.89
       Mean episode length/episode: 7.12
            Mean episode successes: 2.4380
Mean episode consecutive_successes: 5.9863
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 8.35s
                        Total time: 29150.07s
                               ETA: 1005651.4s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.540s, learning 0.163s)
               Value function loss: 165963.5836
                    Surrogate loss: 0.0386
             Mean action noise std: 0.72
                       Mean reward: 1186.54
               Mean episode length: 47.66
                  Mean reward/step: 28.97
       Mean episode length/episode: 7.01
            Mean episode successes: 2.3979
Mean episode consecutive_successes: 5.9526
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 8.70s
                        Total time: 29158.77s
                               ETA: 1005584.3s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.700s, learning 0.160s)
               Value function loss: 104289.0523
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 1167.50
               Mean episode length: 48.90
                  Mean reward/step: 29.23
       Mean episode length/episode: 7.06
            Mean episode successes: 2.3276
Mean episode consecutive_successes: 5.9700
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 8.86s
                        Total time: 29167.63s
                               ETA: 1005522.7s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.400s, learning 0.160s)
               Value function loss: 91709.0898
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 1299.96
               Mean episode length: 48.96
                  Mean reward/step: 27.96
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3428
Mean episode consecutive_successes: 5.8971
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 8.56s
                        Total time: 29176.19s
                               ETA: 1005450.8s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.387s, learning 0.158s)
               Value function loss: 99779.3646
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 1147.65
               Mean episode length: 50.01
                  Mean reward/step: 30.18
       Mean episode length/episode: 7.03
            Mean episode successes: 2.4858
Mean episode consecutive_successes: 5.8614
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 8.55s
                        Total time: 29184.73s
                               ETA: 1005378.4s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.293s, learning 0.178s)
               Value function loss: 101207.0215
                    Surrogate loss: 0.0114
             Mean action noise std: 0.72
                       Mean reward: 1528.15
               Mean episode length: 50.67
                  Mean reward/step: 29.49
       Mean episode length/episode: 7.03
            Mean episode successes: 2.5205
Mean episode consecutive_successes: 5.8858
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 8.47s
                        Total time: 29193.20s
                               ETA: 1005303.5s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.133s, learning 0.159s)
               Value function loss: 93942.8529
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 1206.21
               Mean episode length: 47.29
                  Mean reward/step: 29.26
       Mean episode length/episode: 7.00
            Mean episode successes: 2.3848
Mean episode consecutive_successes: 5.8922
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 8.29s
                        Total time: 29201.50s
                               ETA: 1005222.5s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.090s, learning 0.249s)
               Value function loss: 93868.0082
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 1549.72
               Mean episode length: 48.55
                  Mean reward/step: 27.18
       Mean episode length/episode: 6.95
            Mean episode successes: 2.3130
Mean episode consecutive_successes: 5.8591
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 8.34s
                        Total time: 29209.84s
                               ETA: 1005143.1s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.253s, learning 0.164s)
               Value function loss: 98678.8229
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 1485.66
               Mean episode length: 50.42
                  Mean reward/step: 27.85
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2271
Mean episode consecutive_successes: 5.8637
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 8.42s
                        Total time: 29218.25s
                               ETA: 1005066.5s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.961s, learning 0.156s)
               Value function loss: 129434.0496
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 1436.50
               Mean episode length: 48.00
                  Mean reward/step: 28.78
       Mean episode length/episode: 7.12
            Mean episode successes: 2.4390
Mean episode consecutive_successes: 5.7990
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 8.12s
                        Total time: 29226.37s
                               ETA: 1004979.6s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.397s, learning 0.207s)
               Value function loss: 228178.6719
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 1599.98
               Mean episode length: 49.26
                  Mean reward/step: 30.31
       Mean episode length/episode: 7.00
            Mean episode successes: 2.4731
Mean episode consecutive_successes: 5.8158
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 8.60s
                        Total time: 29234.97s
                               ETA: 1004909.5s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.346s, learning 0.190s)
               Value function loss: 122827.5369
                    Surrogate loss: 0.0125
             Mean action noise std: 0.72
                       Mean reward: 1516.28
               Mean episode length: 51.28
                  Mean reward/step: 29.73
       Mean episode length/episode: 6.97
            Mean episode successes: 2.4409
Mean episode consecutive_successes: 5.7949
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 8.54s
                        Total time: 29243.51s
                               ETA: 1004837.2s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.047s, learning 0.195s)
               Value function loss: 96984.2941
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: 1375.22
               Mean episode length: 49.19
                  Mean reward/step: 29.83
       Mean episode length/episode: 7.07
            Mean episode successes: 2.4526
Mean episode consecutive_successes: 5.8319
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 8.24s
                        Total time: 29251.75s
                               ETA: 1004754.8s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.130s, learning 0.168s)
               Value function loss: 100507.0352
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 2005.91
               Mean episode length: 53.72
                  Mean reward/step: 30.53
       Mean episode length/episode: 7.10
            Mean episode successes: 2.5576
Mean episode consecutive_successes: 5.8691
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 8.30s
                        Total time: 29260.05s
                               ETA: 1004674.3s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.159s, learning 0.157s)
               Value function loss: 104372.4248
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 2061.41
               Mean episode length: 54.60
                  Mean reward/step: 29.17
       Mean episode length/episode: 7.10
            Mean episode successes: 2.4897
Mean episode consecutive_successes: 5.9621
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 8.32s
                        Total time: 29268.37s
                               ETA: 1004594.6s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.393s, learning 0.174s)
               Value function loss: 110347.9959
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 1284.65
               Mean episode length: 48.65
                  Mean reward/step: 29.69
       Mean episode length/episode: 7.00
            Mean episode successes: 2.4707
Mean episode consecutive_successes: 5.9540
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 8.57s
                        Total time: 29276.93s
                               ETA: 1004523.5s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.062s, learning 0.165s)
               Value function loss: 111423.3674
                    Surrogate loss: 0.0018
             Mean action noise std: 0.72
                       Mean reward: 1130.65
               Mean episode length: 50.00
                  Mean reward/step: 28.30
       Mean episode length/episode: 6.97
            Mean episode successes: 2.4048
Mean episode consecutive_successes: 5.9149
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 8.23s
                        Total time: 29285.16s
                               ETA: 1004440.7s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.286s, learning 0.158s)
               Value function loss: 92030.2328
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 1529.14
               Mean episode length: 52.02
                  Mean reward/step: 27.18
       Mean episode length/episode: 7.03
            Mean episode successes: 2.3540
Mean episode consecutive_successes: 5.9225
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 8.44s
                        Total time: 29293.61s
                               ETA: 1004365.5s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.072s, learning 0.160s)
               Value function loss: 89128.0936
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 1316.01
               Mean episode length: 51.10
                  Mean reward/step: 26.43
       Mean episode length/episode: 7.06
            Mean episode successes: 2.4536
Mean episode consecutive_successes: 5.8293
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 8.23s
                        Total time: 29301.84s
                               ETA: 1004283.0s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.154s, learning 0.174s)
               Value function loss: 83746.7105
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 1112.91
               Mean episode length: 49.82
                  Mean reward/step: 24.66
       Mean episode length/episode: 6.92
            Mean episode successes: 2.1597
Mean episode consecutive_successes: 5.8064
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 8.33s
                        Total time: 29310.16s
                               ETA: 1004203.9s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.313s, learning 0.187s)
               Value function loss: 98254.6350
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 1491.00
               Mean episode length: 50.61
                  Mean reward/step: 23.47
       Mean episode length/episode: 6.94
            Mean episode successes: 1.9692
Mean episode consecutive_successes: 5.7955
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 8.50s
                        Total time: 29318.66s
                               ETA: 1004130.7s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.322s, learning 0.162s)
               Value function loss: 85709.1799
                    Surrogate loss: 0.0141
             Mean action noise std: 0.72
                       Mean reward: 1195.26
               Mean episode length: 49.14
                  Mean reward/step: 24.03
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0718
Mean episode consecutive_successes: 5.6783
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 8.48s
                        Total time: 29327.15s
                               ETA: 1004057.0s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.086s, learning 0.173s)
               Value function loss: 91588.5502
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 987.12
               Mean episode length: 48.95
                  Mean reward/step: 24.81
       Mean episode length/episode: 7.02
            Mean episode successes: 2.0532
Mean episode consecutive_successes: 5.6322
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 8.26s
                        Total time: 29335.41s
                               ETA: 1003975.7s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.423s, learning 0.218s)
               Value function loss: 84954.4500
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 1142.86
               Mean episode length: 45.89
                  Mean reward/step: 25.29
       Mean episode length/episode: 7.05
            Mean episode successes: 2.1006
Mean episode consecutive_successes: 5.5550
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 8.64s
                        Total time: 29344.05s
                               ETA: 1003907.5s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.121s, learning 0.199s)
               Value function loss: 98086.2666
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: 1050.13
               Mean episode length: 49.41
                  Mean reward/step: 25.16
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2002
Mean episode consecutive_successes: 5.4570
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 8.32s
                        Total time: 29352.37s
                               ETA: 1003828.3s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.229s, learning 0.212s)
               Value function loss: 117152.8373
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: 1559.28
               Mean episode length: 51.82
                  Mean reward/step: 23.05
       Mean episode length/episode: 7.02
            Mean episode successes: 2.1816
Mean episode consecutive_successes: 5.4218
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 8.44s
                        Total time: 29360.81s
                               ETA: 1003753.4s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.216s, learning 0.185s)
               Value function loss: 127266.6322
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 1365.73
               Mean episode length: 49.99
                  Mean reward/step: 22.98
       Mean episode length/episode: 7.01
            Mean episode successes: 2.1201
Mean episode consecutive_successes: 5.3862
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 8.40s
                        Total time: 29369.21s
                               ETA: 1003677.1s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.626s, learning 0.208s)
               Value function loss: 112704.9463
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 955.60
               Mean episode length: 49.71
                  Mean reward/step: 21.83
       Mean episode length/episode: 7.09
            Mean episode successes: 2.0962
Mean episode consecutive_successes: 5.3284
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 8.83s
                        Total time: 29378.05s
                               ETA: 1003615.6s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.470s, learning 0.186s)
               Value function loss: 121750.6391
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 1210.52
               Mean episode length: 49.73
                  Mean reward/step: 23.08
       Mean episode length/episode: 7.00
            Mean episode successes: 2.0381
Mean episode consecutive_successes: 5.2829
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 16.66s
                        Total time: 29394.70s
                               ETA: 1003821.4s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.087s, learning 0.162s)
               Value function loss: 125140.4311
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 1258.05
               Mean episode length: 50.02
                  Mean reward/step: 23.46
       Mean episode length/episode: 6.94
            Mean episode successes: 2.0679
Mean episode consecutive_successes: 5.2028
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 16.25s
                        Total time: 29410.95s
                               ETA: 1004013.0s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.063s, learning 0.235s)
               Value function loss: 104851.9430
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: 1297.35
               Mean episode length: 53.25
                  Mean reward/step: 23.82
       Mean episode length/episode: 7.05
            Mean episode successes: 2.0234
Mean episode consecutive_successes: 5.1786
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 16.30s
                        Total time: 29427.25s
                               ETA: 1004206.2s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.478s, learning 0.163s)
               Value function loss: 101381.6479
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: 1233.89
               Mean episode length: 51.00
                  Mean reward/step: 23.09
       Mean episode length/episode: 7.04
            Mean episode successes: 2.0366
Mean episode consecutive_successes: 5.1303
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 16.64s
                        Total time: 29443.89s
                               ETA: 1004410.9s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.837s, learning 0.163s)
               Value function loss: 127141.4820
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 1310.69
               Mean episode length: 50.35
                  Mean reward/step: 21.51
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9355
Mean episode consecutive_successes: 5.1125
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 16.00s
                        Total time: 29459.89s
                               ETA: 1004593.7s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.062s, learning 0.187s)
               Value function loss: 95377.7211
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 835.28
               Mean episode length: 46.14
                  Mean reward/step: 21.82
       Mean episode length/episode: 6.94
            Mean episode successes: 1.9238
Mean episode consecutive_successes: 5.0199
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 16.25s
                        Total time: 29476.14s
                               ETA: 1004784.7s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.247s, learning 0.165s)
               Value function loss: 97384.9396
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: 1103.04
               Mean episode length: 49.75
                  Mean reward/step: 21.79
       Mean episode length/episode: 7.02
            Mean episode successes: 1.8657
Mean episode consecutive_successes: 4.9769
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 16.41s
                        Total time: 29492.55s
                               ETA: 1004981.2s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.920s, learning 0.165s)
               Value function loss: 85564.1344
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 955.44
               Mean episode length: 46.10
                  Mean reward/step: 19.97
       Mean episode length/episode: 7.04
            Mean episode successes: 1.8140
Mean episode consecutive_successes: 4.9125
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 16.09s
                        Total time: 29508.64s
                               ETA: 1005166.4s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.024s, learning 0.194s)
               Value function loss: 89064.1852
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 1001.95
               Mean episode length: 48.77
                  Mean reward/step: 21.34
       Mean episode length/episode: 7.02
            Mean episode successes: 1.9033
Mean episode consecutive_successes: 4.7961
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 16.22s
                        Total time: 29524.86s
                               ETA: 1005356.0s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.969s, learning 0.162s)
               Value function loss: 95653.8982
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 1112.74
               Mean episode length: 49.97
                  Mean reward/step: 22.50
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9512
Mean episode consecutive_successes: 4.7613
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 16.13s
                        Total time: 29540.99s
                               ETA: 1005542.5s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.303s, learning 0.187s)
               Value function loss: 83813.8578
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 1083.07
               Mean episode length: 50.12
                  Mean reward/step: 23.69
       Mean episode length/episode: 6.90
            Mean episode successes: 1.9541
Mean episode consecutive_successes: 4.7100
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 16.49s
                        Total time: 29557.48s
                               ETA: 1005741.0s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.855s, learning 0.162s)
               Value function loss: 85537.7768
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 1045.21
               Mean episode length: 49.59
                  Mean reward/step: 21.83
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0234
Mean episode consecutive_successes: 4.6457
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 16.02s
                        Total time: 29573.49s
                               ETA: 1005923.3s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.874s, learning 0.171s)
               Value function loss: 90684.6766
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 1376.79
               Mean episode length: 52.08
                  Mean reward/step: 23.10
       Mean episode length/episode: 7.13
            Mean episode successes: 2.0542
Mean episode consecutive_successes: 4.6832
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 16.05s
                        Total time: 29589.54s
                               ETA: 1006106.5s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.173s, learning 0.184s)
               Value function loss: 87568.8506
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 964.18
               Mean episode length: 48.19
                  Mean reward/step: 23.48
       Mean episode length/episode: 6.95
            Mean episode successes: 1.9878
Mean episode consecutive_successes: 4.6505
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 16.36s
                        Total time: 29605.90s
                               ETA: 1006300.1s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.125s, learning 0.189s)
               Value function loss: 96786.4871
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 1142.88
               Mean episode length: 49.49
                  Mean reward/step: 23.49
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9995
Mean episode consecutive_successes: 4.6400
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 16.31s
                        Total time: 29622.21s
                               ETA: 1006492.0s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.822s, learning 0.163s)
               Value function loss: 95957.7873
                    Surrogate loss: 0.0108
             Mean action noise std: 0.72
                       Mean reward: 1312.14
               Mean episode length: 52.30
                  Mean reward/step: 25.84
       Mean episode length/episode: 7.03
            Mean episode successes: 2.0396
Mean episode consecutive_successes: 4.6539
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 16.98s
                        Total time: 29639.20s
                               ETA: 1006706.7s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.606s, learning 0.166s)
               Value function loss: 91997.0393
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 827.76
               Mean episode length: 45.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1211
Mean episode consecutive_successes: 4.6505
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 15.77s
                        Total time: 29654.97s
                               ETA: 1006879.9s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.098s, learning 0.272s)
               Value function loss: 94108.7254
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 1075.39
               Mean episode length: 49.69
                  Mean reward/step: 25.69
       Mean episode length/episode: 7.04
            Mean episode successes: 2.1670
Mean episode consecutive_successes: 4.6931
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 16.37s
                        Total time: 29671.34s
                               ETA: 1007073.4s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.084s, learning 0.286s)
               Value function loss: 132075.8746
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 1600.53
               Mean episode length: 53.78
                  Mean reward/step: 25.00
       Mean episode length/episode: 6.91
            Mean episode successes: 2.1597
Mean episode consecutive_successes: 4.6788
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 16.37s
                        Total time: 29687.71s
                               ETA: 1007266.7s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1006 steps/s (collection: 15.924s, learning 0.359s)
               Value function loss: 126269.3488
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 1711.18
               Mean episode length: 50.37
                  Mean reward/step: 24.07
       Mean episode length/episode: 6.87
            Mean episode successes: 2.0044
Mean episode consecutive_successes: 4.7388
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 16.28s
                        Total time: 29703.99s
                               ETA: 1007456.9s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.152s, learning 0.207s)
               Value function loss: 72332.2188
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 1257.53
               Mean episode length: 49.67
                  Mean reward/step: 22.71
       Mean episode length/episode: 7.05
            Mean episode successes: 1.8755
Mean episode consecutive_successes: 4.7691
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 16.36s
                        Total time: 29720.35s
                               ETA: 1007649.5s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1011 steps/s (collection: 15.948s, learning 0.253s)
               Value function loss: 76213.6377
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 910.67
               Mean episode length: 46.76
                  Mean reward/step: 25.08
       Mean episode length/episode: 7.01
            Mean episode successes: 1.9951
Mean episode consecutive_successes: 4.6992
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 16.20s
                        Total time: 29736.55s
                               ETA: 1007836.7s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.842s, learning 0.169s)
               Value function loss: 89613.3037
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 1263.70
               Mean episode length: 47.07
                  Mean reward/step: 23.61
       Mean episode length/episode: 6.98
            Mean episode successes: 1.9424
Mean episode consecutive_successes: 4.7234
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 16.01s
                        Total time: 29752.56s
                               ETA: 1008017.2s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.932s, learning 0.216s)
               Value function loss: 99696.4793
                    Surrogate loss: -0.0200
             Mean action noise std: 0.72
                       Mean reward: 1161.20
               Mean episode length: 46.69
                  Mean reward/step: 25.78
       Mean episode length/episode: 6.97
            Mean episode successes: 1.9858
Mean episode consecutive_successes: 4.7141
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 16.15s
                        Total time: 29768.71s
                               ETA: 1008202.3s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.492s, learning 0.220s)
               Value function loss: 80823.1964
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 915.39
               Mean episode length: 46.22
                  Mean reward/step: 26.27
       Mean episode length/episode: 6.95
            Mean episode successes: 2.0742
Mean episode consecutive_successes: 4.6627
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 15.71s
                        Total time: 29784.42s
                               ETA: 1008372.5s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.748s, learning 0.191s)
               Value function loss: 73748.3399
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 1660.87
               Mean episode length: 51.13
                  Mean reward/step: 24.23
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9590
Mean episode consecutive_successes: 4.7795
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 15.94s
                        Total time: 29800.36s
                               ETA: 1008550.2s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.895s, learning 0.163s)
               Value function loss: 87572.3652
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 953.92
               Mean episode length: 47.30
                  Mean reward/step: 25.88
       Mean episode length/episode: 6.98
            Mean episode successes: 2.0493
Mean episode consecutive_successes: 4.7172
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 16.06s
                        Total time: 29816.42s
                               ETA: 1008731.7s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.816s, learning 0.175s)
               Value function loss: 94670.9834
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 1163.68
               Mean episode length: 47.27
                  Mean reward/step: 29.04
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2505
Mean episode consecutive_successes: 4.7152
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 15.99s
                        Total time: 29832.41s
                               ETA: 1008910.9s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.589s, learning 0.185s)
               Value function loss: 89225.3840
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 1595.14
               Mean episode length: 49.13
                  Mean reward/step: 27.29
       Mean episode length/episode: 7.01
            Mean episode successes: 2.2227
Mean episode consecutive_successes: 4.7755
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 15.77s
                        Total time: 29848.18s
                               ETA: 1009082.6s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.864s, learning 0.214s)
               Value function loss: 96432.3873
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 1251.10
               Mean episode length: 51.34
                  Mean reward/step: 26.21
       Mean episode length/episode: 7.11
            Mean episode successes: 2.2026
Mean episode consecutive_successes: 4.8588
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 16.08s
                        Total time: 29864.26s
                               ETA: 1009264.5s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.690s, learning 0.163s)
               Value function loss: 111719.8035
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 1255.87
               Mean episode length: 50.74
                  Mean reward/step: 27.34
       Mean episode length/episode: 6.94
            Mean episode successes: 2.2251
Mean episode consecutive_successes: 4.8714
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 15.85s
                        Total time: 29880.12s
                               ETA: 1009438.7s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.764s, learning 0.162s)
               Value function loss: 105650.6307
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 1601.76
               Mean episode length: 51.66
                  Mean reward/step: 26.73
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2090
Mean episode consecutive_successes: 4.9241
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 15.93s
                        Total time: 29896.04s
                               ETA: 1009615.1s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.179s, learning 0.171s)
               Value function loss: 103907.6650
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 805.29
               Mean episode length: 46.43
                  Mean reward/step: 29.51
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2637
Mean episode consecutive_successes: 4.9084
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 16.35s
                        Total time: 29912.39s
                               ETA: 1009805.7s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.497s, learning 0.162s)
               Value function loss: 110991.7621
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 894.20
               Mean episode length: 48.00
                  Mean reward/step: 28.43
       Mean episode length/episode: 6.99
            Mean episode successes: 2.2949
Mean episode consecutive_successes: 4.9593
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 15.66s
                        Total time: 29928.05s
                               ETA: 1009972.9s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.362s, learning 0.170s)
               Value function loss: 105913.4746
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 1286.72
               Mean episode length: 48.41
                  Mean reward/step: 27.93
       Mean episode length/episode: 7.00
            Mean episode successes: 2.2510
Mean episode consecutive_successes: 5.0047
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 16.53s
                        Total time: 29944.58s
                               ETA: 1010169.4s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.294s, learning 0.191s)
               Value function loss: 132318.8609
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 1682.54
               Mean episode length: 48.62
                  Mean reward/step: 28.14
       Mean episode length/episode: 6.97
            Mean episode successes: 2.3384
Mean episode consecutive_successes: 5.0361
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 16.48s
                        Total time: 29961.07s
                               ETA: 1010364.2s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.090s, learning 0.192s)
               Value function loss: 145520.7191
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 1693.85
               Mean episode length: 50.95
                  Mean reward/step: 26.79
       Mean episode length/episode: 6.94
            Mean episode successes: 2.1528
Mean episode consecutive_successes: 5.0840
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 16.28s
                        Total time: 29977.35s
                               ETA: 1010552.0s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.943s, learning 0.157s)
               Value function loss: 90783.9141
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 1192.38
               Mean episode length: 48.50
                  Mean reward/step: 26.12
       Mean episode length/episode: 6.94
            Mean episode successes: 2.0874
Mean episode consecutive_successes: 5.0701
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 13.10s
                        Total time: 29990.45s
                               ETA: 1010632.4s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.359s, learning 0.212s)
               Value function loss: 83772.4221
                    Surrogate loss: -0.0045
             Mean action noise std: 0.72
                       Mean reward: 885.62
               Mean episode length: 46.35
                  Mean reward/step: 26.24
       Mean episode length/episode: 7.06
            Mean episode successes: 2.2788
Mean episode consecutive_successes: 4.9952
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 8.57s
                        Total time: 29999.02s
                               ETA: 1010560.2s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1995 steps/s (collection: 7.963s, learning 0.246s)
               Value function loss: 91438.7859
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 1281.64
               Mean episode length: 47.41
                  Mean reward/step: 24.73
       Mean episode length/episode: 6.97
            Mean episode successes: 2.2036
Mean episode consecutive_successes: 4.9994
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 8.21s
                        Total time: 30007.23s
                               ETA: 1010475.8s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.109s, learning 0.161s)
               Value function loss: 86638.2457
                    Surrogate loss: -0.0235
             Mean action noise std: 0.72
                       Mean reward: 1578.10
               Mean episode length: 49.79
                  Mean reward/step: 24.77
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1226
Mean episode consecutive_successes: 5.0261
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 8.27s
                        Total time: 30015.50s
                               ETA: 1010393.6s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.886s, learning 0.167s)
               Value function loss: 92884.7984
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 1201.30
               Mean episode length: 47.38
                  Mean reward/step: 23.65
       Mean episode length/episode: 6.95
            Mean episode successes: 2.0859
Mean episode consecutive_successes: 4.9925
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 8.05s
                        Total time: 30023.55s
                               ETA: 1010304.1s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.533s, learning 0.192s)
               Value function loss: 94039.7260
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 1148.04
               Mean episode length: 50.34
                  Mean reward/step: 25.12
       Mean episode length/episode: 6.95
            Mean episode successes: 2.1519
Mean episode consecutive_successes: 4.9308
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 8.73s
                        Total time: 30032.28s
                               ETA: 1010237.2s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.949s, learning 0.169s)
               Value function loss: 102559.5328
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 1274.03
               Mean episode length: 47.35
                  Mean reward/step: 25.24
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1431
Mean episode consecutive_successes: 4.9517
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 8.12s
                        Total time: 30040.40s
                               ETA: 1010150.0s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.073s, learning 0.161s)
               Value function loss: 109967.9340
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 933.11
               Mean episode length: 46.55
                  Mean reward/step: 23.89
       Mean episode length/episode: 6.96
            Mean episode successes: 2.0908
Mean episode consecutive_successes: 4.8824
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 8.23s
                        Total time: 30048.63s
                               ETA: 1010066.7s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.015s, learning 0.169s)
               Value function loss: 113476.8334
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 1126.89
               Mean episode length: 48.28
                  Mean reward/step: 25.70
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2607
Mean episode consecutive_successes: 4.8420
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 8.18s
                        Total time: 30056.81s
                               ETA: 1009981.8s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.220s, learning 0.200s)
               Value function loss: 87803.2443
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 950.40
               Mean episode length: 46.44
                  Mean reward/step: 25.58
       Mean episode length/episode: 7.03
            Mean episode successes: 2.1929
Mean episode consecutive_successes: 4.8711
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 8.42s
                        Total time: 30065.23s
                               ETA: 1009904.9s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.142s, learning 0.164s)
               Value function loss: 85899.1432
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 895.78
               Mean episode length: 46.81
                  Mean reward/step: 24.66
       Mean episode length/episode: 7.04
            Mean episode successes: 2.2168
Mean episode consecutive_successes: 4.8664
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 8.31s
                        Total time: 30073.54s
                               ETA: 1009824.2s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.127s, learning 0.162s)
               Value function loss: 96550.0820
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 1049.92
               Mean episode length: 48.85
                  Mean reward/step: 25.30
       Mean episode length/episode: 6.99
            Mean episode successes: 2.3032
Mean episode consecutive_successes: 4.8053
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 8.29s
                        Total time: 30081.83s
                               ETA: 1009742.9s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.024s, learning 0.172s)
               Value function loss: 107807.5924
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 1236.25
               Mean episode length: 47.35
                  Mean reward/step: 27.47
       Mean episode length/episode: 7.02
            Mean episode successes: 2.3198
Mean episode consecutive_successes: 4.8433
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 8.20s
                        Total time: 30090.03s
                               ETA: 1009658.6s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.129s, learning 0.171s)
               Value function loss: 117233.9061
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 1516.31
               Mean episode length: 51.36
                  Mean reward/step: 28.94
       Mean episode length/episode: 7.05
            Mean episode successes: 2.4150
Mean episode consecutive_successes: 4.8827
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 8.30s
                        Total time: 30098.33s
                               ETA: 1009577.9s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.957s, learning 0.167s)
               Value function loss: 111106.1217
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 1689.23
               Mean episode length: 51.49
                  Mean reward/step: 29.94
       Mean episode length/episode: 7.07
            Mean episode successes: 2.5454
Mean episode consecutive_successes: 4.9314
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 8.12s
                        Total time: 30106.45s
                               ETA: 1009491.2s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.157s, learning 0.155s)
               Value function loss: 109526.6125
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 1507.12
               Mean episode length: 48.64
                  Mean reward/step: 31.17
       Mean episode length/episode: 6.99
            Mean episode successes: 2.6255
Mean episode consecutive_successes: 4.9860
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 8.31s
                        Total time: 30114.76s
                               ETA: 1009411.0s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.503s, learning 0.265s)
               Value function loss: 112023.8256
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 1371.82
               Mean episode length: 48.43
                  Mean reward/step: 30.97
       Mean episode length/episode: 7.03
            Mean episode successes: 2.6143
Mean episode consecutive_successes: 5.0503
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 8.77s
                        Total time: 30123.53s
                               ETA: 1009346.1s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.388s, learning 0.164s)
               Value function loss: 121951.4596
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 1824.36
               Mean episode length: 51.45
                  Mean reward/step: 30.73
       Mean episode length/episode: 7.00
            Mean episode successes: 2.5679
Mean episode consecutive_successes: 5.1400
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 8.55s
                        Total time: 30132.08s
                               ETA: 1009274.0s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.162s)
               Value function loss: 123833.6275
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 1771.17
               Mean episode length: 50.36
                  Mean reward/step: 32.39
       Mean episode length/episode: 7.00
            Mean episode successes: 2.6631
Mean episode consecutive_successes: 5.2094
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 8.24s
                        Total time: 30140.33s
                               ETA: 1009191.6s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.165s, learning 0.189s)
               Value function loss: 155235.0992
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 1769.57
               Mean episode length: 51.77
                  Mean reward/step: 33.41
       Mean episode length/episode: 7.07
            Mean episode successes: 2.8716
Mean episode consecutive_successes: 5.2671
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 8.35s
                        Total time: 30148.68s
                               ETA: 1009113.0s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.206s, learning 0.247s)
               Value function loss: 209381.4559
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 1169.99
               Mean episode length: 49.65
                  Mean reward/step: 34.28
       Mean episode length/episode: 6.98
            Mean episode successes: 2.8320
Mean episode consecutive_successes: 5.3253
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 8.45s
                        Total time: 30157.13s
                               ETA: 1009037.7s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.257s, learning 0.161s)
               Value function loss: 153435.7270
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 1564.00
               Mean episode length: 47.31
                  Mean reward/step: 34.07
       Mean episode length/episode: 7.00
            Mean episode successes: 2.9233
Mean episode consecutive_successes: 5.4410
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 8.42s
                        Total time: 30165.55s
                               ETA: 1008961.3s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.164s)
               Value function loss: 125906.2113
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 1938.84
               Mean episode length: 51.41
                  Mean reward/step: 32.89
       Mean episode length/episode: 7.00
            Mean episode successes: 2.8086
Mean episode consecutive_successes: 5.5708
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 8.39s
                        Total time: 30173.94s
                               ETA: 1008884.0s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.161s)
               Value function loss: 126763.9721
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 1754.98
               Mean episode length: 49.29
                  Mean reward/step: 33.69
       Mean episode length/episode: 7.09
            Mean episode successes: 2.9907
Mean episode consecutive_successes: 5.6058
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 8.45s
                        Total time: 30182.39s
                               ETA: 1008808.7s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.313s, learning 0.159s)
               Value function loss: 135345.3969
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 2306.79
               Mean episode length: 51.11
                  Mean reward/step: 34.44
       Mean episode length/episode: 7.02
            Mean episode successes: 2.9565
Mean episode consecutive_successes: 5.7251
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 8.47s
                        Total time: 30190.86s
                               ETA: 1008734.2s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.288s, learning 0.158s)
               Value function loss: 134844.1020
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 1441.38
               Mean episode length: 46.76
                  Mean reward/step: 33.24
       Mean episode length/episode: 6.97
            Mean episode successes: 3.0825
Mean episode consecutive_successes: 5.6826
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 8.45s
                        Total time: 30199.31s
                               ETA: 1008658.9s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.130s, learning 0.237s)
               Value function loss: 213375.9973
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 2028.57
               Mean episode length: 50.81
                  Mean reward/step: 31.83
       Mean episode length/episode: 6.91
            Mean episode successes: 2.8418
Mean episode consecutive_successes: 5.7896
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 8.37s
                        Total time: 30207.67s
                               ETA: 1008581.0s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.154s, learning 0.183s)
               Value function loss: 163802.9293
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 1502.08
               Mean episode length: 48.43
                  Mean reward/step: 27.64
       Mean episode length/episode: 6.97
            Mean episode successes: 2.5186
Mean episode consecutive_successes: 5.8468
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 8.34s
                        Total time: 30216.01s
                               ETA: 1008502.2s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.081s, learning 0.167s)
               Value function loss: 122139.6092
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 1633.50
               Mean episode length: 50.39
                  Mean reward/step: 27.86
       Mean episode length/episode: 7.06
            Mean episode successes: 2.5312
Mean episode consecutive_successes: 5.8331
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 8.25s
                        Total time: 30224.26s
                               ETA: 1008420.4s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.225s, learning 0.162s)
               Value function loss: 113207.8445
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 1716.13
               Mean episode length: 50.61
                  Mean reward/step: 29.50
       Mean episode length/episode: 6.93
            Mean episode successes: 2.5605
Mean episode consecutive_successes: 5.8296
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 8.39s
                        Total time: 30232.64s
                               ETA: 1008343.3s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.105s, learning 0.172s)
               Value function loss: 116241.0953
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 1239.00
               Mean episode length: 47.77
                  Mean reward/step: 30.97
       Mean episode length/episode: 7.05
            Mean episode successes: 2.6177
Mean episode consecutive_successes: 5.7978
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 8.28s
                        Total time: 30240.92s
                               ETA: 1008262.6s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.082s, learning 0.168s)
               Value function loss: 109620.2373
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 1624.17
               Mean episode length: 48.00
                  Mean reward/step: 34.00
       Mean episode length/episode: 7.04
            Mean episode successes: 2.7959
Mean episode consecutive_successes: 5.8097
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 8.25s
                        Total time: 30249.17s
                               ETA: 1008181.1s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.018s, learning 0.185s)
               Value function loss: 116664.1443
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 1905.27
               Mean episode length: 49.17
                  Mean reward/step: 34.04
       Mean episode length/episode: 7.04
            Mean episode successes: 3.0029
Mean episode consecutive_successes: 5.8014
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 8.20s
                        Total time: 30257.37s
                               ETA: 1008098.0s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.160s)
               Value function loss: 127378.4244
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 1814.58
               Mean episode length: 52.24
                  Mean reward/step: 32.04
       Mean episode length/episode: 7.01
            Mean episode successes: 3.0522
Mean episode consecutive_successes: 5.7905
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 8.31s
                        Total time: 30265.68s
                               ETA: 1008018.5s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.926s, learning 0.161s)
               Value function loss: 125368.5219
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 1692.73
               Mean episode length: 49.63
                  Mean reward/step: 33.37
       Mean episode length/episode: 7.01
            Mean episode successes: 3.0801
Mean episode consecutive_successes: 5.8517
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 8.09s
                        Total time: 30273.77s
                               ETA: 1007931.7s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.093s, learning 0.168s)
               Value function loss: 143160.9973
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 1674.87
               Mean episode length: 52.32
                  Mean reward/step: 34.32
       Mean episode length/episode: 7.02
            Mean episode successes: 2.9224
Mean episode consecutive_successes: 5.9957
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 8.26s
                        Total time: 30282.03s
                               ETA: 1007850.7s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.158s)
               Value function loss: 140635.9387
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 1937.59
               Mean episode length: 48.95
                  Mean reward/step: 33.48
       Mean episode length/episode: 7.07
            Mean episode successes: 2.8281
Mean episode consecutive_successes: 6.1274
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 8.18s
                        Total time: 30290.21s
                               ETA: 1007767.1s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.264s, learning 0.261s)
               Value function loss: 164988.4926
                    Surrogate loss: -0.0068
             Mean action noise std: 0.71
                       Mean reward: 1524.57
               Mean episode length: 48.30
                  Mean reward/step: 32.19
       Mean episode length/episode: 7.04
            Mean episode successes: 2.7715
Mean episode consecutive_successes: 6.1721
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 8.53s
                        Total time: 30298.73s
                               ETA: 1007695.0s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.193s, learning 0.213s)
               Value function loss: 198885.6652
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 1286.56
               Mean episode length: 46.93
                  Mean reward/step: 33.17
       Mean episode length/episode: 6.93
            Mean episode successes: 2.8398
Mean episode consecutive_successes: 6.1159
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 8.41s
                        Total time: 30307.14s
                               ETA: 1007619.0s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.947s, learning 0.158s)
               Value function loss: 166730.6379
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 1674.20
               Mean episode length: 51.99
                  Mean reward/step: 34.31
       Mean episode length/episode: 7.05
            Mean episode successes: 2.8906
Mean episode consecutive_successes: 6.1696
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 8.10s
                        Total time: 30315.24s
                               ETA: 1007533.0s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.397s, learning 0.183s)
               Value function loss: 115121.2982
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 1520.19
               Mean episode length: 49.49
                  Mean reward/step: 32.49
       Mean episode length/episode: 6.98
            Mean episode successes: 2.8750
Mean episode consecutive_successes: 6.1807
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 8.58s
                        Total time: 30323.83s
                               ETA: 1007462.9s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.367s, learning 0.170s)
               Value function loss: 97967.8566
                    Surrogate loss: -0.0241
             Mean action noise std: 0.71
                       Mean reward: 1255.16
               Mean episode length: 45.78
                  Mean reward/step: 32.38
       Mean episode length/episode: 6.91
            Mean episode successes: 2.8960
Mean episode consecutive_successes: 6.1405
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 8.54s
                        Total time: 30332.36s
                               ETA: 1007391.4s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.167s)
               Value function loss: 93450.3316
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 1368.37
               Mean episode length: 50.35
                  Mean reward/step: 30.59
       Mean episode length/episode: 6.99
            Mean episode successes: 2.7559
Mean episode consecutive_successes: 6.1730
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 8.41s
                        Total time: 30340.77s
                               ETA: 1007315.8s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.052s, learning 0.158s)
               Value function loss: 97178.2789
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 1539.82
               Mean episode length: 48.93
                  Mean reward/step: 30.40
       Mean episode length/episode: 7.07
            Mean episode successes: 2.7344
Mean episode consecutive_successes: 6.1876
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 8.21s
                        Total time: 30348.98s
                               ETA: 1007233.5s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.957s, learning 0.210s)
               Value function loss: 93400.9707
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 1445.30
               Mean episode length: 49.42
                  Mean reward/step: 30.00
       Mean episode length/episode: 7.06
            Mean episode successes: 2.7104
Mean episode consecutive_successes: 6.2070
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 8.17s
                        Total time: 30357.15s
                               ETA: 1007149.8s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.147s, learning 0.233s)
               Value function loss: 96376.7830
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 792.55
               Mean episode length: 45.81
                  Mean reward/step: 31.32
       Mean episode length/episode: 6.92
            Mean episode successes: 2.6392
Mean episode consecutive_successes: 6.1634
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 8.38s
                        Total time: 30365.53s
                               ETA: 1007073.3s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.165s)
               Value function loss: 103146.8455
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 1688.12
               Mean episode length: 50.28
                  Mean reward/step: 32.47
       Mean episode length/episode: 7.04
            Mean episode successes: 2.8140
Mean episode consecutive_successes: 6.1310
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 8.46s
                        Total time: 30373.99s
                               ETA: 1006999.4s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.742s, learning 0.174s)
               Value function loss: 108946.7031
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 1743.74
               Mean episode length: 50.55
                  Mean reward/step: 31.93
       Mean episode length/episode: 6.99
            Mean episode successes: 2.8286
Mean episode consecutive_successes: 6.1311
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 7.92s
                        Total time: 30381.91s
                               ETA: 1006907.6s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.291s, learning 0.188s)
               Value function loss: 108112.7254
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 1636.18
               Mean episode length: 51.45
                  Mean reward/step: 30.90
       Mean episode length/episode: 7.07
            Mean episode successes: 2.9141
Mean episode consecutive_successes: 6.1305
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 8.48s
                        Total time: 30390.39s
                               ETA: 1006834.5s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.469s, learning 0.210s)
               Value function loss: 118354.0719
                    Surrogate loss: -0.0171
             Mean action noise std: 0.71
                       Mean reward: 1717.01
               Mean episode length: 47.78
                  Mean reward/step: 31.79
       Mean episode length/episode: 6.94
            Mean episode successes: 2.9126
Mean episode consecutive_successes: 6.0809
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 8.68s
                        Total time: 30399.06s
                               ETA: 1006768.0s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.473s, learning 0.171s)
               Value function loss: 128154.1957
                    Surrogate loss: 0.0447
             Mean action noise std: 0.71
                       Mean reward: 2025.90
               Mean episode length: 49.89
                  Mean reward/step: 33.67
       Mean episode length/episode: 7.03
            Mean episode successes: 2.8428
Mean episode consecutive_successes: 6.1604
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 8.64s
                        Total time: 30407.71s
                               ETA: 1006700.5s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.129s, learning 0.167s)
               Value function loss: 135118.9813
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 1665.11
               Mean episode length: 48.94
                  Mean reward/step: 33.73
       Mean episode length/episode: 7.04
            Mean episode successes: 2.8940
Mean episode consecutive_successes: 6.1847
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 8.30s
                        Total time: 30416.00s
                               ETA: 1006621.4s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.025s, learning 0.231s)
               Value function loss: 134611.9256
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 1199.65
               Mean episode length: 48.50
                  Mean reward/step: 32.35
       Mean episode length/episode: 7.01
            Mean episode successes: 2.9395
Mean episode consecutive_successes: 6.1893
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 8.26s
                        Total time: 30424.26s
                               ETA: 1006541.1s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.937s, learning 0.160s)
               Value function loss: 159355.6863
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 1394.19
               Mean episode length: 48.19
                  Mean reward/step: 32.39
       Mean episode length/episode: 7.05
            Mean episode successes: 3.1665
Mean episode consecutive_successes: 6.1142
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 8.10s
                        Total time: 30432.36s
                               ETA: 1006455.6s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.285s, learning 0.221s)
               Value function loss: 174529.5766
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 1776.21
               Mean episode length: 50.61
                  Mean reward/step: 33.29
       Mean episode length/episode: 7.00
            Mean episode successes: 3.1338
Mean episode consecutive_successes: 6.1385
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 8.51s
                        Total time: 30440.86s
                               ETA: 1006383.7s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.340s, learning 0.167s)
               Value function loss: 178030.6289
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 1352.57
               Mean episode length: 49.25
                  Mean reward/step: 31.00
       Mean episode length/episode: 6.95
            Mean episode successes: 2.8018
Mean episode consecutive_successes: 6.2253
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 8.51s
                        Total time: 30449.37s
                               ETA: 1006311.8s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.810s, learning 0.173s)
               Value function loss: 148152.0377
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 1452.89
               Mean episode length: 49.69
                  Mean reward/step: 30.01
       Mean episode length/episode: 7.02
            Mean episode successes: 2.7251
Mean episode consecutive_successes: 6.2908
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 7.98s
                        Total time: 30457.35s
                               ETA: 1006222.6s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.414s, learning 0.182s)
               Value function loss: 119150.2469
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 1680.45
               Mean episode length: 49.63
                  Mean reward/step: 30.72
       Mean episode length/episode: 7.03
            Mean episode successes: 2.8374
Mean episode consecutive_successes: 6.2236
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 8.60s
                        Total time: 30465.95s
                               ETA: 1006153.8s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.335s, learning 0.261s)
               Value function loss: 123111.5014
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 1683.20
               Mean episode length: 50.45
                  Mean reward/step: 31.21
       Mean episode length/episode: 6.96
            Mean episode successes: 2.8638
Mean episode consecutive_successes: 6.1947
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 8.60s
                        Total time: 30474.55s
                               ETA: 1006085.0s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.240s, learning 0.168s)
               Value function loss: 119893.0652
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 2028.85
               Mean episode length: 51.16
                  Mean reward/step: 32.40
       Mean episode length/episode: 7.02
            Mean episode successes: 3.0029
Mean episode consecutive_successes: 6.1869
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 8.41s
                        Total time: 30482.95s
                               ETA: 1006010.0s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 2008 steps/s (collection: 8.002s, learning 0.155s)
               Value function loss: 142087.3687
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: 1214.58
               Mean episode length: 48.55
                  Mean reward/step: 32.53
       Mean episode length/episode: 7.06
            Mean episode successes: 3.1338
Mean episode consecutive_successes: 6.1137
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 8.16s
                        Total time: 30491.11s
                               ETA: 1005926.8s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.213s, learning 0.163s)
               Value function loss: 124022.3250
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 2023.42
               Mean episode length: 50.35
                  Mean reward/step: 31.74
       Mean episode length/episode: 7.05
            Mean episode successes: 3.0547
Mean episode consecutive_successes: 6.2460
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 8.38s
                        Total time: 30499.49s
                               ETA: 1005850.9s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.811s, learning 0.184s)
               Value function loss: 114404.0275
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 1545.27
               Mean episode length: 49.78
                  Mean reward/step: 32.32
       Mean episode length/episode: 7.01
            Mean episode successes: 2.8486
Mean episode consecutive_successes: 6.2632
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 7.99s
                        Total time: 30507.48s
                               ETA: 1005762.4s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.078s, learning 0.186s)
               Value function loss: 117745.5787
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 2012.68
               Mean episode length: 52.71
                  Mean reward/step: 31.16
       Mean episode length/episode: 7.02
            Mean episode successes: 2.8311
Mean episode consecutive_successes: 6.3013
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 8.26s
                        Total time: 30515.74s
                               ETA: 1005682.9s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.015s, learning 0.166s)
               Value function loss: 119652.2732
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 1679.77
               Mean episode length: 52.63
                  Mean reward/step: 32.13
       Mean episode length/episode: 7.07
            Mean episode successes: 2.7568
Mean episode consecutive_successes: 6.3350
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 8.18s
                        Total time: 30523.93s
                               ETA: 1005600.7s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.423s, learning 0.166s)
               Value function loss: 113035.5684
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 1189.68
               Mean episode length: 49.28
                  Mean reward/step: 33.61
       Mean episode length/episode: 6.96
            Mean episode successes: 2.9995
Mean episode consecutive_successes: 6.2172
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 8.59s
                        Total time: 30532.52s
                               ETA: 1005532.0s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.945s, learning 0.176s)
               Value function loss: 116999.3799
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: 1208.59
               Mean episode length: 46.75
                  Mean reward/step: 33.30
       Mean episode length/episode: 6.98
            Mean episode successes: 3.1455
Mean episode consecutive_successes: 6.1639
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 8.12s
                        Total time: 30540.64s
                               ETA: 1005447.9s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.168s)
               Value function loss: 125965.5156
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 2067.53
               Mean episode length: 56.90
                  Mean reward/step: 32.55
       Mean episode length/episode: 7.00
            Mean episode successes: 3.0552
Mean episode consecutive_successes: 6.2528
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 8.29s
                        Total time: 30548.93s
                               ETA: 1005369.4s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.212s, learning 0.163s)
               Value function loss: 137205.2686
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 1461.12
               Mean episode length: 46.42
                  Mean reward/step: 30.84
       Mean episode length/episode: 6.97
            Mean episode successes: 2.8130
Mean episode consecutive_successes: 6.2893
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 8.38s
                        Total time: 30557.30s
                               ETA: 1005293.8s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.268s, learning 0.194s)
               Value function loss: 120491.3502
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 1558.92
               Mean episode length: 50.39
                  Mean reward/step: 30.71
       Mean episode length/episode: 6.99
            Mean episode successes: 2.9307
Mean episode consecutive_successes: 6.2203
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 8.46s
                        Total time: 30565.76s
                               ETA: 1005221.1s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.438s, learning 0.183s)
               Value function loss: 162916.1621
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 1938.75
               Mean episode length: 51.22
                  Mean reward/step: 30.48
       Mean episode length/episode: 6.97
            Mean episode successes: 2.7754
Mean episode consecutive_successes: 6.2773
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 8.62s
                        Total time: 30574.38s
                               ETA: 1005153.6s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.382s, learning 0.166s)
               Value function loss: 185632.5688
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 1140.39
               Mean episode length: 45.63
                  Mean reward/step: 29.46
       Mean episode length/episode: 7.00
            Mean episode successes: 2.6797
Mean episode consecutive_successes: 6.2408
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 8.55s
                        Total time: 30582.93s
                               ETA: 1005083.8s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.818s, learning 0.168s)
               Value function loss: 130490.1676
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 1156.79
               Mean episode length: 47.84
                  Mean reward/step: 27.95
       Mean episode length/episode: 6.99
            Mean episode successes: 2.7856
Mean episode consecutive_successes: 6.1419
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 7.99s
                        Total time: 30590.92s
                               ETA: 1004995.5s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.317s, learning 0.162s)
               Value function loss: 109106.7641
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 1526.68
               Mean episode length: 47.69
                  Mean reward/step: 29.85
       Mean episode length/episode: 7.01
            Mean episode successes: 2.8369
Mean episode consecutive_successes: 6.1084
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 8.48s
                        Total time: 30599.40s
                               ETA: 1004923.5s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.767s, learning 0.162s)
               Value function loss: 108614.5881
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: 1580.32
               Mean episode length: 48.65
                  Mean reward/step: 30.43
       Mean episode length/episode: 6.90
            Mean episode successes: 2.7749
Mean episode consecutive_successes: 6.0828
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 8.93s
                        Total time: 30608.33s
                               ETA: 1004866.4s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.225s, learning 0.162s)
               Value function loss: 109699.8357
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 1216.46
               Mean episode length: 47.28
                  Mean reward/step: 30.72
       Mean episode length/episode: 6.99
            Mean episode successes: 2.8135
Mean episode consecutive_successes: 6.0582
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 8.39s
                        Total time: 30616.71s
                               ETA: 1004791.5s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.254s, learning 0.163s)
               Value function loss: 119772.4527
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 1722.74
               Mean episode length: 48.68
                  Mean reward/step: 32.05
       Mean episode length/episode: 7.00
            Mean episode successes: 2.7900
Mean episode consecutive_successes: 6.0997
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 8.42s
                        Total time: 30625.13s
                               ETA: 1004717.6s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.200s, learning 0.185s)
               Value function loss: 125970.3369
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 1220.49
               Mean episode length: 45.78
                  Mean reward/step: 31.93
       Mean episode length/episode: 6.97
            Mean episode successes: 3.0278
Mean episode consecutive_successes: 5.9772
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 8.38s
                        Total time: 30633.51s
                               ETA: 1004642.6s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.174s, learning 0.187s)
               Value function loss: 129917.4635
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 1178.66
               Mean episode length: 46.90
                  Mean reward/step: 33.24
       Mean episode length/episode: 6.98
            Mean episode successes: 2.9956
Mean episode consecutive_successes: 5.9892
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 8.36s
                        Total time: 30641.88s
                               ETA: 1004567.0s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.073s, learning 0.163s)
               Value function loss: 133696.3941
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 1622.15
               Mean episode length: 48.52
                  Mean reward/step: 32.87
       Mean episode length/episode: 7.06
            Mean episode successes: 3.2153
Mean episode consecutive_successes: 5.9940
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 8.24s
                        Total time: 30650.11s
                               ETA: 1004487.3s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.219s, learning 0.165s)
               Value function loss: 135148.7355
                    Surrogate loss: -0.0143
             Mean action noise std: 0.71
                       Mean reward: 1679.57
               Mean episode length: 48.62
                  Mean reward/step: 33.92
       Mean episode length/episode: 7.02
            Mean episode successes: 3.1372
Mean episode consecutive_successes: 6.0651
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 8.38s
                        Total time: 30658.50s
                               ETA: 1004412.5s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.036s, learning 0.201s)
               Value function loss: 142327.8258
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 1212.76
               Mean episode length: 45.56
                  Mean reward/step: 34.51
       Mean episode length/episode: 7.00
            Mean episode successes: 3.2017
Mean episode consecutive_successes: 6.0784
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 8.24s
                        Total time: 30666.73s
                               ETA: 1004332.9s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.162s, learning 0.278s)
               Value function loss: 151254.7543
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 1755.51
               Mean episode length: 49.93
                  Mean reward/step: 33.21
       Mean episode length/episode: 7.01
            Mean episode successes: 3.2271
Mean episode consecutive_successes: 6.1435
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 8.44s
                        Total time: 30675.17s
                               ETA: 1004260.0s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.145s, learning 0.155s)
               Value function loss: 150980.2023
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: 1941.90
               Mean episode length: 51.87
                  Mean reward/step: 33.00
       Mean episode length/episode: 6.99
            Mean episode successes: 3.0103
Mean episode consecutive_successes: 6.2585
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 8.30s
                        Total time: 30683.47s
                               ETA: 1004182.6s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.317s, learning 0.164s)
               Value function loss: 148995.0561
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 1662.25
               Mean episode length: 48.42
                  Mean reward/step: 32.83
       Mean episode length/episode: 7.05
            Mean episode successes: 2.9819
Mean episode consecutive_successes: 6.2875
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 8.48s
                        Total time: 30691.95s
                               ETA: 1004111.2s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.092s, learning 0.185s)
               Value function loss: 195661.3852
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 1304.42
               Mean episode length: 48.40
                  Mean reward/step: 33.28
       Mean episode length/episode: 7.02
            Mean episode successes: 2.8477
Mean episode consecutive_successes: 6.3482
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 8.28s
                        Total time: 30700.23s
                               ETA: 1004033.1s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.243s, learning 0.261s)
               Value function loss: 175046.2473
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: 1448.57
               Mean episode length: 47.10
                  Mean reward/step: 32.97
       Mean episode length/episode: 7.07
            Mean episode successes: 2.9302
Mean episode consecutive_successes: 6.3500
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 8.50s
                        Total time: 30708.73s
                               ETA: 1003962.5s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.244s, learning 0.187s)
               Value function loss: 107624.6715
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 1402.61
               Mean episode length: 49.55
                  Mean reward/step: 31.51
       Mean episode length/episode: 6.98
            Mean episode successes: 2.8555
Mean episode consecutive_successes: 6.3359
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 8.43s
                        Total time: 30717.17s
                               ETA: 1003889.5s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.171s, learning 0.184s)
               Value function loss: 104845.6238
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 1750.66
               Mean episode length: 49.99
                  Mean reward/step: 33.36
       Mean episode length/episode: 7.00
            Mean episode successes: 2.9136
Mean episode consecutive_successes: 6.3621
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 8.36s
                        Total time: 30725.52s
                               ETA: 1003814.1s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.227s, learning 0.161s)
               Value function loss: 103486.8588
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 1954.72
               Mean episode length: 52.51
                  Mean reward/step: 34.72
       Mean episode length/episode: 7.06
            Mean episode successes: 2.9155
Mean episode consecutive_successes: 6.4092
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 8.39s
                        Total time: 30733.91s
                               ETA: 1003739.9s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.353s, learning 0.162s)
               Value function loss: 109163.6289
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 1832.74
               Mean episode length: 49.73
                  Mean reward/step: 33.66
       Mean episode length/episode: 7.06
            Mean episode successes: 3.0059
Mean episode consecutive_successes: 6.4321
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 8.51s
                        Total time: 30742.42s
                               ETA: 1003669.8s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.090s, learning 0.206s)
               Value function loss: 123047.2928
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 1816.35
               Mean episode length: 51.09
                  Mean reward/step: 34.13
       Mean episode length/episode: 7.03
            Mean episode successes: 2.9766
Mean episode consecutive_successes: 6.4379
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 8.30s
                        Total time: 30750.72s
                               ETA: 1003592.6s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.098s, learning 0.159s)
               Value function loss: 134778.4473
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 1933.76
               Mean episode length: 50.79
                  Mean reward/step: 33.24
       Mean episode length/episode: 7.02
            Mean episode successes: 2.8667
Mean episode consecutive_successes: 6.5384
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 8.26s
                        Total time: 30758.98s
                               ETA: 1003514.2s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.468s, learning 0.172s)
               Value function loss: 166127.0582
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 1676.49
               Mean episode length: 47.58
                  Mean reward/step: 33.68
       Mean episode length/episode: 7.07
            Mean episode successes: 2.7935
Mean episode consecutive_successes: 6.5893
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 8.64s
                        Total time: 30767.62s
                               ETA: 1003448.3s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.248s, learning 0.163s)
               Value function loss: 138820.7730
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 1774.94
               Mean episode length: 53.08
                  Mean reward/step: 31.51
       Mean episode length/episode: 7.09
            Mean episode successes: 2.7676
Mean episode consecutive_successes: 6.6476
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 8.41s
                        Total time: 30776.03s
                               ETA: 1003375.0s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.157s, learning 0.192s)
               Value function loss: 110452.4486
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 1458.28
               Mean episode length: 50.94
                  Mean reward/step: 31.04
       Mean episode length/episode: 7.01
            Mean episode successes: 2.7148
Mean episode consecutive_successes: 6.6297
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 8.35s
                        Total time: 30784.38s
                               ETA: 1003299.7s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.414s, learning 0.163s)
               Value function loss: 107584.3412
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 1906.90
               Mean episode length: 52.12
                  Mean reward/step: 29.01
       Mean episode length/episode: 6.93
            Mean episode successes: 2.4756
Mean episode consecutive_successes: 6.6720
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 8.58s
                        Total time: 30792.95s
                               ETA: 1003231.9s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.471s, learning 0.184s)
               Value function loss: 98899.6336
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 1649.31
               Mean episode length: 51.32
                  Mean reward/step: 26.05
       Mean episode length/episode: 6.94
            Mean episode successes: 2.0928
Mean episode consecutive_successes: 6.6951
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 8.65s
                        Total time: 30801.61s
                               ETA: 1003166.7s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.120s, learning 0.190s)
               Value function loss: 84720.3359
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 1296.46
               Mean episode length: 51.36
                  Mean reward/step: 24.98
       Mean episode length/episode: 7.04
            Mean episode successes: 1.9541
Mean episode consecutive_successes: 6.6364
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 8.31s
                        Total time: 30809.92s
                               ETA: 1003090.3s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.249s, learning 0.160s)
               Value function loss: 90549.3582
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 1074.08
               Mean episode length: 48.20
                  Mean reward/step: 25.16
       Mean episode length/episode: 6.99
            Mean episode successes: 1.8628
Mean episode consecutive_successes: 6.5079
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 8.41s
                        Total time: 30818.33s
                               ETA: 1003017.1s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.533s, learning 0.169s)
               Value function loss: 98769.6727
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: 1202.43
               Mean episode length: 49.16
                  Mean reward/step: 26.52
       Mean episode length/episode: 6.99
            Mean episode successes: 2.0669
Mean episode consecutive_successes: 6.3371
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 8.70s
                        Total time: 30827.03s
                               ETA: 1002953.5s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.217s, learning 0.158s)
               Value function loss: 97427.3867
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 1440.37
               Mean episode length: 49.31
                  Mean reward/step: 27.57
       Mean episode length/episode: 7.00
            Mean episode successes: 2.1494
Mean episode consecutive_successes: 6.2420
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 8.38s
                        Total time: 30835.40s
                               ETA: 1002879.4s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.394s, learning 0.261s)
               Value function loss: 99174.3938
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 1203.84
               Mean episode length: 47.54
                  Mean reward/step: 29.02
       Mean episode length/episode: 7.10
            Mean episode successes: 2.1455
Mean episode consecutive_successes: 6.1897
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 8.66s
                        Total time: 30844.06s
                               ETA: 1002814.4s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.261s, learning 0.162s)
               Value function loss: 101929.7371
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 1530.54
               Mean episode length: 49.75
                  Mean reward/step: 30.30
       Mean episode length/episode: 7.03
            Mean episode successes: 2.2939
Mean episode consecutive_successes: 6.1167
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 8.42s
                        Total time: 30852.48s
                               ETA: 1002741.9s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.326s, learning 0.164s)
               Value function loss: 108532.4801
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: 1250.21
               Mean episode length: 50.01
                  Mean reward/step: 31.03
       Mean episode length/episode: 7.06
            Mean episode successes: 2.5103
Mean episode consecutive_successes: 6.0160
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 8.49s
                        Total time: 30860.97s
                               ETA: 1002671.6s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.963s, learning 0.164s)
               Value function loss: 129703.8664
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 1714.85
               Mean episode length: 51.98
                  Mean reward/step: 32.14
       Mean episode length/episode: 6.96
            Mean episode successes: 2.5283
Mean episode consecutive_successes: 6.0136
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 8.13s
                        Total time: 30869.10s
                               ETA: 1002589.5s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.075s, learning 0.158s)
               Value function loss: 122714.7977
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 1665.52
               Mean episode length: 50.12
                  Mean reward/step: 32.04
       Mean episode length/episode: 7.00
            Mean episode successes: 2.4590
Mean episode consecutive_successes: 6.0252
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 8.23s
                        Total time: 30877.33s
                               ETA: 1002510.9s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.417s, learning 0.187s)
               Value function loss: 118846.4664
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 1400.37
               Mean episode length: 49.53
                  Mean reward/step: 31.88
       Mean episode length/episode: 7.05
            Mean episode successes: 2.5996
Mean episode consecutive_successes: 5.9847
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 8.60s
                        Total time: 30885.94s
                               ETA: 1002444.4s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.467s, learning 0.165s)
               Value function loss: 127038.0068
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 1229.84
               Mean episode length: 49.72
                  Mean reward/step: 31.27
       Mean episode length/episode: 7.09
            Mean episode successes: 2.6836
Mean episode consecutive_successes: 6.0225
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 8.63s
                        Total time: 30894.57s
                               ETA: 1002378.9s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.919s, learning 0.162s)
               Value function loss: 120590.4783
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 1317.01
               Mean episode length: 48.39
                  Mean reward/step: 31.93
       Mean episode length/episode: 7.02
            Mean episode successes: 2.5039
Mean episode consecutive_successes: 6.1132
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 8.08s
                        Total time: 30902.65s
                               ETA: 1002295.5s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.495s, learning 0.195s)
               Value function loss: 121060.6846
                    Surrogate loss: 0.0075
             Mean action noise std: 0.71
                       Mean reward: 1490.36
               Mean episode length: 49.94
                  Mean reward/step: 31.80
       Mean episode length/episode: 7.04
            Mean episode successes: 2.6650
Mean episode consecutive_successes: 6.0528
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 8.69s
                        Total time: 30911.34s
                               ETA: 1002231.9s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.225s, learning 0.161s)
               Value function loss: 124552.2898
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 1741.65
               Mean episode length: 50.95
                  Mean reward/step: 32.70
       Mean episode length/episode: 6.98
            Mean episode successes: 2.6958
Mean episode consecutive_successes: 6.0557
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 8.39s
                        Total time: 30919.72s
                               ETA: 1002158.6s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.511s, learning 0.186s)
               Value function loss: 125213.2971
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 1524.11
               Mean episode length: 51.69
                  Mean reward/step: 31.22
       Mean episode length/episode: 7.02
            Mean episode successes: 2.6279
Mean episode consecutive_successes: 6.0927
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 8.70s
                        Total time: 30928.42s
                               ETA: 1002095.3s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.860s, learning 0.159s)
               Value function loss: 123461.4254
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 1748.70
               Mean episode length: 49.86
                  Mean reward/step: 32.27
       Mean episode length/episode: 7.07
            Mean episode successes: 2.8228
Mean episode consecutive_successes: 6.0721
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 8.02s
                        Total time: 30936.44s
                               ETA: 1002010.1s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.986s, learning 0.168s)
               Value function loss: 133154.8125
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 2082.43
               Mean episode length: 52.31
                  Mean reward/step: 32.96
       Mean episode length/episode: 7.03
            Mean episode successes: 2.8809
Mean episode consecutive_successes: 6.1121
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 8.15s
                        Total time: 30944.59s
                               ETA: 1001929.4s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.202s, learning 0.158s)
               Value function loss: 127043.1268
                    Surrogate loss: -0.0171
             Mean action noise std: 0.71
                       Mean reward: 1337.12
               Mean episode length: 47.99
                  Mean reward/step: 31.46
       Mean episode length/episode: 6.93
            Mean episode successes: 2.6694
Mean episode consecutive_successes: 6.0877
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 8.36s
                        Total time: 30952.96s
                               ETA: 1001855.3s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.123s, learning 0.192s)
               Value function loss: 133186.7383
                    Surrogate loss: -0.0143
             Mean action noise std: 0.71
                       Mean reward: 1965.03
               Mean episode length: 52.48
                  Mean reward/step: 30.16
       Mean episode length/episode: 7.02
            Mean episode successes: 2.5566
Mean episode consecutive_successes: 6.1676
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 8.31s
                        Total time: 30961.27s
                               ETA: 1001779.9s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.352s, learning 0.173s)
               Value function loss: 142776.6496
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 1681.54
               Mean episode length: 51.40
                  Mean reward/step: 31.38
       Mean episode length/episode: 7.12
            Mean episode successes: 2.6851
Mean episode consecutive_successes: 6.1722
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 8.53s
                        Total time: 30969.79s
                               ETA: 1001711.3s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.044s, learning 0.165s)
               Value function loss: 120911.4547
                    Surrogate loss: 0.0066
             Mean action noise std: 0.71
                       Mean reward: 1714.42
               Mean episode length: 47.59
                  Mean reward/step: 30.93
       Mean episode length/episode: 7.00
            Mean episode successes: 2.7197
Mean episode consecutive_successes: 6.1148
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 8.21s
                        Total time: 30978.00s
                               ETA: 1001632.5s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.322s, learning 0.157s)
               Value function loss: 121535.7219
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 1763.75
               Mean episode length: 50.94
                  Mean reward/step: 32.44
       Mean episode length/episode: 7.03
            Mean episode successes: 2.6763
Mean episode consecutive_successes: 6.1525
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 8.48s
                        Total time: 30986.48s
                               ETA: 1001562.4s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.148s, learning 0.166s)
               Value function loss: 137712.0238
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: 1091.73
               Mean episode length: 48.41
                  Mean reward/step: 29.84
       Mean episode length/episode: 6.96
            Mean episode successes: 2.5674
Mean episode consecutive_successes: 6.1394
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 8.31s
                        Total time: 30994.80s
                               ETA: 1001487.1s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.368s, learning 0.165s)
               Value function loss: 133842.6656
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 1525.84
               Mean episode length: 49.75
                  Mean reward/step: 30.59
       Mean episode length/episode: 7.06
            Mean episode successes: 2.5801
Mean episode consecutive_successes: 6.1480
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 8.53s
                        Total time: 31003.33s
                               ETA: 1001418.9s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.375s, learning 0.267s)
               Value function loss: 143337.9691
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 1664.80
               Mean episode length: 51.94
                  Mean reward/step: 31.83
       Mean episode length/episode: 7.06
            Mean episode successes: 2.6279
Mean episode consecutive_successes: 6.1835
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 8.64s
                        Total time: 31011.97s
                               ETA: 1001354.3s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.258s, learning 0.159s)
               Value function loss: 122215.0691
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 1550.53
               Mean episode length: 50.19
                  Mean reward/step: 33.46
       Mean episode length/episode: 7.06
            Mean episode successes: 2.7725
Mean episode consecutive_successes: 6.1704
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 8.42s
                        Total time: 31020.39s
                               ETA: 1001282.4s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.346s, learning 0.169s)
               Value function loss: 112110.0434
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 1707.26
               Mean episode length: 52.59
                  Mean reward/step: 32.06
       Mean episode length/episode: 7.04
            Mean episode successes: 2.6743
Mean episode consecutive_successes: 6.2439
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 8.51s
                        Total time: 31028.90s
                               ETA: 1001213.7s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.282s, learning 0.161s)
               Value function loss: 110856.9309
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 1981.30
               Mean episode length: 51.06
                  Mean reward/step: 31.25
       Mean episode length/episode: 7.02
            Mean episode successes: 2.6714
Mean episode consecutive_successes: 6.2728
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 8.44s
                        Total time: 31037.35s
                               ETA: 1001142.8s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.282s, learning 0.186s)
               Value function loss: 115232.1711
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 1423.56
               Mean episode length: 47.26
                  Mean reward/step: 33.10
       Mean episode length/episode: 7.05
            Mean episode successes: 2.8242
Mean episode consecutive_successes: 6.2232
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 8.47s
                        Total time: 31045.81s
                               ETA: 1001072.7s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.185s, learning 0.166s)
               Value function loss: 142670.4340
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 1573.77
               Mean episode length: 50.88
                  Mean reward/step: 33.12
       Mean episode length/episode: 7.06
            Mean episode successes: 2.7910
Mean episode consecutive_successes: 6.2883
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 8.35s
                        Total time: 31054.16s
                               ETA: 1000998.9s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.329s, learning 0.188s)
               Value function loss: 180220.0785
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 1952.76
               Mean episode length: 53.30
                  Mean reward/step: 33.31
       Mean episode length/episode: 7.03
            Mean episode successes: 2.8203
Mean episode consecutive_successes: 6.3286
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 8.52s
                        Total time: 31062.68s
                               ETA: 1000930.4s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.052s, learning 0.198s)
               Value function loss: 144678.2551
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 1779.31
               Mean episode length: 51.63
                  Mean reward/step: 31.34
       Mean episode length/episode: 7.10
            Mean episode successes: 2.6914
Mean episode consecutive_successes: 6.3863
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 8.25s
                        Total time: 31070.93s
                               ETA: 1000853.5s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.085s, learning 0.164s)
               Value function loss: 125992.0410
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 1626.31
               Mean episode length: 50.36
                  Mean reward/step: 30.60
       Mean episode length/episode: 6.99
            Mean episode successes: 2.7329
Mean episode consecutive_successes: 6.3464
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 8.25s
                        Total time: 31079.18s
                               ETA: 1000776.5s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.974s, learning 0.186s)
               Value function loss: 122023.9227
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 968.82
               Mean episode length: 47.58
                  Mean reward/step: 31.16
       Mean episode length/episode: 7.05
            Mean episode successes: 2.6929
Mean episode consecutive_successes: 6.3395
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 8.16s
                        Total time: 31087.34s
                               ETA: 1000696.7s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.348s, learning 0.173s)
               Value function loss: 134238.7314
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 1505.50
               Mean episode length: 53.53
                  Mean reward/step: 33.63
       Mean episode length/episode: 7.06
            Mean episode successes: 2.7012
Mean episode consecutive_successes: 6.3524
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 8.52s
                        Total time: 31095.86s
                               ETA: 1000628.6s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.146s, learning 0.173s)
               Value function loss: 136075.8133
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 1958.17
               Mean episode length: 49.82
                  Mean reward/step: 33.99
       Mean episode length/episode: 7.00
            Mean episode successes: 2.9307
Mean episode consecutive_successes: 6.3250
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 8.32s
                        Total time: 31104.18s
                               ETA: 1000553.9s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.894s, learning 0.167s)
               Value function loss: 142056.0020
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 2179.80
               Mean episode length: 52.22
                  Mean reward/step: 33.56
       Mean episode length/episode: 7.10
            Mean episode successes: 3.0024
Mean episode consecutive_successes: 6.3486
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 8.06s
                        Total time: 31112.24s
                               ETA: 1000471.1s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.166s, learning 0.192s)
               Value function loss: 148028.9734
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 1693.90
               Mean episode length: 51.50
                  Mean reward/step: 32.87
       Mean episode length/episode: 7.07
            Mean episode successes: 2.8887
Mean episode consecutive_successes: 6.3798
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 8.36s
                        Total time: 31120.60s
                               ETA: 1000397.8s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.245s, learning 0.160s)
               Value function loss: 167708.5805
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 1351.74
               Mean episode length: 47.96
                  Mean reward/step: 31.81
       Mean episode length/episode: 7.01
            Mean episode successes: 2.6582
Mean episode consecutive_successes: 6.4183
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 8.41s
                        Total time: 31129.01s
                               ETA: 1000326.2s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.810s, learning 0.164s)
               Value function loss: 143993.2832
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 1498.65
               Mean episode length: 46.83
                  Mean reward/step: 34.41
       Mean episode length/episode: 7.04
            Mean episode successes: 2.7485
Mean episode consecutive_successes: 6.4484
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 7.97s
                        Total time: 31136.98s
                               ETA: 1000240.7s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.299s, learning 0.186s)
               Value function loss: 129945.8766
                    Surrogate loss: 0.0051
             Mean action noise std: 0.71
                       Mean reward: 1443.27
               Mean episode length: 50.85
                  Mean reward/step: 35.42
       Mean episode length/episode: 7.09
            Mean episode successes: 2.8584
Mean episode consecutive_successes: 6.4631
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 8.49s
                        Total time: 31145.46s
                               ETA: 1000171.6s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.336s, learning 0.165s)
               Value function loss: 132348.7820
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 1737.08
               Mean episode length: 52.26
                  Mean reward/step: 34.48
       Mean episode length/episode: 7.05
            Mean episode successes: 2.7803
Mean episode consecutive_successes: 6.5468
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 8.50s
                        Total time: 31153.97s
                               ETA: 1000103.2s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.030s, learning 0.208s)
               Value function loss: 126491.6047
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 1907.72
               Mean episode length: 53.35
                  Mean reward/step: 35.44
       Mean episode length/episode: 7.12
            Mean episode successes: 2.8047
Mean episode consecutive_successes: 6.6325
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 8.24s
                        Total time: 31162.20s
                               ETA: 1000026.3s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.236s, learning 0.168s)
               Value function loss: 146109.9461
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 1564.47
               Mean episode length: 48.23
                  Mean reward/step: 37.38
       Mean episode length/episode: 7.06
            Mean episode successes: 3.0132
Mean episode consecutive_successes: 6.5982
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 8.40s
                        Total time: 31170.61s
                               ETA: 999954.8s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.806s, learning 0.166s)
               Value function loss: 186971.5508
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 1993.48
               Mean episode length: 54.54
                  Mean reward/step: 37.31
       Mean episode length/episode: 7.04
            Mean episode successes: 2.9697
Mean episode consecutive_successes: 6.7220
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 8.97s
                        Total time: 31179.58s
                               ETA: 999901.5s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.441s, learning 0.181s)
               Value function loss: 160416.9609
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 1777.20
               Mean episode length: 52.54
                  Mean reward/step: 33.80
       Mean episode length/episode: 7.05
            Mean episode successes: 2.9980
Mean episode consecutive_successes: 6.7127
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 8.62s
                        Total time: 31188.20s
                               ETA: 999837.1s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.015s, learning 0.166s)
               Value function loss: 111742.3500
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 2072.88
               Mean episode length: 49.61
                  Mean reward/step: 33.85
       Mean episode length/episode: 7.05
            Mean episode successes: 2.7544
Mean episode consecutive_successes: 6.8022
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 8.18s
                        Total time: 31196.38s
                               ETA: 999758.5s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.937s, learning 0.168s)
               Value function loss: 107363.1031
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 1962.55
               Mean episode length: 52.80
                  Mean reward/step: 33.53
       Mean episode length/episode: 7.02
            Mean episode successes: 2.7349
Mean episode consecutive_successes: 6.8444
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 8.11s
                        Total time: 31204.49s
                               ETA: 999677.6s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.348s, learning 0.172s)
               Value function loss: 108238.8225
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 1769.71
               Mean episode length: 52.51
                  Mean reward/step: 33.39
       Mean episode length/episode: 7.07
            Mean episode successes: 2.9644
Mean episode consecutive_successes: 6.7602
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 8.52s
                        Total time: 31213.01s
                               ETA: 999610.0s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.379s, learning 0.183s)
               Value function loss: 116520.3359
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 1689.44
               Mean episode length: 51.78
                  Mean reward/step: 33.31
       Mean episode length/episode: 7.05
            Mean episode successes: 2.8516
Mean episode consecutive_successes: 6.7801
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 8.56s
                        Total time: 31221.57s
                               ETA: 999543.8s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.809s, learning 0.159s)
               Value function loss: 127418.7756
                    Surrogate loss: 0.0044
             Mean action noise std: 0.71
                       Mean reward: 2142.17
               Mean episode length: 52.46
                  Mean reward/step: 34.02
       Mean episode length/episode: 7.04
            Mean episode successes: 2.8013
Mean episode consecutive_successes: 6.8454
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 7.97s
                        Total time: 31229.54s
                               ETA: 999458.6s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.052s, learning 0.185s)
               Value function loss: 111702.9580
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 2071.98
               Mean episode length: 51.47
                  Mean reward/step: 33.97
       Mean episode length/episode: 7.03
            Mean episode successes: 2.6626
Mean episode consecutive_successes: 6.8991
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 8.24s
                        Total time: 31237.78s
                               ETA: 999382.1s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.005s, learning 0.168s)
               Value function loss: 121489.2594
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 1765.59
               Mean episode length: 50.39
                  Mean reward/step: 35.30
       Mean episode length/episode: 7.08
            Mean episode successes: 2.8833
Mean episode consecutive_successes: 6.8429
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 8.17s
                        Total time: 31245.95s
                               ETA: 999303.6s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.900s, learning 0.162s)
               Value function loss: 135443.1242
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 1486.12
               Mean episode length: 50.66
                  Mean reward/step: 32.99
       Mean episode length/episode: 7.14
            Mean episode successes: 2.8228
Mean episode consecutive_successes: 6.8976
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 8.06s
                        Total time: 31254.01s
                               ETA: 999221.6s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.117s, learning 0.166s)
               Value function loss: 151641.2633
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 1797.53
               Mean episode length: 52.57
                  Mean reward/step: 34.13
       Mean episode length/episode: 7.00
            Mean episode successes: 2.7666
Mean episode consecutive_successes: 6.9152
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 8.28s
                        Total time: 31262.29s
                               ETA: 999146.6s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.315s, learning 0.175s)
               Value function loss: 171608.7311
                    Surrogate loss: 0.0242
             Mean action noise std: 0.71
                       Mean reward: 1658.98
               Mean episode length: 51.24
                  Mean reward/step: 34.24
       Mean episode length/episode: 7.02
            Mean episode successes: 2.6221
Mean episode consecutive_successes: 6.9721
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 8.49s
                        Total time: 31270.78s
                               ETA: 999078.4s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.889s, learning 0.171s)
               Value function loss: 121218.3807
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 1679.41
               Mean episode length: 52.64
                  Mean reward/step: 33.48
       Mean episode length/episode: 7.04
            Mean episode successes: 2.6611
Mean episode consecutive_successes: 6.9264
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 8.06s
                        Total time: 31278.84s
                               ETA: 998996.4s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.172s, learning 0.204s)
               Value function loss: 122281.1281
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 1759.87
               Mean episode length: 53.05
                  Mean reward/step: 32.04
       Mean episode length/episode: 7.20
            Mean episode successes: 2.7935
Mean episode consecutive_successes: 6.9365
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 8.38s
                        Total time: 31287.22s
                               ETA: 998924.6s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.126s, learning 0.187s)
               Value function loss: 127519.7295
                    Surrogate loss: 0.0047
             Mean action noise std: 0.71
                       Mean reward: 1460.32
               Mean episode length: 49.60
                  Mean reward/step: 30.84
       Mean episode length/episode: 6.97
            Mean episode successes: 2.6406
Mean episode consecutive_successes: 6.9017
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 8.31s
                        Total time: 31295.53s
                               ETA: 998850.8s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.006s, learning 0.181s)
               Value function loss: 124139.2527
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 1512.72
               Mean episode length: 49.69
                  Mean reward/step: 30.23
       Mean episode length/episode: 7.03
            Mean episode successes: 2.5615
Mean episode consecutive_successes: 6.8789
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 8.19s
                        Total time: 31303.72s
                               ETA: 998773.1s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.177s, learning 0.168s)
               Value function loss: 122237.0670
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 1398.73
               Mean episode length: 55.66
                  Mean reward/step: 32.29
       Mean episode length/episode: 6.97
            Mean episode successes: 2.5493
Mean episode consecutive_successes: 6.8078
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 8.34s
                        Total time: 31312.07s
                               ETA: 998700.4s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.057s, learning 0.173s)
               Value function loss: 197528.1102
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 1112.72
               Mean episode length: 49.00
                  Mean reward/step: 32.56
       Mean episode length/episode: 7.11
            Mean episode successes: 2.6836
Mean episode consecutive_successes: 6.7499
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 8.23s
                        Total time: 31320.30s
                               ETA: 998624.1s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.226s, learning 0.204s)
               Value function loss: 162902.2992
                    Surrogate loss: 0.0033
             Mean action noise std: 0.71
                       Mean reward: 1327.63
               Mean episode length: 48.77
                  Mean reward/step: 34.66
       Mean episode length/episode: 7.11
            Mean episode successes: 2.9629
Mean episode consecutive_successes: 6.6822
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 8.43s
                        Total time: 31328.73s
                               ETA: 998554.2s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.232s, learning 0.161s)
               Value function loss: 117374.9449
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 1683.58
               Mean episode length: 50.79
                  Mean reward/step: 36.56
       Mean episode length/episode: 7.04
            Mean episode successes: 3.1636
Mean episode consecutive_successes: 6.7059
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 8.39s
                        Total time: 31337.12s
                               ETA: 998483.2s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.198s, learning 0.162s)
               Value function loss: 125437.9705
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: 1767.06
               Mean episode length: 53.15
                  Mean reward/step: 36.92
       Mean episode length/episode: 7.02
            Mean episode successes: 3.1577
Mean episode consecutive_successes: 6.7330
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 8.36s
                        Total time: 31345.48s
                               ETA: 998411.2s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.275s, learning 0.158s)
               Value function loss: 109328.2449
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 1230.64
               Mean episode length: 49.68
                  Mean reward/step: 34.05
       Mean episode length/episode: 6.88
            Mean episode successes: 3.1313
Mean episode consecutive_successes: 6.6999
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 12.43s
                        Total time: 31357.91s
                               ETA: 998468.9s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.522s, learning 0.162s)
               Value function loss: 113553.9041
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 2351.77
               Mean episode length: 51.50
                  Mean reward/step: 34.59
       Mean episode length/episode: 7.07
            Mean episode successes: 3.0029
Mean episode consecutive_successes: 6.8454
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 15.68s
                        Total time: 31373.60s
                               ETA: 998630.0s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.095s, learning 0.195s)
               Value function loss: 112845.2332
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 1363.17
               Mean episode length: 50.63
                  Mean reward/step: 34.90
       Mean episode length/episode: 7.13
            Mean episode successes: 3.2788
Mean episode consecutive_successes: 6.7642
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 16.29s
                        Total time: 31389.89s
                               ETA: 998810.3s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.021s, learning 0.174s)
               Value function loss: 130943.7545
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 1651.63
               Mean episode length: 51.57
                  Mean reward/step: 34.94
       Mean episode length/episode: 6.94
            Mean episode successes: 3.1821
Mean episode consecutive_successes: 6.7563
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 16.19s
                        Total time: 31406.08s
                               ETA: 998987.5s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.082s, learning 0.173s)
               Value function loss: 177558.1361
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 2190.53
               Mean episode length: 53.23
                  Mean reward/step: 35.36
       Mean episode length/episode: 7.03
            Mean episode successes: 3.1123
Mean episode consecutive_successes: 6.8568
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 16.26s
                        Total time: 31422.34s
                               ETA: 999166.4s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.530s, learning 0.165s)
               Value function loss: 124610.9391
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 1585.41
               Mean episode length: 49.22
                  Mean reward/step: 35.39
       Mean episode length/episode: 7.06
            Mean episode successes: 3.2515
Mean episode consecutive_successes: 6.8037
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 15.69s
                        Total time: 31438.03s
                               ETA: 999327.4s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.179s, learning 0.166s)
               Value function loss: 125339.1127
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 1763.16
               Mean episode length: 49.88
                  Mean reward/step: 36.14
       Mean episode length/episode: 7.09
            Mean episode successes: 3.2715
Mean episode consecutive_successes: 6.8554
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 16.35s
                        Total time: 31454.38s
                               ETA: 999509.0s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.759s, learning 0.165s)
               Value function loss: 130762.6256
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 1765.70
               Mean episode length: 53.92
                  Mean reward/step: 38.91
       Mean episode length/episode: 7.11
            Mean episode successes: 3.4458
Mean episode consecutive_successes: 6.8786
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 15.92s
                        Total time: 31470.30s
                               ETA: 999677.0s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.344s, learning 0.163s)
               Value function loss: 145782.4191
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 1471.82
               Mean episode length: 52.30
                  Mean reward/step: 36.53
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4937
Mean episode consecutive_successes: 6.9144
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 16.51s
                        Total time: 31486.81s
                               ETA: 999863.4s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.159s, learning 0.165s)
               Value function loss: 136399.2574
                    Surrogate loss: 0.0137
             Mean action noise std: 0.71
                       Mean reward: 1995.75
               Mean episode length: 53.65
                  Mean reward/step: 36.65
       Mean episode length/episode: 6.99
            Mean episode successes: 3.3735
Mean episode consecutive_successes: 6.9831
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 16.32s
                        Total time: 31503.13s
                               ETA: 1000043.9s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.103s, learning 0.186s)
               Value function loss: 122726.8016
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 2240.37
               Mean episode length: 53.21
                  Mean reward/step: 35.77
       Mean episode length/episode: 7.07
            Mean episode successes: 3.3262
Mean episode consecutive_successes: 7.0615
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 16.29s
                        Total time: 31519.42s
                               ETA: 1000223.1s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.122s, learning 0.162s)
               Value function loss: 123821.5258
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 1766.38
               Mean episode length: 51.31
                  Mean reward/step: 36.41
       Mean episode length/episode: 7.15
            Mean episode successes: 3.5571
Mean episode consecutive_successes: 7.0437
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 16.28s
                        Total time: 31535.70s
                               ETA: 1000402.1s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.281s, learning 0.173s)
               Value function loss: 136381.4184
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 2162.43
               Mean episode length: 55.82
                  Mean reward/step: 36.00
       Mean episode length/episode: 7.07
            Mean episode successes: 3.4634
Mean episode consecutive_successes: 7.1668
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 16.45s
                        Total time: 31552.16s
                               ETA: 1000586.3s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.335s, learning 0.204s)
               Value function loss: 161272.2719
                    Surrogate loss: 0.0155
             Mean action noise std: 0.71
                       Mean reward: 1681.15
               Mean episode length: 50.82
                  Mean reward/step: 37.61
       Mean episode length/episode: 7.02
            Mean episode successes: 3.4385
Mean episode consecutive_successes: 7.2023
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 16.54s
                        Total time: 31568.70s
                               ETA: 1000773.1s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.719s, learning 0.161s)
               Value function loss: 139100.1885
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 2178.88
               Mean episode length: 55.58
                  Mean reward/step: 39.39
       Mean episode length/episode: 7.01
            Mean episode successes: 3.4395
Mean episode consecutive_successes: 7.2745
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 15.88s
                        Total time: 31584.58s
                               ETA: 1000938.9s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.240s, learning 0.235s)
               Value function loss: 163857.8648
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 1921.34
               Mean episode length: 51.15
                  Mean reward/step: 38.03
       Mean episode length/episode: 7.11
            Mean episode successes: 3.5166
Mean episode consecutive_successes: 7.3334
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 16.47s
                        Total time: 31601.05s
                               ETA: 1001123.4s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.398s, learning 0.168s)
               Value function loss: 193889.8227
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2192.64
               Mean episode length: 52.63
                  Mean reward/step: 38.16
       Mean episode length/episode: 6.96
            Mean episode successes: 3.5049
Mean episode consecutive_successes: 7.3525
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 16.57s
                        Total time: 31617.62s
                               ETA: 1001310.6s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.929s, learning 0.171s)
               Value function loss: 127998.9482
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 2677.42
               Mean episode length: 55.58
                  Mean reward/step: 38.17
       Mean episode length/episode: 7.03
            Mean episode successes: 3.3633
Mean episode consecutive_successes: 7.4539
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 16.10s
                        Total time: 31633.72s
                               ETA: 1001483.0s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.047s, learning 0.259s)
               Value function loss: 117068.4311
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 2243.10
               Mean episode length: 53.22
                  Mean reward/step: 37.91
       Mean episode length/episode: 7.13
            Mean episode successes: 3.5269
Mean episode consecutive_successes: 7.4445
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 16.31s
                        Total time: 31650.02s
                               ETA: 1001661.7s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1007 steps/s (collection: 15.984s, learning 0.273s)
               Value function loss: 123735.9480
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 1663.14
               Mean episode length: 50.22
                  Mean reward/step: 36.64
       Mean episode length/episode: 7.00
            Mean episode successes: 3.2466
Mean episode consecutive_successes: 7.5250
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 16.26s
                        Total time: 31666.28s
                               ETA: 1001838.8s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.975s, learning 0.177s)
               Value function loss: 112354.4775
                    Surrogate loss: 0.0092
             Mean action noise std: 0.71
                       Mean reward: 1367.23
               Mean episode length: 48.52
                  Mean reward/step: 36.61
       Mean episode length/episode: 7.05
            Mean episode successes: 3.3916
Mean episode consecutive_successes: 7.4576
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 16.15s
                        Total time: 31682.43s
                               ETA: 1002012.5s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.263s, learning 0.160s)
               Value function loss: 120238.5973
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 2287.78
               Mean episode length: 53.25
                  Mean reward/step: 37.47
       Mean episode length/episode: 7.16
            Mean episode successes: 3.3472
Mean episode consecutive_successes: 7.5993
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 16.42s
                        Total time: 31698.86s
                               ETA: 1002194.6s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.863s, learning 0.164s)
               Value function loss: 142390.1430
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 2865.26
               Mean episode length: 56.39
                  Mean reward/step: 38.88
       Mean episode length/episode: 7.07
            Mean episode successes: 3.4121
Mean episode consecutive_successes: 7.6256
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 16.03s
                        Total time: 31714.88s
                               ETA: 1002364.0s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.970s, learning 0.164s)
               Value function loss: 152887.6309
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 2195.80
               Mean episode length: 54.44
                  Mean reward/step: 38.50
       Mean episode length/episode: 7.08
            Mean episode successes: 3.2778
Mean episode consecutive_successes: 7.6468
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 16.13s
                        Total time: 31731.02s
                               ETA: 1002536.7s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.098s, learning 0.184s)
               Value function loss: 218729.2566
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 1786.53
               Mean episode length: 52.17
                  Mean reward/step: 36.61
       Mean episode length/episode: 7.11
            Mean episode successes: 3.3975
Mean episode consecutive_successes: 7.5819
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 16.28s
                        Total time: 31747.30s
                               ETA: 1002714.0s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.319s, learning 0.184s)
               Value function loss: 150394.0359
                    Surrogate loss: 0.0090
             Mean action noise std: 0.71
                       Mean reward: 1939.86
               Mean episode length: 52.73
                  Mean reward/step: 37.30
       Mean episode length/episode: 7.08
            Mean episode successes: 3.4331
Mean episode consecutive_successes: 7.5863
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 16.50s
                        Total time: 31763.80s
                               ETA: 1002898.1s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.861s, learning 0.189s)
               Value function loss: 132635.3680
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 2541.97
               Mean episode length: 55.61
                  Mean reward/step: 37.18
       Mean episode length/episode: 7.05
            Mean episode successes: 3.4985
Mean episode consecutive_successes: 7.5881
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 16.05s
                        Total time: 31779.85s
                               ETA: 1003067.7s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.660s, learning 0.183s)
               Value function loss: 126100.2873
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 1918.80
               Mean episode length: 50.83
                  Mean reward/step: 39.07
       Mean episode length/episode: 7.05
            Mean episode successes: 3.3989
Mean episode consecutive_successes: 7.6032
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 16.84s
                        Total time: 31796.69s
                               ETA: 1003262.3s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.943s, learning 0.184s)
               Value function loss: 124048.8518
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 1840.19
               Mean episode length: 52.64
                  Mean reward/step: 38.37
       Mean episode length/episode: 7.10
            Mean episode successes: 3.3682
Mean episode consecutive_successes: 7.6343
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 16.13s
                        Total time: 31812.82s
                               ETA: 1003434.2s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.108s, learning 0.190s)
               Value function loss: 150586.0063
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 1558.69
               Mean episode length: 51.40
                  Mean reward/step: 38.48
       Mean episode length/episode: 7.09
            Mean episode successes: 3.4082
Mean episode consecutive_successes: 7.6639
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 16.30s
                        Total time: 31829.12s
                               ETA: 1003611.3s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.730s, learning 0.191s)
               Value function loss: 158225.1293
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 1657.66
               Mean episode length: 49.80
                  Mean reward/step: 41.89
       Mean episode length/episode: 7.12
            Mean episode successes: 3.6191
Mean episode consecutive_successes: 7.6512
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 15.92s
                        Total time: 31845.04s
                               ETA: 1003776.4s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.795s, learning 0.199s)
               Value function loss: 138568.3754
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 2156.55
               Mean episode length: 55.42
                  Mean reward/step: 40.21
       Mean episode length/episode: 7.09
            Mean episode successes: 3.6514
Mean episode consecutive_successes: 7.7322
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 15.99s
                        Total time: 31861.03s
                               ETA: 1003943.7s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.163s, learning 0.164s)
               Value function loss: 137714.8385
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 1688.60
               Mean episode length: 51.00
                  Mean reward/step: 40.56
       Mean episode length/episode: 7.02
            Mean episode successes: 3.6040
Mean episode consecutive_successes: 7.7668
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 16.33s
                        Total time: 31877.36s
                               ETA: 1004121.3s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.084s, learning 0.162s)
               Value function loss: 136062.6992
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 2381.98
               Mean episode length: 51.34
                  Mean reward/step: 39.73
       Mean episode length/episode: 7.06
            Mean episode successes: 3.6221
Mean episode consecutive_successes: 7.8144
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 16.25s
                        Total time: 31893.61s
                               ETA: 1004296.3s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.474s, learning 0.185s)
               Value function loss: 145392.4146
                    Surrogate loss: 0.0357
             Mean action noise std: 0.71
                       Mean reward: 2386.48
               Mean episode length: 54.41
                  Mean reward/step: 38.98
       Mean episode length/episode: 7.09
            Mean episode successes: 3.5679
Mean episode consecutive_successes: 7.8515
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 16.66s
                        Total time: 31910.27s
                               ETA: 1004484.2s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.307s, learning 0.234s)
               Value function loss: 136704.9637
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 2373.27
               Mean episode length: 53.98
                  Mean reward/step: 36.89
       Mean episode length/episode: 7.08
            Mean episode successes: 3.4663
Mean episode consecutive_successes: 7.8786
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 16.54s
                        Total time: 31926.81s
                               ETA: 1004668.2s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.325s, learning 0.162s)
               Value function loss: 141864.0348
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 1834.73
               Mean episode length: 52.61
                  Mean reward/step: 36.69
       Mean episode length/episode: 7.01
            Mean episode successes: 3.2783
Mean episode consecutive_successes: 7.9443
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 16.49s
                        Total time: 31943.29s
                               ETA: 1004850.4s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.293s, learning 0.162s)
               Value function loss: 137906.0574
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 2561.31
               Mean episode length: 53.81
                  Mean reward/step: 36.92
       Mean episode length/episode: 7.07
            Mean episode successes: 3.1357
Mean episode consecutive_successes: 8.0302
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 16.46s
                        Total time: 31959.75s
                               ETA: 1005031.5s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.614s, learning 0.165s)
               Value function loss: 139366.9916
                    Surrogate loss: -0.0068
             Mean action noise std: 0.71
                       Mean reward: 1863.07
               Mean episode length: 49.90
                  Mean reward/step: 37.18
       Mean episode length/episode: 7.06
            Mean episode successes: 3.0815
Mean episode consecutive_successes: 7.9931
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 9.78s
                        Total time: 31969.53s
                               ETA: 1005002.5s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.226s, learning 0.161s)
               Value function loss: 136371.9775
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 2031.84
               Mean episode length: 51.85
                  Mean reward/step: 39.95
       Mean episode length/episode: 7.12
            Mean episode successes: 3.3188
Mean episode consecutive_successes: 7.9192
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 8.39s
                        Total time: 31977.92s
                               ETA: 1004929.8s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.081s, learning 0.188s)
               Value function loss: 180953.2742
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 1855.21
               Mean episode length: 53.03
                  Mean reward/step: 41.71
       Mean episode length/episode: 7.02
            Mean episode successes: 3.5552
Mean episode consecutive_successes: 7.8606
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 8.27s
                        Total time: 31986.18s
                               ETA: 1004853.5s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.331s, learning 0.165s)
               Value function loss: 200314.9070
                    Surrogate loss: 0.0158
             Mean action noise std: 0.71
                       Mean reward: 2132.37
               Mean episode length: 52.60
                  Mean reward/step: 41.90
       Mean episode length/episode: 7.04
            Mean episode successes: 3.5034
Mean episode consecutive_successes: 7.9296
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 8.50s
                        Total time: 31994.68s
                               ETA: 1004784.3s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.134s, learning 0.164s)
               Value function loss: 157385.0223
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 1922.35
               Mean episode length: 52.14
                  Mean reward/step: 42.81
       Mean episode length/episode: 7.03
            Mean episode successes: 3.3926
Mean episode consecutive_successes: 8.0320
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 8.30s
                        Total time: 32002.98s
                               ETA: 1004709.0s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.216s, learning 0.210s)
               Value function loss: 154219.4289
                    Surrogate loss: 0.0125
             Mean action noise std: 0.71
                       Mean reward: 1949.95
               Mean episode length: 52.63
                  Mean reward/step: 43.12
       Mean episode length/episode: 7.03
            Mean episode successes: 3.5659
Mean episode consecutive_successes: 8.0552
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 8.43s
                        Total time: 32011.40s
                               ETA: 1004637.7s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.348s, learning 0.201s)
               Value function loss: 161293.6973
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 2468.17
               Mean episode length: 52.92
                  Mean reward/step: 43.45
       Mean episode length/episode: 7.09
            Mean episode successes: 3.7310
Mean episode consecutive_successes: 8.1056
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 8.55s
                        Total time: 32019.95s
                               ETA: 1004570.3s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.947s, learning 0.228s)
               Value function loss: 162371.3051
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 2037.53
               Mean episode length: 52.72
                  Mean reward/step: 43.31
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7734
Mean episode consecutive_successes: 8.1514
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 8.18s
                        Total time: 32028.13s
                               ETA: 1004491.2s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.761s, learning 0.227s)
               Value function loss: 179825.8883
                    Surrogate loss: 0.0034
             Mean action noise std: 0.71
                       Mean reward: 2555.07
               Mean episode length: 52.70
                  Mean reward/step: 41.76
       Mean episode length/episode: 6.98
            Mean episode successes: 3.6841
Mean episode consecutive_successes: 8.2185
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 8.99s
                        Total time: 32037.12s
                               ETA: 1004437.7s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.229s, learning 0.184s)
               Value function loss: 159710.5074
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 2403.58
               Mean episode length: 53.95
                  Mean reward/step: 39.01
       Mean episode length/episode: 7.09
            Mean episode successes: 3.3936
Mean episode consecutive_successes: 8.3094
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 8.41s
                        Total time: 32045.53s
                               ETA: 1004366.1s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.339s, learning 0.171s)
               Value function loss: 176640.3266
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 1952.33
               Mean episode length: 51.73
                  Mean reward/step: 37.61
       Mean episode length/episode: 7.05
            Mean episode successes: 3.2959
Mean episode consecutive_successes: 8.3070
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 8.51s
                        Total time: 32054.04s
                               ETA: 1004297.7s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.037s, learning 0.195s)
               Value function loss: 176733.8922
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 1947.59
               Mean episode length: 52.61
                  Mean reward/step: 38.48
       Mean episode length/episode: 7.09
            Mean episode successes: 3.2622
Mean episode consecutive_successes: 8.3113
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 8.23s
                        Total time: 32062.27s
                               ETA: 1004220.6s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.201s, learning 0.161s)
               Value function loss: 179527.4137
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 1806.97
               Mean episode length: 51.51
                  Mean reward/step: 39.06
       Mean episode length/episode: 6.99
            Mean episode successes: 3.2061
Mean episode consecutive_successes: 8.2741
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 8.36s
                        Total time: 32070.63s
                               ETA: 1004147.6s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.339s, learning 0.208s)
               Value function loss: 156635.6457
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 1715.62
               Mean episode length: 49.90
                  Mean reward/step: 38.64
       Mean episode length/episode: 7.06
            Mean episode successes: 3.2554
Mean episode consecutive_successes: 8.2320
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 8.55s
                        Total time: 32079.18s
                               ETA: 1004080.4s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.947s, learning 0.208s)
               Value function loss: 160895.1203
                    Surrogate loss: 0.0445
             Mean action noise std: 0.71
                       Mean reward: 2297.95
               Mean episode length: 52.14
                  Mean reward/step: 39.20
       Mean episode length/episode: 7.09
            Mean episode successes: 3.3813
Mean episode consecutive_successes: 8.1984
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 8.15s
                        Total time: 32087.33s
                               ETA: 1004001.0s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.078s, learning 0.238s)
               Value function loss: 146027.4113
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 1484.65
               Mean episode length: 49.03
                  Mean reward/step: 38.62
       Mean episode length/episode: 7.00
            Mean episode successes: 3.4160
Mean episode consecutive_successes: 8.1323
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 8.32s
                        Total time: 32095.65s
                               ETA: 1003926.7s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.919s, learning 0.208s)
               Value function loss: 142906.2754
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 1312.95
               Mean episode length: 49.92
                  Mean reward/step: 39.07
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4141
Mean episode consecutive_successes: 8.0770
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 8.13s
                        Total time: 32103.78s
                               ETA: 1003846.5s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.015s, learning 0.162s)
               Value function loss: 148489.7980
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 2110.53
               Mean episode length: 52.50
                  Mean reward/step: 36.30
       Mean episode length/episode: 7.04
            Mean episode successes: 3.1777
Mean episode consecutive_successes: 8.1658
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 8.18s
                        Total time: 32111.95s
                               ETA: 1003767.9s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.937s, learning 0.163s)
               Value function loss: 139452.8496
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 1463.26
               Mean episode length: 50.12
                  Mean reward/step: 35.64
       Mean episode length/episode: 7.11
            Mean episode successes: 3.2896
Mean episode consecutive_successes: 8.0676
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 8.10s
                        Total time: 32120.06s
                               ETA: 1003687.0s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.384s, learning 0.186s)
               Value function loss: 153423.9984
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 2162.09
               Mean episode length: 51.77
                  Mean reward/step: 38.16
       Mean episode length/episode: 6.99
            Mean episode successes: 3.3594
Mean episode consecutive_successes: 8.0108
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 8.57s
                        Total time: 32128.63s
                               ETA: 1003620.8s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.022s, learning 0.166s)
               Value function loss: 168982.1250
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 1930.71
               Mean episode length: 49.35
                  Mean reward/step: 39.54
       Mean episode length/episode: 7.13
            Mean episode successes: 3.5518
Mean episode consecutive_successes: 7.9838
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 8.19s
                        Total time: 32136.81s
                               ETA: 1003542.7s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.298s, learning 0.165s)
               Value function loss: 207908.0230
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 1967.39
               Mean episode length: 52.44
                  Mean reward/step: 39.23
       Mean episode length/episode: 7.02
            Mean episode successes: 3.4399
Mean episode consecutive_successes: 7.9979
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 8.46s
                        Total time: 32145.28s
                               ETA: 1003473.2s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 2080 steps/s (collection: 7.704s, learning 0.171s)
               Value function loss: 180326.1379
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 2059.22
               Mean episode length: 51.53
                  Mean reward/step: 40.14
       Mean episode length/episode: 7.05
            Mean episode successes: 3.4443
Mean episode consecutive_successes: 8.0124
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 7.88s
                        Total time: 32153.15s
                               ETA: 1003385.4s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.667s, learning 0.200s)
               Value function loss: 151121.6203
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: 1842.06
               Mean episode length: 52.31
                  Mean reward/step: 40.05
       Mean episode length/episode: 7.02
            Mean episode successes: 3.5547
Mean episode consecutive_successes: 7.9790
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 8.87s
                        Total time: 32162.02s
                               ETA: 1003328.6s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.089s, learning 0.169s)
               Value function loss: 136422.1104
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 2361.00
               Mean episode length: 50.27
                  Mean reward/step: 39.15
       Mean episode length/episode: 7.04
            Mean episode successes: 3.5947
Mean episode consecutive_successes: 8.0074
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 8.26s
                        Total time: 32170.28s
                               ETA: 1003252.9s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.285s, learning 0.275s)
               Value function loss: 148322.9270
                    Surrogate loss: 0.0049
             Mean action noise std: 0.71
                       Mean reward: 2729.44
               Mean episode length: 55.13
                  Mean reward/step: 40.17
       Mean episode length/episode: 7.10
            Mean episode successes: 3.6587
Mean episode consecutive_successes: 8.0563
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 8.56s
                        Total time: 32178.84s
                               ETA: 1003186.6s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.429s, learning 0.162s)
               Value function loss: 142389.9477
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 2029.76
               Mean episode length: 52.56
                  Mean reward/step: 38.61
       Mean episode length/episode: 6.98
            Mean episode successes: 3.2969
Mean episode consecutive_successes: 8.1448
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 8.59s
                        Total time: 32187.43s
                               ETA: 1003121.3s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.065s, learning 0.159s)
               Value function loss: 155779.0207
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 1947.15
               Mean episode length: 52.48
                  Mean reward/step: 38.14
       Mean episode length/episode: 7.06
            Mean episode successes: 3.3647
Mean episode consecutive_successes: 8.0774
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 8.22s
                        Total time: 32195.65s
                               ETA: 1003044.7s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.368s, learning 0.162s)
               Value function loss: 157607.7375
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 2886.67
               Mean episode length: 54.94
                  Mean reward/step: 38.06
       Mean episode length/episode: 7.07
            Mean episode successes: 3.3364
Mean episode consecutive_successes: 8.1095
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 8.53s
                        Total time: 32204.18s
                               ETA: 1002977.5s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.138s, learning 0.206s)
               Value function loss: 153817.6879
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 2167.42
               Mean episode length: 52.40
                  Mean reward/step: 35.51
       Mean episode length/episode: 6.98
            Mean episode successes: 3.0840
Mean episode consecutive_successes: 8.0830
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 8.34s
                        Total time: 32212.53s
                               ETA: 1002904.7s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.382s, learning 0.244s)
               Value function loss: 116781.3537
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 2257.99
               Mean episode length: 53.23
                  Mean reward/step: 36.11
       Mean episode length/episode: 7.10
            Mean episode successes: 3.1060
Mean episode consecutive_successes: 8.0896
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 8.63s
                        Total time: 32221.15s
                               ETA: 1002840.6s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.946s, learning 0.161s)
               Value function loss: 109039.8453
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 1696.80
               Mean episode length: 51.08
                  Mean reward/step: 33.40
       Mean episode length/episode: 7.11
            Mean episode successes: 3.0908
Mean episode consecutive_successes: 8.0114
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 8.11s
                        Total time: 32229.26s
                               ETA: 1002760.5s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.029s, learning 0.165s)
               Value function loss: 115812.7127
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 1899.46
               Mean episode length: 51.85
                  Mean reward/step: 35.08
       Mean episode length/episode: 6.99
            Mean episode successes: 3.0771
Mean episode consecutive_successes: 7.9375
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 8.19s
                        Total time: 32237.45s
                               ETA: 1002683.1s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.693s, learning 0.182s)
               Value function loss: 114612.8496
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 2011.15
               Mean episode length: 54.09
                  Mean reward/step: 34.53
       Mean episode length/episode: 7.01
            Mean episode successes: 3.0977
Mean episode consecutive_successes: 7.8496
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 8.88s
                        Total time: 32246.33s
                               ETA: 1002626.9s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.170s, learning 0.166s)
               Value function loss: 121492.8785
                    Surrogate loss: 0.0055
             Mean action noise std: 0.71
                       Mean reward: 1808.79
               Mean episode length: 50.95
                  Mean reward/step: 34.75
       Mean episode length/episode: 7.06
            Mean episode successes: 2.9937
Mean episode consecutive_successes: 7.7920
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 8.34s
                        Total time: 32254.66s
                               ETA: 1002554.0s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.121s, learning 0.162s)
               Value function loss: 122962.6602
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 2661.96
               Mean episode length: 54.61
                  Mean reward/step: 33.97
       Mean episode length/episode: 7.04
            Mean episode successes: 2.8984
Mean episode consecutive_successes: 7.7853
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 8.28s
                        Total time: 32262.95s
                               ETA: 1002479.5s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.299s, learning 0.186s)
               Value function loss: 130261.6461
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 1586.94
               Mean episode length: 51.19
                  Mean reward/step: 35.56
       Mean episode length/episode: 6.97
            Mean episode successes: 2.9072
Mean episode consecutive_successes: 7.6873
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 8.48s
                        Total time: 32271.43s
                               ETA: 1002411.3s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.996s, learning 0.185s)
               Value function loss: 134371.3832
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 1986.03
               Mean episode length: 50.04
                  Mean reward/step: 37.84
       Mean episode length/episode: 7.10
            Mean episode successes: 3.0859
Mean episode consecutive_successes: 7.6621
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 8.18s
                        Total time: 32279.61s
                               ETA: 1002333.7s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.418s, learning 0.160s)
               Value function loss: 140373.2322
                    Surrogate loss: 0.0090
             Mean action noise std: 0.71
                       Mean reward: 1486.76
               Mean episode length: 52.18
                  Mean reward/step: 39.25
       Mean episode length/episode: 7.06
            Mean episode successes: 3.2397
Mean episode consecutive_successes: 7.6347
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 8.58s
                        Total time: 32288.19s
                               ETA: 1002268.5s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.161s)
               Value function loss: 139304.3000
                    Surrogate loss: 0.0056
             Mean action noise std: 0.71
                       Mean reward: 1891.70
               Mean episode length: 51.39
                  Mean reward/step: 40.74
       Mean episode length/episode: 7.03
            Mean episode successes: 3.2920
Mean episode consecutive_successes: 7.7033
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 8.36s
                        Total time: 32296.55s
                               ETA: 1002196.6s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.368s, learning 0.157s)
               Value function loss: 141211.5811
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 1951.15
               Mean episode length: 54.16
                  Mean reward/step: 42.25
       Mean episode length/episode: 7.08
            Mean episode successes: 3.5981
Mean episode consecutive_successes: 7.6531
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 8.53s
                        Total time: 32305.08s
                               ETA: 1002129.8s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.167s)
               Value function loss: 148171.9650
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 1773.62
               Mean episode length: 50.86
                  Mean reward/step: 41.27
       Mean episode length/episode: 7.05
            Mean episode successes: 3.5508
Mean episode consecutive_successes: 7.7132
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 8.35s
                        Total time: 32313.42s
                               ETA: 1002057.5s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.164s)
               Value function loss: 201672.2613
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 1959.59
               Mean episode length: 52.11
                  Mean reward/step: 38.68
       Mean episode length/episode: 7.00
            Mean episode successes: 3.4990
Mean episode consecutive_successes: 7.7372
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 8.38s
                        Total time: 32321.81s
                               ETA: 1001986.3s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.405s, learning 0.263s)
               Value function loss: 211093.7492
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 2080.10
               Mean episode length: 52.59
                  Mean reward/step: 35.92
       Mean episode length/episode: 7.02
            Mean episode successes: 3.4463
Mean episode consecutive_successes: 7.7148
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 8.67s
                        Total time: 32330.47s
                               ETA: 1001924.1s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.143s, learning 0.167s)
               Value function loss: 175413.4738
                    Surrogate loss: 0.0078
             Mean action noise std: 0.71
                       Mean reward: 1828.34
               Mean episode length: 49.96
                  Mean reward/step: 33.41
       Mean episode length/episode: 7.08
            Mean episode successes: 3.2695
Mean episode consecutive_successes: 7.7114
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 8.31s
                        Total time: 32338.78s
                               ETA: 1001850.8s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.468s, learning 0.162s)
               Value function loss: 122008.6154
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: 2100.98
               Mean episode length: 53.44
                  Mean reward/step: 35.17
       Mean episode length/episode: 7.02
            Mean episode successes: 3.2803
Mean episode consecutive_successes: 7.6947
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 8.63s
                        Total time: 32347.41s
                               ETA: 1001787.4s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.312s, learning 0.238s)
               Value function loss: 113091.4455
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 1977.13
               Mean episode length: 52.11
                  Mean reward/step: 35.64
       Mean episode length/episode: 7.05
            Mean episode successes: 3.3838
Mean episode consecutive_successes: 7.5998
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 8.55s
                        Total time: 32355.96s
                               ETA: 1001721.6s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.932s, learning 0.189s)
               Value function loss: 120273.1500
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 1907.24
               Mean episode length: 52.10
                  Mean reward/step: 38.08
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7227
Mean episode consecutive_successes: 7.4546
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 9.12s
                        Total time: 32365.08s
                               ETA: 1001673.5s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.113s, learning 0.196s)
               Value function loss: 136135.9219
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 1717.20
               Mean episode length: 52.13
                  Mean reward/step: 37.33
       Mean episode length/episode: 6.98
            Mean episode successes: 3.6426
Mean episode consecutive_successes: 7.4786
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 8.31s
                        Total time: 32373.39s
                               ETA: 1001600.3s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.506s, learning 0.164s)
               Value function loss: 166924.0805
                    Surrogate loss: 0.0167
             Mean action noise std: 0.71
                       Mean reward: 2286.23
               Mean episode length: 54.47
                  Mean reward/step: 35.04
       Mean episode length/episode: 6.98
            Mean episode successes: 3.5171
Mean episode consecutive_successes: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 8.67s
                        Total time: 32382.06s
                               ETA: 1001538.3s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.950s, learning 0.167s)
               Value function loss: 126667.9715
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 2283.19
               Mean episode length: 53.34
                  Mean reward/step: 34.85
       Mean episode length/episode: 7.11
            Mean episode successes: 3.6050
Mean episode consecutive_successes: 7.4754
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 8.12s
                        Total time: 32390.18s
                               ETA: 1001459.3s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.300s, learning 0.185s)
               Value function loss: 129541.6291
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 1623.10
               Mean episode length: 50.16
                  Mean reward/step: 33.00
       Mean episode length/episode: 7.00
            Mean episode successes: 3.3560
Mean episode consecutive_successes: 7.4426
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 8.48s
                        Total time: 32398.66s
                               ETA: 1001391.6s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.055s, learning 0.206s)
               Value function loss: 143759.8937
                    Surrogate loss: 0.0040
             Mean action noise std: 0.71
                       Mean reward: 1615.11
               Mean episode length: 49.09
                  Mean reward/step: 34.78
       Mean episode length/episode: 7.08
            Mean episode successes: 3.5195
Mean episode consecutive_successes: 7.3570
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 8.26s
                        Total time: 32406.93s
                               ETA: 1001317.1s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 2083 steps/s (collection: 7.697s, learning 0.168s)
               Value function loss: 144669.8613
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 1672.00
               Mean episode length: 52.55
                  Mean reward/step: 36.66
       Mean episode length/episode: 7.06
            Mean episode successes: 3.5977
Mean episode consecutive_successes: 7.3292
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 7.87s
                        Total time: 32414.79s
                               ETA: 1001230.4s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 2151 steps/s (collection: 7.442s, learning 0.172s)
               Value function loss: 152898.1000
                    Surrogate loss: 0.0639
             Mean action noise std: 0.71
                       Mean reward: 1786.53
               Mean episode length: 50.91
                  Mean reward/step: 37.80
       Mean episode length/episode: 7.06
            Mean episode successes: 3.7051
Mean episode consecutive_successes: 7.2924
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 7.61s
                        Total time: 32422.40s
                               ETA: 1001136.1s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.037s, learning 0.156s)
               Value function loss: 131808.3172
                    Surrogate loss: -0.0046
             Mean action noise std: 0.70
                       Mean reward: 2031.76
               Mean episode length: 51.28
                  Mean reward/step: 35.81
       Mean episode length/episode: 6.99
            Mean episode successes: 3.6885
Mean episode consecutive_successes: 7.3128
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 8.19s
                        Total time: 32430.60s
                               ETA: 1001059.6s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.097s, learning 0.163s)
               Value function loss: 135485.9867
                    Surrogate loss: -0.0003
             Mean action noise std: 0.70
                       Mean reward: 1869.11
               Mean episode length: 50.84
                  Mean reward/step: 35.27
       Mean episode length/episode: 7.05
            Mean episode successes: 3.6602
Mean episode consecutive_successes: 7.2902
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 8.26s
                        Total time: 32438.86s
                               ETA: 1000985.2s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.266s, learning 0.165s)
               Value function loss: 163733.8523
                    Surrogate loss: -0.0083
             Mean action noise std: 0.70
                       Mean reward: 2398.39
               Mean episode length: 53.76
                  Mean reward/step: 34.89
       Mean episode length/episode: 7.02
            Mean episode successes: 3.5396
Mean episode consecutive_successes: 7.3340
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 8.43s
                        Total time: 32447.29s
                               ETA: 1000916.2s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.082s, learning 0.190s)
               Value function loss: 170047.9430
                    Surrogate loss: -0.0052
             Mean action noise std: 0.70
                       Mean reward: 2396.13
               Mean episode length: 53.02
                  Mean reward/step: 33.71
       Mean episode length/episode: 7.02
            Mean episode successes: 3.3433
Mean episode consecutive_successes: 7.3555
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 8.27s
                        Total time: 32455.56s
                               ETA: 1000842.3s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.019s, learning 0.169s)
               Value function loss: 151989.7379
                    Surrogate loss: -0.0059
             Mean action noise std: 0.70
                       Mean reward: 1468.36
               Mean episode length: 49.58
                  Mean reward/step: 34.81
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5796
Mean episode consecutive_successes: 7.2374
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 8.19s
                        Total time: 32463.75s
                               ETA: 1000765.8s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.346s, learning 0.180s)
               Value function loss: 135009.0910
                    Surrogate loss: -0.0073
             Mean action noise std: 0.70
                       Mean reward: 1703.99
               Mean episode length: 50.60
                  Mean reward/step: 34.69
       Mean episode length/episode: 6.97
            Mean episode successes: 3.6143
Mean episode consecutive_successes: 7.1888
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 8.53s
                        Total time: 32472.27s
                               ETA: 1000699.8s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.074s, learning 0.165s)
               Value function loss: 110215.8230
                    Surrogate loss: -0.0108
             Mean action noise std: 0.70
                       Mean reward: 1688.68
               Mean episode length: 50.59
                  Mean reward/step: 34.79
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4795
Mean episode consecutive_successes: 7.1878
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 8.24s
                        Total time: 32480.51s
                               ETA: 1000625.0s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.052s, learning 0.164s)
               Value function loss: 109871.3803
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 1913.37
               Mean episode length: 53.87
                  Mean reward/step: 34.81
       Mean episode length/episode: 7.10
            Mean episode successes: 3.3955
Mean episode consecutive_successes: 7.2992
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 8.22s
                        Total time: 32488.73s
                               ETA: 1000549.6s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.457s, learning 0.210s)
               Value function loss: 108179.9191
                    Surrogate loss: 0.0036
             Mean action noise std: 0.70
                       Mean reward: 1582.47
               Mean episode length: 48.88
                  Mean reward/step: 34.53
       Mean episode length/episode: 7.14
            Mean episode successes: 3.4814
Mean episode consecutive_successes: 7.2516
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 8.67s
                        Total time: 32497.40s
                               ETA: 1000488.0s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.931s, learning 0.164s)
               Value function loss: 121336.9309
                    Surrogate loss: -0.0083
             Mean action noise std: 0.70
                       Mean reward: 1920.83
               Mean episode length: 53.73
                  Mean reward/step: 37.71
       Mean episode length/episode: 7.02
            Mean episode successes: 3.5986
Mean episode consecutive_successes: 7.2321
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 8.09s
                        Total time: 32505.49s
                               ETA: 1000408.9s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.023s, learning 0.160s)
               Value function loss: 129631.9598
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 2187.01
               Mean episode length: 52.14
                  Mean reward/step: 40.51
       Mean episode length/episode: 7.04
            Mean episode successes: 3.6187
Mean episode consecutive_successes: 7.3122
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 8.18s
                        Total time: 32513.67s
                               ETA: 1000332.6s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.209s, learning 0.170s)
               Value function loss: 138733.3475
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 2055.47
               Mean episode length: 53.28
                  Mean reward/step: 41.00
       Mean episode length/episode: 7.06
            Mean episode successes: 3.7925
Mean episode consecutive_successes: 7.3512
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 8.38s
                        Total time: 32522.05s
                               ETA: 1000262.3s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.436s, learning 0.227s)
               Value function loss: 176595.6426
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 1659.69
               Mean episode length: 52.13
                  Mean reward/step: 42.73
       Mean episode length/episode: 7.07
            Mean episode successes: 3.7974
Mean episode consecutive_successes: 7.4067
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 8.66s
                        Total time: 32530.72s
                               ETA: 1000200.8s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.285s, learning 0.175s)
               Value function loss: 152915.6305
                    Surrogate loss: -0.0042
             Mean action noise std: 0.70
                       Mean reward: 1859.30
               Mean episode length: 51.67
                  Mean reward/step: 42.01
       Mean episode length/episode: 7.02
            Mean episode successes: 3.8726
Mean episode consecutive_successes: 7.4922
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 8.46s
                        Total time: 32539.18s
                               ETA: 1000133.1s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.887s, learning 0.169s)
               Value function loss: 124494.3908
                    Surrogate loss: -0.0056
             Mean action noise std: 0.70
                       Mean reward: 2684.00
               Mean episode length: 54.99
                  Mean reward/step: 41.01
       Mean episode length/episode: 7.03
            Mean episode successes: 3.8359
Mean episode consecutive_successes: 7.5939
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 8.06s
                        Total time: 32547.23s
                               ETA: 1000053.0s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.227s, learning 0.165s)
               Value function loss: 124767.6449
                    Surrogate loss: -0.0154
             Mean action noise std: 0.70
                       Mean reward: 1931.21
               Mean episode length: 50.34
                  Mean reward/step: 40.27
       Mean episode length/episode: 7.05
            Mean episode successes: 3.8203
Mean episode consecutive_successes: 7.6303
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 8.39s
                        Total time: 32555.62s
                               ETA: 999983.2s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.423s, learning 0.173s)
               Value function loss: 139815.3469
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 1870.95
               Mean episode length: 50.51
                  Mean reward/step: 40.33
       Mean episode length/episode: 6.96
            Mean episode successes: 3.5142
Mean episode consecutive_successes: 7.7442
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 8.60s
                        Total time: 32564.22s
                               ETA: 999919.8s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.471s, learning 0.186s)
               Value function loss: 163666.9609
                    Surrogate loss: -0.0075
             Mean action noise std: 0.70
                       Mean reward: 1649.88
               Mean episode length: 49.10
                  Mean reward/step: 40.85
       Mean episode length/episode: 7.06
            Mean episode successes: 3.5864
Mean episode consecutive_successes: 7.7714
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 8.66s
                        Total time: 32572.88s
                               ETA: 999858.3s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.119s, learning 0.224s)
               Value function loss: 148170.7021
                    Surrogate loss: 0.0039
             Mean action noise std: 0.70
                       Mean reward: 2061.88
               Mean episode length: 51.51
                  Mean reward/step: 40.72
       Mean episode length/episode: 7.03
            Mean episode successes: 3.5127
Mean episode consecutive_successes: 7.8676
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 8.34s
                        Total time: 32581.22s
                               ETA: 999787.2s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.402s, learning 0.225s)
               Value function loss: 120039.3512
                    Surrogate loss: -0.0105
             Mean action noise std: 0.70
                       Mean reward: 1860.29
               Mean episode length: 48.95
                  Mean reward/step: 40.60
       Mean episode length/episode: 7.09
            Mean episode successes: 3.5781
Mean episode consecutive_successes: 7.8746
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 8.63s
                        Total time: 32589.85s
                               ETA: 999724.8s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.084s, learning 0.212s)
               Value function loss: 127150.8637
                    Surrogate loss: -0.0106
             Mean action noise std: 0.70
                       Mean reward: 2131.08
               Mean episode length: 50.32
                  Mean reward/step: 41.14
       Mean episode length/episode: 7.03
            Mean episode successes: 3.5220
Mean episode consecutive_successes: 7.9289
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 8.30s
                        Total time: 32598.14s
                               ETA: 999652.3s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.437s, learning 0.165s)
               Value function loss: 126211.9512
                    Surrogate loss: -0.0104
             Mean action noise std: 0.70
                       Mean reward: 2071.66
               Mean episode length: 51.15
                  Mean reward/step: 41.34
       Mean episode length/episode: 7.16
            Mean episode successes: 3.7148
Mean episode consecutive_successes: 7.9561
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 8.60s
                        Total time: 32606.75s
                               ETA: 999589.2s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.278s, learning 0.183s)
               Value function loss: 144085.4535
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2007.00
               Mean episode length: 51.95
                  Mean reward/step: 43.38
       Mean episode length/episode: 7.05
            Mean episode successes: 3.9453
Mean episode consecutive_successes: 7.9799
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 8.46s
                        Total time: 32615.21s
                               ETA: 999521.9s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.346s, learning 0.168s)
               Value function loss: 158224.7393
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 1981.98
               Mean episode length: 51.76
                  Mean reward/step: 41.00
       Mean episode length/episode: 7.01
            Mean episode successes: 3.7783
Mean episode consecutive_successes: 7.9993
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 8.51s
                        Total time: 32623.72s
                               ETA: 999456.2s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.054s, learning 0.158s)
               Value function loss: 155194.3238
                    Surrogate loss: -0.0059
             Mean action noise std: 0.70
                       Mean reward: 1383.61
               Mean episode length: 50.32
                  Mean reward/step: 41.54
       Mean episode length/episode: 7.01
            Mean episode successes: 3.8193
Mean episode consecutive_successes: 7.9827
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 8.21s
                        Total time: 32631.93s
                               ETA: 999381.3s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.207s, learning 0.159s)
               Value function loss: 177948.2969
                    Surrogate loss: -0.0009
             Mean action noise std: 0.70
                       Mean reward: 2385.32
               Mean episode length: 52.79
                  Mean reward/step: 40.61
       Mean episode length/episode: 6.97
            Mean episode successes: 3.5386
Mean episode consecutive_successes: 8.1475
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 8.37s
                        Total time: 32640.30s
                               ETA: 999311.2s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.386s, learning 0.203s)
               Value function loss: 239959.7719
                    Surrogate loss: -0.0110
             Mean action noise std: 0.70
                       Mean reward: 2262.82
               Mean episode length: 53.11
                  Mean reward/step: 38.99
       Mean episode length/episode: 7.06
            Mean episode successes: 3.3984
Mean episode consecutive_successes: 8.1681
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 8.59s
                        Total time: 32648.89s
                               ETA: 999247.9s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.251s, learning 0.166s)
               Value function loss: 166482.4895
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 1854.10
               Mean episode length: 51.67
                  Mean reward/step: 39.12
       Mean episode length/episode: 7.06
            Mean episode successes: 3.4814
Mean episode consecutive_successes: 8.1237
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 8.42s
                        Total time: 32657.30s
                               ETA: 999179.4s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.825s, learning 0.162s)
               Value function loss: 133464.1824
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 1846.27
               Mean episode length: 50.36
                  Mean reward/step: 41.15
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4688
Mean episode consecutive_successes: 8.1259
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 8.99s
                        Total time: 32666.29s
                               ETA: 999128.3s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.037s, learning 0.168s)
               Value function loss: 129018.0637
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 2130.71
               Mean episode length: 53.16
                  Mean reward/step: 42.36
       Mean episode length/episode: 7.13
            Mean episode successes: 3.8784
Mean episode consecutive_successes: 8.0277
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 8.20s
                        Total time: 32674.49s
                               ETA: 999053.4s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.276s, learning 0.166s)
               Value function loss: 144097.8730
                    Surrogate loss: -0.0075
             Mean action noise std: 0.70
                       Mean reward: 2497.62
               Mean episode length: 52.39
                  Mean reward/step: 41.13
       Mean episode length/episode: 7.00
            Mean episode successes: 3.7974
Mean episode consecutive_successes: 8.0710
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 8.44s
                        Total time: 32682.94s
                               ETA: 998985.7s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.077s, learning 0.163s)
               Value function loss: 144498.1195
                    Surrogate loss: -0.0030
             Mean action noise std: 0.70
                       Mean reward: 1880.32
               Mean episode length: 49.15
                  Mean reward/step: 40.52
       Mean episode length/episode: 7.01
            Mean episode successes: 3.8276
Mean episode consecutive_successes: 8.0605
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 8.24s
                        Total time: 32691.18s
                               ETA: 998912.0s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.507s, learning 0.213s)
               Value function loss: 138386.0023
                    Surrogate loss: -0.0139
             Mean action noise std: 0.70
                       Mean reward: 2372.67
               Mean episode length: 52.57
                  Mean reward/step: 37.59
       Mean episode length/episode: 7.08
            Mean episode successes: 3.7090
Mean episode consecutive_successes: 8.1171
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 8.72s
                        Total time: 32699.90s
                               ETA: 998852.9s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.056s, learning 0.185s)
               Value function loss: 140375.7652
                    Surrogate loss: -0.0069
             Mean action noise std: 0.70
                       Mean reward: 1994.65
               Mean episode length: 51.49
                  Mean reward/step: 37.96
       Mean episode length/episode: 7.07
            Mean episode successes: 3.6084
Mean episode consecutive_successes: 8.1267
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 8.24s
                        Total time: 32708.14s
                               ETA: 998779.3s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.399s, learning 0.164s)
               Value function loss: 159434.7871
                    Surrogate loss: -0.0141
             Mean action noise std: 0.70
                       Mean reward: 1894.28
               Mean episode length: 50.67
                  Mean reward/step: 37.95
       Mean episode length/episode: 6.90
            Mean episode successes: 3.3242
Mean episode consecutive_successes: 8.1267
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 8.56s
                        Total time: 32716.70s
                               ETA: 998715.5s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.408s, learning 0.187s)
               Value function loss: 150340.1250
                    Surrogate loss: -0.0068
             Mean action noise std: 0.70
                       Mean reward: 1562.34
               Mean episode length: 48.64
                  Mean reward/step: 39.48
       Mean episode length/episode: 7.05
            Mean episode successes: 3.4478
Mean episode consecutive_successes: 8.0274
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 8.60s
                        Total time: 32725.30s
                               ETA: 998652.7s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.383s, learning 0.176s)
               Value function loss: 153005.2180
                    Surrogate loss: 0.0166
             Mean action noise std: 0.70
                       Mean reward: 1661.16
               Mean episode length: 50.23
                  Mean reward/step: 41.03
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7842
Mean episode consecutive_successes: 7.9365
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 8.56s
                        Total time: 32733.85s
                               ETA: 998588.8s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.389s, learning 0.244s)
               Value function loss: 148490.7930
                    Surrogate loss: -0.0111
             Mean action noise std: 0.70
                       Mean reward: 1774.89
               Mean episode length: 48.42
                  Mean reward/step: 41.11
       Mean episode length/episode: 6.95
            Mean episode successes: 3.7021
Mean episode consecutive_successes: 7.9282
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 8.63s
                        Total time: 32742.49s
                               ETA: 998527.3s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.265s, learning 0.171s)
               Value function loss: 147436.7605
                    Surrogate loss: -0.0071
             Mean action noise std: 0.70
                       Mean reward: 1747.01
               Mean episode length: 47.77
                  Mean reward/step: 38.44
       Mean episode length/episode: 7.02
            Mean episode successes: 3.5713
Mean episode consecutive_successes: 7.9604
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 8.44s
                        Total time: 32750.92s
                               ETA: 998459.7s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.191s, learning 0.186s)
               Value function loss: 183800.2164
                    Surrogate loss: -0.0032
             Mean action noise std: 0.70
                       Mean reward: 2026.48
               Mean episode length: 46.53
                  Mean reward/step: 39.64
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5762
Mean episode consecutive_successes: 7.9714
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 8.38s
                        Total time: 32759.30s
                               ETA: 998390.5s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.150s, learning 0.193s)
               Value function loss: 295841.3875
                    Surrogate loss: -0.0069
             Mean action noise std: 0.70
                       Mean reward: 2192.74
               Mean episode length: 52.16
                  Mean reward/step: 40.34
       Mean episode length/episode: 7.06
            Mean episode successes: 3.6816
Mean episode consecutive_successes: 7.9804
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 8.34s
                        Total time: 32767.64s
                               ETA: 998320.2s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.360s, learning 0.187s)
               Value function loss: 172996.9570
                    Surrogate loss: -0.0066
             Mean action noise std: 0.70
                       Mean reward: 1765.77
               Mean episode length: 49.41
                  Mean reward/step: 38.97
       Mean episode length/episode: 7.05
            Mean episode successes: 3.5806
Mean episode consecutive_successes: 7.9917
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 8.55s
                        Total time: 32776.19s
                               ETA: 998256.1s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.361s, learning 0.202s)
               Value function loss: 149220.8125
                    Surrogate loss: -0.0056
             Mean action noise std: 0.70
                       Mean reward: 2393.59
               Mean episode length: 49.82
                  Mean reward/step: 40.45
       Mean episode length/episode: 7.11
            Mean episode successes: 3.8271
Mean episode consecutive_successes: 7.9691
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 8.56s
                        Total time: 32784.75s
                               ETA: 998192.6s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.466s, learning 0.174s)
               Value function loss: 146675.8398
                    Surrogate loss: -0.0137
             Mean action noise std: 0.70
                       Mean reward: 1813.99
               Mean episode length: 50.54
                  Mean reward/step: 40.18
       Mean episode length/episode: 6.99
            Mean episode successes: 3.3716
Mean episode consecutive_successes: 8.1197
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 8.64s
                        Total time: 32793.39s
                               ETA: 998131.5s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.070s, learning 0.228s)
               Value function loss: 160124.4953
                    Surrogate loss: -0.0104
             Mean action noise std: 0.70
                       Mean reward: 1662.67
               Mean episode length: 48.35
                  Mean reward/step: 40.34
       Mean episode length/episode: 6.96
            Mean episode successes: 3.5952
Mean episode consecutive_successes: 7.9640
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 8.30s
                        Total time: 32801.69s
                               ETA: 998060.0s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.332s, learning 0.172s)
               Value function loss: 173520.2008
                    Surrogate loss: -0.0090
             Mean action noise std: 0.70
                       Mean reward: 2062.33
               Mean episode length: 52.05
                  Mean reward/step: 40.14
       Mean episode length/episode: 7.14
            Mean episode successes: 3.7153
Mean episode consecutive_successes: 7.9587
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 8.50s
                        Total time: 32810.19s
                               ETA: 997994.8s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.167s, learning 0.161s)
               Value function loss: 210393.5301
                    Surrogate loss: 0.0012
             Mean action noise std: 0.70
                       Mean reward: 2350.93
               Mean episode length: 52.91
                  Mean reward/step: 40.14
       Mean episode length/episode: 7.05
            Mean episode successes: 3.6011
Mean episode consecutive_successes: 8.0943
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 8.33s
                        Total time: 32818.52s
                               ETA: 997924.3s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.242s, learning 0.162s)
               Value function loss: 134943.9730
                    Surrogate loss: -0.0070
             Mean action noise std: 0.70
                       Mean reward: 2038.57
               Mean episode length: 50.05
                  Mean reward/step: 39.72
       Mean episode length/episode: 6.98
            Mean episode successes: 3.4521
Mean episode consecutive_successes: 8.0907
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 8.40s
                        Total time: 32826.93s
                               ETA: 997856.1s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.286s, learning 0.181s)
               Value function loss: 130947.9863
                    Surrogate loss: -0.0089
             Mean action noise std: 0.70
                       Mean reward: 2213.58
               Mean episode length: 49.68
                  Mean reward/step: 38.84
       Mean episode length/episode: 7.03
            Mean episode successes: 3.6226
Mean episode consecutive_successes: 8.0169
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 8.47s
                        Total time: 32835.39s
                               ETA: 997789.9s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.060s, learning 0.166s)
               Value function loss: 124733.7039
                    Surrogate loss: -0.0130
             Mean action noise std: 0.70
                       Mean reward: 2101.35
               Mean episode length: 50.06
                  Mean reward/step: 37.78
       Mean episode length/episode: 7.06
            Mean episode successes: 3.5645
Mean episode consecutive_successes: 8.0199
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 8.23s
                        Total time: 32843.62s
                               ETA: 997716.4s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.280s, learning 0.161s)
               Value function loss: 126415.2773
                    Surrogate loss: -0.0177
             Mean action noise std: 0.70
                       Mean reward: 1920.92
               Mean episode length: 49.53
                  Mean reward/step: 37.15
       Mean episode length/episode: 6.98
            Mean episode successes: 3.4604
Mean episode consecutive_successes: 7.9917
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 8.44s
                        Total time: 32852.06s
                               ETA: 997649.4s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.146s, learning 0.190s)
               Value function loss: 128153.0350
                    Surrogate loss: -0.0156
             Mean action noise std: 0.70
                       Mean reward: 1853.10
               Mean episode length: 49.13
                  Mean reward/step: 35.94
       Mean episode length/episode: 7.01
            Mean episode successes: 3.4624
Mean episode consecutive_successes: 7.9584
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 8.34s
                        Total time: 32860.40s
                               ETA: 997579.4s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.580s, learning 0.158s)
               Value function loss: 140981.3305
                    Surrogate loss: -0.0166
             Mean action noise std: 0.70
                       Mean reward: 2239.25
               Mean episode length: 51.05
                  Mean reward/step: 34.59
       Mean episode length/episode: 6.99
            Mean episode successes: 3.3066
Mean episode consecutive_successes: 7.9388
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 8.74s
                        Total time: 32869.13s
                               ETA: 997521.5s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.332s, learning 0.161s)
               Value function loss: 146070.4660
                    Surrogate loss: 0.0071
             Mean action noise std: 0.70
                       Mean reward: 1700.85
               Mean episode length: 49.69
                  Mean reward/step: 32.04
       Mean episode length/episode: 7.03
            Mean episode successes: 2.9458
Mean episode consecutive_successes: 7.9387
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 8.49s
                        Total time: 32877.63s
                               ETA: 997456.3s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.198s, learning 0.167s)
               Value function loss: 126151.0223
                    Surrogate loss: -0.0129
             Mean action noise std: 0.70
                       Mean reward: 1686.22
               Mean episode length: 50.64
                  Mean reward/step: 36.87
       Mean episode length/episode: 7.03
            Mean episode successes: 3.0859
Mean episode consecutive_successes: 7.8524
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 8.37s
                        Total time: 32885.99s
                               ETA: 997387.2s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.376s, learning 0.299s)
               Value function loss: 151410.7582
                    Surrogate loss: -0.0163
             Mean action noise std: 0.70
                       Mean reward: 1795.63
               Mean episode length: 49.48
                  Mean reward/step: 39.23
       Mean episode length/episode: 7.07
            Mean episode successes: 3.3115
Mean episode consecutive_successes: 7.7596
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 8.67s
                        Total time: 32894.67s
                               ETA: 997327.5s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.088s, learning 0.197s)
               Value function loss: 162748.4613
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 1936.53
               Mean episode length: 50.84
                  Mean reward/step: 40.86
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4858
Mean episode consecutive_successes: 7.7203
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 8.29s
                        Total time: 32902.95s
                               ETA: 997256.1s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.001s, learning 0.165s)
               Value function loss: 167497.5430
                    Surrogate loss: -0.0138
             Mean action noise std: 0.70
                       Mean reward: 2087.40
               Mean episode length: 52.07
                  Mean reward/step: 41.35
       Mean episode length/episode: 7.07
            Mean episode successes: 3.6816
Mean episode consecutive_successes: 7.6947
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 8.17s
                        Total time: 32911.12s
                               ETA: 997181.1s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.376s, learning 0.163s)
               Value function loss: 241239.5152
                    Surrogate loss: -0.0033
             Mean action noise std: 0.70
                       Mean reward: 2297.02
               Mean episode length: 51.84
                  Mean reward/step: 40.79
       Mean episode length/episode: 7.08
            Mean episode successes: 3.7129
Mean episode consecutive_successes: 7.7623
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 8.54s
                        Total time: 32919.66s
                               ETA: 997117.4s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.043s, learning 0.220s)
               Value function loss: 362210.6469
                    Surrogate loss: 0.0061
             Mean action noise std: 0.70
                       Mean reward: 2440.17
               Mean episode length: 56.67
                  Mean reward/step: 40.58
       Mean episode length/episode: 7.20
            Mean episode successes: 3.6255
Mean episode consecutive_successes: 7.9282
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 8.26s
                        Total time: 32927.92s
                               ETA: 997045.5s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.099s, learning 0.157s)
               Value function loss: 194328.8559
                    Surrogate loss: -0.0037
             Mean action noise std: 0.70
                       Mean reward: 2201.53
               Mean episode length: 50.78
                  Mean reward/step: 40.12
       Mean episode length/episode: 7.00
            Mean episode successes: 3.5947
Mean episode consecutive_successes: 7.9277
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 8.26s
                        Total time: 32936.18s
                               ETA: 996973.3s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.222s, learning 0.166s)
               Value function loss: 148222.6434
                    Surrogate loss: -0.0019
             Mean action noise std: 0.70
                       Mean reward: 1885.25
               Mean episode length: 53.48
                  Mean reward/step: 39.68
       Mean episode length/episode: 6.99
            Mean episode successes: 3.5288
Mean episode consecutive_successes: 7.9486
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 8.39s
                        Total time: 32944.56s
                               ETA: 996905.2s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.381s, learning 0.160s)
               Value function loss: 136327.1742
                    Surrogate loss: -0.0047
             Mean action noise std: 0.70
                       Mean reward: 2143.46
               Mean episode length: 53.90
                  Mean reward/step: 38.64
       Mean episode length/episode: 7.09
            Mean episode successes: 3.6118
Mean episode consecutive_successes: 7.9665
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 8.54s
                        Total time: 32953.10s
                               ETA: 996841.7s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.270s, learning 0.164s)
               Value function loss: 142937.6437
                    Surrogate loss: 0.0060
             Mean action noise std: 0.70
                       Mean reward: 1727.04
               Mean episode length: 51.49
                  Mean reward/step: 36.79
       Mean episode length/episode: 7.00
            Mean episode successes: 3.4561
Mean episode consecutive_successes: 7.9207
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 8.43s
                        Total time: 32961.54s
                               ETA: 996775.0s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.162s)
               Value function loss: 137098.3793
                    Surrogate loss: -0.0144
             Mean action noise std: 0.70
                       Mean reward: 1463.21
               Mean episode length: 50.15
                  Mean reward/step: 37.04
       Mean episode length/episode: 7.10
            Mean episode successes: 3.5078
Mean episode consecutive_successes: 7.8705
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 8.10s
                        Total time: 32969.64s
                               ETA: 996698.4s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.100s, learning 0.193s)
               Value function loss: 135062.7664
                    Surrogate loss: -0.0110
             Mean action noise std: 0.70
                       Mean reward: 2233.94
               Mean episode length: 54.85
                  Mean reward/step: 38.19
       Mean episode length/episode: 7.09
            Mean episode successes: 3.4067
Mean episode consecutive_successes: 7.9786
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 8.29s
                        Total time: 32977.94s
                               ETA: 996627.6s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.217s, learning 0.164s)
               Value function loss: 145706.1582
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 1904.17
               Mean episode length: 51.42
                  Mean reward/step: 36.86
       Mean episode length/episode: 6.96
            Mean episode successes: 3.3921
Mean episode consecutive_successes: 7.8975
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 8.38s
                        Total time: 32986.32s
                               ETA: 996559.4s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.202s, learning 0.188s)
               Value function loss: 142205.5750
                    Surrogate loss: -0.0019
             Mean action noise std: 0.70
                       Mean reward: 2301.59
               Mean episode length: 54.23
                  Mean reward/step: 37.41
       Mean episode length/episode: 6.98
            Mean episode successes: 3.5010
Mean episode consecutive_successes: 7.8321
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 8.39s
                        Total time: 32994.71s
                               ETA: 996491.6s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.969s, learning 0.162s)
               Value function loss: 129627.0459
                    Surrogate loss: -0.0063
             Mean action noise std: 0.70
                       Mean reward: 1305.50
               Mean episode length: 49.96
                  Mean reward/step: 37.80
       Mean episode length/episode: 7.09
            Mean episode successes: 3.5947
Mean episode consecutive_successes: 7.7579
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 8.13s
                        Total time: 33002.84s
                               ETA: 996416.0s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.047s, learning 0.161s)
               Value function loss: 138019.4439
                    Surrogate loss: -0.0123
             Mean action noise std: 0.70
                       Mean reward: 2102.56
               Mean episode length: 52.97
                  Mean reward/step: 39.13
       Mean episode length/episode: 6.98
            Mean episode successes: 3.7524
Mean episode consecutive_successes: 7.6775
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 8.21s
                        Total time: 33011.04s
                               ETA: 996342.7s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.987s, learning 0.175s)
               Value function loss: 149268.7166
                    Surrogate loss: 0.0020
             Mean action noise std: 0.70
                       Mean reward: 2142.26
               Mean episode length: 52.13
                  Mean reward/step: 37.49
       Mean episode length/episode: 7.05
            Mean episode successes: 3.5425
Mean episode consecutive_successes: 7.7609
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 8.16s
                        Total time: 33019.21s
                               ETA: 996268.1s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.221s, learning 0.173s)
               Value function loss: 132219.0816
                    Surrogate loss: -0.0106
             Mean action noise std: 0.70
                       Mean reward: 1983.63
               Mean episode length: 50.32
                  Mean reward/step: 39.80
       Mean episode length/episode: 7.06
            Mean episode successes: 3.6191
Mean episode consecutive_successes: 7.7493
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 8.39s
                        Total time: 33027.60s
                               ETA: 996200.5s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.117s, learning 0.205s)
               Value function loss: 140080.8742
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2370.76
               Mean episode length: 53.23
                  Mean reward/step: 40.30
       Mean episode length/episode: 7.08
            Mean episode successes: 3.6680
Mean episode consecutive_successes: 7.8467
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 8.32s
                        Total time: 33035.92s
                               ETA: 996130.8s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.066s, learning 0.164s)
               Value function loss: 148967.5426
                    Surrogate loss: -0.0111
             Mean action noise std: 0.70
                       Mean reward: 1497.89
               Mean episode length: 50.11
                  Mean reward/step: 38.84
       Mean episode length/episode: 7.00
            Mean episode successes: 3.6938
Mean episode consecutive_successes: 7.7767
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 8.23s
                        Total time: 33044.15s
                               ETA: 996058.4s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.757s, learning 0.160s)
               Value function loss: 140874.1248
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 1394.67
               Mean episode length: 48.40
                  Mean reward/step: 39.42
       Mean episode length/episode: 7.10
            Mean episode successes: 3.8491
Mean episode consecutive_successes: 7.7273
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 7.92s
                        Total time: 33052.07s
                               ETA: 995976.6s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.216s, learning 0.194s)
               Value function loss: 144958.4738
                    Surrogate loss: -0.0008
             Mean action noise std: 0.70
                       Mean reward: 2104.72
               Mean episode length: 51.90
                  Mean reward/step: 37.12
       Mean episode length/episode: 6.97
            Mean episode successes: 3.8896
Mean episode consecutive_successes: 7.7132
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 8.41s
                        Total time: 33060.48s
                               ETA: 995909.6s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.377s, learning 0.159s)
               Value function loss: 161073.9121
                    Surrogate loss: -0.0110
             Mean action noise std: 0.70
                       Mean reward: 2321.39
               Mean episode length: 54.77
                  Mean reward/step: 36.12
       Mean episode length/episode: 6.97
            Mean episode successes: 3.6826
Mean episode consecutive_successes: 7.7203
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 8.54s
                        Total time: 33069.01s
                               ETA: 995846.5s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.208s, learning 0.172s)
               Value function loss: 157355.1977
                    Surrogate loss: -0.0122
             Mean action noise std: 0.70
                       Mean reward: 1403.89
               Mean episode length: 46.76
                  Mean reward/step: 35.60
       Mean episode length/episode: 7.00
            Mean episode successes: 3.5220
Mean episode consecutive_successes: 7.7039
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 8.38s
                        Total time: 33077.40s
                               ETA: 995778.8s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.075s, learning 0.218s)
               Value function loss: 160794.1848
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 1447.59
               Mean episode length: 49.04
                  Mean reward/step: 35.58
       Mean episode length/episode: 7.03
            Mean episode successes: 3.3398
Mean episode consecutive_successes: 7.7383
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 8.29s
                        Total time: 33085.69s
                               ETA: 995708.5s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.337s, learning 0.160s)
               Value function loss: 148473.5281
                    Surrogate loss: -0.0096
             Mean action noise std: 0.70
                       Mean reward: 1665.39
               Mean episode length: 49.03
                  Mean reward/step: 34.05
       Mean episode length/episode: 6.96
            Mean episode successes: 3.1772
Mean episode consecutive_successes: 7.7374
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 8.50s
                        Total time: 33094.19s
                               ETA: 995644.3s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.011s, learning 0.226s)
               Value function loss: 147563.2822
                    Surrogate loss: -0.0018
             Mean action noise std: 0.70
                       Mean reward: 1912.01
               Mean episode length: 51.30
                  Mean reward/step: 34.51
       Mean episode length/episode: 7.05
            Mean episode successes: 3.3721
Mean episode consecutive_successes: 7.6214
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 8.24s
                        Total time: 33102.42s
                               ETA: 995572.4s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.105s, learning 0.155s)
               Value function loss: 153273.5588
                    Surrogate loss: 0.0011
             Mean action noise std: 0.70
                       Mean reward: 1369.73
               Mean episode length: 48.00
                  Mean reward/step: 35.46
       Mean episode length/episode: 6.99
            Mean episode successes: 3.4146
Mean episode consecutive_successes: 7.5253
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 8.26s
                        Total time: 33110.68s
                               ETA: 995501.2s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.105s, learning 0.165s)
               Value function loss: 145965.6309
                    Surrogate loss: -0.0092
             Mean action noise std: 0.70
                       Mean reward: 2708.83
               Mean episode length: 54.11
                  Mean reward/step: 34.90
       Mean episode length/episode: 6.97
            Mean episode successes: 3.2710
Mean episode consecutive_successes: 7.6147
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 8.27s
                        Total time: 33118.95s
                               ETA: 995430.3s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.192s, learning 0.155s)
               Value function loss: 147938.0473
                    Surrogate loss: -0.0111
             Mean action noise std: 0.70
                       Mean reward: 2039.08
               Mean episode length: 51.70
                  Mean reward/step: 37.11
       Mean episode length/episode: 7.05
            Mean episode successes: 3.3467
Mean episode consecutive_successes: 7.4999
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 8.35s
                        Total time: 33127.30s
                               ETA: 995361.8s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.360s, learning 0.168s)
               Value function loss: 147497.5400
                    Surrogate loss: -0.0016
             Mean action noise std: 0.70
                       Mean reward: 1499.93
               Mean episode length: 48.62
                  Mean reward/step: 36.87
       Mean episode length/episode: 7.02
            Mean episode successes: 3.4727
Mean episode consecutive_successes: 7.4273
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 8.53s
                        Total time: 33135.83s
                               ETA: 995298.7s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.081s, learning 0.220s)
               Value function loss: 147104.4918
                    Surrogate loss: -0.0104
             Mean action noise std: 0.70
                       Mean reward: 1855.25
               Mean episode length: 48.76
                  Mean reward/step: 36.38
       Mean episode length/episode: 7.01
            Mean episode successes: 3.4536
Mean episode consecutive_successes: 7.4186
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 8.30s
                        Total time: 33144.13s
                               ETA: 995228.9s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.434s, learning 0.161s)
               Value function loss: 155684.6613
                    Surrogate loss: -0.0102
             Mean action noise std: 0.70
                       Mean reward: 1575.74
               Mean episode length: 50.52
                  Mean reward/step: 37.41
       Mean episode length/episode: 7.16
            Mean episode successes: 3.5996
Mean episode consecutive_successes: 7.4076
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 8.60s
                        Total time: 33152.73s
                               ETA: 995167.9s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.041s, learning 0.160s)
               Value function loss: 176323.2949
                    Surrogate loss: -0.0117
             Mean action noise std: 0.70
                       Mean reward: 1318.25
               Mean episode length: 45.89
                  Mean reward/step: 37.12
       Mean episode length/episode: 6.96
            Mean episode successes: 3.5400
Mean episode consecutive_successes: 7.3265
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 8.20s
                        Total time: 33160.93s
                               ETA: 995095.2s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.045s, learning 0.194s)
               Value function loss: 223028.8617
                    Surrogate loss: -0.0112
             Mean action noise std: 0.70
                       Mean reward: 2051.03
               Mean episode length: 54.06
                  Mean reward/step: 37.16
       Mean episode length/episode: 6.97
            Mean episode successes: 3.5269
Mean episode consecutive_successes: 7.3511
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 8.24s
                        Total time: 33169.17s
                               ETA: 995023.6s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.140s, learning 0.164s)
               Value function loss: 207225.5816
                    Surrogate loss: -0.0053
             Mean action noise std: 0.70
                       Mean reward: 1711.94
               Mean episode length: 51.20
                  Mean reward/step: 37.34
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4756
Mean episode consecutive_successes: 7.4271
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 8.30s
                        Total time: 33177.47s
                               ETA: 994954.0s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.163s, learning 0.163s)
               Value function loss: 161304.3809
                    Surrogate loss: -0.0057
             Mean action noise std: 0.70
                       Mean reward: 1843.52
               Mean episode length: 49.64
                  Mean reward/step: 37.10
       Mean episode length/episode: 7.06
            Mean episode successes: 3.4287
Mean episode consecutive_successes: 7.4207
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 8.33s
                        Total time: 33185.80s
                               ETA: 994885.1s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.152s, learning 0.166s)
               Value function loss: 143111.6078
                    Surrogate loss: 0.0052
             Mean action noise std: 0.70
                       Mean reward: 1764.74
               Mean episode length: 52.02
                  Mean reward/step: 36.46
       Mean episode length/episode: 6.92
            Mean episode successes: 3.4482
Mean episode consecutive_successes: 7.3754
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 8.32s
                        Total time: 33194.11s
                               ETA: 994816.0s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.339s, learning 0.205s)
               Value function loss: 147022.6088
                    Surrogate loss: -0.0044
             Mean action noise std: 0.70
                       Mean reward: 1829.69
               Mean episode length: 52.14
                  Mean reward/step: 36.81
       Mean episode length/episode: 7.03
            Mean episode successes: 3.2715
Mean episode consecutive_successes: 7.4489
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 8.54s
                        Total time: 33202.66s
                               ETA: 994753.7s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.069s, learning 0.210s)
               Value function loss: 148218.2730
                    Surrogate loss: 0.0115
             Mean action noise std: 0.70
                       Mean reward: 2198.46
               Mean episode length: 53.61
                  Mean reward/step: 38.20
       Mean episode length/episode: 7.04
            Mean episode successes: 3.2881
Mean episode consecutive_successes: 7.5101
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 8.28s
                        Total time: 33210.94s
                               ETA: 994683.5s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.249s, learning 0.171s)
               Value function loss: 145826.2111
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 1666.83
               Mean episode length: 51.54
                  Mean reward/step: 39.29
       Mean episode length/episode: 7.04
            Mean episode successes: 3.4272
Mean episode consecutive_successes: 7.4673
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 8.42s
                        Total time: 33219.36s
                               ETA: 994617.5s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.163s)
               Value function loss: 150011.7156
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 1655.33
               Mean episode length: 48.85
                  Mean reward/step: 39.78
       Mean episode length/episode: 7.05
            Mean episode successes: 3.6318
Mean episode consecutive_successes: 7.4506
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 8.34s
                        Total time: 33227.70s
                               ETA: 994549.3s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.344s, learning 0.170s)
               Value function loss: 151288.1504
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 1805.07
               Mean episode length: 48.29
                  Mean reward/step: 39.29
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7256
Mean episode consecutive_successes: 7.4458
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 8.51s
                        Total time: 33236.21s
                               ETA: 994486.3s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.030s, learning 0.163s)
               Value function loss: 171622.4453
                    Surrogate loss: -0.0140
             Mean action noise std: 0.70
                       Mean reward: 2038.35
               Mean episode length: 53.21
                  Mean reward/step: 37.63
       Mean episode length/episode: 7.06
            Mean episode successes: 3.7344
Mean episode consecutive_successes: 7.4658
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 8.19s
                        Total time: 33244.41s
                               ETA: 994413.7s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.163s)
               Value function loss: 220461.2793
                    Surrogate loss: -0.0104
             Mean action noise std: 0.70
                       Mean reward: 2715.93
               Mean episode length: 54.09
                  Mean reward/step: 35.86
       Mean episode length/episode: 6.92
            Mean episode successes: 3.5400
Mean episode consecutive_successes: 7.5169
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 8.34s
                        Total time: 33252.75s
                               ETA: 994345.6s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.453s, learning 0.210s)
               Value function loss: 222247.7680
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 2263.55
               Mean episode length: 53.86
                  Mean reward/step: 34.40
       Mean episode length/episode: 7.01
            Mean episode successes: 3.5522
Mean episode consecutive_successes: 7.4764
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 8.66s
                        Total time: 33261.41s
                               ETA: 994287.1s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.983s, learning 0.192s)
               Value function loss: 122061.0184
                    Surrogate loss: 0.0037
             Mean action noise std: 0.70
                       Mean reward: 1652.78
               Mean episode length: 48.93
                  Mean reward/step: 32.95
       Mean episode length/episode: 7.00
            Mean episode successes: 3.1299
Mean episode consecutive_successes: 7.5428
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 8.17s
                        Total time: 33269.59s
                               ETA: 994214.0s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.241s, learning 0.181s)
               Value function loss: 127533.0160
                    Surrogate loss: -0.0124
             Mean action noise std: 0.70
                       Mean reward: 1745.23
               Mean episode length: 48.83
                  Mean reward/step: 32.45
       Mean episode length/episode: 6.98
            Mean episode successes: 2.9741
Mean episode consecutive_successes: 7.5172
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 8.42s
                        Total time: 33278.01s
                               ETA: 994148.4s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.988s, learning 0.162s)
               Value function loss: 104622.6938
                    Surrogate loss: -0.0131
             Mean action noise std: 0.70
                       Mean reward: 1730.70
               Mean episode length: 49.65
                  Mean reward/step: 34.99
       Mean episode length/episode: 7.07
            Mean episode successes: 3.0459
Mean episode consecutive_successes: 7.4695
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 8.15s
                        Total time: 33286.16s
                               ETA: 994074.7s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.037s, learning 0.161s)
               Value function loss: 119602.7354
                    Surrogate loss: -0.0099
             Mean action noise std: 0.70
                       Mean reward: 1232.90
               Mean episode length: 49.03
                  Mean reward/step: 34.68
       Mean episode length/episode: 6.94
            Mean episode successes: 3.2925
Mean episode consecutive_successes: 7.2627
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 8.20s
                        Total time: 33294.36s
                               ETA: 994002.5s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.024s, learning 0.162s)
               Value function loss: 135361.4412
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 1195.52
               Mean episode length: 49.63
                  Mean reward/step: 36.41
       Mean episode length/episode: 6.99
            Mean episode successes: 3.4062
Mean episode consecutive_successes: 7.1929
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 8.19s
                        Total time: 33302.54s
                               ETA: 993929.9s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.130s, learning 0.166s)
               Value function loss: 140710.9295
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 1867.16
               Mean episode length: 47.88
                  Mean reward/step: 37.34
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5586
Mean episode consecutive_successes: 7.1938
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 8.30s
                        Total time: 33310.84s
                               ETA: 993860.7s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.228s, learning 0.163s)
               Value function loss: 125226.5158
                    Surrogate loss: -0.0137
             Mean action noise std: 0.70
                       Mean reward: 1530.58
               Mean episode length: 49.52
                  Mean reward/step: 34.92
       Mean episode length/episode: 6.99
            Mean episode successes: 3.4775
Mean episode consecutive_successes: 7.2077
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 8.39s
                        Total time: 33319.23s
                               ETA: 993794.3s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.312s, learning 0.208s)
               Value function loss: 137824.3611
                    Surrogate loss: -0.0106
             Mean action noise std: 0.70
                       Mean reward: 1731.40
               Mean episode length: 50.78
                  Mean reward/step: 33.80
       Mean episode length/episode: 6.94
            Mean episode successes: 3.2925
Mean episode consecutive_successes: 7.2122
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 15.52s
                        Total time: 33334.75s
                               ETA: 993940.6s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.103s, learning 0.175s)
               Value function loss: 151423.6207
                    Surrogate loss: -0.0158
             Mean action noise std: 0.70
                       Mean reward: 1641.00
               Mean episode length: 50.24
                  Mean reward/step: 34.53
       Mean episode length/episode: 7.04
            Mean episode successes: 3.3809
Mean episode consecutive_successes: 7.1724
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 16.28s
                        Total time: 33351.03s
                               ETA: 994109.3s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.741s, learning 0.164s)
               Value function loss: 173681.5266
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 1963.14
               Mean episode length: 49.46
                  Mean reward/step: 35.15
       Mean episode length/episode: 7.06
            Mean episode successes: 3.2749
Mean episode consecutive_successes: 7.2429
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 15.90s
                        Total time: 33366.93s
                               ETA: 994266.8s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.777s, learning 0.253s)
               Value function loss: 164601.5324
                    Surrogate loss: -0.0135
             Mean action noise std: 0.70
                       Mean reward: 2053.83
               Mean episode length: 50.55
                  Mean reward/step: 33.51
       Mean episode length/episode: 6.98
            Mean episode successes: 3.2173
Mean episode consecutive_successes: 7.1999
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 16.03s
                        Total time: 33382.96s
                               ETA: 994427.9s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.283s, learning 0.274s)
               Value function loss: 146265.4969
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 1056.58
               Mean episode length: 47.65
                  Mean reward/step: 34.53
       Mean episode length/episode: 7.07
            Mean episode successes: 3.3960
Mean episode consecutive_successes: 7.0771
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 16.56s
                        Total time: 33399.52s
                               ETA: 994604.6s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.047s, learning 0.211s)
               Value function loss: 149971.0727
                    Surrogate loss: -0.0110
             Mean action noise std: 0.70
                       Mean reward: 1749.59
               Mean episode length: 52.03
                  Mean reward/step: 35.01
       Mean episode length/episode: 6.93
            Mean episode successes: 3.3882
Mean episode consecutive_successes: 7.0573
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 16.26s
                        Total time: 33415.78s
                               ETA: 994772.3s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.037s, learning 0.206s)
               Value function loss: 146036.9227
                    Surrogate loss: -0.0037
             Mean action noise std: 0.70
                       Mean reward: 2074.14
               Mean episode length: 51.54
                  Mean reward/step: 35.57
       Mean episode length/episode: 7.08
            Mean episode successes: 3.4380
Mean episode consecutive_successes: 7.0880
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 16.24s
                        Total time: 33432.02s
                               ETA: 994939.4s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.015s, learning 0.166s)
               Value function loss: 148087.7742
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 1435.48
               Mean episode length: 49.39
                  Mean reward/step: 35.89
       Mean episode length/episode: 6.98
            Mean episode successes: 3.2339
Mean episode consecutive_successes: 7.0905
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 16.18s
                        Total time: 33448.20s
                               ETA: 995104.5s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.919s, learning 0.165s)
               Value function loss: 143665.3480
                    Surrogate loss: -0.0144
             Mean action noise std: 0.70
                       Mean reward: 1591.12
               Mean episode length: 50.00
                  Mean reward/step: 36.36
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4585
Mean episode consecutive_successes: 7.0290
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 16.08s
                        Total time: 33464.28s
                               ETA: 995266.7s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.872s, learning 0.161s)
               Value function loss: 155957.5164
                    Surrogate loss: 0.0404
             Mean action noise std: 0.70
                       Mean reward: 1320.80
               Mean episode length: 46.37
                  Mean reward/step: 37.33
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4932
Mean episode consecutive_successes: 7.0223
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 16.03s
                        Total time: 33480.32s
                               ETA: 995427.2s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1011 steps/s (collection: 15.960s, learning 0.231s)
               Value function loss: 144842.6293
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 1111.84
               Mean episode length: 47.86
                  Mean reward/step: 37.21
       Mean episode length/episode: 7.09
            Mean episode successes: 3.5786
Mean episode consecutive_successes: 7.0244
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 16.19s
                        Total time: 33496.51s
                               ETA: 995592.4s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.385s, learning 0.281s)
               Value function loss: 153450.4574
                    Surrogate loss: -0.0124
             Mean action noise std: 0.70
                       Mean reward: 2282.51
               Mean episode length: 52.23
                  Mean reward/step: 38.22
       Mean episode length/episode: 7.02
            Mean episode successes: 3.8052
Mean episode consecutive_successes: 7.0161
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 16.67s
                        Total time: 33513.17s
                               ETA: 995771.5s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.980s, learning 0.173s)
               Value function loss: 164724.0121
                    Surrogate loss: -0.0052
             Mean action noise std: 0.70
                       Mean reward: 1753.17
               Mean episode length: 49.42
                  Mean reward/step: 37.11
       Mean episode length/episode: 6.91
            Mean episode successes: 3.6758
Mean episode consecutive_successes: 7.0287
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 16.15s
                        Total time: 33529.33s
                               ETA: 995935.3s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.962s, learning 0.185s)
               Value function loss: 158870.9613
                    Surrogate loss: -0.0102
             Mean action noise std: 0.70
                       Mean reward: 1940.73
               Mean episode length: 49.39
                  Mean reward/step: 37.83
       Mean episode length/episode: 7.07
            Mean episode successes: 3.6895
Mean episode consecutive_successes: 7.1283
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 16.15s
                        Total time: 33545.47s
                               ETA: 996098.8s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.105s, learning 0.261s)
               Value function loss: 179241.9121
                    Surrogate loss: -0.0058
             Mean action noise std: 0.70
                       Mean reward: 1667.68
               Mean episode length: 49.09
                  Mean reward/step: 39.51
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7153
Mean episode consecutive_successes: 7.1719
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 16.37s
                        Total time: 33561.84s
                               ETA: 996268.6s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.134s, learning 0.304s)
               Value function loss: 191210.5723
                    Surrogate loss: 0.0032
             Mean action noise std: 0.70
                       Mean reward: 2053.99
               Mean episode length: 50.33
                  Mean reward/step: 40.58
       Mean episode length/episode: 6.97
            Mean episode successes: 3.7969
Mean episode consecutive_successes: 7.2146
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 16.44s
                        Total time: 33578.28s
                               ETA: 996440.5s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.897s, learning 0.191s)
               Value function loss: 173809.7805
                    Surrogate loss: -0.0079
             Mean action noise std: 0.70
                       Mean reward: 1715.49
               Mean episode length: 49.00
                  Mean reward/step: 39.24
       Mean episode length/episode: 7.01
            Mean episode successes: 3.7993
Mean episode consecutive_successes: 7.2140
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 16.09s
                        Total time: 33594.37s
                               ETA: 996601.9s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.573s, learning 0.213s)
               Value function loss: 168704.1371
                    Surrogate loss: -0.0073
             Mean action noise std: 0.70
                       Mean reward: 1964.92
               Mean episode length: 48.38
                  Mean reward/step: 36.27
       Mean episode length/episode: 6.94
            Mean episode successes: 3.6011
Mean episode consecutive_successes: 7.2792
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 16.79s
                        Total time: 33611.15s
                               ETA: 996783.9s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.143s, learning 0.214s)
               Value function loss: 165980.6609
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 1855.97
               Mean episode length: 49.97
                  Mean reward/step: 35.83
       Mean episode length/episode: 7.09
            Mean episode successes: 3.6201
Mean episode consecutive_successes: 7.2924
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 16.36s
                        Total time: 33627.51s
                               ETA: 996953.1s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.749s, learning 0.233s)
               Value function loss: 173474.6898
                    Surrogate loss: -0.0158
             Mean action noise std: 0.70
                       Mean reward: 1956.32
               Mean episode length: 50.54
                  Mean reward/step: 34.38
       Mean episode length/episode: 7.04
            Mean episode successes: 3.4814
Mean episode consecutive_successes: 7.3293
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 15.98s
                        Total time: 33643.49s
                               ETA: 997111.0s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.986s, learning 0.161s)
               Value function loss: 203620.6156
                    Surrogate loss: -0.0154
             Mean action noise std: 0.70
                       Mean reward: 1996.76
               Mean episode length: 51.10
                  Mean reward/step: 36.50
       Mean episode length/episode: 6.95
            Mean episode successes: 3.2515
Mean episode consecutive_successes: 7.3499
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 16.15s
                        Total time: 33659.64s
                               ETA: 997273.7s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.070s, learning 0.185s)
               Value function loss: 168929.9219
                    Surrogate loss: -0.0159
             Mean action noise std: 0.70
                       Mean reward: 1759.22
               Mean episode length: 51.57
                  Mean reward/step: 36.21
       Mean episode length/episode: 7.00
            Mean episode successes: 3.3730
Mean episode consecutive_successes: 7.2755
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 16.25s
                        Total time: 33675.89s
                               ETA: 997439.5s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.325s, learning 0.181s)
               Value function loss: 127121.4549
                    Surrogate loss: -0.0078
             Mean action noise std: 0.70
                       Mean reward: 1313.14
               Mean episode length: 45.30
                  Mean reward/step: 36.39
       Mean episode length/episode: 7.02
            Mean episode successes: 3.3408
Mean episode consecutive_successes: 7.2258
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 16.51s
                        Total time: 33692.40s
                               ETA: 997612.6s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.317s, learning 0.155s)
               Value function loss: 119465.9488
                    Surrogate loss: -0.0066
             Mean action noise std: 0.70
                       Mean reward: 1381.08
               Mean episode length: 45.99
                  Mean reward/step: 35.35
       Mean episode length/episode: 7.07
            Mean episode successes: 3.4326
Mean episode consecutive_successes: 7.1941
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 16.47s
                        Total time: 33708.87s
                               ETA: 997784.7s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.504s, learning 0.161s)
               Value function loss: 125839.4775
                    Surrogate loss: -0.0057
             Mean action noise std: 0.70
                       Mean reward: 1837.09
               Mean episode length: 47.99
                  Mean reward/step: 34.25
       Mean episode length/episode: 6.90
            Mean episode successes: 3.3672
Mean episode consecutive_successes: 7.1261
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 16.67s
                        Total time: 33725.54s
                               ETA: 997962.3s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.412s, learning 0.184s)
               Value function loss: 133078.7430
                    Surrogate loss: -0.0036
             Mean action noise std: 0.70
                       Mean reward: 1833.18
               Mean episode length: 49.24
                  Mean reward/step: 34.54
       Mean episode length/episode: 6.96
            Mean episode successes: 3.4858
Mean episode consecutive_successes: 7.0537
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 16.60s
                        Total time: 33742.13s
                               ETA: 998137.7s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.331s, learning 0.164s)
               Value function loss: 142448.3230
                    Surrogate loss: 0.0080
             Mean action noise std: 0.70
                       Mean reward: 1775.30
               Mean episode length: 52.35
                  Mean reward/step: 35.33
       Mean episode length/episode: 6.89
            Mean episode successes: 3.3931
Mean episode consecutive_successes: 7.0458
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 16.50s
                        Total time: 33758.63s
                               ETA: 998310.0s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.205s, learning 0.168s)
               Value function loss: 134783.2809
                    Surrogate loss: -0.0126
             Mean action noise std: 0.70
                       Mean reward: 1909.40
               Mean episode length: 50.46
                  Mean reward/step: 31.51
       Mean episode length/episode: 6.92
            Mean episode successes: 3.1855
Mean episode consecutive_successes: 7.0043
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 16.37s
                        Total time: 33775.00s
                               ETA: 998478.6s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.291s, learning 0.164s)
               Value function loss: 138241.0877
                    Surrogate loss: -0.0187
             Mean action noise std: 0.70
                       Mean reward: 1343.08
               Mean episode length: 45.30
                  Mean reward/step: 30.72
       Mean episode length/episode: 7.01
            Mean episode successes: 3.0537
Mean episode consecutive_successes: 6.9916
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 16.45s
                        Total time: 33791.46s
                               ETA: 998649.5s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.940s, learning 0.164s)
               Value function loss: 128188.2281
                    Surrogate loss: -0.0168
             Mean action noise std: 0.70
                       Mean reward: 1703.36
               Mean episode length: 45.26
                  Mean reward/step: 30.68
       Mean episode length/episode: 6.94
            Mean episode successes: 2.9741
Mean episode consecutive_successes: 6.9181
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 16.10s
                        Total time: 33807.56s
                               ETA: 998809.9s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.665s, learning 0.198s)
               Value function loss: 136433.9363
                    Surrogate loss: -0.0172
             Mean action noise std: 0.70
                       Mean reward: 1465.86
               Mean episode length: 45.49
                  Mean reward/step: 30.74
       Mean episode length/episode: 6.93
            Mean episode successes: 2.8926
Mean episode consecutive_successes: 6.8284
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 15.86s
                        Total time: 33823.42s
                               ETA: 998963.2s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.245s, learning 0.182s)
               Value function loss: 135789.5479
                    Surrogate loss: 0.0451
             Mean action noise std: 0.70
                       Mean reward: 1262.71
               Mean episode length: 47.80
                  Mean reward/step: 33.14
       Mean episode length/episode: 7.00
            Mean episode successes: 2.9922
Mean episode consecutive_successes: 6.7180
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 16.43s
                        Total time: 33839.85s
                               ETA: 999132.9s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.029s, learning 0.213s)
               Value function loss: 123144.1885
                    Surrogate loss: 0.0004
             Mean action noise std: 0.70
                       Mean reward: 1396.75
               Mean episode length: 47.51
                  Mean reward/step: 32.48
       Mean episode length/episode: 7.04
            Mean episode successes: 3.0596
Mean episode consecutive_successes: 6.6787
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 16.24s
                        Total time: 33856.09s
                               ETA: 999297.1s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.117s, learning 0.221s)
               Value function loss: 131196.2777
                    Surrogate loss: -0.0168
             Mean action noise std: 0.70
                       Mean reward: 1589.26
               Mean episode length: 47.59
                  Mean reward/step: 34.72
       Mean episode length/episode: 7.00
            Mean episode successes: 3.1011
Mean episode consecutive_successes: 6.6712
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 16.34s
                        Total time: 33872.43s
                               ETA: 999464.0s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.343s, learning 0.157s)
               Value function loss: 154164.5172
                    Surrogate loss: -0.0126
             Mean action noise std: 0.70
                       Mean reward: 1764.71
               Mean episode length: 47.56
                  Mean reward/step: 36.38
       Mean episode length/episode: 7.00
            Mean episode successes: 3.3228
Mean episode consecutive_successes: 6.6175
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 16.50s
                        Total time: 33888.93s
                               ETA: 999635.5s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.010s, learning 0.176s)
               Value function loss: 210598.3684
                    Surrogate loss: -0.0108
             Mean action noise std: 0.70
                       Mean reward: 2235.71
               Mean episode length: 53.29
                  Mean reward/step: 36.71
       Mean episode length/episode: 7.02
            Mean episode successes: 3.2490
Mean episode consecutive_successes: 6.7125
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 16.19s
                        Total time: 33905.11s
                               ETA: 999797.7s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.312s, learning 0.182s)
               Value function loss: 191162.7668
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 1243.87
               Mean episode length: 46.89
                  Mean reward/step: 36.89
       Mean episode length/episode: 7.06
            Mean episode successes: 3.3599
Mean episode consecutive_successes: 6.6834
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 16.49s
                        Total time: 33921.61s
                               ETA: 999968.9s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1123 steps/s (collection: 14.425s, learning 0.158s)
               Value function loss: 128444.9225
                    Surrogate loss: -0.0168
             Mean action noise std: 0.70
                       Mean reward: 1307.64
               Mean episode length: 48.33
                  Mean reward/step: 37.35
       Mean episode length/episode: 7.03
            Mean episode successes: 3.6113
Mean episode consecutive_successes: 6.6259
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 14.58s
                        Total time: 33936.19s
                               ETA: 1000083.6s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.107s, learning 0.169s)
               Value function loss: 143629.9668
                    Surrogate loss: -0.0089
             Mean action noise std: 0.70
                       Mean reward: 2020.10
               Mean episode length: 51.99
                  Mean reward/step: 38.04
       Mean episode length/episode: 7.09
            Mean episode successes: 3.7827
Mean episode consecutive_successes: 6.6715
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 8.28s
                        Total time: 33944.47s
                               ETA: 1000012.5s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.246s, learning 0.160s)
               Value function loss: 143264.0187
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 2479.94
               Mean episode length: 52.23
                  Mean reward/step: 37.41
       Mean episode length/episode: 7.09
            Mean episode successes: 3.6030
Mean episode consecutive_successes: 6.8849
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 8.41s
                        Total time: 33952.87s
                               ETA: 999945.2s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.052s, learning 0.161s)
               Value function loss: 132691.6643
                    Surrogate loss: -0.0113
             Mean action noise std: 0.70
                       Mean reward: 1712.41
               Mean episode length: 52.61
                  Mean reward/step: 34.53
       Mean episode length/episode: 7.04
            Mean episode successes: 3.4580
Mean episode consecutive_successes: 6.9282
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 8.21s
                        Total time: 33961.09s
                               ETA: 999872.3s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.581s, learning 0.160s)
               Value function loss: 135902.7934
                    Surrogate loss: -0.0119
             Mean action noise std: 0.70
                       Mean reward: 2244.55
               Mean episode length: 51.07
                  Mean reward/step: 37.14
       Mean episode length/episode: 7.03
            Mean episode successes: 3.3530
Mean episode consecutive_successes: 7.0461
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 8.74s
                        Total time: 33969.83s
                               ETA: 999814.9s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.099s, learning 0.164s)
               Value function loss: 200209.0082
                    Surrogate loss: -0.0127
             Mean action noise std: 0.70
                       Mean reward: 1455.94
               Mean episode length: 50.33
                  Mean reward/step: 39.45
       Mean episode length/episode: 7.10
            Mean episode successes: 3.4785
Mean episode consecutive_successes: 7.0546
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 8.26s
                        Total time: 33978.09s
                               ETA: 999743.5s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.171s, learning 0.183s)
               Value function loss: 288417.8258
                    Surrogate loss: -0.0119
             Mean action noise std: 0.70
                       Mean reward: 1987.61
               Mean episode length: 52.20
                  Mean reward/step: 39.11
       Mean episode length/episode: 7.01
            Mean episode successes: 3.6265
Mean episode consecutive_successes: 7.0878
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 8.35s
                        Total time: 33986.44s
                               ETA: 999674.9s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.111s, learning 0.167s)
               Value function loss: 176647.9000
                    Surrogate loss: -0.0058
             Mean action noise std: 0.70
                       Mean reward: 2149.43
               Mean episode length: 51.38
                  Mean reward/step: 37.91
       Mean episode length/episode: 7.07
            Mean episode successes: 3.6133
Mean episode consecutive_successes: 7.0983
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 8.28s
                        Total time: 33994.72s
                               ETA: 999604.0s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.269s, learning 0.168s)
               Value function loss: 144539.8219
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 2180.31
               Mean episode length: 51.82
                  Mean reward/step: 38.91
       Mean episode length/episode: 7.12
            Mean episode successes: 3.7632
Mean episode consecutive_successes: 7.1388
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 8.44s
                        Total time: 34003.16s
                               ETA: 999537.8s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.901s, learning 0.210s)
               Value function loss: 160299.8395
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 2359.65
               Mean episode length: 54.88
                  Mean reward/step: 37.28
       Mean episode length/episode: 6.98
            Mean episode successes: 3.3486
Mean episode consecutive_successes: 7.3168
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 8.11s
                        Total time: 34011.27s
                               ETA: 999462.1s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.186s, learning 0.258s)
               Value function loss: 128468.6687
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 1734.34
               Mean episode length: 51.26
                  Mean reward/step: 36.53
       Mean episode length/episode: 7.06
            Mean episode successes: 3.4609
Mean episode consecutive_successes: 7.2493
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 8.44s
                        Total time: 34019.71s
                               ETA: 999396.2s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.305s, learning 0.191s)
               Value function loss: 128361.3807
                    Surrogate loss: -0.0160
             Mean action noise std: 0.70
                       Mean reward: 1762.37
               Mean episode length: 52.21
                  Mean reward/step: 36.99
       Mean episode length/episode: 7.15
            Mean episode successes: 3.5698
Mean episode consecutive_successes: 7.2951
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 8.50s
                        Total time: 34028.21s
                               ETA: 999331.9s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.135s, learning 0.215s)
               Value function loss: 152829.4316
                    Surrogate loss: -0.0151
             Mean action noise std: 0.70
                       Mean reward: 2380.53
               Mean episode length: 56.56
                  Mean reward/step: 37.58
       Mean episode length/episode: 7.05
            Mean episode successes: 3.5342
Mean episode consecutive_successes: 7.3416
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 8.35s
                        Total time: 34036.56s
                               ETA: 999263.3s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.478s, learning 0.185s)
               Value function loss: 191703.0238
                    Surrogate loss: -0.0006
             Mean action noise std: 0.70
                       Mean reward: 2303.98
               Mean episode length: 55.06
                  Mean reward/step: 40.31
       Mean episode length/episode: 7.10
            Mean episode successes: 3.6821
Mean episode consecutive_successes: 7.3934
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 8.66s
                        Total time: 34045.22s
                               ETA: 999204.0s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.325s, learning 0.176s)
               Value function loss: 135378.8248
                    Surrogate loss: 0.0170
             Mean action noise std: 0.70
                       Mean reward: 2155.40
               Mean episode length: 53.31
                  Mean reward/step: 39.43
       Mean episode length/episode: 7.06
            Mean episode successes: 3.7788
Mean episode consecutive_successes: 7.3994
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 8.50s
                        Total time: 34053.72s
                               ETA: 999139.9s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.277s, learning 0.181s)
               Value function loss: 139506.4004
                    Surrogate loss: -0.0044
             Mean action noise std: 0.70
                       Mean reward: 1957.21
               Mean episode length: 52.34
                  Mean reward/step: 40.21
       Mean episode length/episode: 7.04
            Mean episode successes: 3.6934
Mean episode consecutive_successes: 7.4959
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 8.46s
                        Total time: 34062.18s
                               ETA: 999074.6s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.312s, learning 0.183s)
               Value function loss: 137870.2029
                    Surrogate loss: -0.0072
             Mean action noise std: 0.70
                       Mean reward: 1719.29
               Mean episode length: 51.30
                  Mean reward/step: 39.44
       Mean episode length/episode: 7.02
            Mean episode successes: 3.5005
Mean episode consecutive_successes: 7.5987
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 8.50s
                        Total time: 34070.68s
                               ETA: 999010.5s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.404s, learning 0.158s)
               Value function loss: 140101.0979
                    Surrogate loss: -0.0084
             Mean action noise std: 0.70
                       Mean reward: 1661.62
               Mean episode length: 51.19
                  Mean reward/step: 38.04
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7642
Mean episode consecutive_successes: 7.5083
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 8.56s
                        Total time: 34079.24s
                               ETA: 998948.3s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.440s, learning 0.185s)
               Value function loss: 155208.2309
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 2338.37
               Mean episode length: 54.01
                  Mean reward/step: 39.13
       Mean episode length/episode: 7.10
            Mean episode successes: 3.6938
Mean episode consecutive_successes: 7.6547
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 8.63s
                        Total time: 34087.86s
                               ETA: 998888.0s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.291s, learning 0.164s)
               Value function loss: 189168.1879
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 2317.48
               Mean episode length: 52.11
                  Mean reward/step: 38.72
       Mean episode length/episode: 7.00
            Mean episode successes: 3.6460
Mean episode consecutive_successes: 7.6670
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 8.45s
                        Total time: 34096.32s
                               ETA: 998822.7s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.400s, learning 0.258s)
               Value function loss: 145676.4711
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 1614.04
               Mean episode length: 51.23
                  Mean reward/step: 37.75
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5366
Mean episode consecutive_successes: 7.6986
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 8.66s
                        Total time: 34104.98s
                               ETA: 998763.5s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.203s, learning 0.185s)
               Value function loss: 153212.2984
                    Surrogate loss: -0.0102
             Mean action noise std: 0.70
                       Mean reward: 2579.19
               Mean episode length: 54.89
                  Mean reward/step: 37.43
       Mean episode length/episode: 7.10
            Mean episode successes: 3.4414
Mean episode consecutive_successes: 7.8299
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 8.39s
                        Total time: 34113.36s
                               ETA: 998696.3s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.358s, learning 0.163s)
               Value function loss: 183898.6301
                    Surrogate loss: -0.0044
             Mean action noise std: 0.70
                       Mean reward: 1382.50
               Mean episode length: 48.86
                  Mean reward/step: 37.71
       Mean episode length/episode: 6.99
            Mean episode successes: 3.7031
Mean episode consecutive_successes: 7.6706
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 8.52s
                        Total time: 34121.88s
                               ETA: 998633.1s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.255s, learning 0.163s)
               Value function loss: 170591.8344
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 1981.95
               Mean episode length: 55.82
                  Mean reward/step: 38.47
       Mean episode length/episode: 7.12
            Mean episode successes: 3.8052
Mean episode consecutive_successes: 7.6672
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 8.42s
                        Total time: 34130.30s
                               ETA: 998566.9s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.232s, learning 0.232s)
               Value function loss: 142822.6102
                    Surrogate loss: -0.0084
             Mean action noise std: 0.70
                       Mean reward: 2265.27
               Mean episode length: 52.82
                  Mean reward/step: 37.10
       Mean episode length/episode: 7.06
            Mean episode successes: 3.6924
Mean episode consecutive_successes: 7.7249
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 8.46s
                        Total time: 34138.77s
                               ETA: 998502.1s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.401s, learning 0.161s)
               Value function loss: 132552.4129
                    Surrogate loss: -0.0085
             Mean action noise std: 0.70
                       Mean reward: 1915.36
               Mean episode length: 53.19
                  Mean reward/step: 37.05
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5933
Mean episode consecutive_successes: 7.7238
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 8.56s
                        Total time: 34147.33s
                               ETA: 998440.2s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.271s, learning 0.158s)
               Value function loss: 178394.8547
                    Surrogate loss: -0.0092
             Mean action noise std: 0.70
                       Mean reward: 2011.06
               Mean episode length: 50.17
                  Mean reward/step: 37.03
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4448
Mean episode consecutive_successes: 7.7902
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 8.43s
                        Total time: 34155.76s
                               ETA: 998374.4s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.171s, learning 0.186s)
               Value function loss: 217448.7445
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 2009.93
               Mean episode length: 52.36
                  Mean reward/step: 37.51
       Mean episode length/episode: 7.14
            Mean episode successes: 3.5811
Mean episode consecutive_successes: 7.7992
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 8.36s
                        Total time: 34164.11s
                               ETA: 998306.6s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.226s, learning 0.176s)
               Value function loss: 129466.7002
                    Surrogate loss: -0.0144
             Mean action noise std: 0.70
                       Mean reward: 1954.94
               Mean episode length: 52.10
                  Mean reward/step: 36.51
       Mean episode length/episode: 7.02
            Mean episode successes: 3.4229
Mean episode consecutive_successes: 7.7861
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 8.40s
                        Total time: 34172.52s
                               ETA: 998240.1s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.260s, learning 0.225s)
               Value function loss: 120519.5141
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 1551.12
               Mean episode length: 50.12
                  Mean reward/step: 38.38
       Mean episode length/episode: 6.98
            Mean episode successes: 3.5039
Mean episode consecutive_successes: 7.7260
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 8.48s
                        Total time: 34181.00s
                               ETA: 998176.1s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.341s, learning 0.159s)
               Value function loss: 113969.2316
                    Surrogate loss: -0.0172
             Mean action noise std: 0.70
                       Mean reward: 1864.72
               Mean episode length: 52.48
                  Mean reward/step: 37.48
       Mean episode length/episode: 7.09
            Mean episode successes: 3.4473
Mean episode consecutive_successes: 7.7793
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 8.50s
                        Total time: 34189.50s
                               ETA: 998112.5s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.166s)
               Value function loss: 123631.3439
                    Surrogate loss: -0.0185
             Mean action noise std: 0.70
                       Mean reward: 2136.10
               Mean episode length: 53.91
                  Mean reward/step: 38.12
       Mean episode length/episode: 7.06
            Mean episode successes: 3.5283
Mean episode consecutive_successes: 7.7723
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 8.46s
                        Total time: 34197.96s
                               ETA: 998047.7s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.443s, learning 0.161s)
               Value function loss: 146656.7197
                    Surrogate loss: 0.0006
             Mean action noise std: 0.70
                       Mean reward: 2167.97
               Mean episode length: 52.83
                  Mean reward/step: 38.20
       Mean episode length/episode: 7.02
            Mean episode successes: 3.4531
Mean episode consecutive_successes: 7.8009
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 8.60s
                        Total time: 34206.56s
                               ETA: 997987.2s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.164s, learning 0.163s)
               Value function loss: 141113.6291
                    Surrogate loss: -0.0070
             Mean action noise std: 0.70
                       Mean reward: 1769.14
               Mean episode length: 50.83
                  Mean reward/step: 39.46
       Mean episode length/episode: 7.17
            Mean episode successes: 3.5347
Mean episode consecutive_successes: 7.8295
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 8.33s
                        Total time: 34214.89s
                               ETA: 997918.7s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.499s, learning 0.179s)
               Value function loss: 153486.1809
                    Surrogate loss: -0.0141
             Mean action noise std: 0.70
                       Mean reward: 2498.25
               Mean episode length: 53.46
                  Mean reward/step: 40.72
       Mean episode length/episode: 7.09
            Mean episode successes: 3.7104
Mean episode consecutive_successes: 7.8672
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 8.68s
                        Total time: 34223.56s
                               ETA: 997860.5s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.449s, learning 0.165s)
               Value function loss: 168247.2387
                    Surrogate loss: 0.0033
             Mean action noise std: 0.70
                       Mean reward: 2136.87
               Mean episode length: 51.47
                  Mean reward/step: 40.67
       Mean episode length/episode: 7.07
            Mean episode successes: 3.6338
Mean episode consecutive_successes: 7.9523
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 8.61s
                        Total time: 34232.18s
                               ETA: 997800.4s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.057s, learning 0.170s)
               Value function loss: 156324.5395
                    Surrogate loss: -0.0164
             Mean action noise std: 0.70
                       Mean reward: 1691.77
               Mean episode length: 51.25
                  Mean reward/step: 40.21
       Mean episode length/episode: 7.07
            Mean episode successes: 3.7510
Mean episode consecutive_successes: 7.8897
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 8.23s
                        Total time: 34240.41s
                               ETA: 997729.1s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.339s, learning 0.172s)
               Value function loss: 159260.2094
                    Surrogate loss: -0.0147
             Mean action noise std: 0.70
                       Mean reward: 2197.65
               Mean episode length: 56.28
                  Mean reward/step: 39.42
       Mean episode length/episode: 6.94
            Mean episode successes: 3.6230
Mean episode consecutive_successes: 7.9079
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 8.51s
                        Total time: 34248.92s
                               ETA: 997666.1s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.372s, learning 0.263s)
               Value function loss: 164956.3988
                    Surrogate loss: -0.0157
             Mean action noise std: 0.70
                       Mean reward: 2377.33
               Mean episode length: 55.67
                  Mean reward/step: 36.08
       Mean episode length/episode: 7.03
            Mean episode successes: 3.4819
Mean episode consecutive_successes: 7.9821
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 8.64s
                        Total time: 34257.55s
                               ETA: 997606.7s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.409s, learning 0.160s)
               Value function loss: 143798.1547
                    Surrogate loss: -0.0184
             Mean action noise std: 0.70
                       Mean reward: 1957.62
               Mean episode length: 49.12
                  Mean reward/step: 34.63
       Mean episode length/episode: 7.04
            Mean episode successes: 3.2632
Mean episode consecutive_successes: 8.0113
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 8.57s
                        Total time: 34266.12s
                               ETA: 997545.5s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.374s, learning 0.235s)
               Value function loss: 134656.8986
                    Surrogate loss: -0.0077
             Mean action noise std: 0.70
                       Mean reward: 1585.25
               Mean episode length: 49.31
                  Mean reward/step: 35.06
       Mean episode length/episode: 7.11
            Mean episode successes: 3.0908
Mean episode consecutive_successes: 8.0354
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 8.61s
                        Total time: 34274.73s
                               ETA: 997485.4s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.191s, learning 0.228s)
               Value function loss: 133489.9229
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 1802.34
               Mean episode length: 52.59
                  Mean reward/step: 35.09
       Mean episode length/episode: 7.01
            Mean episode successes: 3.1567
Mean episode consecutive_successes: 7.9087
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 8.42s
                        Total time: 34283.15s
                               ETA: 997419.9s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.137s, learning 0.185s)
               Value function loss: 136372.4375
                    Surrogate loss: -0.0139
             Mean action noise std: 0.70
                       Mean reward: 1473.61
               Mean episode length: 50.60
                  Mean reward/step: 34.71
       Mean episode length/episode: 7.09
            Mean episode successes: 3.3711
Mean episode consecutive_successes: 7.7685
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 8.32s
                        Total time: 34291.47s
                               ETA: 997351.5s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.209s, learning 0.166s)
               Value function loss: 173687.2867
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 1972.71
               Mean episode length: 52.99
                  Mean reward/step: 35.65
       Mean episode length/episode: 7.07
            Mean episode successes: 3.3242
Mean episode consecutive_successes: 7.7785
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 8.37s
                        Total time: 34299.84s
                               ETA: 997284.7s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.260s, learning 0.166s)
               Value function loss: 139489.6178
                    Surrogate loss: -0.0020
             Mean action noise std: 0.70
                       Mean reward: 1496.93
               Mean episode length: 48.30
                  Mean reward/step: 39.02
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5903
Mean episode consecutive_successes: 7.6862
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 8.43s
                        Total time: 34308.27s
                               ETA: 997219.5s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.431s, learning 0.161s)
               Value function loss: 128560.4178
                    Surrogate loss: -0.0119
             Mean action noise std: 0.70
                       Mean reward: 2056.99
               Mean episode length: 51.97
                  Mean reward/step: 38.31
       Mean episode length/episode: 7.02
            Mean episode successes: 3.4526
Mean episode consecutive_successes: 7.7340
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 8.59s
                        Total time: 34316.86s
                               ETA: 997159.1s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.130s, learning 0.159s)
               Value function loss: 129876.2135
                    Surrogate loss: -0.0135
             Mean action noise std: 0.70
                       Mean reward: 1829.84
               Mean episode length: 53.00
                  Mean reward/step: 39.09
       Mean episode length/episode: 7.01
            Mean episode successes: 3.5830
Mean episode consecutive_successes: 7.7071
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 8.29s
                        Total time: 34325.15s
                               ETA: 997089.9s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.424s, learning 0.229s)
               Value function loss: 137852.2590
                    Surrogate loss: -0.0129
             Mean action noise std: 0.70
                       Mean reward: 1731.06
               Mean episode length: 50.55
                  Mean reward/step: 37.68
       Mean episode length/episode: 7.06
            Mean episode successes: 3.5537
Mean episode consecutive_successes: 7.6903
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 8.65s
                        Total time: 34333.80s
                               ETA: 997031.4s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.046s, learning 0.204s)
               Value function loss: 141106.4422
                    Surrogate loss: -0.0143
             Mean action noise std: 0.70
                       Mean reward: 1951.23
               Mean episode length: 50.61
                  Mean reward/step: 37.90
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5635
Mean episode consecutive_successes: 7.6989
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 8.25s
                        Total time: 34342.05s
                               ETA: 996961.2s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.525s, learning 0.189s)
               Value function loss: 148090.4566
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 1248.97
               Mean episode length: 47.63
                  Mean reward/step: 40.93
       Mean episode length/episode: 7.11
            Mean episode successes: 3.8481
Mean episode consecutive_successes: 7.6576
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 8.71s
                        Total time: 34350.77s
                               ETA: 996904.5s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.452s, learning 0.160s)
               Value function loss: 158245.9082
                    Surrogate loss: -0.0114
             Mean action noise std: 0.70
                       Mean reward: 2129.79
               Mean episode length: 52.18
                  Mean reward/step: 39.19
       Mean episode length/episode: 7.05
            Mean episode successes: 3.8384
Mean episode consecutive_successes: 7.7177
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 8.61s
                        Total time: 34359.38s
                               ETA: 996844.8s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.188s, learning 0.159s)
               Value function loss: 158652.9867
                    Surrogate loss: -0.0156
             Mean action noise std: 0.70
                       Mean reward: 2024.79
               Mean episode length: 52.70
                  Mean reward/step: 38.42
       Mean episode length/episode: 7.03
            Mean episode successes: 3.7793
Mean episode consecutive_successes: 7.7336
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 8.35s
                        Total time: 34367.73s
                               ETA: 996777.5s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.351s, learning 0.185s)
               Value function loss: 151607.3438
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 2042.66
               Mean episode length: 52.85
                  Mean reward/step: 36.90
       Mean episode length/episode: 7.05
            Mean episode successes: 3.6646
Mean episode consecutive_successes: 7.7652
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 8.54s
                        Total time: 34376.26s
                               ETA: 996715.7s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.248s, learning 0.161s)
               Value function loss: 248474.1973
                    Surrogate loss: -0.0099
             Mean action noise std: 0.70
                       Mean reward: 2515.89
               Mean episode length: 53.96
                  Mean reward/step: 37.89
       Mean episode length/episode: 7.15
            Mean episode successes: 3.5093
Mean episode consecutive_successes: 7.9441
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 8.41s
                        Total time: 34384.67s
                               ETA: 996650.3s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.252s, learning 0.169s)
               Value function loss: 139045.6867
                    Surrogate loss: -0.0162
             Mean action noise std: 0.70
                       Mean reward: 1992.23
               Mean episode length: 52.27
                  Mean reward/step: 38.58
       Mean episode length/episode: 7.11
            Mean episode successes: 3.5850
Mean episode consecutive_successes: 7.9453
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 8.42s
                        Total time: 34393.09s
                               ETA: 996585.3s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.535s, learning 0.165s)
               Value function loss: 134176.5295
                    Surrogate loss: -0.0103
             Mean action noise std: 0.70
                       Mean reward: 2266.03
               Mean episode length: 54.37
                  Mean reward/step: 39.80
       Mean episode length/episode: 7.04
            Mean episode successes: 3.6304
Mean episode consecutive_successes: 7.9313
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 8.70s
                        Total time: 34401.79s
                               ETA: 996528.3s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.214s, learning 0.322s)
               Value function loss: 120330.6898
                    Surrogate loss: -0.0157
             Mean action noise std: 0.70
                       Mean reward: 2033.26
               Mean episode length: 54.02
                  Mean reward/step: 41.02
       Mean episode length/episode: 7.08
            Mean episode successes: 3.6724
Mean episode consecutive_successes: 7.9712
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 8.54s
                        Total time: 34410.33s
                               ETA: 996466.6s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.163s)
               Value function loss: 142718.0863
                    Surrogate loss: -0.0113
             Mean action noise std: 0.70
                       Mean reward: 1948.06
               Mean episode length: 53.51
                  Mean reward/step: 42.54
       Mean episode length/episode: 7.08
            Mean episode successes: 3.8086
Mean episode consecutive_successes: 7.9672
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 8.37s
                        Total time: 34418.70s
                               ETA: 996400.3s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.102s, learning 0.163s)
               Value function loss: 146736.5406
                    Surrogate loss: -0.0094
             Mean action noise std: 0.70
                       Mean reward: 2001.97
               Mean episode length: 52.21
                  Mean reward/step: 42.55
       Mean episode length/episode: 7.09
            Mean episode successes: 3.8940
Mean episode consecutive_successes: 7.9939
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 8.27s
                        Total time: 34426.97s
                               ETA: 996330.9s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.325s, learning 0.172s)
               Value function loss: 173819.7414
                    Surrogate loss: -0.0063
             Mean action noise std: 0.70
                       Mean reward: 2030.15
               Mean episode length: 53.78
                  Mean reward/step: 43.30
       Mean episode length/episode: 7.07
            Mean episode successes: 4.0454
Mean episode consecutive_successes: 8.0678
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.50s
                        Total time: 34435.46s
                               ETA: 996268.2s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.578s, learning 0.245s)
               Value function loss: 152465.8830
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 2362.90
               Mean episode length: 53.07
                  Mean reward/step: 45.27
       Mean episode length/episode: 7.17
            Mean episode successes: 4.0029
Mean episode consecutive_successes: 8.2482
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 8.82s
                        Total time: 34444.29s
                               ETA: 996215.0s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.397s, learning 0.339s)
               Value function loss: 157145.9320
                    Surrogate loss: -0.0093
             Mean action noise std: 0.70
                       Mean reward: 3116.77
               Mean episode length: 55.15
                  Mean reward/step: 45.65
       Mean episode length/episode: 7.02
            Mean episode successes: 4.1548
Mean episode consecutive_successes: 8.3349
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 8.74s
                        Total time: 34453.02s
                               ETA: 996159.2s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.337s, learning 0.157s)
               Value function loss: 167154.4754
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 1917.47
               Mean episode length: 52.60
                  Mean reward/step: 44.54
       Mean episode length/episode: 7.04
            Mean episode successes: 3.9023
Mean episode consecutive_successes: 8.4447
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 8.49s
                        Total time: 34461.52s
                               ETA: 996096.6s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.098s, learning 0.174s)
               Value function loss: 164344.6922
                    Surrogate loss: -0.0017
             Mean action noise std: 0.70
                       Mean reward: 2333.38
               Mean episode length: 54.26
                  Mean reward/step: 45.38
       Mean episode length/episode: 7.09
            Mean episode successes: 4.0391
Mean episode consecutive_successes: 8.4976
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 8.27s
                        Total time: 34469.79s
                               ETA: 996027.5s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.344s, learning 0.161s)
               Value function loss: 146021.4129
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 2438.88
               Mean episode length: 54.65
                  Mean reward/step: 45.83
       Mean episode length/episode: 7.10
            Mean episode successes: 4.2319
Mean episode consecutive_successes: 8.5467
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 8.51s
                        Total time: 34478.29s
                               ETA: 995965.2s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.050s, learning 0.157s)
               Value function loss: 164823.6453
                    Surrogate loss: -0.0103
             Mean action noise std: 0.70
                       Mean reward: 2696.67
               Mean episode length: 55.03
                  Mean reward/step: 45.23
       Mean episode length/episode: 7.00
            Mean episode successes: 4.0757
Mean episode consecutive_successes: 8.6734
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 8.21s
                        Total time: 34486.50s
                               ETA: 995894.3s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.063s, learning 0.164s)
               Value function loss: 208732.3195
                    Surrogate loss: -0.0008
             Mean action noise std: 0.70
                       Mean reward: 2518.69
               Mean episode length: 54.38
                  Mean reward/step: 43.76
       Mean episode length/episode: 7.00
            Mean episode successes: 3.8765
Mean episode consecutive_successes: 8.7815
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 8.23s
                        Total time: 34494.73s
                               ETA: 995824.1s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.749s, learning 0.172s)
               Value function loss: 163707.2484
                    Surrogate loss: -0.0050
             Mean action noise std: 0.70
                       Mean reward: 2195.09
               Mean episode length: 53.40
                  Mean reward/step: 43.31
       Mean episode length/episode: 7.04
            Mean episode successes: 3.9922
Mean episode consecutive_successes: 8.7056
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 7.92s
                        Total time: 34502.65s
                               ETA: 995745.0s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.188s, learning 0.159s)
               Value function loss: 145138.6584
                    Surrogate loss: -0.0071
             Mean action noise std: 0.70
                       Mean reward: 2153.63
               Mean episode length: 54.16
                  Mean reward/step: 42.94
       Mean episode length/episode: 7.15
            Mean episode successes: 3.9570
Mean episode consecutive_successes: 8.8552
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 8.35s
                        Total time: 34511.00s
                               ETA: 995678.3s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.237s, learning 0.179s)
               Value function loss: 186206.7379
                    Surrogate loss: 0.0003
             Mean action noise std: 0.70
                       Mean reward: 2386.89
               Mean episode length: 51.90
                  Mean reward/step: 44.54
       Mean episode length/episode: 7.11
            Mean episode successes: 4.1152
Mean episode consecutive_successes: 8.8120
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 8.42s
                        Total time: 34519.41s
                               ETA: 995613.6s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.204s, learning 0.180s)
               Value function loss: 164715.9793
                    Surrogate loss: 0.0046
             Mean action noise std: 0.70
                       Mean reward: 2971.98
               Mean episode length: 55.36
                  Mean reward/step: 44.15
       Mean episode length/episode: 7.06
            Mean episode successes: 4.0317
Mean episode consecutive_successes: 8.9135
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 8.38s
                        Total time: 34527.80s
                               ETA: 995548.0s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.461s, learning 0.156s)
               Value function loss: 153029.6082
                    Surrogate loss: -0.0123
             Mean action noise std: 0.70
                       Mean reward: 2884.04
               Mean episode length: 54.87
                  Mean reward/step: 42.84
       Mean episode length/episode: 7.16
            Mean episode successes: 4.0298
Mean episode consecutive_successes: 9.0162
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 8.62s
                        Total time: 34536.41s
                               ETA: 995489.2s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.164s)
               Value function loss: 156568.7555
                    Surrogate loss: -0.0167
             Mean action noise std: 0.70
                       Mean reward: 1757.47
               Mean episode length: 53.36
                  Mean reward/step: 44.61
       Mean episode length/episode: 7.03
            Mean episode successes: 4.1235
Mean episode consecutive_successes: 8.9630
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 8.37s
                        Total time: 34544.78s
                               ETA: 995423.3s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.535s, learning 0.170s)
               Value function loss: 156053.6172
                    Surrogate loss: 0.0187
             Mean action noise std: 0.70
                       Mean reward: 1910.98
               Mean episode length: 50.63
                  Mean reward/step: 43.48
       Mean episode length/episode: 7.08
            Mean episode successes: 4.0420
Mean episode consecutive_successes: 8.9933
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 8.71s
                        Total time: 34553.49s
                               ETA: 995367.1s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.157s, learning 0.167s)
               Value function loss: 151257.9574
                    Surrogate loss: -0.0139
             Mean action noise std: 0.70
                       Mean reward: 2157.74
               Mean episode length: 53.19
                  Mean reward/step: 45.66
       Mean episode length/episode: 7.00
            Mean episode successes: 4.0098
Mean episode consecutive_successes: 9.0208
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 8.32s
                        Total time: 34561.81s
                               ETA: 995299.9s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.328s, learning 0.187s)
               Value function loss: 165111.1855
                    Surrogate loss: -0.0088
             Mean action noise std: 0.70
                       Mean reward: 2866.98
               Mean episode length: 55.73
                  Mean reward/step: 46.13
       Mean episode length/episode: 7.11
            Mean episode successes: 4.0923
Mean episode consecutive_successes: 9.0910
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 8.51s
                        Total time: 34570.33s
                               ETA: 995238.3s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.283s, learning 0.185s)
               Value function loss: 217946.7055
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 2269.33
               Mean episode length: 51.86
                  Mean reward/step: 45.63
       Mean episode length/episode: 7.06
            Mean episode successes: 4.0186
Mean episode consecutive_successes: 9.1554
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 8.47s
                        Total time: 34578.80s
                               ETA: 995175.3s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.379s, learning 0.162s)
               Value function loss: 250996.1543
                    Surrogate loss: -0.0065
             Mean action noise std: 0.70
                       Mean reward: 2174.69
               Mean episode length: 52.15
                  Mean reward/step: 45.55
       Mean episode length/episode: 7.13
            Mean episode successes: 4.0664
Mean episode consecutive_successes: 9.1854
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 8.54s
                        Total time: 34587.34s
                               ETA: 995114.5s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.059s, learning 0.165s)
               Value function loss: 376106.7617
                    Surrogate loss: 0.0123
             Mean action noise std: 0.70
                       Mean reward: 2079.45
               Mean episode length: 51.85
                  Mean reward/step: 45.75
       Mean episode length/episode: 7.10
            Mean episode successes: 4.0957
Mean episode consecutive_successes: 9.1947
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 8.22s
                        Total time: 34595.56s
                               ETA: 995044.6s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.457s, learning 0.272s)
               Value function loss: 186915.2691
                    Surrogate loss: -0.0037
             Mean action noise std: 0.70
                       Mean reward: 2838.59
               Mean episode length: 57.48
                  Mean reward/step: 47.98
       Mean episode length/episode: 7.10
            Mean episode successes: 4.2422
Mean episode consecutive_successes: 9.2662
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 8.73s
                        Total time: 34604.29s
                               ETA: 994989.2s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.924s, learning 0.166s)
               Value function loss: 158224.6957
                    Surrogate loss: -0.0098
             Mean action noise std: 0.70
                       Mean reward: 2281.24
               Mean episode length: 50.26
                  Mean reward/step: 47.59
       Mean episode length/episode: 7.05
            Mean episode successes: 4.2065
Mean episode consecutive_successes: 9.2894
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 8.09s
                        Total time: 34612.38s
                               ETA: 994915.5s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.201s, learning 0.174s)
               Value function loss: 176246.7430
                    Surrogate loss: -0.0098
             Mean action noise std: 0.70
                       Mean reward: 2538.01
               Mean episode length: 52.96
                  Mean reward/step: 48.87
       Mean episode length/episode: 7.06
            Mean episode successes: 4.1470
Mean episode consecutive_successes: 9.4008
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 8.38s
                        Total time: 34620.76s
                               ETA: 994850.0s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.026s, learning 0.159s)
               Value function loss: 194722.9949
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 2315.35
               Mean episode length: 52.94
                  Mean reward/step: 47.17
       Mean episode length/episode: 7.15
            Mean episode successes: 4.3203
Mean episode consecutive_successes: 9.4234
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 8.19s
                        Total time: 34628.94s
                               ETA: 994779.2s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.197s, learning 0.194s)
               Value function loss: 188251.2477
                    Surrogate loss: -0.0040
             Mean action noise std: 0.70
                       Mean reward: 2446.71
               Mean episode length: 55.41
                  Mean reward/step: 46.36
       Mean episode length/episode: 7.02
            Mean episode successes: 4.1631
Mean episode consecutive_successes: 9.4782
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 8.39s
                        Total time: 34637.33s
                               ETA: 994714.2s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.170s, learning 0.166s)
               Value function loss: 173389.9477
                    Surrogate loss: 0.0146
             Mean action noise std: 0.70
                       Mean reward: 2720.19
               Mean episode length: 56.56
                  Mean reward/step: 45.72
       Mean episode length/episode: 7.04
            Mean episode successes: 4.1279
Mean episode consecutive_successes: 9.5033
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 8.34s
                        Total time: 34645.67s
                               ETA: 994647.7s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.006s, learning 0.167s)
               Value function loss: 175108.3598
                    Surrogate loss: -0.0078
             Mean action noise std: 0.70
                       Mean reward: 2780.50
               Mean episode length: 56.41
                  Mean reward/step: 47.21
       Mean episode length/episode: 7.13
            Mean episode successes: 4.1704
Mean episode consecutive_successes: 9.5537
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 8.17s
                        Total time: 34653.84s
                               ETA: 994576.6s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.179s, learning 0.163s)
               Value function loss: 174942.6078
                    Surrogate loss: -0.0135
             Mean action noise std: 0.70
                       Mean reward: 2814.70
               Mean episode length: 55.44
                  Mean reward/step: 45.35
       Mean episode length/episode: 7.07
            Mean episode successes: 3.9243
Mean episode consecutive_successes: 9.6630
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 8.34s
                        Total time: 34662.18s
                               ETA: 994510.3s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.157s, learning 0.189s)
               Value function loss: 182749.4699
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 2314.15
               Mean episode length: 50.97
                  Mean reward/step: 46.57
       Mean episode length/episode: 7.04
            Mean episode successes: 4.0435
Mean episode consecutive_successes: 9.5859
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 8.35s
                        Total time: 34670.53s
                               ETA: 994444.3s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.008s, learning 0.157s)
               Value function loss: 163333.9312
                    Surrogate loss: -0.0131
             Mean action noise std: 0.70
                       Mean reward: 1913.66
               Mean episode length: 50.40
                  Mean reward/step: 47.60
       Mean episode length/episode: 7.19
            Mean episode successes: 4.3276
Mean episode consecutive_successes: 9.5449
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 8.17s
                        Total time: 34678.70s
                               ETA: 994373.0s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.263s, learning 0.161s)
               Value function loss: 213995.7508
                    Surrogate loss: -0.0102
             Mean action noise std: 0.70
                       Mean reward: 2571.37
               Mean episode length: 54.54
                  Mean reward/step: 48.78
       Mean episode length/episode: 7.05
            Mean episode successes: 4.3555
Mean episode consecutive_successes: 9.5884
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 8.42s
                        Total time: 34687.12s
                               ETA: 994309.2s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.255s, learning 0.195s)
               Value function loss: 383543.0695
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 3038.50
               Mean episode length: 53.23
                  Mean reward/step: 47.55
       Mean episode length/episode: 7.01
            Mean episode successes: 4.1465
Mean episode consecutive_successes: 9.6906
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 8.45s
                        Total time: 34695.57s
                               ETA: 994246.2s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.940s, learning 0.168s)
               Value function loss: 241909.4516
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 2641.42
               Mean episode length: 54.71
                  Mean reward/step: 46.72
       Mean episode length/episode: 7.04
            Mean episode successes: 4.1357
Mean episode consecutive_successes: 9.7214
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 8.11s
                        Total time: 34703.68s
                               ETA: 994173.5s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.446s, learning 0.165s)
               Value function loss: 175686.2504
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 2863.78
               Mean episode length: 54.30
                  Mean reward/step: 46.92
       Mean episode length/episode: 7.13
            Mean episode successes: 4.1211
Mean episode consecutive_successes: 9.7757
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 8.61s
                        Total time: 34712.29s
                               ETA: 994115.2s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.019s, learning 0.228s)
               Value function loss: 156711.3820
                    Surrogate loss: -0.0106
             Mean action noise std: 0.70
                       Mean reward: 2590.44
               Mean episode length: 53.36
                  Mean reward/step: 44.12
       Mean episode length/episode: 7.07
            Mean episode successes: 3.9766
Mean episode consecutive_successes: 9.7788
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 8.25s
                        Total time: 34720.54s
                               ETA: 994046.4s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.301s, learning 0.265s)
               Value function loss: 169272.1711
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 2119.07
               Mean episode length: 55.03
                  Mean reward/step: 44.26
       Mean episode length/episode: 7.09
            Mean episode successes: 3.9985
Mean episode consecutive_successes: 9.7730
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 8.57s
                        Total time: 34729.10s
                               ETA: 993986.9s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.275s, learning 0.280s)
               Value function loss: 159826.8125
                    Surrogate loss: 0.0049
             Mean action noise std: 0.70
                       Mean reward: 2633.54
               Mean episode length: 54.22
                  Mean reward/step: 44.80
       Mean episode length/episode: 7.02
            Mean episode successes: 3.8848
Mean episode consecutive_successes: 9.7866
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 8.55s
                        Total time: 34737.66s
                               ETA: 993927.0s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.085s, learning 0.186s)
               Value function loss: 145325.0434
                    Surrogate loss: -0.0094
             Mean action noise std: 0.70
                       Mean reward: 3132.29
               Mean episode length: 56.16
                  Mean reward/step: 42.09
       Mean episode length/episode: 7.11
            Mean episode successes: 3.8516
Mean episode consecutive_successes: 9.7956
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 8.27s
                        Total time: 34745.93s
                               ETA: 993859.1s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.379s, learning 0.167s)
               Value function loss: 154335.0680
                    Surrogate loss: -0.0068
             Mean action noise std: 0.70
                       Mean reward: 2201.65
               Mean episode length: 55.52
                  Mean reward/step: 42.03
       Mean episode length/episode: 7.09
            Mean episode successes: 3.8706
Mean episode consecutive_successes: 9.6939
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 8.55s
                        Total time: 34754.48s
                               ETA: 993799.0s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.039s, learning 0.166s)
               Value function loss: 199938.9410
                    Surrogate loss: -0.0043
             Mean action noise std: 0.70
                       Mean reward: 2592.58
               Mean episode length: 56.28
                  Mean reward/step: 44.03
       Mean episode length/episode: 7.14
            Mean episode successes: 3.9526
Mean episode consecutive_successes: 9.6817
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 8.21s
                        Total time: 34762.68s
                               ETA: 993729.3s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.161s)
               Value function loss: 286962.2570
                    Surrogate loss: 0.0142
             Mean action noise std: 0.70
                       Mean reward: 1992.30
               Mean episode length: 56.05
                  Mean reward/step: 44.79
       Mean episode length/episode: 7.02
            Mean episode successes: 3.8438
Mean episode consecutive_successes: 9.6393
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 8.43s
                        Total time: 34771.11s
                               ETA: 993665.9s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.311s, learning 0.181s)
               Value function loss: 165193.5859
                    Surrogate loss: -0.0089
             Mean action noise std: 0.70
                       Mean reward: 2650.69
               Mean episode length: 53.90
                  Mean reward/step: 44.58
       Mean episode length/episode: 7.08
            Mean episode successes: 3.7974
Mean episode consecutive_successes: 9.7039
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 8.49s
                        Total time: 34779.60s
                               ETA: 993604.4s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.164s)
               Value function loss: 142255.5773
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 2605.88
               Mean episode length: 54.09
                  Mean reward/step: 44.31
       Mean episode length/episode: 7.15
            Mean episode successes: 4.0596
Mean episode consecutive_successes: 9.6109
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 8.36s
                        Total time: 34787.96s
                               ETA: 993539.1s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.138s, learning 0.191s)
               Value function loss: 149719.6316
                    Surrogate loss: 0.0020
             Mean action noise std: 0.70
                       Mean reward: 2545.77
               Mean episode length: 53.42
                  Mean reward/step: 43.05
       Mean episode length/episode: 7.01
            Mean episode successes: 3.9170
Mean episode consecutive_successes: 9.5969
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 8.33s
                        Total time: 34796.29s
                               ETA: 993473.1s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.214s, learning 0.187s)
               Value function loss: 143354.0957
                    Surrogate loss: -0.0083
             Mean action noise std: 0.70
                       Mean reward: 1919.52
               Mean episode length: 52.05
                  Mean reward/step: 42.57
       Mean episode length/episode: 7.09
            Mean episode successes: 3.8813
Mean episode consecutive_successes: 9.5182
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 8.40s
                        Total time: 34804.69s
                               ETA: 993409.1s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.982s, learning 0.191s)
               Value function loss: 140792.2281
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 1970.68
               Mean episode length: 53.90
                  Mean reward/step: 39.73
       Mean episode length/episode: 6.95
            Mean episode successes: 3.4883
Mean episode consecutive_successes: 9.4907
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 8.17s
                        Total time: 34812.86s
                               ETA: 993338.6s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.238s, learning 0.158s)
               Value function loss: 138383.1645
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 2115.43
               Mean episode length: 53.69
                  Mean reward/step: 41.62
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7930
Mean episode consecutive_successes: 9.3502
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 8.40s
                        Total time: 34821.26s
                               ETA: 993274.6s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.729s, learning 0.162s)
               Value function loss: 145123.5824
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 2021.42
               Mean episode length: 51.02
                  Mean reward/step: 38.67
       Mean episode length/episode: 7.00
            Mean episode successes: 3.5254
Mean episode consecutive_successes: 9.3470
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 8.89s
                        Total time: 34830.15s
                               ETA: 993224.7s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.162s)
               Value function loss: 232585.6480
                    Surrogate loss: -0.0070
             Mean action noise std: 0.70
                       Mean reward: 1922.88
               Mean episode length: 49.14
                  Mean reward/step: 38.83
       Mean episode length/episode: 7.16
            Mean episode successes: 3.5020
Mean episode consecutive_successes: 9.2781
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 8.36s
                        Total time: 34838.51s
                               ETA: 993159.6s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.128s, learning 0.163s)
               Value function loss: 184319.2770
                    Surrogate loss: -0.0049
             Mean action noise std: 0.70
                       Mean reward: 2214.82
               Mean episode length: 52.91
                  Mean reward/step: 40.30
       Mean episode length/episode: 7.07
            Mean episode successes: 3.4111
Mean episode consecutive_successes: 9.2818
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 8.29s
                        Total time: 34846.80s
                               ETA: 993092.7s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.359s, learning 0.163s)
               Value function loss: 156045.7199
                    Surrogate loss: 0.0968
             Mean action noise std: 0.70
                       Mean reward: 2164.21
               Mean episode length: 51.73
                  Mean reward/step: 42.61
       Mean episode length/episode: 7.12
            Mean episode successes: 3.6709
Mean episode consecutive_successes: 9.1530
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 8.52s
                        Total time: 34855.32s
                               ETA: 993032.3s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.116s, learning 0.164s)
               Value function loss: 133151.4039
                    Surrogate loss: 0.0060
             Mean action noise std: 0.70
                       Mean reward: 1579.63
               Mean episode length: 52.34
                  Mean reward/step: 44.02
       Mean episode length/episode: 7.00
            Mean episode successes: 3.7554
Mean episode consecutive_successes: 9.0380
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 8.28s
                        Total time: 34863.60s
                               ETA: 992965.1s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.472s, learning 0.158s)
               Value function loss: 135908.2061
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 1818.93
               Mean episode length: 51.84
                  Mean reward/step: 43.93
       Mean episode length/episode: 7.05
            Mean episode successes: 4.1040
Mean episode consecutive_successes: 8.8978
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 8.63s
                        Total time: 34872.23s
                               ETA: 992907.9s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.957s, learning 0.172s)
               Value function loss: 149399.2168
                    Surrogate loss: -0.0074
             Mean action noise std: 0.70
                       Mean reward: 2374.72
               Mean episode length: 55.36
                  Mean reward/step: 43.52
       Mean episode length/episode: 7.06
            Mean episode successes: 3.9214
Mean episode consecutive_successes: 8.9816
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 8.13s
                        Total time: 34880.36s
                               ETA: 992836.5s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.061s, learning 0.196s)
               Value function loss: 200632.3875
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 2331.31
               Mean episode length: 54.54
                  Mean reward/step: 43.98
       Mean episode length/episode: 7.05
            Mean episode successes: 3.8423
Mean episode consecutive_successes: 9.0005
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 8.26s
                        Total time: 34888.62s
                               ETA: 992768.7s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.464s, learning 0.164s)
               Value function loss: 184809.7008
                    Surrogate loss: 0.0034
             Mean action noise std: 0.70
                       Mean reward: 2192.49
               Mean episode length: 53.27
                  Mean reward/step: 44.77
       Mean episode length/episode: 7.08
            Mean episode successes: 3.9683
Mean episode consecutive_successes: 9.0222
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 8.63s
                        Total time: 34897.25s
                               ETA: 992711.6s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.352s, learning 0.157s)
               Value function loss: 159013.4109
                    Surrogate loss: -0.0086
             Mean action noise std: 0.70
                       Mean reward: 2961.00
               Mean episode length: 53.93
                  Mean reward/step: 44.39
       Mean episode length/episode: 7.09
            Mean episode successes: 3.9102
Mean episode consecutive_successes: 9.0962
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 8.51s
                        Total time: 34905.76s
                               ETA: 992651.0s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.292s, learning 0.270s)
               Value function loss: 158144.8859
                    Surrogate loss: 0.0009
             Mean action noise std: 0.70
                       Mean reward: 2730.51
               Mean episode length: 53.26
                  Mean reward/step: 42.83
       Mean episode length/episode: 7.12
            Mean episode successes: 3.7202
Mean episode consecutive_successes: 9.1969
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 8.56s
                        Total time: 34914.32s
                               ETA: 992592.1s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 1995 steps/s (collection: 7.964s, learning 0.246s)
               Value function loss: 152555.3598
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 2138.47
               Mean episode length: 50.25
                  Mean reward/step: 42.10
       Mean episode length/episode: 6.99
            Mean episode successes: 3.6572
Mean episode consecutive_successes: 9.1550
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 8.21s
                        Total time: 34922.53s
                               ETA: 992523.1s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.035s, learning 0.161s)
               Value function loss: 144375.4504
                    Surrogate loss: -0.0108
             Mean action noise std: 0.70
                       Mean reward: 2089.00
               Mean episode length: 50.85
                  Mean reward/step: 43.16
       Mean episode length/episode: 7.06
            Mean episode successes: 3.8198
Mean episode consecutive_successes: 9.0658
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 8.20s
                        Total time: 34930.72s
                               ETA: 992453.8s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.044s, learning 0.158s)
               Value function loss: 167000.7812
                    Surrogate loss: -0.0035
             Mean action noise std: 0.70
                       Mean reward: 1947.11
               Mean episode length: 51.79
                  Mean reward/step: 43.77
       Mean episode length/episode: 7.00
            Mean episode successes: 3.7842
Mean episode consecutive_successes: 9.0571
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 8.20s
                        Total time: 34938.93s
                               ETA: 992384.6s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.130s, learning 0.165s)
               Value function loss: 159505.7348
                    Surrogate loss: -0.0080
             Mean action noise std: 0.70
                       Mean reward: 2062.04
               Mean episode length: 52.02
                  Mean reward/step: 43.62
       Mean episode length/episode: 7.09
            Mean episode successes: 3.7490
Mean episode consecutive_successes: 9.1037
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 8.29s
                        Total time: 34947.22s
                               ETA: 992318.2s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.796s, learning 0.167s)
               Value function loss: 173679.0758
                    Surrogate loss: -0.0114
             Mean action noise std: 0.70
                       Mean reward: 2504.32
               Mean episode length: 50.94
                  Mean reward/step: 42.41
       Mean episode length/episode: 7.05
            Mean episode successes: 3.6558
Mean episode consecutive_successes: 9.1418
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 7.96s
                        Total time: 34955.18s
                               ETA: 992242.4s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.120s, learning 0.161s)
               Value function loss: 172184.0566
                    Surrogate loss: -0.0113
             Mean action noise std: 0.70
                       Mean reward: 2174.01
               Mean episode length: 51.24
                  Mean reward/step: 42.62
       Mean episode length/episode: 7.09
            Mean episode successes: 3.6865
Mean episode consecutive_successes: 9.1064
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 8.28s
                        Total time: 34963.46s
                               ETA: 992175.6s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.809s, learning 0.168s)
               Value function loss: 183011.9969
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2571.42
               Mean episode length: 54.71
                  Mean reward/step: 42.81
       Mean episode length/episode: 7.03
            Mean episode successes: 3.6499
Mean episode consecutive_successes: 9.0830
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 7.98s
                        Total time: 34971.44s
                               ETA: 992100.2s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.997s, learning 0.169s)
               Value function loss: 151820.5414
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 2351.34
               Mean episode length: 54.76
                  Mean reward/step: 43.87
       Mean episode length/episode: 7.07
            Mean episode successes: 3.8872
Mean episode consecutive_successes: 9.0190
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 8.17s
                        Total time: 34979.61s
                               ETA: 992030.2s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.108s, learning 0.185s)
               Value function loss: 224634.9637
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 2309.06
               Mean episode length: 50.78
                  Mean reward/step: 45.70
       Mean episode length/episode: 7.15
            Mean episode successes: 4.0825
Mean episode consecutive_successes: 9.0151
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 8.29s
                        Total time: 34987.90s
                               ETA: 991963.9s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.489s, learning 0.169s)
               Value function loss: 201129.4660
                    Surrogate loss: -0.0112
             Mean action noise std: 0.70
                       Mean reward: 2533.67
               Mean episode length: 54.33
                  Mean reward/step: 45.76
       Mean episode length/episode: 7.12
            Mean episode successes: 4.1890
Mean episode consecutive_successes: 9.0537
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 8.66s
                        Total time: 34996.56s
                               ETA: 991908.0s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.368s, learning 0.164s)
               Value function loss: 151610.6566
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 2272.13
               Mean episode length: 52.24
                  Mean reward/step: 47.66
       Mean episode length/episode: 7.06
            Mean episode successes: 4.2827
Mean episode consecutive_successes: 9.0570
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 8.53s
                        Total time: 35005.09s
                               ETA: 991848.5s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.099s, learning 0.186s)
               Value function loss: 159855.5461
                    Surrogate loss: -0.0098
             Mean action noise std: 0.70
                       Mean reward: 2163.18
               Mean episode length: 54.21
                  Mean reward/step: 46.10
       Mean episode length/episode: 7.04
            Mean episode successes: 4.1348
Mean episode consecutive_successes: 9.1031
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 8.28s
                        Total time: 35013.37s
                               ETA: 991782.0s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.967s, learning 0.172s)
               Value function loss: 140821.5086
                    Surrogate loss: -0.0062
             Mean action noise std: 0.70
                       Mean reward: 2365.70
               Mean episode length: 53.45
                  Mean reward/step: 45.43
       Mean episode length/episode: 7.13
            Mean episode successes: 4.1328
Mean episode consecutive_successes: 9.1611
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 8.14s
                        Total time: 35021.51s
                               ETA: 991711.5s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.966s, learning 0.165s)
               Value function loss: 148062.2988
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2738.81
               Mean episode length: 54.54
                  Mean reward/step: 45.52
       Mean episode length/episode: 7.00
            Mean episode successes: 3.9233
Mean episode consecutive_successes: 9.2533
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 8.13s
                        Total time: 35029.64s
                               ETA: 991640.7s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.007s, learning 0.189s)
               Value function loss: 146929.1234
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 2412.34
               Mean episode length: 52.71
                  Mean reward/step: 46.40
       Mean episode length/episode: 7.04
            Mean episode successes: 4.0854
Mean episode consecutive_successes: 9.2141
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 8.20s
                        Total time: 35037.84s
                               ETA: 991571.9s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.901s, learning 0.207s)
               Value function loss: 157624.2348
                    Surrogate loss: -0.0110
             Mean action noise std: 0.70
                       Mean reward: 2559.81
               Mean episode length: 52.21
                  Mean reward/step: 45.72
       Mean episode length/episode: 7.05
            Mean episode successes: 3.9766
Mean episode consecutive_successes: 9.2712
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 8.11s
                        Total time: 35045.95s
                               ETA: 991500.5s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.151s, learning 0.161s)
               Value function loss: 162250.1566
                    Surrogate loss: -0.0073
             Mean action noise std: 0.70
                       Mean reward: 2441.45
               Mean episode length: 51.17
                  Mean reward/step: 46.14
       Mean episode length/episode: 7.12
            Mean episode successes: 4.2603
Mean episode consecutive_successes: 9.1771
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 8.31s
                        Total time: 35054.26s
                               ETA: 991435.0s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.055s, learning 0.192s)
               Value function loss: 191303.7836
                    Surrogate loss: -0.0149
             Mean action noise std: 0.70
                       Mean reward: 1987.55
               Mean episode length: 48.84
                  Mean reward/step: 46.52
       Mean episode length/episode: 7.01
            Mean episode successes: 4.1890
Mean episode consecutive_successes: 9.1707
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 8.25s
                        Total time: 35062.51s
                               ETA: 991367.7s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.053s, learning 0.171s)
               Value function loss: 288624.1312
                    Surrogate loss: -0.0096
             Mean action noise std: 0.70
                       Mean reward: 2264.45
               Mean episode length: 51.85
                  Mean reward/step: 46.86
       Mean episode length/episode: 7.08
            Mean episode successes: 4.3872
Mean episode consecutive_successes: 9.1526
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 8.22s
                        Total time: 35070.73s
                               ETA: 991299.8s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.230s, learning 0.162s)
               Value function loss: 160712.6559
                    Surrogate loss: -0.0018
             Mean action noise std: 0.70
                       Mean reward: 2308.14
               Mean episode length: 49.83
                  Mean reward/step: 45.28
       Mean episode length/episode: 7.00
            Mean episode successes: 4.2144
Mean episode consecutive_successes: 9.2313
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 8.39s
                        Total time: 35079.12s
                               ETA: 991236.6s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.175s, learning 0.166s)
               Value function loss: 137462.7125
                    Surrogate loss: 0.0072
             Mean action noise std: 0.70
                       Mean reward: 2153.78
               Mean episode length: 50.92
                  Mean reward/step: 44.09
       Mean episode length/episode: 7.08
            Mean episode successes: 4.1064
Mean episode consecutive_successes: 9.3044
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 8.34s
                        Total time: 35087.46s
                               ETA: 991172.1s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.222s, learning 0.172s)
               Value function loss: 130284.4498
                    Surrogate loss: 0.0069
             Mean action noise std: 0.70
                       Mean reward: 2130.73
               Mean episode length: 50.41
                  Mean reward/step: 43.34
       Mean episode length/episode: 7.08
            Mean episode successes: 4.1240
Mean episode consecutive_successes: 9.2652
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 8.39s
                        Total time: 35095.86s
                               ETA: 991109.0s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.933s, learning 0.214s)
               Value function loss: 139975.3953
                    Surrogate loss: -0.0149
             Mean action noise std: 0.70
                       Mean reward: 2630.17
               Mean episode length: 52.89
                  Mean reward/step: 42.41
       Mean episode length/episode: 7.02
            Mean episode successes: 4.0181
Mean episode consecutive_successes: 9.2909
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 8.15s
                        Total time: 35104.00s
                               ETA: 991039.1s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.067s, learning 0.262s)
               Value function loss: 139247.2631
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 2171.69
               Mean episode length: 51.08
                  Mean reward/step: 42.38
       Mean episode length/episode: 7.05
            Mean episode successes: 3.9492
Mean episode consecutive_successes: 9.2252
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 8.33s
                        Total time: 35112.33s
                               ETA: 990974.2s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.444s, learning 0.162s)
               Value function loss: 131343.6875
                    Surrogate loss: -0.0069
             Mean action noise std: 0.70
                       Mean reward: 2202.61
               Mean episode length: 53.18
                  Mean reward/step: 39.29
       Mean episode length/episode: 7.10
            Mean episode successes: 3.8740
Mean episode consecutive_successes: 9.2237
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 8.61s
                        Total time: 35120.94s
                               ETA: 990917.3s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.937s, learning 0.175s)
               Value function loss: 143672.9586
                    Surrogate loss: -0.0106
             Mean action noise std: 0.70
                       Mean reward: 1608.94
               Mean episode length: 51.57
                  Mean reward/step: 39.27
       Mean episode length/episode: 7.05
            Mean episode successes: 3.8633
Mean episode consecutive_successes: 9.1101
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 8.11s
                        Total time: 35129.05s
                               ETA: 990846.4s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.314s, learning 0.248s)
               Value function loss: 134243.7924
                    Surrogate loss: -0.0094
             Mean action noise std: 0.70
                       Mean reward: 2355.27
               Mean episode length: 53.22
                  Mean reward/step: 39.02
       Mean episode length/episode: 7.03
            Mean episode successes: 3.7148
Mean episode consecutive_successes: 9.0875
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 8.56s
                        Total time: 35137.61s
                               ETA: 990788.3s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.075s, learning 0.300s)
               Value function loss: 137612.6916
                    Surrogate loss: -0.0152
             Mean action noise std: 0.70
                       Mean reward: 1713.60
               Mean episode length: 50.76
                  Mean reward/step: 38.51
       Mean episode length/episode: 7.10
            Mean episode successes: 3.7812
Mean episode consecutive_successes: 8.9718
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 8.37s
                        Total time: 35145.98s
                               ETA: 990724.9s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.456s, learning 0.160s)
               Value function loss: 145644.3529
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 2150.25
               Mean episode length: 53.43
                  Mean reward/step: 41.32
       Mean episode length/episode: 7.11
            Mean episode successes: 3.9722
Mean episode consecutive_successes: 8.9117
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 11.62s
                        Total time: 35157.60s
                               ETA: 990752.9s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.894s, learning 0.162s)
               Value function loss: 148173.6027
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 1929.47
               Mean episode length: 51.93
                  Mean reward/step: 41.75
       Mean episode length/episode: 7.10
            Mean episode successes: 4.1440
Mean episode consecutive_successes: 8.8345
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 16.06s
                        Total time: 35173.66s
                               ETA: 990905.9s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.988s, learning 0.167s)
               Value function loss: 137937.8471
                    Surrogate loss: -0.0137
             Mean action noise std: 0.70
                       Mean reward: 2415.02
               Mean episode length: 52.71
                  Mean reward/step: 40.74
       Mean episode length/episode: 6.97
            Mean episode successes: 3.9360
Mean episode consecutive_successes: 8.8488
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 16.15s
                        Total time: 35189.81s
                               ETA: 991061.7s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.945s, learning 0.169s)
               Value function loss: 154550.6939
                    Surrogate loss: -0.0096
             Mean action noise std: 0.70
                       Mean reward: 2849.64
               Mean episode length: 56.06
                  Mean reward/step: 42.60
       Mean episode length/episode: 7.06
            Mean episode successes: 3.9561
Mean episode consecutive_successes: 8.8974
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 16.11s
                        Total time: 35205.93s
                               ETA: 991216.2s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.984s, learning 0.164s)
               Value function loss: 152673.6242
                    Surrogate loss: -0.0160
             Mean action noise std: 0.70
                       Mean reward: 2059.13
               Mean episode length: 54.46
                  Mean reward/step: 42.36
       Mean episode length/episode: 7.13
            Mean episode successes: 3.9370
Mean episode consecutive_successes: 8.9120
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 16.15s
                        Total time: 35222.07s
                               ETA: 991371.5s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.521s, learning 0.167s)
               Value function loss: 195754.2191
                    Surrogate loss: -0.0165
             Mean action noise std: 0.70
                       Mean reward: 2244.17
               Mean episode length: 55.08
                  Mean reward/step: 43.91
       Mean episode length/episode: 7.15
            Mean episode successes: 4.1196
Mean episode consecutive_successes: 8.8730
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 15.69s
                        Total time: 35237.76s
                               ETA: 991513.8s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.963s, learning 0.161s)
               Value function loss: 239384.0500
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 2380.06
               Mean episode length: 51.85
                  Mean reward/step: 45.29
       Mean episode length/episode: 7.03
            Mean episode successes: 4.1289
Mean episode consecutive_successes: 8.9049
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 16.12s
                        Total time: 35253.89s
                               ETA: 991668.3s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.119s, learning 0.168s)
               Value function loss: 141221.7188
                    Surrogate loss: 0.0061
             Mean action noise std: 0.70
                       Mean reward: 2472.69
               Mean episode length: 53.13
                  Mean reward/step: 47.11
       Mean episode length/episode: 7.07
            Mean episode successes: 4.1333
Mean episode consecutive_successes: 8.9722
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 16.29s
                        Total time: 35270.17s
                               ETA: 991827.2s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.929s, learning 0.162s)
               Value function loss: 132797.7768
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 1915.13
               Mean episode length: 53.38
                  Mean reward/step: 45.12
       Mean episode length/episode: 7.07
            Mean episode successes: 4.2598
Mean episode consecutive_successes: 8.9633
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 16.09s
                        Total time: 35286.26s
                               ETA: 991980.6s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.791s, learning 0.160s)
               Value function loss: 137665.4506
                    Surrogate loss: -0.0153
             Mean action noise std: 0.70
                       Mean reward: 2318.75
               Mean episode length: 54.71
                  Mean reward/step: 47.24
       Mean episode length/episode: 7.01
            Mean episode successes: 4.3809
Mean episode consecutive_successes: 8.9877
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 15.95s
                        Total time: 35302.21s
                               ETA: 992129.9s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1011 steps/s (collection: 15.966s, learning 0.239s)
               Value function loss: 146738.2516
                    Surrogate loss: -0.0092
             Mean action noise std: 0.70
                       Mean reward: 2415.90
               Mean episode length: 53.93
                  Mean reward/step: 46.05
       Mean episode length/episode: 7.05
            Mean episode successes: 4.3101
Mean episode consecutive_successes: 9.0540
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 16.21s
                        Total time: 35318.42s
                               ETA: 992286.3s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.038s, learning 0.206s)
               Value function loss: 148315.4098
                    Surrogate loss: 0.0011
             Mean action noise std: 0.70
                       Mean reward: 3121.28
               Mean episode length: 54.04
                  Mean reward/step: 45.05
       Mean episode length/episode: 7.10
            Mean episode successes: 4.2393
Mean episode consecutive_successes: 9.1725
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 16.24s
                        Total time: 35334.66s
                               ETA: 992443.6s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.407s, learning 0.184s)
               Value function loss: 157166.0184
                    Surrogate loss: -0.0114
             Mean action noise std: 0.70
                       Mean reward: 2781.05
               Mean episode length: 53.82
                  Mean reward/step: 43.67
       Mean episode length/episode: 7.07
            Mean episode successes: 3.9189
Mean episode consecutive_successes: 9.3335
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 16.59s
                        Total time: 35351.25s
                               ETA: 992610.6s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.817s, learning 0.164s)
               Value function loss: 198519.4941
                    Surrogate loss: -0.0047
             Mean action noise std: 0.70
                       Mean reward: 2353.13
               Mean episode length: 50.14
                  Mean reward/step: 44.82
       Mean episode length/episode: 7.08
            Mean episode successes: 3.9507
Mean episode consecutive_successes: 9.2970
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 15.98s
                        Total time: 35367.23s
                               ETA: 992760.3s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.358s, learning 0.166s)
               Value function loss: 155112.3391
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 2049.18
               Mean episode length: 51.37
                  Mean reward/step: 45.55
       Mean episode length/episode: 7.14
            Mean episode successes: 4.2832
Mean episode consecutive_successes: 9.2104
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 15.52s
                        Total time: 35382.76s
                               ETA: 992897.2s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.328s, learning 0.175s)
               Value function loss: 201897.4863
                    Surrogate loss: -0.0129
             Mean action noise std: 0.70
                       Mean reward: 2611.88
               Mean episode length: 55.75
                  Mean reward/step: 45.11
       Mean episode length/episode: 7.06
            Mean episode successes: 4.2280
Mean episode consecutive_successes: 9.2428
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 16.50s
                        Total time: 35399.26s
                               ETA: 993061.4s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.677s, learning 0.164s)
               Value function loss: 244509.4574
                    Surrogate loss: -0.0149
             Mean action noise std: 0.70
                       Mean reward: 2575.09
               Mean episode length: 56.66
                  Mean reward/step: 43.77
       Mean episode length/episode: 6.99
            Mean episode successes: 4.0293
Mean episode consecutive_successes: 9.2318
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 15.84s
                        Total time: 35415.10s
                               ETA: 993206.9s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.995s, learning 0.163s)
               Value function loss: 178822.9531
                    Surrogate loss: -0.0072
             Mean action noise std: 0.70
                       Mean reward: 2408.00
               Mean episode length: 53.15
                  Mean reward/step: 44.08
       Mean episode length/episode: 7.09
            Mean episode successes: 3.9688
Mean episode consecutive_successes: 9.3096
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 16.16s
                        Total time: 35431.26s
                               ETA: 993361.3s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.801s, learning 0.158s)
               Value function loss: 158257.4293
                    Surrogate loss: -0.0104
             Mean action noise std: 0.70
                       Mean reward: 2491.53
               Mean episode length: 55.03
                  Mean reward/step: 43.56
       Mean episode length/episode: 7.17
            Mean episode successes: 3.9541
Mean episode consecutive_successes: 9.3969
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 15.96s
                        Total time: 35447.22s
                               ETA: 993509.9s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.823s, learning 0.290s)
               Value function loss: 151549.6871
                    Surrogate loss: -0.0149
             Mean action noise std: 0.70
                       Mean reward: 2410.30
               Mean episode length: 52.81
                  Mean reward/step: 44.97
       Mean episode length/episode: 7.01
            Mean episode successes: 3.8784
Mean episode consecutive_successes: 9.3960
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 16.11s
                        Total time: 35463.33s
                               ETA: 993662.8s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1007 steps/s (collection: 15.954s, learning 0.315s)
               Value function loss: 174719.4949
                    Surrogate loss: -0.0113
             Mean action noise std: 0.70
                       Mean reward: 2088.49
               Mean episode length: 54.19
                  Mean reward/step: 44.33
       Mean episode length/episode: 7.04
            Mean episode successes: 3.8906
Mean episode consecutive_successes: 9.3540
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 16.27s
                        Total time: 35479.60s
                               ETA: 993820.0s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.098s, learning 0.236s)
               Value function loss: 206016.2316
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 2075.08
               Mean episode length: 53.05
                  Mean reward/step: 44.31
       Mean episode length/episode: 7.13
            Mean episode successes: 4.1289
Mean episode consecutive_successes: 9.2524
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 16.33s
                        Total time: 35495.94s
                               ETA: 993978.8s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.307s, learning 0.180s)
               Value function loss: 171976.5867
                    Surrogate loss: -0.0109
             Mean action noise std: 0.70
                       Mean reward: 2463.32
               Mean episode length: 53.48
                  Mean reward/step: 47.08
       Mean episode length/episode: 7.10
            Mean episode successes: 4.3716
Mean episode consecutive_successes: 9.2552
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 16.49s
                        Total time: 35512.42s
                               ETA: 994141.9s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.237s, learning 0.256s)
               Value function loss: 190800.6008
                    Surrogate loss: -0.0141
             Mean action noise std: 0.70
                       Mean reward: 2723.46
               Mean episode length: 53.63
                  Mean reward/step: 47.60
       Mean episode length/episode: 7.10
            Mean episode successes: 4.3667
Mean episode consecutive_successes: 9.3740
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 16.49s
                        Total time: 35528.92s
                               ETA: 994305.0s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.921s, learning 0.199s)
               Value function loss: 194257.4227
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 2474.47
               Mean episode length: 55.37
                  Mean reward/step: 46.23
       Mean episode length/episode: 7.13
            Mean episode successes: 4.2700
Mean episode consecutive_successes: 9.4294
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 16.12s
                        Total time: 35545.04s
                               ETA: 994457.6s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.913s, learning 0.181s)
               Value function loss: 330889.7953
                    Surrogate loss: -0.0094
             Mean action noise std: 0.70
                       Mean reward: 2809.10
               Mean episode length: 55.11
                  Mean reward/step: 46.19
       Mean episode length/episode: 7.00
            Mean episode successes: 3.9946
Mean episode consecutive_successes: 9.4958
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 16.09s
                        Total time: 35561.13s
                               ETA: 994609.4s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.666s, learning 0.157s)
               Value function loss: 204053.8000
                    Surrogate loss: 0.0083
             Mean action noise std: 0.70
                       Mean reward: 2023.70
               Mean episode length: 50.85
                  Mean reward/step: 45.39
       Mean episode length/episode: 7.13
            Mean episode successes: 4.1250
Mean episode consecutive_successes: 9.4861
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 15.82s
                        Total time: 35576.95s
                               ETA: 994753.5s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.738s, learning 0.335s)
               Value function loss: 168113.9371
                    Surrogate loss: -0.0148
             Mean action noise std: 0.70
                       Mean reward: 2287.39
               Mean episode length: 51.99
                  Mean reward/step: 45.63
       Mean episode length/episode: 7.04
            Mean episode successes: 4.2764
Mean episode consecutive_successes: 9.4593
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 16.07s
                        Total time: 35593.03s
                               ETA: 994904.4s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.434s, learning 0.159s)
               Value function loss: 164083.8551
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 2856.36
               Mean episode length: 54.03
                  Mean reward/step: 44.98
       Mean episode length/episode: 7.06
            Mean episode successes: 4.1128
Mean episode consecutive_successes: 9.5738
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 16.59s
                        Total time: 35609.62s
                               ETA: 995069.8s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.922s, learning 0.157s)
               Value function loss: 154948.5832
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 2692.29
               Mean episode length: 55.73
                  Mean reward/step: 44.04
       Mean episode length/episode: 7.04
            Mean episode successes: 3.9585
Mean episode consecutive_successes: 9.6029
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 16.08s
                        Total time: 35625.70s
                               ETA: 995220.8s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.025s, learning 0.196s)
               Value function loss: 148983.0479
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 2171.65
               Mean episode length: 54.91
                  Mean reward/step: 43.31
       Mean episode length/episode: 7.12
            Mean episode successes: 4.0044
Mean episode consecutive_successes: 9.5514
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 16.22s
                        Total time: 35641.92s
                               ETA: 995375.6s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.643s, learning 0.211s)
               Value function loss: 159888.6758
                    Surrogate loss: 0.0066
             Mean action noise std: 0.70
                       Mean reward: 2513.53
               Mean episode length: 53.76
                  Mean reward/step: 45.42
       Mean episode length/episode: 7.14
            Mean episode successes: 4.0342
Mean episode consecutive_successes: 9.6051
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 15.85s
                        Total time: 35657.77s
                               ETA: 995520.1s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.135s, learning 0.161s)
               Value function loss: 156777.7188
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 2314.89
               Mean episode length: 52.87
                  Mean reward/step: 47.13
       Mean episode length/episode: 7.14
            Mean episode successes: 4.1812
Mean episode consecutive_successes: 9.5952
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 16.30s
                        Total time: 35674.07s
                               ETA: 995676.8s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.897s, learning 0.237s)
               Value function loss: 165575.2863
                    Surrogate loss: -0.0137
             Mean action noise std: 0.70
                       Mean reward: 2504.52
               Mean episode length: 52.05
                  Mean reward/step: 51.08
       Mean episode length/episode: 7.09
            Mean episode successes: 4.4614
Mean episode consecutive_successes: 9.5482
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 16.13s
                        Total time: 35690.20s
                               ETA: 995828.9s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.905s, learning 0.188s)
               Value function loss: 184548.6832
                    Surrogate loss: -0.0079
             Mean action noise std: 0.70
                       Mean reward: 2677.09
               Mean episode length: 55.31
                  Mean reward/step: 49.42
       Mean episode length/episode: 7.12
            Mean episode successes: 4.5425
Mean episode consecutive_successes: 9.5906
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 16.09s
                        Total time: 35706.30s
                               ETA: 995979.8s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.011s, learning 0.161s)
               Value function loss: 179054.3270
                    Surrogate loss: -0.0171
             Mean action noise std: 0.70
                       Mean reward: 2660.42
               Mean episode length: 52.94
                  Mean reward/step: 48.93
       Mean episode length/episode: 7.10
            Mean episode successes: 4.4365
Mean episode consecutive_successes: 9.6984
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 16.17s
                        Total time: 35722.47s
                               ETA: 996132.7s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.089s, learning 0.157s)
               Value function loss: 228590.7172
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 2917.83
               Mean episode length: 53.15
                  Mean reward/step: 50.15
       Mean episode length/episode: 7.14
            Mean episode successes: 4.5664
Mean episode consecutive_successes: 9.7946
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 16.25s
                        Total time: 35738.72s
                               ETA: 996287.6s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.005s, learning 0.157s)
               Value function loss: 319976.8773
                    Surrogate loss: -0.0022
             Mean action noise std: 0.70
                       Mean reward: 3327.09
               Mean episode length: 55.28
                  Mean reward/step: 50.84
       Mean episode length/episode: 7.10
            Mean episode successes: 4.6899
Mean episode consecutive_successes: 9.8185
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 16.16s
                        Total time: 35754.88s
                               ETA: 996440.1s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.491s, learning 0.209s)
               Value function loss: 198143.0449
                    Surrogate loss: -0.0092
             Mean action noise std: 0.70
                       Mean reward: 3141.15
               Mean episode length: 54.24
                  Mean reward/step: 49.13
       Mean episode length/episode: 7.02
            Mean episode successes: 4.3496
Mean episode consecutive_successes: 9.9876
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 10.70s
                        Total time: 35765.58s
                               ETA: 996440.4s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.057s, learning 0.168s)
               Value function loss: 180181.5734
                    Surrogate loss: -0.0084
             Mean action noise std: 0.70
                       Mean reward: 2337.07
               Mean episode length: 52.15
                  Mean reward/step: 48.59
       Mean episode length/episode: 7.14
            Mean episode successes: 4.4551
Mean episode consecutive_successes: 9.9500
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 8.23s
                        Total time: 35773.80s
                               ETA: 996371.7s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.308s, learning 0.225s)
               Value function loss: 188226.3168
                    Surrogate loss: -0.0175
             Mean action noise std: 0.70
                       Mean reward: 3104.17
               Mean episode length: 58.37
                  Mean reward/step: 47.83
       Mean episode length/episode: 7.14
            Mean episode successes: 4.4580
Mean episode consecutive_successes: 10.1009
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 8.53s
                        Total time: 35782.34s
                               ETA: 996311.5s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.163s)
               Value function loss: 189901.5234
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 2711.91
               Mean episode length: 51.35
                  Mean reward/step: 46.78
       Mean episode length/episode: 7.11
            Mean episode successes: 4.4263
Mean episode consecutive_successes: 10.0738
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 8.48s
                        Total time: 35790.81s
                               ETA: 996249.9s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.161s)
               Value function loss: 204112.1586
                    Surrogate loss: -0.0093
             Mean action noise std: 0.70
                       Mean reward: 2113.79
               Mean episode length: 50.40
                  Mean reward/step: 48.46
       Mean episode length/episode: 7.04
            Mean episode successes: 4.4561
Mean episode consecutive_successes: 10.0480
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 8.41s
                        Total time: 35799.23s
                               ETA: 996186.5s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.150s, learning 0.159s)
               Value function loss: 334796.7359
                    Surrogate loss: -0.0154
             Mean action noise std: 0.70
                       Mean reward: 2811.36
               Mean episode length: 54.79
                  Mean reward/step: 51.61
       Mean episode length/episode: 7.12
            Mean episode successes: 4.4814
Mean episode consecutive_successes: 10.1547
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 8.31s
                        Total time: 35807.54s
                               ETA: 996120.3s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.083s, learning 0.191s)
               Value function loss: 218323.1828
                    Surrogate loss: -0.0112
             Mean action noise std: 0.70
                       Mean reward: 2072.57
               Mean episode length: 53.42
                  Mean reward/step: 50.38
       Mean episode length/episode: 7.09
            Mean episode successes: 4.6040
Mean episode consecutive_successes: 10.0660
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 8.27s
                        Total time: 35815.81s
                               ETA: 996053.1s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.162s)
               Value function loss: 175821.8184
                    Surrogate loss: -0.0099
             Mean action noise std: 0.70
                       Mean reward: 2645.53
               Mean episode length: 52.86
                  Mean reward/step: 49.16
       Mean episode length/episode: 7.06
            Mean episode successes: 4.5322
Mean episode consecutive_successes: 10.1505
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 8.40s
                        Total time: 35824.21s
                               ETA: 995989.5s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.135s, learning 0.166s)
               Value function loss: 169365.9277
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 2447.83
               Mean episode length: 53.52
                  Mean reward/step: 45.49
       Mean episode length/episode: 7.09
            Mean episode successes: 4.5786
Mean episode consecutive_successes: 10.1211
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 8.30s
                        Total time: 35832.51s
                               ETA: 995923.1s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.156s, learning 0.224s)
               Value function loss: 179856.1477
                    Surrogate loss: -0.0131
             Mean action noise std: 0.70
                       Mean reward: 3036.96
               Mean episode length: 55.21
                  Mean reward/step: 43.47
       Mean episode length/episode: 7.02
            Mean episode successes: 4.1465
Mean episode consecutive_successes: 10.2062
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 8.38s
                        Total time: 35840.89s
                               ETA: 995859.0s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.434s, learning 0.171s)
               Value function loss: 224799.5164
                    Surrogate loss: -0.0084
             Mean action noise std: 0.70
                       Mean reward: 2350.15
               Mean episode length: 52.46
                  Mean reward/step: 43.64
       Mean episode length/episode: 7.10
            Mean episode successes: 4.0269
Mean episode consecutive_successes: 10.1682
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 8.61s
                        Total time: 35849.50s
                               ETA: 995801.1s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.284s, learning 0.282s)
               Value function loss: 167113.6355
                    Surrogate loss: -0.0062
             Mean action noise std: 0.70
                       Mean reward: 2663.59
               Mean episode length: 53.84
                  Mean reward/step: 45.02
       Mean episode length/episode: 7.10
            Mean episode successes: 4.1567
Mean episode consecutive_successes: 10.0757
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 8.57s
                        Total time: 35858.07s
                               ETA: 995742.2s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.145s, learning 0.182s)
               Value function loss: 164617.2563
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 2806.26
               Mean episode length: 54.26
                  Mean reward/step: 45.42
       Mean episode length/episode: 7.08
            Mean episode successes: 4.2944
Mean episode consecutive_successes: 10.0147
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 8.33s
                        Total time: 35866.39s
                               ETA: 995676.6s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.350s, learning 0.241s)
               Value function loss: 173331.0824
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 2284.02
               Mean episode length: 50.86
                  Mean reward/step: 45.54
       Mean episode length/episode: 7.00
            Mean episode successes: 4.1099
Mean episode consecutive_successes: 9.9954
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 8.59s
                        Total time: 35874.98s
                               ETA: 995618.5s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.137s, learning 0.173s)
               Value function loss: 194008.2906
                    Surrogate loss: -0.0065
             Mean action noise std: 0.70
                       Mean reward: 2591.97
               Mean episode length: 52.10
                  Mean reward/step: 45.95
       Mean episode length/episode: 7.17
            Mean episode successes: 4.2656
Mean episode consecutive_successes: 9.9910
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 8.31s
                        Total time: 35883.29s
                               ETA: 995552.6s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.847s, learning 0.171s)
               Value function loss: 214326.2242
                    Surrogate loss: -0.0085
             Mean action noise std: 0.70
                       Mean reward: 2998.04
               Mean episode length: 53.25
                  Mean reward/step: 47.68
       Mean episode length/episode: 7.11
            Mean episode successes: 4.2554
Mean episode consecutive_successes: 10.0713
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 8.02s
                        Total time: 35891.31s
                               ETA: 995478.6s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.122s, learning 0.197s)
               Value function loss: 183807.5402
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 2279.44
               Mean episode length: 51.66
                  Mean reward/step: 49.25
       Mean episode length/episode: 6.99
            Mean episode successes: 4.4155
Mean episode consecutive_successes: 9.9571
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 8.32s
                        Total time: 35899.63s
                               ETA: 995413.0s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.956s, learning 0.191s)
               Value function loss: 218371.1789
                    Surrogate loss: -0.0154
             Mean action noise std: 0.70
                       Mean reward: 2991.30
               Mean episode length: 54.42
                  Mean reward/step: 50.80
       Mean episode length/episode: 7.06
            Mean episode successes: 4.5215
Mean episode consecutive_successes: 9.9957
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 8.15s
                        Total time: 35907.78s
                               ETA: 995342.6s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.525s, learning 0.160s)
               Value function loss: 245271.2148
                    Surrogate loss: 0.0086
             Mean action noise std: 0.70
                       Mean reward: 2634.12
               Mean episode length: 54.93
                  Mean reward/step: 50.60
       Mean episode length/episode: 7.08
            Mean episode successes: 4.3589
Mean episode consecutive_successes: 10.1290
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 8.68s
                        Total time: 35916.46s
                               ETA: 995287.2s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.080s, learning 0.156s)
               Value function loss: 166599.0168
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 2717.62
               Mean episode length: 52.46
                  Mean reward/step: 49.98
       Mean episode length/episode: 7.04
            Mean episode successes: 4.4473
Mean episode consecutive_successes: 10.1502
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 8.24s
                        Total time: 35924.70s
                               ETA: 995219.4s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.429s, learning 0.185s)
               Value function loss: 155642.3672
                    Surrogate loss: -0.0108
             Mean action noise std: 0.70
                       Mean reward: 2721.89
               Mean episode length: 51.71
                  Mean reward/step: 49.23
       Mean episode length/episode: 7.07
            Mean episode successes: 4.4365
Mean episode consecutive_successes: 10.1976
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 8.61s
                        Total time: 35933.31s
                               ETA: 995162.1s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.398s, learning 0.165s)
               Value function loss: 198022.1125
                    Surrogate loss: -0.0033
             Mean action noise std: 0.70
                       Mean reward: 3034.47
               Mean episode length: 55.49
                  Mean reward/step: 48.74
       Mean episode length/episode: 7.11
            Mean episode successes: 4.5728
Mean episode consecutive_successes: 10.1906
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 8.56s
                        Total time: 35941.88s
                               ETA: 995103.4s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.233s, learning 0.169s)
               Value function loss: 225200.1355
                    Surrogate loss: 0.0100
             Mean action noise std: 0.70
                       Mean reward: 3159.67
               Mean episode length: 55.91
                  Mean reward/step: 48.41
       Mean episode length/episode: 7.03
            Mean episode successes: 4.3013
Mean episode consecutive_successes: 10.2870
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 8.40s
                        Total time: 35950.28s
                               ETA: 995040.2s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.334s, learning 0.161s)
               Value function loss: 162837.7359
                    Surrogate loss: 0.0124
             Mean action noise std: 0.70
                       Mean reward: 2704.80
               Mean episode length: 56.06
                  Mean reward/step: 47.47
       Mean episode length/episode: 7.10
            Mean episode successes: 4.2563
Mean episode consecutive_successes: 10.3122
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 8.49s
                        Total time: 35958.77s
                               ETA: 994979.7s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.039s, learning 0.172s)
               Value function loss: 156892.7508
                    Surrogate loss: -0.0006
             Mean action noise std: 0.70
                       Mean reward: 3049.47
               Mean episode length: 54.53
                  Mean reward/step: 46.80
       Mean episode length/episode: 7.14
            Mean episode successes: 4.1719
Mean episode consecutive_successes: 10.3765
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 8.21s
                        Total time: 35966.98s
                               ETA: 994911.3s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.086s, learning 0.189s)
               Value function loss: 157404.9684
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2356.46
               Mean episode length: 54.45
                  Mean reward/step: 44.73
       Mean episode length/episode: 7.06
            Mean episode successes: 4.0171
Mean episode consecutive_successes: 10.3348
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 8.27s
                        Total time: 35975.26s
                               ETA: 994844.8s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.058s, learning 0.213s)
               Value function loss: 146320.4695
                    Surrogate loss: -0.0072
             Mean action noise std: 0.70
                       Mean reward: 2928.54
               Mean episode length: 57.43
                  Mean reward/step: 43.85
       Mean episode length/episode: 7.15
            Mean episode successes: 3.9536
Mean episode consecutive_successes: 10.3558
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 8.27s
                        Total time: 35983.53s
                               ETA: 994778.1s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.476s, learning 0.172s)
               Value function loss: 148728.4648
                    Surrogate loss: -0.0058
             Mean action noise std: 0.70
                       Mean reward: 1870.81
               Mean episode length: 50.75
                  Mean reward/step: 44.15
       Mean episode length/episode: 7.03
            Mean episode successes: 4.0391
Mean episode consecutive_successes: 10.2090
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 8.65s
                        Total time: 35992.18s
                               ETA: 994722.0s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.178s, learning 0.167s)
               Value function loss: 161681.0660
                    Surrogate loss: -0.0131
             Mean action noise std: 0.70
                       Mean reward: 2173.31
               Mean episode length: 54.31
                  Mean reward/step: 45.25
       Mean episode length/episode: 7.06
            Mean episode successes: 3.8364
Mean episode consecutive_successes: 10.2370
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 8.34s
                        Total time: 36000.52s
                               ETA: 994657.4s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.908s, learning 0.169s)
               Value function loss: 165136.4035
                    Surrogate loss: -0.0127
             Mean action noise std: 0.70
                       Mean reward: 2505.75
               Mean episode length: 57.36
                  Mean reward/step: 44.50
       Mean episode length/episode: 7.12
            Mean episode successes: 3.7432
Mean episode consecutive_successes: 10.2660
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 8.08s
                        Total time: 36008.60s
                               ETA: 994585.5s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.973s, learning 0.162s)
               Value function loss: 168355.8227
                    Surrogate loss: -0.0073
             Mean action noise std: 0.70
                       Mean reward: 1994.72
               Mean episode length: 52.90
                  Mean reward/step: 44.82
       Mean episode length/episode: 7.10
            Mean episode successes: 3.9741
Mean episode consecutive_successes: 10.1206
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 8.13s
                        Total time: 36016.73s
                               ETA: 994515.3s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.480s, learning 0.254s)
               Value function loss: 173740.7934
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 2396.15
               Mean episode length: 55.11
                  Mean reward/step: 42.07
       Mean episode length/episode: 7.03
            Mean episode successes: 3.8281
Mean episode consecutive_successes: 10.0752
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 8.73s
                        Total time: 36025.47s
                               ETA: 994461.6s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.064s, learning 0.169s)
               Value function loss: 226732.4492
                    Surrogate loss: -0.0082
             Mean action noise std: 0.70
                       Mean reward: 1760.27
               Mean episode length: 49.49
                  Mean reward/step: 42.21
       Mean episode length/episode: 7.07
            Mean episode successes: 3.7148
Mean episode consecutive_successes: 10.0170
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 8.23s
                        Total time: 36033.70s
                               ETA: 994394.1s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.437s, learning 0.160s)
               Value function loss: 241115.6184
                    Surrogate loss: 0.0030
             Mean action noise std: 0.70
                       Mean reward: 2371.39
               Mean episode length: 50.81
                  Mean reward/step: 45.25
       Mean episode length/episode: 7.22
            Mean episode successes: 3.8906
Mean episode consecutive_successes: 10.0259
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 8.60s
                        Total time: 36042.30s
                               ETA: 994336.7s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.344s, learning 0.212s)
               Value function loss: 151192.5184
                    Surrogate loss: -0.0086
             Mean action noise std: 0.70
                       Mean reward: 2572.34
               Mean episode length: 52.35
                  Mean reward/step: 47.61
       Mean episode length/episode: 7.06
            Mean episode successes: 3.9033
Mean episode consecutive_successes: 10.0667
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 8.56s
                        Total time: 36050.85s
                               ETA: 994278.2s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.407s, learning 0.188s)
               Value function loss: 162015.9336
                    Surrogate loss: -0.0074
             Mean action noise std: 0.70
                       Mean reward: 1960.21
               Mean episode length: 53.22
                  Mean reward/step: 46.96
       Mean episode length/episode: 6.98
            Mean episode successes: 3.9639
Mean episode consecutive_successes: 9.9569
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 8.59s
                        Total time: 36059.45s
                               ETA: 994220.8s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.323s, learning 0.161s)
               Value function loss: 173345.8094
                    Surrogate loss: 0.0076
             Mean action noise std: 0.70
                       Mean reward: 2824.78
               Mean episode length: 55.31
                  Mean reward/step: 46.75
       Mean episode length/episode: 7.05
            Mean episode successes: 4.0083
Mean episode consecutive_successes: 9.9592
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 8.48s
                        Total time: 36067.93s
                               ETA: 994160.4s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.146s, learning 0.170s)
               Value function loss: 165790.4039
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 2639.74
               Mean episode length: 55.94
                  Mean reward/step: 44.24
       Mean episode length/episode: 7.10
            Mean episode successes: 3.8486
Mean episode consecutive_successes: 10.0381
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 8.32s
                        Total time: 36076.25s
                               ETA: 994095.3s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.081s, learning 0.160s)
               Value function loss: 167414.0020
                    Surrogate loss: -0.0024
             Mean action noise std: 0.70
                       Mean reward: 2472.92
               Mean episode length: 52.79
                  Mean reward/step: 44.19
       Mean episode length/episode: 7.07
            Mean episode successes: 3.6328
Mean episode consecutive_successes: 10.0646
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 8.24s
                        Total time: 36084.49s
                               ETA: 994028.3s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.542s, learning 0.177s)
               Value function loss: 162690.9535
                    Surrogate loss: -0.0099
             Mean action noise std: 0.70
                       Mean reward: 2536.44
               Mean episode length: 54.14
                  Mean reward/step: 42.97
       Mean episode length/episode: 6.99
            Mean episode successes: 3.5366
Mean episode consecutive_successes: 9.9928
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 8.72s
                        Total time: 36093.21s
                               ETA: 993974.4s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.407s, learning 0.161s)
               Value function loss: 166292.1035
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 2605.98
               Mean episode length: 53.53
                  Mean reward/step: 43.29
       Mean episode length/episode: 7.10
            Mean episode successes: 3.6235
Mean episode consecutive_successes: 9.8958
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 8.57s
                        Total time: 36101.78s
                               ETA: 993916.4s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.247s, learning 0.171s)
               Value function loss: 158480.8914
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 2143.93
               Mean episode length: 51.39
                  Mean reward/step: 42.62
       Mean episode length/episode: 7.05
            Mean episode successes: 3.6797
Mean episode consecutive_successes: 9.8047
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 8.42s
                        Total time: 36110.19s
                               ETA: 993854.3s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.380s, learning 0.240s)
               Value function loss: 171990.3973
                    Surrogate loss: -0.0069
             Mean action noise std: 0.70
                       Mean reward: 1713.31
               Mean episode length: 50.83
                  Mean reward/step: 42.84
       Mean episode length/episode: 7.12
            Mean episode successes: 3.7568
Mean episode consecutive_successes: 9.7012
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 8.62s
                        Total time: 36118.81s
                               ETA: 993797.8s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.087s, learning 0.212s)
               Value function loss: 171040.3055
                    Surrogate loss: -0.0022
             Mean action noise std: 0.70
                       Mean reward: 2286.05
               Mean episode length: 54.74
                  Mean reward/step: 44.70
       Mean episode length/episode: 7.10
            Mean episode successes: 3.8130
Mean episode consecutive_successes: 9.7112
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 8.30s
                        Total time: 36127.11s
                               ETA: 993732.5s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.298s, learning 0.168s)
               Value function loss: 165384.0895
                    Surrogate loss: -0.0048
             Mean action noise std: 0.70
                       Mean reward: 2528.04
               Mean episode length: 53.54
                  Mean reward/step: 45.35
       Mean episode length/episode: 7.04
            Mean episode successes: 3.6851
Mean episode consecutive_successes: 9.7512
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 8.47s
                        Total time: 36135.58s
                               ETA: 993671.8s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.228s, learning 0.162s)
               Value function loss: 166053.4129
                    Surrogate loss: -0.0026
             Mean action noise std: 0.70
                       Mean reward: 2519.06
               Mean episode length: 54.69
                  Mean reward/step: 43.84
       Mean episode length/episode: 7.05
            Mean episode successes: 3.6782
Mean episode consecutive_successes: 9.6757
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 8.39s
                        Total time: 36143.97s
                               ETA: 993609.0s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.519s, learning 0.157s)
               Value function loss: 163722.1777
                    Surrogate loss: -0.0049
             Mean action noise std: 0.70
                       Mean reward: 2148.31
               Mean episode length: 50.06
                  Mean reward/step: 43.58
       Mean episode length/episode: 7.04
            Mean episode successes: 3.7847
Mean episode consecutive_successes: 9.6062
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 8.68s
                        Total time: 36152.64s
                               ETA: 993554.2s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.335s, learning 0.183s)
               Value function loss: 174909.6598
                    Surrogate loss: -0.0059
             Mean action noise std: 0.70
                       Mean reward: 2730.50
               Mean episode length: 56.73
                  Mean reward/step: 44.03
       Mean episode length/episode: 7.17
            Mean episode successes: 3.8765
Mean episode consecutive_successes: 9.6283
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 8.52s
                        Total time: 36161.16s
                               ETA: 993495.0s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.286s, learning 0.178s)
               Value function loss: 249240.7797
                    Surrogate loss: -0.0099
             Mean action noise std: 0.70
                       Mean reward: 2757.42
               Mean episode length: 55.86
                  Mean reward/step: 44.09
       Mean episode length/episode: 7.06
            Mean episode successes: 3.7588
Mean episode consecutive_successes: 9.6538
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 8.46s
                        Total time: 36169.63s
                               ETA: 993434.4s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.161s)
               Value function loss: 229060.9129
                    Surrogate loss: -0.0065
             Mean action noise std: 0.70
                       Mean reward: 1416.90
               Mean episode length: 52.73
                  Mean reward/step: 43.83
       Mean episode length/episode: 7.06
            Mean episode successes: 3.9121
Mean episode consecutive_successes: 9.4901
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 8.45s
                        Total time: 36178.07s
                               ETA: 993373.3s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.162s)
               Value function loss: 199638.5473
                    Surrogate loss: -0.0158
             Mean action noise std: 0.70
                       Mean reward: 2968.67
               Mean episode length: 57.46
                  Mean reward/step: 46.99
       Mean episode length/episode: 7.19
            Mean episode successes: 3.9224
Mean episode consecutive_successes: 9.6398
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 8.48s
                        Total time: 36186.55s
                               ETA: 993313.1s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.319s, learning 0.211s)
               Value function loss: 297372.2945
                    Surrogate loss: -0.0067
             Mean action noise std: 0.70
                       Mean reward: 1955.14
               Mean episode length: 52.40
                  Mean reward/step: 45.16
       Mean episode length/episode: 7.02
            Mean episode successes: 3.8813
Mean episode consecutive_successes: 9.6304
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 8.53s
                        Total time: 36195.08s
                               ETA: 993254.4s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.125s, learning 0.208s)
               Value function loss: 154067.4445
                    Surrogate loss: 0.0128
             Mean action noise std: 0.70
                       Mean reward: 1530.94
               Mean episode length: 50.71
                  Mean reward/step: 45.08
       Mean episode length/episode: 7.03
            Mean episode successes: 3.9727
Mean episode consecutive_successes: 9.5609
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 8.33s
                        Total time: 36203.41s
                               ETA: 993190.2s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.102s, learning 0.213s)
               Value function loss: 146807.8703
                    Surrogate loss: 0.0013
             Mean action noise std: 0.70
                       Mean reward: 2985.97
               Mean episode length: 57.63
                  Mean reward/step: 44.11
       Mean episode length/episode: 7.04
            Mean episode successes: 3.7964
Mean episode consecutive_successes: 9.6556
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 8.31s
                        Total time: 36211.73s
                               ETA: 993125.7s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.227s, learning 0.159s)
               Value function loss: 127712.4652
                    Surrogate loss: -0.0113
             Mean action noise std: 0.70
                       Mean reward: 2732.77
               Mean episode length: 52.77
                  Mean reward/step: 43.85
       Mean episode length/episode: 7.06
            Mean episode successes: 3.7871
Mean episode consecutive_successes: 9.6517
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 8.39s
                        Total time: 36220.11s
                               ETA: 993063.1s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.239s, learning 0.166s)
               Value function loss: 132786.8135
                    Surrogate loss: 0.0035
             Mean action noise std: 0.70
                       Mean reward: 2616.23
               Mean episode length: 54.15
                  Mean reward/step: 41.20
       Mean episode length/episode: 7.10
            Mean episode successes: 3.5732
Mean episode consecutive_successes: 9.7138
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 8.40s
                        Total time: 36228.52s
                               ETA: 993001.1s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.250s, learning 0.164s)
               Value function loss: 129429.6201
                    Surrogate loss: -0.0086
             Mean action noise std: 0.70
                       Mean reward: 1640.99
               Mean episode length: 54.84
                  Mean reward/step: 41.62
       Mean episode length/episode: 7.13
            Mean episode successes: 3.6245
Mean episode consecutive_successes: 9.6549
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 8.41s
                        Total time: 36236.93s
                               ETA: 992939.3s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.265s, learning 0.187s)
               Value function loss: 152441.3262
                    Surrogate loss: 0.0151
             Mean action noise std: 0.70
                       Mean reward: 2117.20
               Mean episode length: 52.02
                  Mean reward/step: 40.79
       Mean episode length/episode: 7.09
            Mean episode successes: 3.7314
Mean episode consecutive_successes: 9.5591
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 8.45s
                        Total time: 36245.38s
                               ETA: 992878.6s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.114s, learning 0.171s)
               Value function loss: 144226.1703
                    Surrogate loss: -0.0018
             Mean action noise std: 0.70
                       Mean reward: 2301.42
               Mean episode length: 54.70
                  Mean reward/step: 39.17
       Mean episode length/episode: 7.07
            Mean episode successes: 3.3179
Mean episode consecutive_successes: 9.6355
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 8.29s
                        Total time: 36253.67s
                               ETA: 992813.4s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.277s, learning 0.159s)
               Value function loss: 140642.5771
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 2006.02
               Mean episode length: 54.94
                  Mean reward/step: 39.27
       Mean episode length/episode: 7.14
            Mean episode successes: 3.5117
Mean episode consecutive_successes: 9.4894
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 8.44s
                        Total time: 36262.11s
                               ETA: 992752.3s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.225s, learning 0.179s)
               Value function loss: 156559.2105
                    Surrogate loss: -0.0067
             Mean action noise std: 0.70
                       Mean reward: 2688.30
               Mean episode length: 57.58
                  Mean reward/step: 42.86
       Mean episode length/episode: 7.07
            Mean episode successes: 3.5039
Mean episode consecutive_successes: 9.4292
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 8.40s
                        Total time: 36270.51s
                               ETA: 992690.4s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.406s, learning 0.161s)
               Value function loss: 151887.4172
                    Surrogate loss: -0.0072
             Mean action noise std: 0.70
                       Mean reward: 1875.21
               Mean episode length: 52.90
                  Mean reward/step: 42.01
       Mean episode length/episode: 7.07
            Mean episode successes: 3.4849
Mean episode consecutive_successes: 9.3904
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 8.57s
                        Total time: 36279.08s
                               ETA: 992633.0s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.264s, learning 0.169s)
               Value function loss: 166299.7430
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2232.81
               Mean episode length: 57.46
                  Mean reward/step: 41.52
       Mean episode length/episode: 7.10
            Mean episode successes: 3.5835
Mean episode consecutive_successes: 9.2993
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 8.43s
                        Total time: 36287.51s
                               ETA: 992572.0s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.053s, learning 0.236s)
               Value function loss: 157593.0496
                    Surrogate loss: -0.0066
             Mean action noise std: 0.70
                       Mean reward: 1871.11
               Mean episode length: 54.64
                  Mean reward/step: 41.01
       Mean episode length/episode: 7.01
            Mean episode successes: 3.4976
Mean episode consecutive_successes: 9.2283
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 8.29s
                        Total time: 36295.80s
                               ETA: 992507.0s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.460s, learning 0.167s)
               Value function loss: 158609.3691
                    Surrogate loss: -0.0034
             Mean action noise std: 0.70
                       Mean reward: 1976.57
               Mean episode length: 50.92
                  Mean reward/step: 39.47
       Mean episode length/episode: 7.17
            Mean episode successes: 3.5908
Mean episode consecutive_successes: 9.1759
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 8.63s
                        Total time: 36304.43s
                               ETA: 992451.3s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.329s, learning 0.186s)
               Value function loss: 173685.1082
                    Surrogate loss: 0.0021
             Mean action noise std: 0.70
                       Mean reward: 1830.48
               Mean episode length: 54.11
                  Mean reward/step: 39.16
       Mean episode length/episode: 7.08
            Mean episode successes: 3.4111
Mean episode consecutive_successes: 9.1213
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 8.51s
                        Total time: 36312.94s
                               ETA: 992392.6s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.123s, learning 0.203s)
               Value function loss: 163608.9141
                    Surrogate loss: -0.0053
             Mean action noise std: 0.70
                       Mean reward: 2151.89
               Mean episode length: 51.93
                  Mean reward/step: 42.49
       Mean episode length/episode: 7.13
            Mean episode successes: 3.6206
Mean episode consecutive_successes: 9.0631
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 8.33s
                        Total time: 36321.27s
                               ETA: 992328.7s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.982s, learning 0.161s)
               Value function loss: 169611.5121
                    Surrogate loss: 0.0170
             Mean action noise std: 0.70
                       Mean reward: 3017.80
               Mean episode length: 56.48
                  Mean reward/step: 46.38
       Mean episode length/episode: 7.05
            Mean episode successes: 3.7939
Mean episode consecutive_successes: 9.1095
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 8.14s
                        Total time: 36329.41s
                               ETA: 992259.9s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.006s, learning 0.156s)
               Value function loss: 158759.9660
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 1905.17
               Mean episode length: 53.53
                  Mean reward/step: 46.06
       Mean episode length/episode: 7.08
            Mean episode successes: 3.6431
Mean episode consecutive_successes: 9.1045
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 8.16s
                        Total time: 36337.57s
                               ETA: 992191.6s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.277s, learning 0.190s)
               Value function loss: 168555.9008
                    Surrogate loss: -0.0063
             Mean action noise std: 0.70
                       Mean reward: 2018.30
               Mean episode length: 54.41
                  Mean reward/step: 46.15
       Mean episode length/episode: 7.16
            Mean episode successes: 3.9736
Mean episode consecutive_successes: 9.0898
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 8.47s
                        Total time: 36346.04s
                               ETA: 992131.7s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.122s, learning 0.164s)
               Value function loss: 190471.3086
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 2490.57
               Mean episode length: 53.16
                  Mean reward/step: 46.75
       Mean episode length/episode: 7.06
            Mean episode successes: 4.1499
Mean episode consecutive_successes: 9.0963
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 8.29s
                        Total time: 36354.33s
                               ETA: 992066.9s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.843s, learning 0.167s)
               Value function loss: 183893.8312
                    Surrogate loss: -0.0129
             Mean action noise std: 0.69
                       Mean reward: 3127.57
               Mean episode length: 56.31
                  Mean reward/step: 45.90
       Mean episode length/episode: 7.05
            Mean episode successes: 3.9673
Mean episode consecutive_successes: 9.2226
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 8.01s
                        Total time: 36362.34s
                               ETA: 991994.5s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.371s, learning 0.165s)
               Value function loss: 231520.6359
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 2966.70
               Mean episode length: 55.14
                  Mean reward/step: 44.59
       Mean episode length/episode: 7.03
            Mean episode successes: 3.8340
Mean episode consecutive_successes: 9.2970
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 8.54s
                        Total time: 36370.87s
                               ETA: 991936.6s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.261s, learning 0.162s)
               Value function loss: 154799.3332
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 2153.27
               Mean episode length: 53.72
                  Mean reward/step: 43.58
       Mean episode length/episode: 7.08
            Mean episode successes: 3.6426
Mean episode consecutive_successes: 9.3406
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 8.42s
                        Total time: 36379.30s
                               ETA: 991875.6s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.258s, learning 0.211s)
               Value function loss: 156599.6113
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 2122.71
               Mean episode length: 52.95
                  Mean reward/step: 41.95
       Mean episode length/episode: 7.11
            Mean episode successes: 3.5459
Mean episode consecutive_successes: 9.3513
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 8.47s
                        Total time: 36387.76s
                               ETA: 991815.9s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.223s, learning 0.191s)
               Value function loss: 201880.0445
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 2196.32
               Mean episode length: 50.04
                  Mean reward/step: 45.43
       Mean episode length/episode: 7.09
            Mean episode successes: 3.8013
Mean episode consecutive_successes: 9.2621
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 8.41s
                        Total time: 36396.18s
                               ETA: 991754.7s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.228s, learning 0.239s)
               Value function loss: 303766.8820
                    Surrogate loss: -0.0071
             Mean action noise std: 0.69
                       Mean reward: 2692.98
               Mean episode length: 57.28
                  Mean reward/step: 48.98
       Mean episode length/episode: 7.15
            Mean episode successes: 4.0171
Mean episode consecutive_successes: 9.3284
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 8.47s
                        Total time: 36404.65s
                               ETA: 991695.0s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.016s, learning 0.165s)
               Value function loss: 211576.6371
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2720.18
               Mean episode length: 56.14
                  Mean reward/step: 50.09
       Mean episode length/episode: 7.11
            Mean episode successes: 4.1992
Mean episode consecutive_successes: 9.4014
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 8.18s
                        Total time: 36412.83s
                               ETA: 991627.6s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.438s, learning 0.164s)
               Value function loss: 177208.9695
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 2579.27
               Mean episode length: 54.81
                  Mean reward/step: 49.28
       Mean episode length/episode: 7.04
            Mean episode successes: 4.1382
Mean episode consecutive_successes: 9.3991
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 8.60s
                        Total time: 36421.43s
                               ETA: 991571.6s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.088s, learning 0.159s)
               Value function loss: 165634.5516
                    Surrogate loss: -0.0126
             Mean action noise std: 0.69
                       Mean reward: 2778.28
               Mean episode length: 53.22
                  Mean reward/step: 48.88
       Mean episode length/episode: 7.02
            Mean episode successes: 4.2432
Mean episode consecutive_successes: 9.4074
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 8.25s
                        Total time: 36429.68s
                               ETA: 991506.0s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.277s, learning 0.166s)
               Value function loss: 170485.0957
                    Surrogate loss: 0.0023
             Mean action noise std: 0.69
                       Mean reward: 2764.89
               Mean episode length: 56.42
                  Mean reward/step: 47.78
       Mean episode length/episode: 7.10
            Mean episode successes: 4.0659
Mean episode consecutive_successes: 9.5921
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 8.44s
                        Total time: 36438.12s
                               ETA: 991445.8s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.045s, learning 0.163s)
               Value function loss: 180955.4688
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 2414.88
               Mean episode length: 52.37
                  Mean reward/step: 46.64
       Mean episode length/episode: 7.07
            Mean episode successes: 3.9468
Mean episode consecutive_successes: 9.6369
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 8.21s
                        Total time: 36446.33s
                               ETA: 991379.2s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.917s, learning 0.174s)
               Value function loss: 171544.4531
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 2371.23
               Mean episode length: 54.41
                  Mean reward/step: 46.27
       Mean episode length/episode: 6.98
            Mean episode successes: 3.9053
Mean episode consecutive_successes: 9.6206
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 8.09s
                        Total time: 36454.42s
                               ETA: 991309.4s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.930s, learning 0.188s)
               Value function loss: 230791.6086
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2423.01
               Mean episode length: 53.13
                  Mean reward/step: 45.56
       Mean episode length/episode: 7.11
            Mean episode successes: 3.9629
Mean episode consecutive_successes: 9.6001
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 8.12s
                        Total time: 36462.54s
                               ETA: 991240.4s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.290s, learning 0.206s)
               Value function loss: 182675.9469
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 2287.21
               Mean episode length: 51.86
                  Mean reward/step: 45.70
       Mean episode length/episode: 7.13
            Mean episode successes: 3.9014
Mean episode consecutive_successes: 9.6269
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 8.50s
                        Total time: 36471.03s
                               ETA: 991181.8s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.267s, learning 0.211s)
               Value function loss: 175625.3133
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 2363.41
               Mean episode length: 54.38
                  Mean reward/step: 47.92
       Mean episode length/episode: 7.10
            Mean episode successes: 4.0942
Mean episode consecutive_successes: 9.5995
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 8.48s
                        Total time: 36479.51s
                               ETA: 991122.6s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.018s, learning 0.165s)
               Value function loss: 166102.3262
                    Surrogate loss: -0.0126
             Mean action noise std: 0.69
                       Mean reward: 2011.49
               Mean episode length: 51.39
                  Mean reward/step: 48.58
       Mean episode length/episode: 7.08
            Mean episode successes: 4.1880
Mean episode consecutive_successes: 9.5962
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 8.18s
                        Total time: 36487.69s
                               ETA: 991055.5s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.426s, learning 0.171s)
               Value function loss: 179151.6895
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 2944.72
               Mean episode length: 55.73
                  Mean reward/step: 49.45
       Mean episode length/episode: 7.10
            Mean episode successes: 4.2651
Mean episode consecutive_successes: 9.7384
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 8.60s
                        Total time: 36496.29s
                               ETA: 990999.7s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.776s, learning 0.166s)
               Value function loss: 165572.6184
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2720.92
               Mean episode length: 53.80
                  Mean reward/step: 49.42
       Mean episode length/episode: 7.03
            Mean episode successes: 4.3428
Mean episode consecutive_successes: 9.7129
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 7.94s
                        Total time: 36504.23s
                               ETA: 990926.1s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.185s, learning 0.161s)
               Value function loss: 171597.5746
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 3080.68
               Mean episode length: 53.41
                  Mean reward/step: 46.79
       Mean episode length/episode: 7.13
            Mean episode successes: 4.1367
Mean episode consecutive_successes: 9.8976
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 8.35s
                        Total time: 36512.58s
                               ETA: 990863.5s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.348s, learning 0.180s)
               Value function loss: 163012.2957
                    Surrogate loss: -0.0113
             Mean action noise std: 0.69
                       Mean reward: 1750.64
               Mean episode length: 53.91
                  Mean reward/step: 45.96
       Mean episode length/episode: 7.14
            Mean episode successes: 4.1880
Mean episode consecutive_successes: 9.9160
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 8.53s
                        Total time: 36521.11s
                               ETA: 990805.9s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.292s, learning 0.203s)
               Value function loss: 167968.5703
                    Surrogate loss: -0.0147
             Mean action noise std: 0.69
                       Mean reward: 2561.30
               Mean episode length: 54.29
                  Mean reward/step: 46.66
       Mean episode length/episode: 7.04
            Mean episode successes: 4.0566
Mean episode consecutive_successes: 10.0089
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 8.50s
                        Total time: 36529.60s
                               ETA: 990747.4s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.247s, learning 0.160s)
               Value function loss: 210616.2738
                    Surrogate loss: 0.0210
             Mean action noise std: 0.69
                       Mean reward: 2741.99
               Mean episode length: 55.83
                  Mean reward/step: 47.02
       Mean episode length/episode: 7.04
            Mean episode successes: 3.8608
Mean episode consecutive_successes: 10.0927
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 8.41s
                        Total time: 36538.01s
                               ETA: 990686.5s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.315s, learning 0.170s)
               Value function loss: 189825.7180
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3022.40
               Mean episode length: 56.17
                  Mean reward/step: 47.83
       Mean episode length/episode: 7.13
            Mean episode successes: 4.1118
Mean episode consecutive_successes: 10.1061
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 8.49s
                        Total time: 36546.50s
                               ETA: 990627.8s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.996s, learning 0.163s)
               Value function loss: 175572.2922
                    Surrogate loss: 0.0106
             Mean action noise std: 0.69
                       Mean reward: 2375.75
               Mean episode length: 54.43
                  Mean reward/step: 47.07
       Mean episode length/episode: 7.10
            Mean episode successes: 4.2163
Mean episode consecutive_successes: 10.0358
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 8.16s
                        Total time: 36554.66s
                               ETA: 990560.3s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.319s, learning 0.211s)
               Value function loss: 168498.9488
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 3295.89
               Mean episode length: 57.48
                  Mean reward/step: 49.01
       Mean episode length/episode: 7.11
            Mean episode successes: 4.2969
Mean episode consecutive_successes: 10.0930
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 8.53s
                        Total time: 36563.18s
                               ETA: 990502.8s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.277s, learning 0.163s)
               Value function loss: 162782.8820
                    Surrogate loss: -0.0129
             Mean action noise std: 0.69
                       Mean reward: 2367.77
               Mean episode length: 53.12
                  Mean reward/step: 49.22
       Mean episode length/episode: 7.07
            Mean episode successes: 4.2080
Mean episode consecutive_successes: 10.1286
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 8.44s
                        Total time: 36571.62s
                               ETA: 990443.0s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.493s, learning 0.176s)
               Value function loss: 172681.8176
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2543.23
               Mean episode length: 53.36
                  Mean reward/step: 49.46
       Mean episode length/episode: 7.13
            Mean episode successes: 4.0967
Mean episode consecutive_successes: 10.2079
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 8.67s
                        Total time: 36580.29s
                               ETA: 990389.4s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.005s, learning 0.158s)
               Value function loss: 322635.2719
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 2539.34
               Mean episode length: 55.69
                  Mean reward/step: 49.44
       Mean episode length/episode: 7.06
            Mean episode successes: 4.3149
Mean episode consecutive_successes: 10.1128
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 8.16s
                        Total time: 36588.46s
                               ETA: 990322.1s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.175s, learning 0.166s)
               Value function loss: 261732.4152
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 2321.85
               Mean episode length: 51.57
                  Mean reward/step: 51.48
       Mean episode length/episode: 7.09
            Mean episode successes: 4.5112
Mean episode consecutive_successes: 10.1128
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 8.34s
                        Total time: 36596.80s
                               ETA: 990259.7s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.075s, learning 0.221s)
               Value function loss: 195381.6297
                    Surrogate loss: -0.0139
             Mean action noise std: 0.69
                       Mean reward: 2777.91
               Mean episode length: 53.50
                  Mean reward/step: 50.74
       Mean episode length/episode: 7.09
            Mean episode successes: 4.4678
Mean episode consecutive_successes: 10.1922
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 8.30s
                        Total time: 36605.09s
                               ETA: 990196.0s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.365s, learning 0.172s)
               Value function loss: 165684.3297
                    Surrogate loss: 0.0175
             Mean action noise std: 0.69
                       Mean reward: 2943.75
               Mean episode length: 54.14
                  Mean reward/step: 49.56
       Mean episode length/episode: 7.04
            Mean episode successes: 4.2754
Mean episode consecutive_successes: 10.2699
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 8.54s
                        Total time: 36613.63s
                               ETA: 990139.0s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.547s, learning 0.220s)
               Value function loss: 159279.8770
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 2494.82
               Mean episode length: 52.38
                  Mean reward/step: 48.22
       Mean episode length/episode: 7.03
            Mean episode successes: 4.1479
Mean episode consecutive_successes: 10.3093
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 8.77s
                        Total time: 36622.40s
                               ETA: 990088.1s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.020s, learning 0.176s)
               Value function loss: 146842.8672
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 2587.40
               Mean episode length: 52.66
                  Mean reward/step: 46.37
       Mean episode length/episode: 7.08
            Mean episode successes: 4.0640
Mean episode consecutive_successes: 10.3013
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 8.20s
                        Total time: 36630.59s
                               ETA: 990021.9s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.287s, learning 0.210s)
               Value function loss: 155867.3332
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2506.18
               Mean episode length: 50.70
                  Mean reward/step: 46.13
       Mean episode length/episode: 7.05
            Mean episode successes: 3.9780
Mean episode consecutive_successes: 10.2781
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 8.50s
                        Total time: 36639.09s
                               ETA: 989963.8s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.184s, learning 0.237s)
               Value function loss: 161559.4816
                    Surrogate loss: -0.0158
             Mean action noise std: 0.69
                       Mean reward: 2144.19
               Mean episode length: 50.78
                  Mean reward/step: 46.78
       Mean episode length/episode: 7.04
            Mean episode successes: 4.0332
Mean episode consecutive_successes: 10.1790
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 8.42s
                        Total time: 36647.51s
                               ETA: 989903.7s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.409s, learning 0.162s)
               Value function loss: 172476.0199
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 2638.36
               Mean episode length: 53.40
                  Mean reward/step: 48.49
       Mean episode length/episode: 7.07
            Mean episode successes: 4.0151
Mean episode consecutive_successes: 10.2032
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 8.57s
                        Total time: 36656.08s
                               ETA: 989847.7s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.960s, learning 0.181s)
               Value function loss: 180712.4555
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 2319.38
               Mean episode length: 51.24
                  Mean reward/step: 46.55
       Mean episode length/episode: 6.99
            Mean episode successes: 3.8130
Mean episode consecutive_successes: 10.2301
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 8.14s
                        Total time: 36664.22s
                               ETA: 989780.1s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.154s, learning 0.188s)
               Value function loss: 175912.3172
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 2428.29
               Mean episode length: 50.24
                  Mean reward/step: 46.49
       Mean episode length/episode: 7.07
            Mean episode successes: 4.0503
Mean episode consecutive_successes: 10.1124
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 8.34s
                        Total time: 36672.57s
                               ETA: 989718.0s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.144s, learning 0.159s)
               Value function loss: 183093.2129
                    Surrogate loss: 0.0025
             Mean action noise std: 0.69
                       Mean reward: 2333.42
               Mean episode length: 49.40
                  Mean reward/step: 47.48
       Mean episode length/episode: 7.05
            Mean episode successes: 4.1421
Mean episode consecutive_successes: 10.0407
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 8.30s
                        Total time: 36680.87s
                               ETA: 989654.8s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.166s)
               Value function loss: 188230.0527
                    Surrogate loss: -0.0119
             Mean action noise std: 0.69
                       Mean reward: 2745.20
               Mean episode length: 52.84
                  Mean reward/step: 50.17
       Mean episode length/episode: 7.07
            Mean episode successes: 4.4185
Mean episode consecutive_successes: 10.0100
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 8.37s
                        Total time: 36689.24s
                               ETA: 989593.4s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.501s, learning 0.209s)
               Value function loss: 179137.1605
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 2234.69
               Mean episode length: 52.12
                  Mean reward/step: 48.36
       Mean episode length/episode: 7.05
            Mean episode successes: 4.2759
Mean episode consecutive_successes: 10.0106
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 8.71s
                        Total time: 36697.95s
                               ETA: 989541.3s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.241s, learning 0.183s)
               Value function loss: 176829.9633
                    Surrogate loss: -0.0111
             Mean action noise std: 0.69
                       Mean reward: 2412.99
               Mean episode length: 53.31
                  Mean reward/step: 48.91
       Mean episode length/episode: 7.14
            Mean episode successes: 4.3291
Mean episode consecutive_successes: 10.0392
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 8.42s
                        Total time: 36706.37s
                               ETA: 989481.4s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.461s, learning 0.173s)
               Value function loss: 216693.6922
                    Surrogate loss: 0.0127
             Mean action noise std: 0.69
                       Mean reward: 2878.61
               Mean episode length: 54.13
                  Mean reward/step: 48.61
       Mean episode length/episode: 7.08
            Mean episode successes: 4.4409
Mean episode consecutive_successes: 10.1127
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 8.63s
                        Total time: 36715.01s
                               ETA: 989427.3s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.343s, learning 0.157s)
               Value function loss: 169289.4109
                    Surrogate loss: -0.0110
             Mean action noise std: 0.69
                       Mean reward: 2537.08
               Mean episode length: 52.07
                  Mean reward/step: 47.80
       Mean episode length/episode: 7.03
            Mean episode successes: 4.2529
Mean episode consecutive_successes: 10.1136
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 8.50s
                        Total time: 36723.51s
                               ETA: 989369.6s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.422s, learning 0.168s)
               Value function loss: 170499.5965
                    Surrogate loss: -0.0123
             Mean action noise std: 0.69
                       Mean reward: 2313.13
               Mean episode length: 50.56
                  Mean reward/step: 48.47
       Mean episode length/episode: 7.01
            Mean episode successes: 4.2798
Mean episode consecutive_successes: 10.0444
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 8.59s
                        Total time: 36732.10s
                               ETA: 989314.3s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.998s, learning 0.160s)
               Value function loss: 174881.4539
                    Surrogate loss: 0.0035
             Mean action noise std: 0.69
                       Mean reward: 2482.42
               Mean episode length: 51.68
                  Mean reward/step: 49.16
       Mean episode length/episode: 6.97
            Mean episode successes: 4.0864
Mean episode consecutive_successes: 10.1000
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 8.16s
                        Total time: 36740.25s
                               ETA: 989247.5s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.331s, learning 0.214s)
               Value function loss: 176698.4105
                    Surrogate loss: -0.0100
             Mean action noise std: 0.69
                       Mean reward: 1934.22
               Mean episode length: 51.07
                  Mean reward/step: 47.49
       Mean episode length/episode: 7.02
            Mean episode successes: 4.1528
Mean episode consecutive_successes: 10.0693
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 8.54s
                        Total time: 36748.80s
                               ETA: 989191.1s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.470s, learning 0.162s)
               Value function loss: 202123.9312
                    Surrogate loss: -0.0095
             Mean action noise std: 0.69
                       Mean reward: 2655.88
               Mean episode length: 53.64
                  Mean reward/step: 46.13
       Mean episode length/episode: 7.07
            Mean episode successes: 4.1201
Mean episode consecutive_successes: 10.1058
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 8.63s
                        Total time: 36757.43s
                               ETA: 989137.0s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.523s, learning 0.261s)
               Value function loss: 243139.9992
                    Surrogate loss: 0.0061
             Mean action noise std: 0.69
                       Mean reward: 2826.30
               Mean episode length: 54.26
                  Mean reward/step: 46.66
       Mean episode length/episode: 7.01
            Mean episode successes: 4.0747
Mean episode consecutive_successes: 10.0606
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 8.78s
                        Total time: 36766.21s
                               ETA: 989087.1s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.291s, learning 0.165s)
               Value function loss: 170406.5586
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 2528.66
               Mean episode length: 53.61
                  Mean reward/step: 47.95
       Mean episode length/episode: 7.14
            Mean episode successes: 4.1553
Mean episode consecutive_successes: 9.9568
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 8.46s
                        Total time: 36774.67s
                               ETA: 989028.3s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.283s, learning 0.170s)
               Value function loss: 227737.1617
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 2253.06
               Mean episode length: 50.13
                  Mean reward/step: 49.80
       Mean episode length/episode: 7.05
            Mean episode successes: 4.1396
Mean episode consecutive_successes: 10.0339
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 8.45s
                        Total time: 36783.12s
                               ETA: 988969.5s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.160s, learning 0.176s)
               Value function loss: 318052.9789
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2577.45
               Mean episode length: 52.50
                  Mean reward/step: 50.56
       Mean episode length/episode: 7.10
            Mean episode successes: 4.3271
Mean episode consecutive_successes: 10.0191
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 8.34s
                        Total time: 36791.46s
                               ETA: 988907.6s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.095s, learning 0.160s)
               Value function loss: 245959.6945
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2546.05
               Mean episode length: 54.26
                  Mean reward/step: 50.49
       Mean episode length/episode: 7.11
            Mean episode successes: 4.6655
Mean episode consecutive_successes: 9.9688
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 8.26s
                        Total time: 36799.71s
                               ETA: 988843.6s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.277s, learning 0.193s)
               Value function loss: 210345.7582
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 2598.36
               Mean episode length: 57.50
                  Mean reward/step: 50.55
       Mean episode length/episode: 7.00
            Mean episode successes: 4.5376
Mean episode consecutive_successes: 10.0295
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 8.47s
                        Total time: 36808.18s
                               ETA: 988785.3s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.009s, learning 0.165s)
               Value function loss: 202298.4352
                    Surrogate loss: -0.0110
             Mean action noise std: 0.69
                       Mean reward: 2921.20
               Mean episode length: 54.79
                  Mean reward/step: 49.62
       Mean episode length/episode: 7.03
            Mean episode successes: 4.2949
Mean episode consecutive_successes: 10.1288
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 8.17s
                        Total time: 36816.36s
                               ETA: 988719.1s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.268s, learning 0.180s)
               Value function loss: 158864.3660
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 2816.22
               Mean episode length: 53.95
                  Mean reward/step: 47.40
       Mean episode length/episode: 7.08
            Mean episode successes: 4.3115
Mean episode consecutive_successes: 10.1829
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 8.45s
                        Total time: 36824.80s
                               ETA: 988660.4s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.115s, learning 0.164s)
               Value function loss: 149278.0836
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 2457.49
               Mean episode length: 52.19
                  Mean reward/step: 47.23
       Mean episode length/episode: 7.09
            Mean episode successes: 4.3291
Mean episode consecutive_successes: 10.1719
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 8.28s
                        Total time: 36833.08s
                               ETA: 988597.1s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.235s, learning 0.182s)
               Value function loss: 159650.4035
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 2262.05
               Mean episode length: 51.59
                  Mean reward/step: 47.32
       Mean episode length/episode: 7.02
            Mean episode successes: 4.1558
Mean episode consecutive_successes: 10.1755
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 8.42s
                        Total time: 36841.50s
                               ETA: 988537.5s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.101s, learning 0.211s)
               Value function loss: 154050.7809
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 2265.74
               Mean episode length: 52.74
                  Mean reward/step: 47.59
       Mean episode length/episode: 7.10
            Mean episode successes: 4.2056
Mean episode consecutive_successes: 10.1618
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 8.31s
                        Total time: 36849.81s
                               ETA: 988475.2s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.900s, learning 0.227s)
               Value function loss: 163533.2875
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2280.99
               Mean episode length: 53.88
                  Mean reward/step: 50.85
       Mean episode length/episode: 7.05
            Mean episode successes: 4.4033
Mean episode consecutive_successes: 10.0686
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 8.13s
                        Total time: 36857.94s
                               ETA: 988407.9s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.333s, learning 0.160s)
               Value function loss: 157996.4746
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 2592.51
               Mean episode length: 52.71
                  Mean reward/step: 51.87
       Mean episode length/episode: 7.15
            Mean episode successes: 4.5229
Mean episode consecutive_successes: 10.1405
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 8.49s
                        Total time: 36866.43s
                               ETA: 988350.5s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.149s, learning 0.190s)
               Value function loss: 202937.8500
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 2541.99
               Mean episode length: 51.77
                  Mean reward/step: 52.13
       Mean episode length/episode: 7.05
            Mean episode successes: 4.6987
Mean episode consecutive_successes: 10.1139
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 8.34s
                        Total time: 36874.77s
                               ETA: 988289.0s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.958s, learning 0.185s)
               Value function loss: 323568.3109
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 3120.85
               Mean episode length: 53.88
                  Mean reward/step: 51.65
       Mean episode length/episode: 7.03
            Mean episode successes: 4.5142
Mean episode consecutive_successes: 10.1986
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 8.14s
                        Total time: 36882.91s
                               ETA: 988222.2s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.199s, learning 0.173s)
               Value function loss: 172517.6891
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3030.18
               Mean episode length: 52.09
                  Mean reward/step: 50.98
       Mean episode length/episode: 7.10
            Mean episode successes: 4.5781
Mean episode consecutive_successes: 10.2096
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 8.37s
                        Total time: 36891.29s
                               ETA: 988161.7s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 2077 steps/s (collection: 7.726s, learning 0.160s)
               Value function loss: 181874.4824
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 3714.28
               Mean episode length: 54.97
                  Mean reward/step: 51.15
       Mean episode length/episode: 7.13
            Mean episode successes: 4.5796
Mean episode consecutive_successes: 10.3325
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 7.89s
                        Total time: 36899.17s
                               ETA: 988088.1s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.296s, learning 0.182s)
               Value function loss: 174209.2719
                    Surrogate loss: 0.0048
             Mean action noise std: 0.69
                       Mean reward: 3100.73
               Mean episode length: 53.19
                  Mean reward/step: 49.21
       Mean episode length/episode: 7.02
            Mean episode successes: 4.2319
Mean episode consecutive_successes: 10.3769
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 8.48s
                        Total time: 36907.65s
                               ETA: 988030.4s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.985s, learning 0.195s)
               Value function loss: 164489.2172
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 2855.76
               Mean episode length: 56.74
                  Mean reward/step: 51.13
       Mean episode length/episode: 7.10
            Mean episode successes: 4.3452
Mean episode consecutive_successes: 10.3597
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 8.18s
                        Total time: 36915.83s
                               ETA: 987964.7s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.039s, learning 0.204s)
               Value function loss: 163432.3191
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 3088.67
               Mean episode length: 54.58
                  Mean reward/step: 50.81
       Mean episode length/episode: 7.09
            Mean episode successes: 4.3511
Mean episode consecutive_successes: 10.3875
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 8.24s
                        Total time: 36924.07s
                               ETA: 987900.8s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.164s)
               Value function loss: 160198.5918
                    Surrogate loss: -0.0153
             Mean action noise std: 0.69
                       Mean reward: 2693.17
               Mean episode length: 53.30
                  Mean reward/step: 52.16
       Mean episode length/episode: 7.04
            Mean episode successes: 4.4932
Mean episode consecutive_successes: 10.3556
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 8.48s
                        Total time: 36932.55s
                               ETA: 987843.2s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.287s, learning 0.189s)
               Value function loss: 170970.1676
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 2601.59
               Mean episode length: 50.89
                  Mean reward/step: 52.69
       Mean episode length/episode: 7.09
            Mean episode successes: 4.7676
Mean episode consecutive_successes: 10.2805
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 8.48s
                        Total time: 36941.03s
                               ETA: 987785.6s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.322s, learning 0.235s)
               Value function loss: 175913.6945
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 3636.32
               Mean episode length: 57.90
                  Mean reward/step: 53.94
       Mean episode length/episode: 7.07
            Mean episode successes: 4.6099
Mean episode consecutive_successes: 10.4630
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 8.56s
                        Total time: 36949.58s
                               ETA: 987730.2s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.122s, learning 0.170s)
               Value function loss: 169930.3594
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 2602.03
               Mean episode length: 52.15
                  Mean reward/step: 52.37
       Mean episode length/episode: 7.08
            Mean episode successes: 4.5630
Mean episode consecutive_successes: 10.4943
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 8.29s
                        Total time: 36957.88s
                               ETA: 987667.7s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.296s, learning 0.156s)
               Value function loss: 168761.7762
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 2792.31
               Mean episode length: 52.85
                  Mean reward/step: 53.73
       Mean episode length/episode: 6.97
            Mean episode successes: 4.5732
Mean episode consecutive_successes: 10.4810
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 8.45s
                        Total time: 36966.33s
                               ETA: 987609.5s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.169s, learning 0.160s)
               Value function loss: 197071.2301
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 2798.22
               Mean episode length: 53.75
                  Mean reward/step: 52.74
       Mean episode length/episode: 7.17
            Mean episode successes: 4.7236
Mean episode consecutive_successes: 10.5743
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 8.33s
                        Total time: 36974.66s
                               ETA: 987548.1s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.051s, learning 0.265s)
               Value function loss: 184052.8129
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 3454.58
               Mean episode length: 55.06
                  Mean reward/step: 53.78
       Mean episode length/episode: 7.11
            Mean episode successes: 4.6914
Mean episode consecutive_successes: 10.6601
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 8.32s
                        Total time: 36982.97s
                               ETA: 987486.4s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.099s, learning 0.208s)
               Value function loss: 162123.9934
                    Surrogate loss: -0.0128
             Mean action noise std: 0.69
                       Mean reward: 2013.95
               Mean episode length: 51.44
                  Mean reward/step: 53.96
       Mean episode length/episode: 7.06
            Mean episode successes: 4.8242
Mean episode consecutive_successes: 10.6393
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 8.31s
                        Total time: 36991.28s
                               ETA: 987424.4s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.124s, learning 0.180s)
               Value function loss: 204092.6277
                    Surrogate loss: 0.0022
             Mean action noise std: 0.69
                       Mean reward: 2547.38
               Mean episode length: 52.41
                  Mean reward/step: 56.46
       Mean episode length/episode: 6.99
            Mean episode successes: 4.7642
Mean episode consecutive_successes: 10.7213
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 8.30s
                        Total time: 36999.58s
                               ETA: 987362.4s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.241s, learning 0.172s)
               Value function loss: 188575.3855
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3075.45
               Mean episode length: 56.38
                  Mean reward/step: 55.70
       Mean episode length/episode: 7.20
            Mean episode successes: 4.8828
Mean episode consecutive_successes: 10.8237
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 8.41s
                        Total time: 37008.00s
                               ETA: 987303.3s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.423s, learning 0.209s)
               Value function loss: 190573.9332
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 2933.51
               Mean episode length: 54.30
                  Mean reward/step: 55.46
       Mean episode length/episode: 7.09
            Mean episode successes: 4.8662
Mean episode consecutive_successes: 10.9345
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 8.63s
                        Total time: 37016.63s
                               ETA: 987250.1s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.134s, learning 0.207s)
               Value function loss: 246077.4730
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 3227.89
               Mean episode length: 53.43
                  Mean reward/step: 54.33
       Mean episode length/episode: 7.10
            Mean episode successes: 4.8247
Mean episode consecutive_successes: 11.0456
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 8.34s
                        Total time: 37024.97s
                               ETA: 987189.2s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.266s, learning 0.165s)
               Value function loss: 316877.3914
                    Surrogate loss: 0.0045
             Mean action noise std: 0.69
                       Mean reward: 3118.83
               Mean episode length: 54.55
                  Mean reward/step: 57.06
       Mean episode length/episode: 6.99
            Mean episode successes: 4.9194
Mean episode consecutive_successes: 11.0546
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 8.43s
                        Total time: 37033.40s
                               ETA: 987130.7s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.063s, learning 0.246s)
               Value function loss: 221636.4594
                    Surrogate loss: -0.0091
             Mean action noise std: 0.69
                       Mean reward: 3087.15
               Mean episode length: 55.59
                  Mean reward/step: 56.57
       Mean episode length/episode: 7.13
            Mean episode successes: 4.9189
Mean episode consecutive_successes: 11.1936
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 8.31s
                        Total time: 37041.71s
                               ETA: 987068.9s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.342s, learning 0.184s)
               Value function loss: 207278.8602
                    Surrogate loss: 0.0274
             Mean action noise std: 0.69
                       Mean reward: 2936.94
               Mean episode length: 51.93
                  Mean reward/step: 53.92
       Mean episode length/episode: 7.09
            Mean episode successes: 4.8545
Mean episode consecutive_successes: 11.3032
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 8.53s
                        Total time: 37050.24s
                               ETA: 987013.0s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.894s, learning 0.180s)
               Value function loss: 177800.4977
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 2545.42
               Mean episode length: 53.13
                  Mean reward/step: 53.99
       Mean episode length/episode: 7.13
            Mean episode successes: 5.0508
Mean episode consecutive_successes: 11.2724
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 8.07s
                        Total time: 37058.31s
                               ETA: 986945.1s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.170s)
               Value function loss: 172213.4066
                    Surrogate loss: 0.0072
             Mean action noise std: 0.69
                       Mean reward: 2843.86
               Mean episode length: 54.32
                  Mean reward/step: 54.79
       Mean episode length/episode: 7.08
            Mean episode successes: 4.9043
Mean episode consecutive_successes: 11.4003
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 8.46s
                        Total time: 37066.77s
                               ETA: 986887.3s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.132s, learning 0.168s)
               Value function loss: 193428.4848
                    Surrogate loss: -0.0063
             Mean action noise std: 0.69
                       Mean reward: 2607.92
               Mean episode length: 53.41
                  Mean reward/step: 55.64
       Mean episode length/episode: 7.03
            Mean episode successes: 4.5010
Mean episode consecutive_successes: 11.5407
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 8.30s
                        Total time: 37075.07s
                               ETA: 986825.5s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.369s, learning 0.159s)
               Value function loss: 195050.8121
                    Surrogate loss: -0.0106
             Mean action noise std: 0.69
                       Mean reward: 3098.33
               Mean episode length: 57.22
                  Mean reward/step: 53.52
       Mean episode length/episode: 7.16
            Mean episode successes: 4.7456
Mean episode consecutive_successes: 11.5192
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 8.53s
                        Total time: 37083.60s
                               ETA: 986769.7s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.158s, learning 0.163s)
               Value function loss: 331389.3594
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 3423.71
               Mean episode length: 57.85
                  Mean reward/step: 56.44
       Mean episode length/episode: 7.13
            Mean episode successes: 4.7251
Mean episode consecutive_successes: 11.6294
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 8.32s
                        Total time: 37091.92s
                               ETA: 986708.5s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.217s, learning 0.185s)
               Value function loss: 247349.7273
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 2976.52
               Mean episode length: 55.08
                  Mean reward/step: 53.57
       Mean episode length/episode: 7.02
            Mean episode successes: 4.9009
Mean episode consecutive_successes: 11.5063
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 8.40s
                        Total time: 37100.32s
                               ETA: 986649.4s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.451s, learning 0.159s)
               Value function loss: 180601.9918
                    Surrogate loss: 0.0057
             Mean action noise std: 0.69
                       Mean reward: 2707.48
               Mean episode length: 52.05
                  Mean reward/step: 51.83
       Mean episode length/episode: 7.11
            Mean episode successes: 4.7290
Mean episode consecutive_successes: 11.5689
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 8.61s
                        Total time: 37108.93s
                               ETA: 986595.9s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.317s, learning 0.230s)
               Value function loss: 165323.3656
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 2463.54
               Mean episode length: 54.14
                  Mean reward/step: 49.63
       Mean episode length/episode: 7.01
            Mean episode successes: 4.5781
Mean episode consecutive_successes: 11.4977
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 8.55s
                        Total time: 37117.48s
                               ETA: 986540.7s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.487s, learning 0.203s)
               Value function loss: 164326.7621
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3248.67
               Mean episode length: 58.18
                  Mean reward/step: 49.66
       Mean episode length/episode: 7.04
            Mean episode successes: 4.5088
Mean episode consecutive_successes: 11.4261
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 8.69s
                        Total time: 37126.17s
                               ETA: 986489.4s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.041s, learning 0.164s)
               Value function loss: 169932.0496
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 3069.68
               Mean episode length: 55.95
                  Mean reward/step: 50.17
       Mean episode length/episode: 7.05
            Mean episode successes: 4.4370
Mean episode consecutive_successes: 11.4161
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 8.21s
                        Total time: 37134.37s
                               ETA: 986425.2s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.104s, learning 0.164s)
               Value function loss: 172277.1125
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 3141.02
               Mean episode length: 54.82
                  Mean reward/step: 49.75
       Mean episode length/episode: 7.15
            Mean episode successes: 4.2939
Mean episode consecutive_successes: 11.5143
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 8.27s
                        Total time: 37142.64s
                               ETA: 986362.7s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.925s, learning 0.172s)
               Value function loss: 216158.9379
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 2420.37
               Mean episode length: 53.65
                  Mean reward/step: 51.57
       Mean episode length/episode: 7.08
            Mean episode successes: 4.5063
Mean episode consecutive_successes: 11.3726
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 8.10s
                        Total time: 37150.74s
                               ETA: 986295.8s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.023s, learning 0.166s)
               Value function loss: 230837.4000
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2374.59
               Mean episode length: 51.62
                  Mean reward/step: 52.25
       Mean episode length/episode: 7.07
            Mean episode successes: 4.4443
Mean episode consecutive_successes: 11.3355
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 17.19s
                        Total time: 37167.93s
                               ETA: 986470.1s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.034s, learning 0.167s)
               Value function loss: 188984.2895
                    Surrogate loss: -0.0162
             Mean action noise std: 0.69
                       Mean reward: 2713.22
               Mean episode length: 53.46
                  Mean reward/step: 54.36
       Mean episode length/episode: 7.08
            Mean episode successes: 4.6816
Mean episode consecutive_successes: 11.2733
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 16.20s
                        Total time: 37184.13s
                               ETA: 986618.2s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.328s, learning 0.166s)
               Value function loss: 204391.9762
                    Surrogate loss: -0.0119
             Mean action noise std: 0.69
                       Mean reward: 3026.86
               Mean episode length: 55.30
                  Mean reward/step: 55.00
       Mean episode length/episode: 7.16
            Mean episode successes: 4.8979
Mean episode consecutive_successes: 11.3180
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 16.49s
                        Total time: 37200.62s
                               ETA: 986773.9s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.231s, learning 0.184s)
               Value function loss: 229631.3816
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 2673.84
               Mean episode length: 50.70
                  Mean reward/step: 53.92
       Mean episode length/episode: 7.03
            Mean episode successes: 4.7866
Mean episode consecutive_successes: 11.2838
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 16.42s
                        Total time: 37217.04s
                               ETA: 986927.4s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.349s, learning 0.169s)
               Value function loss: 224032.9223
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 2967.11
               Mean episode length: 52.07
                  Mean reward/step: 51.66
       Mean episode length/episode: 7.01
            Mean episode successes: 4.5278
Mean episode consecutive_successes: 11.3186
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 16.52s
                        Total time: 37233.55s
                               ETA: 987083.6s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.227s, learning 0.222s)
               Value function loss: 199909.9457
                    Surrogate loss: -0.0131
             Mean action noise std: 0.69
                       Mean reward: 3482.85
               Mean episode length: 55.67
                  Mean reward/step: 49.34
       Mean episode length/episode: 7.00
            Mean episode successes: 4.1665
Mean episode consecutive_successes: 11.3762
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 16.45s
                        Total time: 37250.00s
                               ETA: 987237.8s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.329s, learning 0.242s)
               Value function loss: 218588.0926
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3049.29
               Mean episode length: 55.37
                  Mean reward/step: 47.49
       Mean episode length/episode: 7.03
            Mean episode successes: 3.9438
Mean episode consecutive_successes: 11.4015
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 16.57s
                        Total time: 37266.58s
                               ETA: 987395.2s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 1040 steps/s (collection: 15.586s, learning 0.162s)
               Value function loss: 201704.3031
                    Surrogate loss: -0.0110
             Mean action noise std: 0.69
                       Mean reward: 1757.66
               Mean episode length: 49.03
                  Mean reward/step: 46.45
       Mean episode length/episode: 7.08
            Mean episode successes: 4.1040
Mean episode consecutive_successes: 11.1294
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 15.75s
                        Total time: 37282.32s
                               ETA: 987530.7s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.025s, learning 0.169s)
               Value function loss: 192206.4406
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 3165.40
               Mean episode length: 53.20
                  Mean reward/step: 46.35
       Mean episode length/episode: 6.99
            Mean episode successes: 3.9883
Mean episode consecutive_successes: 11.0852
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 16.19s
                        Total time: 37298.52s
                               ETA: 987677.9s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.840s, learning 0.174s)
               Value function loss: 194745.2266
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 2173.42
               Mean episode length: 49.98
                  Mean reward/step: 48.04
       Mean episode length/episode: 7.09
            Mean episode successes: 3.9199
Mean episode consecutive_successes: 11.0011
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 16.01s
                        Total time: 37314.53s
                               ETA: 987820.2s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.857s, learning 0.162s)
               Value function loss: 185005.0457
                    Surrogate loss: -0.0104
             Mean action noise std: 0.69
                       Mean reward: 2048.40
               Mean episode length: 50.88
                  Mean reward/step: 48.58
       Mean episode length/episode: 7.04
            Mean episode successes: 4.0532
Mean episode consecutive_successes: 10.8709
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 16.02s
                        Total time: 37330.55s
                               ETA: 987962.6s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.509s, learning 0.166s)
               Value function loss: 198066.4121
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 1830.27
               Mean episode length: 50.15
                  Mean reward/step: 48.81
       Mean episode length/episode: 7.12
            Mean episode successes: 4.2388
Mean episode consecutive_successes: 10.7447
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 15.67s
                        Total time: 37346.22s
                               ETA: 988095.8s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 1075 steps/s (collection: 15.071s, learning 0.163s)
               Value function loss: 183912.6898
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 2417.63
               Mean episode length: 53.59
                  Mean reward/step: 49.83
       Mean episode length/episode: 7.00
            Mean episode successes: 4.2222
Mean episode consecutive_successes: 10.7095
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 15.23s
                        Total time: 37361.46s
                               ETA: 988217.3s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.108s, learning 0.282s)
               Value function loss: 185471.1020
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 2717.09
               Mean episode length: 52.45
                  Mean reward/step: 51.68
       Mean episode length/episode: 7.13
            Mean episode successes: 4.2671
Mean episode consecutive_successes: 10.7214
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 16.39s
                        Total time: 37377.85s
                               ETA: 988369.2s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.816s, learning 0.164s)
               Value function loss: 179265.4312
                    Surrogate loss: -0.0149
             Mean action noise std: 0.69
                       Mean reward: 2443.28
               Mean episode length: 50.35
                  Mean reward/step: 53.40
       Mean episode length/episode: 7.15
            Mean episode successes: 4.5107
Mean episode consecutive_successes: 10.7220
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 15.98s
                        Total time: 37393.83s
                               ETA: 988510.3s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.915s, learning 0.158s)
               Value function loss: 196064.0824
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 2772.82
               Mean episode length: 53.30
                  Mean reward/step: 53.57
       Mean episode length/episode: 7.15
            Mean episode successes: 4.6055
Mean episode consecutive_successes: 10.7706
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 16.07s
                        Total time: 37409.90s
                               ETA: 988653.7s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.905s, learning 0.159s)
               Value function loss: 218337.1734
                    Surrogate loss: 0.0405
             Mean action noise std: 0.69
                       Mean reward: 2881.61
               Mean episode length: 55.22
                  Mean reward/step: 52.28
       Mean episode length/episode: 7.01
            Mean episode successes: 4.6250
Mean episode consecutive_successes: 10.7523
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 16.06s
                        Total time: 37425.97s
                               ETA: 988796.8s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.204s, learning 0.234s)
               Value function loss: 189241.8605
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3373.49
               Mean episode length: 57.52
                  Mean reward/step: 53.96
       Mean episode length/episode: 7.04
            Mean episode successes: 4.6455
Mean episode consecutive_successes: 10.8119
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 16.44s
                        Total time: 37442.41s
                               ETA: 988949.6s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.109s, learning 0.166s)
               Value function loss: 190514.9270
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 3028.18
               Mean episode length: 53.40
                  Mean reward/step: 54.96
       Mean episode length/episode: 7.10
            Mean episode successes: 4.7676
Mean episode consecutive_successes: 10.8499
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 16.28s
                        Total time: 37458.68s
                               ETA: 989098.1s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.062s, learning 0.264s)
               Value function loss: 209645.0277
                    Surrogate loss: -0.0146
             Mean action noise std: 0.69
                       Mean reward: 3302.21
               Mean episode length: 55.53
                  Mean reward/step: 55.15
       Mean episode length/episode: 7.16
            Mean episode successes: 4.7397
Mean episode consecutive_successes: 10.9996
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 16.33s
                        Total time: 37475.01s
                               ETA: 989247.8s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.092s, learning 0.209s)
               Value function loss: 208877.1922
                    Surrogate loss: 0.0037
             Mean action noise std: 0.69
                       Mean reward: 2613.64
               Mean episode length: 51.23
                  Mean reward/step: 53.14
       Mean episode length/episode: 7.06
            Mean episode successes: 4.6104
Mean episode consecutive_successes: 11.0396
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 16.30s
                        Total time: 37491.31s
                               ETA: 989396.8s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.047s, learning 0.234s)
               Value function loss: 210949.4363
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2911.23
               Mean episode length: 53.94
                  Mean reward/step: 53.76
       Mean episode length/episode: 7.03
            Mean episode successes: 4.5947
Mean episode consecutive_successes: 11.0507
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 16.28s
                        Total time: 37507.59s
                               ETA: 989545.1s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.497s, learning 0.166s)
               Value function loss: 197051.3945
                    Surrogate loss: 0.0056
             Mean action noise std: 0.69
                       Mean reward: 3047.96
               Mean episode length: 53.45
                  Mean reward/step: 52.06
       Mean episode length/episode: 7.07
            Mean episode successes: 4.3794
Mean episode consecutive_successes: 11.1299
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 16.66s
                        Total time: 37524.25s
                               ETA: 989703.5s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.046s, learning 0.167s)
               Value function loss: 208502.7266
                    Surrogate loss: 0.0100
             Mean action noise std: 0.69
                       Mean reward: 3166.19
               Mean episode length: 54.08
                  Mean reward/step: 53.03
       Mean episode length/episode: 7.13
            Mean episode successes: 4.5752
Mean episode consecutive_successes: 11.0835
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 16.21s
                        Total time: 37540.47s
                               ETA: 989849.8s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.264s, learning 0.169s)
               Value function loss: 207910.4035
                    Surrogate loss: 0.0043
             Mean action noise std: 0.69
                       Mean reward: 2530.08
               Mean episode length: 53.14
                  Mean reward/step: 51.99
       Mean episode length/episode: 7.11
            Mean episode successes: 4.5166
Mean episode consecutive_successes: 11.1026
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 16.43s
                        Total time: 37556.90s
                               ETA: 990001.9s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.085s, learning 0.158s)
               Value function loss: 212219.4539
                    Surrogate loss: 0.0105
             Mean action noise std: 0.69
                       Mean reward: 2003.19
               Mean episode length: 50.18
                  Mean reward/step: 51.11
       Mean episode length/episode: 7.00
            Mean episode successes: 4.4761
Mean episode consecutive_successes: 11.0322
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 16.24s
                        Total time: 37573.14s
                               ETA: 990148.9s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.049s, learning 0.170s)
               Value function loss: 206665.1242
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 3051.74
               Mean episode length: 55.93
                  Mean reward/step: 51.38
       Mean episode length/episode: 7.08
            Mean episode successes: 4.4385
Mean episode consecutive_successes: 11.0966
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 16.22s
                        Total time: 37589.36s
                               ETA: 990295.2s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.902s, learning 0.166s)
               Value function loss: 304168.8246
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 2073.47
               Mean episode length: 51.16
                  Mean reward/step: 50.89
       Mean episode length/episode: 7.10
            Mean episode successes: 4.4575
Mean episode consecutive_successes: 11.0478
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 16.07s
                        Total time: 37605.43s
                               ETA: 990437.4s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.007s, learning 0.173s)
               Value function loss: 454350.6508
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2612.29
               Mean episode length: 52.17
                  Mean reward/step: 51.32
       Mean episode length/episode: 7.12
            Mean episode successes: 4.5479
Mean episode consecutive_successes: 11.0444
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 16.18s
                        Total time: 37621.61s
                               ETA: 990582.4s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.836s, learning 0.247s)
               Value function loss: 232896.6125
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2379.15
               Mean episode length: 51.37
                  Mean reward/step: 52.36
       Mean episode length/episode: 7.10
            Mean episode successes: 4.5703
Mean episode consecutive_successes: 11.0288
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 16.08s
                        Total time: 37637.69s
                               ETA: 990724.9s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.694s, learning 0.166s)
               Value function loss: 174535.7039
                    Surrogate loss: -0.0108
             Mean action noise std: 0.69
                       Mean reward: 3085.33
               Mean episode length: 57.19
                  Mean reward/step: 52.34
       Mean episode length/episode: 7.08
            Mean episode successes: 4.5176
Mean episode consecutive_successes: 11.0971
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 15.86s
                        Total time: 37653.55s
                               ETA: 990861.3s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.203s, learning 0.228s)
               Value function loss: 169506.8512
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 2835.84
               Mean episode length: 54.36
                  Mean reward/step: 50.41
       Mean episode length/episode: 7.02
            Mean episode successes: 4.3125
Mean episode consecutive_successes: 11.0928
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 16.43s
                        Total time: 37669.98s
                               ETA: 991012.7s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.745s, learning 0.267s)
               Value function loss: 173418.3180
                    Surrogate loss: -0.0105
             Mean action noise std: 0.69
                       Mean reward: 2936.25
               Mean episode length: 54.80
                  Mean reward/step: 52.55
       Mean episode length/episode: 7.17
            Mean episode successes: 4.5342
Mean episode consecutive_successes: 11.0995
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 16.01s
                        Total time: 37686.00s
                               ETA: 991153.0s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.154s, learning 0.167s)
               Value function loss: 186590.7395
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2970.62
               Mean episode length: 54.20
                  Mean reward/step: 51.86
       Mean episode length/episode: 7.06
            Mean episode successes: 4.3096
Mean episode consecutive_successes: 11.1770
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 16.32s
                        Total time: 37702.32s
                               ETA: 991301.3s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.114s, learning 0.160s)
               Value function loss: 205520.0211
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 3258.60
               Mean episode length: 54.59
                  Mean reward/step: 50.20
       Mean episode length/episode: 7.14
            Mean episode successes: 4.4419
Mean episode consecutive_successes: 11.1580
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 16.27s
                        Total time: 37718.59s
                               ETA: 991448.4s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.048s, learning 0.179s)
               Value function loss: 234129.7398
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 2886.60
               Mean episode length: 55.23
                  Mean reward/step: 52.06
       Mean episode length/episode: 7.07
            Mean episode successes: 4.3984
Mean episode consecutive_successes: 11.1953
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 16.23s
                        Total time: 37734.82s
                               ETA: 991594.0s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.164s, learning 0.168s)
               Value function loss: 192677.6711
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 2274.40
               Mean episode length: 52.08
                  Mean reward/step: 50.82
       Mean episode length/episode: 7.10
            Mean episode successes: 4.3726
Mean episode consecutive_successes: 11.1692
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 16.33s
                        Total time: 37751.15s
                               ETA: 991742.4s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 1205 steps/s (collection: 13.423s, learning 0.165s)
               Value function loss: 158091.6172
                    Surrogate loss: 0.0031
             Mean action noise std: 0.69
                       Mean reward: 2360.28
               Mean episode length: 53.37
                  Mean reward/step: 50.91
       Mean episode length/episode: 7.04
            Mean episode successes: 4.4253
Mean episode consecutive_successes: 11.1240
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 13.59s
                        Total time: 37764.74s
                               ETA: 991818.6s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.411s, learning 0.167s)
               Value function loss: 159919.9805
                    Surrogate loss: 0.0011
             Mean action noise std: 0.69
                       Mean reward: 3265.77
               Mean episode length: 53.74
                  Mean reward/step: 53.66
       Mean episode length/episode: 7.14
            Mean episode successes: 4.5771
Mean episode consecutive_successes: 11.1639
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 8.58s
                        Total time: 37773.32s
                               ETA: 991763.2s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.392s, learning 0.164s)
               Value function loss: 185415.6328
                    Surrogate loss: -0.0094
             Mean action noise std: 0.69
                       Mean reward: 3253.58
               Mean episode length: 57.52
                  Mean reward/step: 54.44
       Mean episode length/episode: 7.08
            Mean episode successes: 4.6177
Mean episode consecutive_successes: 11.1940
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 8.56s
                        Total time: 37781.87s
                               ETA: 991707.3s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.155s, learning 0.172s)
               Value function loss: 189608.3008
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 2811.78
               Mean episode length: 55.46
                  Mean reward/step: 54.57
       Mean episode length/episode: 7.06
            Mean episode successes: 4.4316
Mean episode consecutive_successes: 11.2698
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 8.33s
                        Total time: 37790.20s
                               ETA: 991645.4s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.634s, learning 0.165s)
               Value function loss: 178796.3555
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2984.60
               Mean episode length: 55.65
                  Mean reward/step: 52.32
       Mean episode length/episode: 7.06
            Mean episode successes: 4.5464
Mean episode consecutive_successes: 11.1354
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 8.80s
                        Total time: 37799.00s
                               ETA: 991595.8s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.758s, learning 0.161s)
               Value function loss: 175176.9883
                    Surrogate loss: -0.0130
             Mean action noise std: 0.69
                       Mean reward: 2610.24
               Mean episode length: 53.11
                  Mean reward/step: 51.74
       Mean episode length/episode: 7.03
            Mean episode successes: 4.5210
Mean episode consecutive_successes: 11.1223
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 7.92s
                        Total time: 37806.92s
                               ETA: 991523.3s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.025s, learning 0.167s)
               Value function loss: 199341.8359
                    Surrogate loss: 0.0390
             Mean action noise std: 0.69
                       Mean reward: 2525.41
               Mean episode length: 52.84
                  Mean reward/step: 49.89
       Mean episode length/episode: 7.09
            Mean episode successes: 4.4146
Mean episode consecutive_successes: 11.0976
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 8.19s
                        Total time: 37815.11s
                               ETA: 991457.9s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.336s, learning 0.233s)
               Value function loss: 181651.3824
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 2690.99
               Mean episode length: 53.94
                  Mean reward/step: 50.34
       Mean episode length/episode: 7.13
            Mean episode successes: 4.2939
Mean episode consecutive_successes: 11.1958
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 8.57s
                        Total time: 37823.68s
                               ETA: 991402.4s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.258s, learning 0.188s)
               Value function loss: 177016.8402
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 2576.36
               Mean episode length: 51.60
                  Mean reward/step: 51.30
       Mean episode length/episode: 7.07
            Mean episode successes: 4.4263
Mean episode consecutive_successes: 11.1241
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 8.45s
                        Total time: 37832.13s
                               ETA: 991343.7s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.188s, learning 0.164s)
               Value function loss: 195832.1031
                    Surrogate loss: 0.0208
             Mean action noise std: 0.69
                       Mean reward: 2645.97
               Mean episode length: 54.43
                  Mean reward/step: 53.32
       Mean episode length/episode: 7.01
            Mean episode successes: 4.5010
Mean episode consecutive_successes: 11.0397
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 8.35s
                        Total time: 37840.48s
                               ETA: 991282.6s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.147s, learning 0.164s)
               Value function loss: 187962.0125
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 2670.10
               Mean episode length: 52.59
                  Mean reward/step: 54.98
       Mean episode length/episode: 7.11
            Mean episode successes: 4.8608
Mean episode consecutive_successes: 10.9665
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 8.31s
                        Total time: 37848.79s
                               ETA: 991220.5s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.012s, learning 0.166s)
               Value function loss: 207728.6961
                    Surrogate loss: 0.0020
             Mean action noise std: 0.69
                       Mean reward: 2843.14
               Mean episode length: 53.26
                  Mean reward/step: 54.68
       Mean episode length/episode: 7.07
            Mean episode successes: 4.8794
Mean episode consecutive_successes: 11.0369
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 8.18s
                        Total time: 37856.97s
                               ETA: 991154.9s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.116s, learning 0.193s)
               Value function loss: 207419.9695
                    Surrogate loss: -0.0070
             Mean action noise std: 0.69
                       Mean reward: 3482.41
               Mean episode length: 56.24
                  Mean reward/step: 53.50
       Mean episode length/episode: 7.06
            Mean episode successes: 4.6973
Mean episode consecutive_successes: 11.1237
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 8.31s
                        Total time: 37865.28s
                               ETA: 991092.8s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.407s, learning 0.228s)
               Value function loss: 226910.7750
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 2847.68
               Mean episode length: 52.89
                  Mean reward/step: 52.71
       Mean episode length/episode: 7.02
            Mean episode successes: 4.4453
Mean episode consecutive_successes: 11.1675
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 8.63s
                        Total time: 37873.91s
                               ETA: 991039.2s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.065s, learning 0.169s)
               Value function loss: 237751.6832
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 2841.29
               Mean episode length: 54.88
                  Mean reward/step: 54.04
       Mean episode length/episode: 6.99
            Mean episode successes: 4.4976
Mean episode consecutive_successes: 11.0965
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 8.23s
                        Total time: 37882.15s
                               ETA: 990975.1s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.170s)
               Value function loss: 220112.1176
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 2718.13
               Mean episode length: 53.31
                  Mean reward/step: 53.58
       Mean episode length/episode: 7.13
            Mean episode successes: 4.3906
Mean episode consecutive_successes: 11.2329
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 8.55s
                        Total time: 37890.70s
                               ETA: 990919.5s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.123s, learning 0.162s)
               Value function loss: 273080.7344
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2500.82
               Mean episode length: 50.23
                  Mean reward/step: 53.42
       Mean episode length/episode: 7.08
            Mean episode successes: 4.5181
Mean episode consecutive_successes: 11.1681
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 8.28s
                        Total time: 37898.99s
                               ETA: 990856.8s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.632s, learning 0.165s)
               Value function loss: 216847.1309
                    Surrogate loss: 0.0012
             Mean action noise std: 0.69
                       Mean reward: 3020.84
               Mean episode length: 53.49
                  Mean reward/step: 55.17
       Mean episode length/episode: 7.07
            Mean episode successes: 4.5337
Mean episode consecutive_successes: 11.1823
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 8.80s
                        Total time: 37907.78s
                               ETA: 990807.6s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.040s, learning 0.185s)
               Value function loss: 168547.9078
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3375.88
               Mean episode length: 53.50
                  Mean reward/step: 54.14
       Mean episode length/episode: 6.98
            Mean episode successes: 4.6943
Mean episode consecutive_successes: 11.1032
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 8.23s
                        Total time: 37916.01s
                               ETA: 990743.4s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.545s, learning 0.210s)
               Value function loss: 186286.0277
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 2916.32
               Mean episode length: 54.78
                  Mean reward/step: 52.64
       Mean episode length/episode: 7.10
            Mean episode successes: 4.5713
Mean episode consecutive_successes: 11.1440
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 8.76s
                        Total time: 37924.76s
                               ETA: 990693.2s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.377s, learning 0.256s)
               Value function loss: 169594.8488
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 2811.20
               Mean episode length: 55.22
                  Mean reward/step: 53.89
       Mean episode length/episode: 7.09
            Mean episode successes: 4.6182
Mean episode consecutive_successes: 11.1858
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 8.63s
                        Total time: 37933.40s
                               ETA: 990639.7s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.152s, learning 0.160s)
               Value function loss: 171405.4652
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 3135.52
               Mean episode length: 53.13
                  Mean reward/step: 52.34
       Mean episode length/episode: 7.10
            Mean episode successes: 4.4922
Mean episode consecutive_successes: 11.2193
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 8.31s
                        Total time: 37941.71s
                               ETA: 990577.9s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.393s, learning 0.280s)
               Value function loss: 175555.2863
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 3155.34
               Mean episode length: 52.74
                  Mean reward/step: 51.44
       Mean episode length/episode: 7.02
            Mean episode successes: 4.3545
Mean episode consecutive_successes: 11.2182
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 8.67s
                        Total time: 37950.38s
                               ETA: 990525.5s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.302s, learning 0.172s)
               Value function loss: 172889.0637
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 2507.31
               Mean episode length: 52.36
                  Mean reward/step: 51.19
       Mean episode length/episode: 7.03
            Mean episode successes: 4.3130
Mean episode consecutive_successes: 11.2009
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 8.47s
                        Total time: 37958.86s
                               ETA: 990468.0s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.274s, learning 0.159s)
               Value function loss: 180349.9852
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 3415.07
               Mean episode length: 56.53
                  Mean reward/step: 49.79
       Mean episode length/episode: 7.08
            Mean episode successes: 4.1860
Mean episode consecutive_successes: 11.2447
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 8.43s
                        Total time: 37967.29s
                               ETA: 990409.4s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.172s, learning 0.170s)
               Value function loss: 173516.8727
                    Surrogate loss: -0.0111
             Mean action noise std: 0.69
                       Mean reward: 2159.54
               Mean episode length: 48.35
                  Mean reward/step: 52.16
       Mean episode length/episode: 7.06
            Mean episode successes: 4.3628
Mean episode consecutive_successes: 11.1398
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 8.34s
                        Total time: 37975.63s
                               ETA: 990348.5s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.050s, learning 0.163s)
               Value function loss: 180801.5633
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 2918.12
               Mean episode length: 53.51
                  Mean reward/step: 52.82
       Mean episode length/episode: 7.10
            Mean episode successes: 4.5054
Mean episode consecutive_successes: 11.1277
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 8.21s
                        Total time: 37983.84s
                               ETA: 990284.3s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.167s, learning 0.167s)
               Value function loss: 238031.2805
                    Surrogate loss: -0.0079
             Mean action noise std: 0.69
                       Mean reward: 2216.84
               Mean episode length: 51.43
                  Mean reward/step: 52.11
       Mean episode length/episode: 7.03
            Mean episode successes: 4.6313
Mean episode consecutive_successes: 11.0223
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 8.33s
                        Total time: 37992.18s
                               ETA: 990223.2s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.233s, learning 0.194s)
               Value function loss: 279193.3992
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 2036.50
               Mean episode length: 51.47
                  Mean reward/step: 51.48
       Mean episode length/episode: 7.03
            Mean episode successes: 4.4321
Mean episode consecutive_successes: 11.0279
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 8.43s
                        Total time: 38000.60s
                               ETA: 990164.6s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.004s, learning 0.169s)
               Value function loss: 189527.2352
                    Surrogate loss: 0.0062
             Mean action noise std: 0.69
                       Mean reward: 2994.83
               Mean episode length: 51.62
                  Mean reward/step: 49.05
       Mean episode length/episode: 7.08
            Mean episode successes: 4.3774
Mean episode consecutive_successes: 11.0468
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 8.17s
                        Total time: 38008.78s
                               ETA: 990099.4s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.259s, learning 0.168s)
               Value function loss: 169179.7188
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 2720.60
               Mean episode length: 53.28
                  Mean reward/step: 47.96
       Mean episode length/episode: 7.05
            Mean episode successes: 4.2842
Mean episode consecutive_successes: 10.9817
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 8.43s
                        Total time: 38017.20s
                               ETA: 990040.8s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.166s, learning 0.193s)
               Value function loss: 171807.1207
                    Surrogate loss: -0.0135
             Mean action noise std: 0.69
                       Mean reward: 3210.22
               Mean episode length: 56.68
                  Mean reward/step: 48.86
       Mean episode length/episode: 7.06
            Mean episode successes: 4.2334
Mean episode consecutive_successes: 10.9224
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 8.36s
                        Total time: 38025.56s
                               ETA: 989980.5s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.051s, learning 0.163s)
               Value function loss: 165273.3906
                    Surrogate loss: 0.0303
             Mean action noise std: 0.69
                       Mean reward: 2587.43
               Mean episode length: 52.45
                  Mean reward/step: 48.68
       Mean episode length/episode: 7.03
            Mean episode successes: 4.0586
Mean episode consecutive_successes: 10.9000
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 8.21s
                        Total time: 38033.78s
                               ETA: 989916.4s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.141s, learning 0.163s)
               Value function loss: 156211.1160
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 2276.82
               Mean episode length: 51.90
                  Mean reward/step: 50.48
       Mean episode length/episode: 7.02
            Mean episode successes: 4.2603
Mean episode consecutive_successes: 10.7619
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 8.30s
                        Total time: 38042.08s
                               ETA: 989854.7s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.644s, learning 0.163s)
               Value function loss: 160507.0750
                    Surrogate loss: -0.0104
             Mean action noise std: 0.69
                       Mean reward: 2429.42
               Mean episode length: 52.41
                  Mean reward/step: 50.10
       Mean episode length/episode: 7.11
            Mean episode successes: 4.4355
Mean episode consecutive_successes: 10.6573
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 8.81s
                        Total time: 38050.89s
                               ETA: 989806.2s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.267s, learning 0.166s)
               Value function loss: 165846.8211
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 2323.52
               Mean episode length: 50.26
                  Mean reward/step: 48.49
       Mean episode length/episode: 7.09
            Mean episode successes: 4.2446
Mean episode consecutive_successes: 10.7180
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 8.43s
                        Total time: 38059.32s
                               ETA: 989747.9s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.520s, learning 0.185s)
               Value function loss: 210541.7500
                    Surrogate loss: -0.0074
             Mean action noise std: 0.69
                       Mean reward: 2499.75
               Mean episode length: 52.36
                  Mean reward/step: 49.77
       Mean episode length/episode: 7.02
            Mean episode successes: 4.1235
Mean episode consecutive_successes: 10.7348
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 8.70s
                        Total time: 38068.03s
                               ETA: 989696.7s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.333s, learning 0.158s)
               Value function loss: 164830.8328
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 1517.45
               Mean episode length: 49.29
                  Mean reward/step: 49.59
       Mean episode length/episode: 7.08
            Mean episode successes: 4.3042
Mean episode consecutive_successes: 10.5788
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 8.49s
                        Total time: 38076.52s
                               ETA: 989640.1s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.560s, learning 0.247s)
               Value function loss: 179271.8516
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 2833.96
               Mean episode length: 50.98
                  Mean reward/step: 50.74
       Mean episode length/episode: 7.10
            Mean episode successes: 4.3726
Mean episode consecutive_successes: 10.6326
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 8.81s
                        Total time: 38085.33s
                               ETA: 989591.6s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.037s, learning 0.250s)
               Value function loss: 181854.3594
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 3173.11
               Mean episode length: 57.59
                  Mean reward/step: 52.24
       Mean episode length/episode: 7.08
            Mean episode successes: 4.4702
Mean episode consecutive_successes: 10.6298
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 8.29s
                        Total time: 38093.61s
                               ETA: 989529.6s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.369s, learning 0.157s)
               Value function loss: 206730.5703
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 2952.88
               Mean episode length: 53.46
                  Mean reward/step: 54.27
       Mean episode length/episode: 7.08
            Mean episode successes: 4.7549
Mean episode consecutive_successes: 10.5814
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 8.53s
                        Total time: 38102.14s
                               ETA: 989473.9s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.255s, learning 0.168s)
               Value function loss: 297890.0469
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2880.73
               Mean episode length: 53.86
                  Mean reward/step: 55.29
       Mean episode length/episode: 7.12
            Mean episode successes: 4.9360
Mean episode consecutive_successes: 10.6303
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 8.42s
                        Total time: 38110.56s
                               ETA: 989415.5s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.335s, learning 0.170s)
               Value function loss: 240707.2977
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 3188.86
               Mean episode length: 54.41
                  Mean reward/step: 54.07
       Mean episode length/episode: 7.12
            Mean episode successes: 4.7236
Mean episode consecutive_successes: 10.8322
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 8.50s
                        Total time: 38119.07s
                               ETA: 989359.3s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.366s, learning 0.217s)
               Value function loss: 231246.8234
                    Surrogate loss: 0.0144
             Mean action noise std: 0.69
                       Mean reward: 2306.82
               Mean episode length: 51.58
                  Mean reward/step: 53.31
       Mean episode length/episode: 7.08
            Mean episode successes: 4.6929
Mean episode consecutive_successes: 10.8717
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 8.58s
                        Total time: 38127.65s
                               ETA: 989305.1s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.297s, learning 0.189s)
               Value function loss: 194551.0520
                    Surrogate loss: 0.0033
             Mean action noise std: 0.69
                       Mean reward: 3000.95
               Mean episode length: 53.82
                  Mean reward/step: 52.13
       Mean episode length/episode: 7.03
            Mean episode successes: 4.5957
Mean episode consecutive_successes: 10.9430
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 8.49s
                        Total time: 38136.13s
                               ETA: 989248.4s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.151s, learning 0.161s)
               Value function loss: 178807.2207
                    Surrogate loss: 0.0080
             Mean action noise std: 0.69
                       Mean reward: 3028.03
               Mean episode length: 57.23
                  Mean reward/step: 51.13
       Mean episode length/episode: 7.15
            Mean episode successes: 4.9004
Mean episode consecutive_successes: 10.9013
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 8.31s
                        Total time: 38144.45s
                               ETA: 989187.3s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.046s, learning 0.165s)
               Value function loss: 176070.3598
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2839.71
               Mean episode length: 55.65
                  Mean reward/step: 51.81
       Mean episode length/episode: 7.09
            Mean episode successes: 4.5195
Mean episode consecutive_successes: 11.0032
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 8.21s
                        Total time: 38152.66s
                               ETA: 989123.5s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.437s, learning 0.163s)
               Value function loss: 169399.9090
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 2893.56
               Mean episode length: 54.38
                  Mean reward/step: 49.20
       Mean episode length/episode: 7.07
            Mean episode successes: 4.2197
Mean episode consecutive_successes: 11.1080
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 8.60s
                        Total time: 38161.26s
                               ETA: 989069.9s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.564s, learning 0.163s)
               Value function loss: 168839.7031
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3078.35
               Mean episode length: 54.27
                  Mean reward/step: 47.98
       Mean episode length/episode: 7.11
            Mean episode successes: 4.1577
Mean episode consecutive_successes: 11.1292
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 8.73s
                        Total time: 38169.98s
                               ETA: 989019.6s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.432s, learning 0.155s)
               Value function loss: 190512.8566
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 2889.74
               Mean episode length: 56.35
                  Mean reward/step: 49.72
       Mean episode length/episode: 7.12
            Mean episode successes: 4.3145
Mean episode consecutive_successes: 11.0318
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 8.59s
                        Total time: 38178.57s
                               ETA: 988965.7s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.074s, learning 0.212s)
               Value function loss: 218386.1496
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 2109.46
               Mean episode length: 51.65
                  Mean reward/step: 50.37
       Mean episode length/episode: 7.11
            Mean episode successes: 4.5469
Mean episode consecutive_successes: 10.9172
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 8.29s
                        Total time: 38186.86s
                               ETA: 988904.0s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.365s, learning 0.169s)
               Value function loss: 200285.5059
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 3353.61
               Mean episode length: 57.51
                  Mean reward/step: 51.85
       Mean episode length/episode: 7.05
            Mean episode successes: 4.4468
Mean episode consecutive_successes: 10.9985
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 8.53s
                        Total time: 38195.39s
                               ETA: 988848.8s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.121s, learning 0.186s)
               Value function loss: 237533.4805
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 2542.97
               Mean episode length: 53.41
                  Mean reward/step: 51.78
       Mean episode length/episode: 7.14
            Mean episode successes: 4.6357
Mean episode consecutive_successes: 10.9455
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 8.31s
                        Total time: 38203.70s
                               ETA: 988787.7s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.088s, learning 0.214s)
               Value function loss: 198919.9129
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 2625.00
               Mean episode length: 56.15
                  Mean reward/step: 51.40
       Mean episode length/episode: 7.05
            Mean episode successes: 4.6489
Mean episode consecutive_successes: 10.8982
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 8.30s
                        Total time: 38212.00s
                               ETA: 988726.5s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.388s, learning 0.247s)
               Value function loss: 160324.5824
                    Surrogate loss: -0.0095
             Mean action noise std: 0.69
                       Mean reward: 3153.75
               Mean episode length: 58.37
                  Mean reward/step: 51.78
       Mean episode length/episode: 7.16
            Mean episode successes: 4.9858
Mean episode consecutive_successes: 10.9297
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 8.63s
                        Total time: 38220.63s
                               ETA: 988674.0s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.224s, learning 0.208s)
               Value function loss: 145548.6914
                    Surrogate loss: 0.0078
             Mean action noise std: 0.69
                       Mean reward: 3459.49
               Mean episode length: 55.25
                  Mean reward/step: 50.20
       Mean episode length/episode: 7.06
            Mean episode successes: 4.6729
Mean episode consecutive_successes: 11.0408
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 8.43s
                        Total time: 38229.07s
                               ETA: 988616.2s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.316s, learning 0.171s)
               Value function loss: 149300.8957
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 2973.82
               Mean episode length: 54.60
                  Mean reward/step: 50.36
       Mean episode length/episode: 7.05
            Mean episode successes: 4.5503
Mean episode consecutive_successes: 11.0295
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 8.49s
                        Total time: 38237.56s
                               ETA: 988559.9s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.326s, learning 0.233s)
               Value function loss: 150074.5758
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 3004.70
               Mean episode length: 55.87
                  Mean reward/step: 50.12
       Mean episode length/episode: 7.11
            Mean episode successes: 4.4941
Mean episode consecutive_successes: 11.0813
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 8.56s
                        Total time: 38246.11s
                               ETA: 988505.5s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.434s, learning 0.161s)
               Value function loss: 153298.3199
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 3340.13
               Mean episode length: 56.33
                  Mean reward/step: 50.11
       Mean episode length/episode: 7.18
            Mean episode successes: 4.5415
Mean episode consecutive_successes: 11.1532
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 8.59s
                        Total time: 38254.71s
                               ETA: 988452.0s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.841s, learning 0.185s)
               Value function loss: 175114.3703
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 2890.31
               Mean episode length: 56.97
                  Mean reward/step: 54.15
       Mean episode length/episode: 7.04
            Mean episode successes: 4.5752
Mean episode consecutive_successes: 11.0989
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 8.03s
                        Total time: 38262.73s
                               ETA: 988383.8s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.240s, learning 0.169s)
               Value function loss: 168495.8223
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 2631.19
               Mean episode length: 54.39
                  Mean reward/step: 53.45
       Mean episode length/episode: 7.14
            Mean episode successes: 4.7256
Mean episode consecutive_successes: 11.0457
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 8.41s
                        Total time: 38271.14s
                               ETA: 988325.6s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.138s, learning 0.168s)
               Value function loss: 181233.5125
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 2525.89
               Mean episode length: 54.31
                  Mean reward/step: 54.79
       Mean episode length/episode: 7.11
            Mean episode successes: 4.9390
Mean episode consecutive_successes: 11.0469
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 8.31s
                        Total time: 38279.45s
                               ETA: 988264.7s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.881s, learning 0.171s)
               Value function loss: 214033.5562
                    Surrogate loss: -0.0116
             Mean action noise std: 0.69
                       Mean reward: 3597.90
               Mean episode length: 56.55
                  Mean reward/step: 57.22
       Mean episode length/episode: 7.08
            Mean episode successes: 4.8257
Mean episode consecutive_successes: 11.2233
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 8.05s
                        Total time: 38287.50s
                               ETA: 988197.3s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.250s, learning 0.161s)
               Value function loss: 311205.7039
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3504.98
               Mean episode length: 56.41
                  Mean reward/step: 57.22
       Mean episode length/episode: 7.10
            Mean episode successes: 5.0522
Mean episode consecutive_successes: 11.2436
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 8.41s
                        Total time: 38295.91s
                               ETA: 988139.3s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.349s, learning 0.161s)
               Value function loss: 359718.8734
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3056.10
               Mean episode length: 54.14
                  Mean reward/step: 55.51
       Mean episode length/episode: 7.14
            Mean episode successes: 4.9414
Mean episode consecutive_successes: 11.4401
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 8.51s
                        Total time: 38304.42s
                               ETA: 988083.7s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.237s, learning 0.172s)
               Value function loss: 233371.8402
                    Surrogate loss: 0.0047
             Mean action noise std: 0.69
                       Mean reward: 2933.62
               Mean episode length: 57.74
                  Mean reward/step: 56.79
       Mean episode length/episode: 7.09
            Mean episode successes: 5.1289
Mean episode consecutive_successes: 11.3868
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 8.41s
                        Total time: 38312.83s
                               ETA: 988025.6s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.903s, learning 0.169s)
               Value function loss: 195850.2586
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 2664.39
               Mean episode length: 55.63
                  Mean reward/step: 57.45
       Mean episode length/episode: 7.12
            Mean episode successes: 5.1191
Mean episode consecutive_successes: 11.5026
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 8.07s
                        Total time: 38320.90s
                               ETA: 987958.8s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.181s, learning 0.156s)
               Value function loss: 202601.9891
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3078.20
               Mean episode length: 53.11
                  Mean reward/step: 58.33
       Mean episode length/episode: 7.07
            Mean episode successes: 4.8340
Mean episode consecutive_successes: 11.7300
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 8.34s
                        Total time: 38329.24s
                               ETA: 987899.0s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.228s, learning 0.164s)
               Value function loss: 183344.8004
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 3272.72
               Mean episode length: 57.00
                  Mean reward/step: 56.68
       Mean episode length/episode: 7.12
            Mean episode successes: 4.8652
Mean episode consecutive_successes: 11.8018
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 8.39s
                        Total time: 38337.63s
                               ETA: 987840.5s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.862s, learning 0.165s)
               Value function loss: 173153.0039
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 3073.84
               Mean episode length: 54.17
                  Mean reward/step: 55.61
       Mean episode length/episode: 7.10
            Mean episode successes: 4.6675
Mean episode consecutive_successes: 11.8798
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 8.03s
                        Total time: 38345.66s
                               ETA: 987772.7s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.163s)
               Value function loss: 175618.4531
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 2777.55
               Mean episode length: 52.78
                  Mean reward/step: 57.62
       Mean episode length/episode: 7.14
            Mean episode successes: 5.0083
Mean episode consecutive_successes: 11.7971
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 8.29s
                        Total time: 38353.95s
                               ETA: 987711.7s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.341s, learning 0.169s)
               Value function loss: 198009.1918
                    Surrogate loss: -0.0104
             Mean action noise std: 0.69
                       Mean reward: 2786.99
               Mean episode length: 56.05
                  Mean reward/step: 58.02
       Mean episode length/episode: 7.07
            Mean episode successes: 5.0327
Mean episode consecutive_successes: 11.7826
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 8.51s
                        Total time: 38362.46s
                               ETA: 987656.4s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.093s, learning 0.185s)
               Value function loss: 225038.9977
                    Surrogate loss: -0.0156
             Mean action noise std: 0.69
                       Mean reward: 3884.26
               Mean episode length: 58.58
                  Mean reward/step: 55.15
       Mean episode length/episode: 7.02
            Mean episode successes: 4.6978
Mean episode consecutive_successes: 11.9884
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 8.28s
                        Total time: 38370.74s
                               ETA: 987595.1s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.507s, learning 0.182s)
               Value function loss: 232928.3898
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2380.11
               Mean episode length: 52.80
                  Mean reward/step: 54.83
       Mean episode length/episode: 7.22
            Mean episode successes: 4.9365
Mean episode consecutive_successes: 11.8937
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 8.69s
                        Total time: 38379.43s
                               ETA: 987544.5s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.090s, learning 0.168s)
               Value function loss: 194109.1230
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 3820.32
               Mean episode length: 56.04
                  Mean reward/step: 55.71
       Mean episode length/episode: 7.10
            Mean episode successes: 4.7661
Mean episode consecutive_successes: 12.0816
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 8.26s
                        Total time: 38387.69s
                               ETA: 987482.7s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.610s, learning 0.310s)
               Value function loss: 197722.8027
                    Surrogate loss: -0.0143
             Mean action noise std: 0.69
                       Mean reward: 2825.48
               Mean episode length: 53.60
                  Mean reward/step: 57.66
       Mean episode length/episode: 7.11
            Mean episode successes: 4.8896
Mean episode consecutive_successes: 12.0418
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 8.92s
                        Total time: 38396.61s
                               ETA: 987438.1s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.166s, learning 0.216s)
               Value function loss: 217819.0121
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 2964.48
               Mean episode length: 55.44
                  Mean reward/step: 60.97
       Mean episode length/episode: 7.09
            Mean episode successes: 4.9897
Mean episode consecutive_successes: 12.0585
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 8.38s
                        Total time: 38404.99s
                               ETA: 987379.6s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.658s, learning 0.166s)
               Value function loss: 214498.6453
                    Surrogate loss: 0.0033
             Mean action noise std: 0.69
                       Mean reward: 3870.41
               Mean episode length: 56.72
                  Mean reward/step: 59.85
       Mean episode length/episode: 7.14
            Mean episode successes: 5.2339
Mean episode consecutive_successes: 12.1138
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 8.82s
                        Total time: 38413.81s
                               ETA: 987332.4s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.791s, learning 0.172s)
               Value function loss: 209349.4102
                    Surrogate loss: -0.0095
             Mean action noise std: 0.69
                       Mean reward: 3350.04
               Mean episode length: 56.23
                  Mean reward/step: 57.66
       Mean episode length/episode: 7.14
            Mean episode successes: 5.1870
Mean episode consecutive_successes: 12.1791
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 7.96s
                        Total time: 38421.78s
                               ETA: 987263.2s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.153s, learning 0.157s)
               Value function loss: 199820.8801
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 2644.42
               Mean episode length: 52.06
                  Mean reward/step: 57.34
       Mean episode length/episode: 7.11
            Mean episode successes: 5.2871
Mean episode consecutive_successes: 12.1243
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 8.31s
                        Total time: 38430.09s
                               ETA: 987203.0s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.453s, learning 0.187s)
               Value function loss: 214622.4488
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 3443.38
               Mean episode length: 57.30
                  Mean reward/step: 56.07
       Mean episode length/episode: 7.10
            Mean episode successes: 4.9683
Mean episode consecutive_successes: 12.2843
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 8.64s
                        Total time: 38438.73s
                               ETA: 987151.2s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.937s, learning 0.166s)
               Value function loss: 202896.9406
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 3139.80
               Mean episode length: 56.40
                  Mean reward/step: 56.30
       Mean episode length/episode: 7.04
            Mean episode successes: 4.6030
Mean episode consecutive_successes: 12.4233
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 8.10s
                        Total time: 38446.83s
                               ETA: 987085.7s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.164s)
               Value function loss: 190646.1801
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 3036.79
               Mean episode length: 55.37
                  Mean reward/step: 55.09
       Mean episode length/episode: 7.10
            Mean episode successes: 4.6270
Mean episode consecutive_successes: 12.3612
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 8.29s
                        Total time: 38455.12s
                               ETA: 987025.1s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.165s, learning 0.191s)
               Value function loss: 240911.4586
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 3194.38
               Mean episode length: 55.12
                  Mean reward/step: 55.51
       Mean episode length/episode: 7.12
            Mean episode successes: 4.7695
Mean episode consecutive_successes: 12.3311
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 8.36s
                        Total time: 38463.48s
                               ETA: 986966.1s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.237s, learning 0.169s)
               Value function loss: 397963.6078
                    Surrogate loss: 0.0132
             Mean action noise std: 0.69
                       Mean reward: 2710.87
               Mean episode length: 54.13
                  Mean reward/step: 55.94
       Mean episode length/episode: 7.06
            Mean episode successes: 4.7256
Mean episode consecutive_successes: 12.2732
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 8.41s
                        Total time: 38471.89s
                               ETA: 986908.5s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.525s, learning 0.160s)
               Value function loss: 201302.4840
                    Surrogate loss: 0.0012
             Mean action noise std: 0.69
                       Mean reward: 3217.01
               Mean episode length: 55.35
                  Mean reward/step: 55.60
       Mean episode length/episode: 7.12
            Mean episode successes: 4.8198
Mean episode consecutive_successes: 12.2694
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 8.68s
                        Total time: 38480.57s
                               ETA: 986858.0s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.509s, learning 0.159s)
               Value function loss: 181757.2941
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 2427.79
               Mean episode length: 52.77
                  Mean reward/step: 56.15
       Mean episode length/episode: 7.11
            Mean episode successes: 4.7490
Mean episode consecutive_successes: 12.2174
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 8.67s
                        Total time: 38489.24s
                               ETA: 986807.1s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.940s, learning 0.161s)
               Value function loss: 185096.6523
                    Surrogate loss: 0.0087
             Mean action noise std: 0.69
                       Mean reward: 2692.34
               Mean episode length: 52.11
                  Mean reward/step: 55.84
       Mean episode length/episode: 7.15
            Mean episode successes: 4.8428
Mean episode consecutive_successes: 12.2445
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 8.10s
                        Total time: 38497.34s
                               ETA: 986741.7s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.319s, learning 0.158s)
               Value function loss: 175247.9789
                    Surrogate loss: -0.0083
             Mean action noise std: 0.69
                       Mean reward: 2948.10
               Mean episode length: 57.47
                  Mean reward/step: 56.00
       Mean episode length/episode: 7.12
            Mean episode successes: 4.9678
Mean episode consecutive_successes: 12.2192
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 8.48s
                        Total time: 38505.82s
                               ETA: 986685.9s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.092s, learning 0.188s)
               Value function loss: 181691.2062
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 3426.85
               Mean episode length: 56.00
                  Mean reward/step: 56.31
       Mean episode length/episode: 7.08
            Mean episode successes: 4.9097
Mean episode consecutive_successes: 12.2495
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 8.28s
                        Total time: 38514.10s
                               ETA: 986625.2s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.089s, learning 0.189s)
               Value function loss: 185665.0395
                    Surrogate loss: -0.0129
             Mean action noise std: 0.69
                       Mean reward: 3336.96
               Mean episode length: 55.94
                  Mean reward/step: 56.64
       Mean episode length/episode: 7.06
            Mean episode successes: 4.9019
Mean episode consecutive_successes: 12.3040
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 8.28s
                        Total time: 38522.38s
                               ETA: 986564.4s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.173s, learning 0.202s)
               Value function loss: 224135.6168
                    Surrogate loss: -0.0091
             Mean action noise std: 0.69
                       Mean reward: 3722.18
               Mean episode length: 58.90
                  Mean reward/step: 56.26
       Mean episode length/episode: 7.06
            Mean episode successes: 4.6597
Mean episode consecutive_successes: 12.4086
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 8.37s
                        Total time: 38530.75s
                               ETA: 986506.1s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.205s, learning 0.175s)
               Value function loss: 186847.2359
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 2619.72
               Mean episode length: 56.73
                  Mean reward/step: 54.99
       Mean episode length/episode: 7.15
            Mean episode successes: 4.6812
Mean episode consecutive_successes: 12.3728
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 8.38s
                        Total time: 38539.13s
                               ETA: 986448.0s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.780s, learning 0.159s)
               Value function loss: 161298.9188
                    Surrogate loss: 0.0111
             Mean action noise std: 0.69
                       Mean reward: 2864.44
               Mean episode length: 55.78
                  Mean reward/step: 55.05
       Mean episode length/episode: 7.14
            Mean episode successes: 4.7183
Mean episode consecutive_successes: 12.3708
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 7.94s
                        Total time: 38547.07s
                               ETA: 986378.6s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.275s, learning 0.160s)
               Value function loss: 168645.9793
                    Surrogate loss: -0.0110
             Mean action noise std: 0.69
                       Mean reward: 3041.37
               Mean episode length: 54.91
                  Mean reward/step: 55.49
       Mean episode length/episode: 7.12
            Mean episode successes: 4.8970
Mean episode consecutive_successes: 12.3300
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 8.43s
                        Total time: 38555.50s
                               ETA: 986321.9s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.919s, learning 0.159s)
               Value function loss: 171913.2137
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 3092.03
               Mean episode length: 55.27
                  Mean reward/step: 57.75
       Mean episode length/episode: 7.17
            Mean episode successes: 5.0361
Mean episode consecutive_successes: 12.3429
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 8.08s
                        Total time: 38563.58s
                               ETA: 986256.2s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.141s, learning 0.170s)
               Value function loss: 198361.3324
                    Surrogate loss: 0.0178
             Mean action noise std: 0.69
                       Mean reward: 2769.21
               Mean episode length: 54.70
                  Mean reward/step: 57.74
       Mean episode length/episode: 7.09
            Mean episode successes: 4.7354
Mean episode consecutive_successes: 12.4273
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 8.31s
                        Total time: 38571.89s
                               ETA: 986196.4s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.153s, learning 0.173s)
               Value function loss: 183373.7484
                    Surrogate loss: -0.0083
             Mean action noise std: 0.69
                       Mean reward: 3078.26
               Mean episode length: 57.48
                  Mean reward/step: 56.92
       Mean episode length/episode: 7.15
            Mean episode successes: 4.9038
Mean episode consecutive_successes: 12.4477
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 8.33s
                        Total time: 38580.22s
                               ETA: 986137.0s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.176s, learning 0.286s)
               Value function loss: 185090.6875
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 3735.78
               Mean episode length: 57.60
                  Mean reward/step: 55.45
       Mean episode length/episode: 7.09
            Mean episode successes: 4.8447
Mean episode consecutive_successes: 12.4431
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 8.46s
                        Total time: 38588.68s
                               ETA: 986081.2s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.151s, learning 0.170s)
               Value function loss: 251182.4359
                    Surrogate loss: -0.0101
             Mean action noise std: 0.69
                       Mean reward: 3092.21
               Mean episode length: 56.18
                  Mean reward/step: 53.83
       Mean episode length/episode: 7.13
            Mean episode successes: 4.7456
Mean episode consecutive_successes: 12.4004
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 8.32s
                        Total time: 38597.00s
                               ETA: 986021.7s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.170s, learning 0.161s)
               Value function loss: 229779.4195
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2874.69
               Mean episode length: 55.37
                  Mean reward/step: 52.15
       Mean episode length/episode: 7.05
            Mean episode successes: 4.4561
Mean episode consecutive_successes: 12.4246
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 8.33s
                        Total time: 38605.33s
                               ETA: 985962.6s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.161s)
               Value function loss: 196292.1047
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 3211.97
               Mean episode length: 55.43
                  Mean reward/step: 52.82
       Mean episode length/episode: 7.16
            Mean episode successes: 4.5903
Mean episode consecutive_successes: 12.3836
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 8.43s
                        Total time: 38613.76s
                               ETA: 985905.9s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.091s, learning 0.183s)
               Value function loss: 230163.1531
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 2606.03
               Mean episode length: 54.47
                  Mean reward/step: 53.74
       Mean episode length/episode: 7.15
            Mean episode successes: 4.5625
Mean episode consecutive_successes: 12.3123
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 8.27s
                        Total time: 38622.03s
                               ETA: 985845.3s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.263s, learning 0.214s)
               Value function loss: 174757.5789
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 2806.42
               Mean episode length: 54.41
                  Mean reward/step: 55.44
       Mean episode length/episode: 7.12
            Mean episode successes: 4.7168
Mean episode consecutive_successes: 12.2538
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 8.48s
                        Total time: 38630.51s
                               ETA: 985790.0s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.177s, learning 0.179s)
               Value function loss: 165862.9750
                    Surrogate loss: -0.0094
             Mean action noise std: 0.69
                       Mean reward: 3262.43
               Mean episode length: 55.41
                  Mean reward/step: 56.26
       Mean episode length/episode: 7.09
            Mean episode successes: 4.7251
Mean episode consecutive_successes: 12.2415
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 8.36s
                        Total time: 38638.87s
                               ETA: 985731.6s

################################################################################
                    [1m Learning iteration 3772/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.345s, learning 0.163s)
               Value function loss: 183674.9234
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 2872.09
               Mean episode length: 56.40
                  Mean reward/step: 57.22
       Mean episode length/episode: 7.11
            Mean episode successes: 4.9136
Mean episode consecutive_successes: 12.1666
--------------------------------------------------------------------------------
                   Total timesteps: 61816832
                    Iteration time: 8.51s
                        Total time: 38647.37s
                               ETA: 985677.1s

################################################################################
                    [1m Learning iteration 3773/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.272s, learning 0.166s)
               Value function loss: 196362.3012
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 2782.78
               Mean episode length: 52.80
                  Mean reward/step: 54.66
       Mean episode length/episode: 7.06
            Mean episode successes: 4.7817
Mean episode consecutive_successes: 12.2081
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 8.44s
                        Total time: 38655.81s
                               ETA: 985620.8s

################################################################################
                    [1m Learning iteration 3774/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.131s, learning 0.176s)
               Value function loss: 223574.8977
                    Surrogate loss: -0.0101
             Mean action noise std: 0.69
                       Mean reward: 3217.63
               Mean episode length: 56.41
                  Mean reward/step: 53.40
       Mean episode length/episode: 7.07
            Mean episode successes: 4.4629
Mean episode consecutive_successes: 12.2844
--------------------------------------------------------------------------------
                   Total timesteps: 61849600
                    Iteration time: 8.31s
                        Total time: 38664.12s
                               ETA: 985561.2s

################################################################################
                    [1m Learning iteration 3775/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.128s, learning 0.174s)
               Value function loss: 213906.7664
                    Surrogate loss: 0.0032
             Mean action noise std: 0.69
                       Mean reward: 3408.44
               Mean episode length: 57.11
                  Mean reward/step: 53.19
       Mean episode length/episode: 7.11
            Mean episode successes: 4.5103
Mean episode consecutive_successes: 12.2545
--------------------------------------------------------------------------------
                   Total timesteps: 61865984
                    Iteration time: 8.30s
                        Total time: 38672.42s
                               ETA: 985501.5s

################################################################################
                    [1m Learning iteration 3776/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.399s, learning 0.161s)
               Value function loss: 157937.1066
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 3463.86
               Mean episode length: 58.15
                  Mean reward/step: 53.53
       Mean episode length/episode: 7.04
            Mean episode successes: 4.4678
Mean episode consecutive_successes: 12.2836
--------------------------------------------------------------------------------
                   Total timesteps: 61882368
                    Iteration time: 8.56s
                        Total time: 38680.98s
                               ETA: 985448.4s

################################################################################
                    [1m Learning iteration 3777/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.181s, learning 0.193s)
               Value function loss: 148010.3758
                    Surrogate loss: 0.0347
             Mean action noise std: 0.69
                       Mean reward: 3628.39
               Mean episode length: 57.44
                  Mean reward/step: 55.57
       Mean episode length/episode: 7.18
            Mean episode successes: 4.7241
Mean episode consecutive_successes: 12.2086
--------------------------------------------------------------------------------
                   Total timesteps: 61898752
                    Iteration time: 8.37s
                        Total time: 38689.36s
                               ETA: 985390.6s

################################################################################
                    [1m Learning iteration 3778/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.176s, learning 0.161s)
               Value function loss: 152219.8434
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2759.10
               Mean episode length: 55.51
                  Mean reward/step: 59.50
       Mean episode length/episode: 7.10
            Mean episode successes: 4.9385
Mean episode consecutive_successes: 12.1278
--------------------------------------------------------------------------------
                   Total timesteps: 61915136
                    Iteration time: 8.34s
                        Total time: 38697.69s
                               ETA: 985331.9s

################################################################################
                    [1m Learning iteration 3779/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.001s, learning 0.195s)
               Value function loss: 166276.2363
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 3738.74
               Mean episode length: 58.29
                  Mean reward/step: 57.79
       Mean episode length/episode: 7.15
            Mean episode successes: 5.0776
Mean episode consecutive_successes: 12.2006
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 8.20s
                        Total time: 38705.89s
                               ETA: 985269.7s

################################################################################
                    [1m Learning iteration 3780/100000 [0m                    

                       Computation: 1988 steps/s (collection: 7.983s, learning 0.256s)
               Value function loss: 163778.4703
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 3204.44
               Mean episode length: 55.29
                  Mean reward/step: 58.11
       Mean episode length/episode: 7.14
            Mean episode successes: 5.0264
Mean episode consecutive_successes: 12.2693
--------------------------------------------------------------------------------
                   Total timesteps: 61947904
                    Iteration time: 8.24s
                        Total time: 38714.13s
                               ETA: 985208.5s

################################################################################
                    [1m Learning iteration 3781/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.181s, learning 0.169s)
               Value function loss: 171971.1875
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3099.47
               Mean episode length: 55.95
                  Mean reward/step: 56.92
       Mean episode length/episode: 7.13
            Mean episode successes: 5.0269
Mean episode consecutive_successes: 12.3114
--------------------------------------------------------------------------------
                   Total timesteps: 61964288
                    Iteration time: 8.35s
                        Total time: 38722.48s
                               ETA: 985150.2s

################################################################################
                    [1m Learning iteration 3782/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.204s, learning 0.200s)
               Value function loss: 184019.3664
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 3625.83
               Mean episode length: 57.33
                  Mean reward/step: 58.68
       Mean episode length/episode: 7.14
            Mean episode successes: 5.0044
Mean episode consecutive_successes: 12.4550
--------------------------------------------------------------------------------
                   Total timesteps: 61980672
                    Iteration time: 8.40s
                        Total time: 38730.88s
                               ETA: 985093.3s

################################################################################
                    [1m Learning iteration 3783/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.280s, learning 0.178s)
               Value function loss: 194330.1531
                    Surrogate loss: 0.0015
             Mean action noise std: 0.69
                       Mean reward: 2871.57
               Mean episode length: 58.11
                  Mean reward/step: 57.96
       Mean episode length/episode: 7.08
            Mean episode successes: 4.7251
Mean episode consecutive_successes: 12.6064
--------------------------------------------------------------------------------
                   Total timesteps: 61997056
                    Iteration time: 8.46s
                        Total time: 38739.34s
                               ETA: 985037.8s

################################################################################
                    [1m Learning iteration 3784/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.288s, learning 0.172s)
               Value function loss: 179032.9586
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 3133.42
               Mean episode length: 57.74
                  Mean reward/step: 57.38
       Mean episode length/episode: 7.07
            Mean episode successes: 4.7432
Mean episode consecutive_successes: 12.5926
--------------------------------------------------------------------------------
                   Total timesteps: 62013440
                    Iteration time: 8.46s
                        Total time: 38747.80s
                               ETA: 984982.4s

################################################################################
                    [1m Learning iteration 3785/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.182s, learning 0.171s)
               Value function loss: 208580.4809
                    Surrogate loss: 0.0026
             Mean action noise std: 0.69
                       Mean reward: 3593.15
               Mean episode length: 57.21
                  Mean reward/step: 56.52
       Mean episode length/episode: 7.13
            Mean episode successes: 4.7861
Mean episode consecutive_successes: 12.5897
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 8.35s
                        Total time: 38756.15s
                               ETA: 984924.2s

################################################################################
                    [1m Learning iteration 3786/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.062s, learning 0.180s)
               Value function loss: 190908.1637
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 2465.18
               Mean episode length: 56.66
                  Mean reward/step: 54.89
       Mean episode length/episode: 7.11
            Mean episode successes: 4.6587
Mean episode consecutive_successes: 12.5711
--------------------------------------------------------------------------------
                   Total timesteps: 62046208
                    Iteration time: 8.24s
                        Total time: 38764.39s
                               ETA: 984863.3s

################################################################################
                    [1m Learning iteration 3787/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.284s, learning 0.164s)
               Value function loss: 250645.5316
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 2997.36
               Mean episode length: 56.17
                  Mean reward/step: 55.29
       Mean episode length/episode: 7.13
            Mean episode successes: 4.7041
Mean episode consecutive_successes: 12.5543
--------------------------------------------------------------------------------
                   Total timesteps: 62062592
                    Iteration time: 8.45s
                        Total time: 38772.84s
                               ETA: 984807.7s

################################################################################
                    [1m Learning iteration 3788/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.906s, learning 0.162s)
               Value function loss: 253463.5906
                    Surrogate loss: 0.0047
             Mean action noise std: 0.69
                       Mean reward: 3351.34
               Mean episode length: 57.67
                  Mean reward/step: 54.95
       Mean episode length/episode: 7.11
            Mean episode successes: 4.6221
Mean episode consecutive_successes: 12.6311
--------------------------------------------------------------------------------
                   Total timesteps: 62078976
                    Iteration time: 8.07s
                        Total time: 38780.91s
                               ETA: 984742.4s

################################################################################
                    [1m Learning iteration 3789/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.168s, learning 0.232s)
               Value function loss: 235922.2020
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2436.29
               Mean episode length: 54.30
                  Mean reward/step: 57.25
       Mean episode length/episode: 7.09
            Mean episode successes: 4.9814
Mean episode consecutive_successes: 12.4060
--------------------------------------------------------------------------------
                   Total timesteps: 62095360
                    Iteration time: 8.40s
                        Total time: 38789.31s
                               ETA: 984685.6s

################################################################################
                    [1m Learning iteration 3790/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.035s, learning 0.214s)
               Value function loss: 192002.1055
                    Surrogate loss: 0.0105
             Mean action noise std: 0.69
                       Mean reward: 3266.05
               Mean episode length: 57.45
                  Mean reward/step: 57.14
       Mean episode length/episode: 7.12
            Mean episode successes: 5.0342
Mean episode consecutive_successes: 12.4228
--------------------------------------------------------------------------------
                   Total timesteps: 62111744
                    Iteration time: 8.25s
                        Total time: 38797.56s
                               ETA: 984624.9s

################################################################################
                    [1m Learning iteration 3791/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.209s, learning 0.167s)
               Value function loss: 183332.0832
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 3195.67
               Mean episode length: 57.19
                  Mean reward/step: 56.84
       Mean episode length/episode: 7.07
            Mean episode successes: 4.9771
Mean episode consecutive_successes: 12.4073
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 8.38s
                        Total time: 38805.94s
                               ETA: 984567.6s

################################################################################
                    [1m Learning iteration 3792/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.015s, learning 0.175s)
               Value function loss: 174673.7934
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 2529.07
               Mean episode length: 54.45
                  Mean reward/step: 57.00
       Mean episode length/episode: 7.16
            Mean episode successes: 5.0137
Mean episode consecutive_successes: 12.4438
--------------------------------------------------------------------------------
                   Total timesteps: 62144512
                    Iteration time: 8.19s
                        Total time: 38814.12s
                               ETA: 984505.5s

################################################################################
                    [1m Learning iteration 3793/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.242s, learning 0.235s)
               Value function loss: 199481.0805
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 3284.58
               Mean episode length: 55.95
                  Mean reward/step: 57.69
       Mean episode length/episode: 7.16
            Mean episode successes: 4.9907
Mean episode consecutive_successes: 12.5548
--------------------------------------------------------------------------------
                   Total timesteps: 62160896
                    Iteration time: 8.48s
                        Total time: 38822.60s
                               ETA: 984450.7s

################################################################################
                    [1m Learning iteration 3794/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.436s, learning 0.168s)
               Value function loss: 226785.7801
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 2778.62
               Mean episode length: 54.19
                  Mean reward/step: 56.43
       Mean episode length/episode: 7.05
            Mean episode successes: 4.8394
Mean episode consecutive_successes: 12.5133
--------------------------------------------------------------------------------
                   Total timesteps: 62177280
                    Iteration time: 8.60s
                        Total time: 38831.21s
                               ETA: 984399.2s

################################################################################
                    [1m Learning iteration 3795/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.086s, learning 0.160s)
               Value function loss: 188191.6430
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 3268.83
               Mean episode length: 58.01
                  Mean reward/step: 58.08
       Mean episode length/episode: 7.07
            Mean episode successes: 4.8481
Mean episode consecutive_successes: 12.5690
--------------------------------------------------------------------------------
                   Total timesteps: 62193664
                    Iteration time: 8.25s
                        Total time: 38839.45s
                               ETA: 984338.6s

################################################################################
                    [1m Learning iteration 3796/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.330s, learning 0.160s)
               Value function loss: 191650.3637
                    Surrogate loss: 0.0013
             Mean action noise std: 0.69
                       Mean reward: 3877.23
               Mean episode length: 59.73
                  Mean reward/step: 59.85
       Mean episode length/episode: 7.16
            Mean episode successes: 5.0366
Mean episode consecutive_successes: 12.6238
--------------------------------------------------------------------------------
                   Total timesteps: 62210048
                    Iteration time: 8.49s
                        Total time: 38847.94s
                               ETA: 984284.3s

################################################################################
                    [1m Learning iteration 3797/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.364s, learning 0.177s)
               Value function loss: 168719.2867
                    Surrogate loss: 0.0382
             Mean action noise std: 0.69
                       Mean reward: 2906.11
               Mean episode length: 53.79
                  Mean reward/step: 60.45
       Mean episode length/episode: 7.10
            Mean episode successes: 5.2065
Mean episode consecutive_successes: 12.5888
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 8.54s
                        Total time: 38856.48s
                               ETA: 984231.2s

################################################################################
                    [1m Learning iteration 3798/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.368s, learning 0.167s)
               Value function loss: 173705.6418
                    Surrogate loss: 0.0120
             Mean action noise std: 0.69
                       Mean reward: 3035.14
               Mean episode length: 56.11
                  Mean reward/step: 60.89
       Mean episode length/episode: 7.11
            Mean episode successes: 5.2544
Mean episode consecutive_successes: 12.5569
--------------------------------------------------------------------------------
                   Total timesteps: 62242816
                    Iteration time: 8.53s
                        Total time: 38865.02s
                               ETA: 984178.0s

################################################################################
                    [1m Learning iteration 3799/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.032s, learning 0.222s)
               Value function loss: 189204.6672
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 3666.57
               Mean episode length: 58.29
                  Mean reward/step: 61.24
       Mean episode length/episode: 7.05
            Mean episode successes: 5.2642
Mean episode consecutive_successes: 12.6819
--------------------------------------------------------------------------------
                   Total timesteps: 62259200
                    Iteration time: 8.25s
                        Total time: 38873.27s
                               ETA: 984117.8s

################################################################################
                    [1m Learning iteration 3800/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.045s, learning 0.271s)
               Value function loss: 188528.6434
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 4379.82
               Mean episode length: 59.07
                  Mean reward/step: 59.64
       Mean episode length/episode: 7.23
            Mean episode successes: 5.2695
Mean episode consecutive_successes: 12.9087
--------------------------------------------------------------------------------
                   Total timesteps: 62275584
                    Iteration time: 8.32s
                        Total time: 38881.59s
                               ETA: 984059.1s

################################################################################
                    [1m Learning iteration 3801/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.442s, learning 0.170s)
               Value function loss: 183719.6176
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 3033.61
               Mean episode length: 54.30
                  Mean reward/step: 59.65
       Mean episode length/episode: 7.14
            Mean episode successes: 5.4023
Mean episode consecutive_successes: 12.8206
--------------------------------------------------------------------------------
                   Total timesteps: 62291968
                    Iteration time: 8.61s
                        Total time: 38890.20s
                               ETA: 984007.9s

################################################################################
                    [1m Learning iteration 3802/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.414s, learning 0.175s)
               Value function loss: 201961.2785
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 2895.57
               Mean episode length: 53.56
                  Mean reward/step: 58.55
       Mean episode length/episode: 7.05
            Mean episode successes: 5.1299
Mean episode consecutive_successes: 12.8515
--------------------------------------------------------------------------------
                   Total timesteps: 62308352
                    Iteration time: 8.59s
                        Total time: 38898.79s
                               ETA: 983956.2s

################################################################################
                    [1m Learning iteration 3803/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.970s, learning 0.173s)
               Value function loss: 219385.0496
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 3424.61
               Mean episode length: 58.99
                  Mean reward/step: 58.28
       Mean episode length/episode: 7.03
            Mean episode successes: 4.8730
Mean episode consecutive_successes: 12.9377
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 8.14s
                        Total time: 38906.93s
                               ETA: 983893.3s

################################################################################
                    [1m Learning iteration 3804/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.041s, learning 0.185s)
               Value function loss: 189110.1840
                    Surrogate loss: 0.0026
             Mean action noise std: 0.69
                       Mean reward: 3340.76
               Mean episode length: 57.55
                  Mean reward/step: 54.50
       Mean episode length/episode: 7.15
            Mean episode successes: 4.8955
Mean episode consecutive_successes: 12.9148
--------------------------------------------------------------------------------
                   Total timesteps: 62341120
                    Iteration time: 8.23s
                        Total time: 38915.16s
                               ETA: 983832.5s

################################################################################
                    [1m Learning iteration 3805/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.161s, learning 0.255s)
               Value function loss: 193439.7687
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 3009.87
               Mean episode length: 56.58
                  Mean reward/step: 55.04
       Mean episode length/episode: 7.15
            Mean episode successes: 4.7188
Mean episode consecutive_successes: 12.9111
--------------------------------------------------------------------------------
                   Total timesteps: 62357504
                    Iteration time: 8.42s
                        Total time: 38923.57s
                               ETA: 983776.4s

################################################################################
                    [1m Learning iteration 3806/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.382s, learning 0.167s)
               Value function loss: 224950.2402
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 3282.40
               Mean episode length: 55.69
                  Mean reward/step: 56.37
       Mean episode length/episode: 7.16
            Mean episode successes: 4.8921
Mean episode consecutive_successes: 12.8793
--------------------------------------------------------------------------------
                   Total timesteps: 62373888
                    Iteration time: 8.55s
                        Total time: 38932.12s
                               ETA: 983723.8s

################################################################################
                    [1m Learning iteration 3807/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.934s, learning 0.181s)
               Value function loss: 289182.2539
                    Surrogate loss: 0.0060
             Mean action noise std: 0.69
                       Mean reward: 3177.42
               Mean episode length: 55.92
                  Mean reward/step: 56.51
       Mean episode length/episode: 7.15
            Mean episode successes: 4.7524
Mean episode consecutive_successes: 12.9288
--------------------------------------------------------------------------------
                   Total timesteps: 62390272
                    Iteration time: 8.11s
                        Total time: 38940.24s
                               ETA: 983660.3s

################################################################################
                    [1m Learning iteration 3808/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.874s, learning 0.192s)
               Value function loss: 213767.7734
                    Surrogate loss: -0.0067
             Mean action noise std: 0.69
                       Mean reward: 2592.32
               Mean episode length: 52.49
                  Mean reward/step: 57.91
       Mean episode length/episode: 7.09
            Mean episode successes: 5.0024
Mean episode consecutive_successes: 12.7766
--------------------------------------------------------------------------------
                   Total timesteps: 62406656
                    Iteration time: 8.07s
                        Total time: 38948.30s
                               ETA: 983595.5s

################################################################################
                    [1m Learning iteration 3809/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.334s, learning 0.177s)
               Value function loss: 199428.7293
                    Surrogate loss: 0.0065
             Mean action noise std: 0.69
                       Mean reward: 3376.42
               Mean episode length: 58.29
                  Mean reward/step: 58.16
       Mean episode length/episode: 7.13
            Mean episode successes: 5.1035
Mean episode consecutive_successes: 12.7086
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 8.51s
                        Total time: 38956.81s
                               ETA: 983542.0s

################################################################################
                    [1m Learning iteration 3810/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.898s, learning 0.160s)
               Value function loss: 173785.2832
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 3516.55
               Mean episode length: 58.33
                  Mean reward/step: 57.39
       Mean episode length/episode: 7.17
            Mean episode successes: 5.0884
Mean episode consecutive_successes: 12.8027
--------------------------------------------------------------------------------
                   Total timesteps: 62439424
                    Iteration time: 8.06s
                        Total time: 38964.87s
                               ETA: 983477.1s

################################################################################
                    [1m Learning iteration 3811/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.500s, learning 0.202s)
               Value function loss: 174034.0242
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2787.18
               Mean episode length: 55.63
                  Mean reward/step: 57.27
       Mean episode length/episode: 7.07
            Mean episode successes: 4.9390
Mean episode consecutive_successes: 12.7948
--------------------------------------------------------------------------------
                   Total timesteps: 62455808
                    Iteration time: 8.70s
                        Total time: 38973.57s
                               ETA: 983428.4s

################################################################################
                    [1m Learning iteration 3812/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.111s, learning 0.178s)
               Value function loss: 171305.8816
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 3132.27
               Mean episode length: 56.02
                  Mean reward/step: 55.05
       Mean episode length/episode: 7.14
            Mean episode successes: 4.8066
Mean episode consecutive_successes: 12.8026
--------------------------------------------------------------------------------
                   Total timesteps: 62472192
                    Iteration time: 8.29s
                        Total time: 38981.86s
                               ETA: 983369.4s

################################################################################
                    [1m Learning iteration 3813/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.265s, learning 0.197s)
               Value function loss: 168736.9582
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 2750.03
               Mean episode length: 53.10
                  Mean reward/step: 56.51
       Mean episode length/episode: 7.21
            Mean episode successes: 4.9556
Mean episode consecutive_successes: 12.7859
--------------------------------------------------------------------------------
                   Total timesteps: 62488576
                    Iteration time: 8.46s
                        Total time: 38990.33s
                               ETA: 983314.7s

################################################################################
                    [1m Learning iteration 3814/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.129s, learning 0.170s)
               Value function loss: 174877.0434
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 2962.75
               Mean episode length: 53.08
                  Mean reward/step: 54.85
       Mean episode length/episode: 7.08
            Mean episode successes: 4.7808
Mean episode consecutive_successes: 12.8105
--------------------------------------------------------------------------------
                   Total timesteps: 62504960
                    Iteration time: 8.30s
                        Total time: 38998.62s
                               ETA: 983256.0s

################################################################################
                    [1m Learning iteration 3815/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.194s, learning 0.171s)
               Value function loss: 179706.6187
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2673.35
               Mean episode length: 53.84
                  Mean reward/step: 56.35
       Mean episode length/episode: 7.14
            Mean episode successes: 4.9473
Mean episode consecutive_successes: 12.7018
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 8.36s
                        Total time: 39006.99s
                               ETA: 983198.9s

################################################################################
                    [1m Learning iteration 3816/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.159s, learning 0.169s)
               Value function loss: 190191.0746
                    Surrogate loss: -0.0087
             Mean action noise std: 0.69
                       Mean reward: 3375.24
               Mean episode length: 56.74
                  Mean reward/step: 54.07
       Mean episode length/episode: 7.07
            Mean episode successes: 4.6899
Mean episode consecutive_successes: 12.7393
--------------------------------------------------------------------------------
                   Total timesteps: 62537728
                    Iteration time: 8.33s
                        Total time: 39015.32s
                               ETA: 983141.0s

################################################################################
                    [1m Learning iteration 3817/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.251s, learning 0.171s)
               Value function loss: 191705.2293
                    Surrogate loss: -0.0151
             Mean action noise std: 0.69
                       Mean reward: 2851.28
               Mean episode length: 54.32
                  Mean reward/step: 55.43
       Mean episode length/episode: 7.10
            Mean episode successes: 4.8301
Mean episode consecutive_successes: 12.6364
--------------------------------------------------------------------------------
                   Total timesteps: 62554112
                    Iteration time: 8.42s
                        Total time: 39023.74s
                               ETA: 983085.5s

################################################################################
                    [1m Learning iteration 3818/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.124s, learning 0.218s)
               Value function loss: 245668.2805
                    Surrogate loss: -0.0094
             Mean action noise std: 0.69
                       Mean reward: 2938.62
               Mean episode length: 54.11
                  Mean reward/step: 53.28
       Mean episode length/episode: 7.07
            Mean episode successes: 4.4810
Mean episode consecutive_successes: 12.6897
--------------------------------------------------------------------------------
                   Total timesteps: 62570496
                    Iteration time: 8.34s
                        Total time: 39032.08s
                               ETA: 983027.9s

################################################################################
                    [1m Learning iteration 3819/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.189s, learning 0.168s)
               Value function loss: 196320.6574
                    Surrogate loss: 0.0065
             Mean action noise std: 0.69
                       Mean reward: 2716.19
               Mean episode length: 54.77
                  Mean reward/step: 53.28
       Mean episode length/episode: 7.18
            Mean episode successes: 4.6436
Mean episode consecutive_successes: 12.6145
--------------------------------------------------------------------------------
                   Total timesteps: 62586880
                    Iteration time: 8.36s
                        Total time: 39040.44s
                               ETA: 982970.8s

################################################################################
                    [1m Learning iteration 3820/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.138s, learning 0.176s)
               Value function loss: 174767.6750
                    Surrogate loss: -0.0118
             Mean action noise std: 0.69
                       Mean reward: 2799.55
               Mean episode length: 55.31
                  Mean reward/step: 52.40
       Mean episode length/episode: 7.07
            Mean episode successes: 4.6025
Mean episode consecutive_successes: 12.5333
--------------------------------------------------------------------------------
                   Total timesteps: 62603264
                    Iteration time: 8.31s
                        Total time: 39048.75s
                               ETA: 982912.6s

################################################################################
                    [1m Learning iteration 3821/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.040s, learning 0.268s)
               Value function loss: 172376.6250
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 2880.86
               Mean episode length: 54.32
                  Mean reward/step: 52.86
       Mean episode length/episode: 7.16
            Mean episode successes: 4.4351
Mean episode consecutive_successes: 12.6116
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 8.31s
                        Total time: 39057.06s
                               ETA: 982854.3s

################################################################################
                    [1m Learning iteration 3822/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.220s, learning 0.192s)
               Value function loss: 179353.1711
                    Surrogate loss: 0.0079
             Mean action noise std: 0.69
                       Mean reward: 3275.45
               Mean episode length: 56.79
                  Mean reward/step: 55.96
       Mean episode length/episode: 7.14
            Mean episode successes: 4.7280
Mean episode consecutive_successes: 12.5404
--------------------------------------------------------------------------------
                   Total timesteps: 62636032
                    Iteration time: 8.41s
                        Total time: 39065.47s
                               ETA: 982798.6s

################################################################################
                    [1m Learning iteration 3823/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.328s, learning 0.160s)
               Value function loss: 183375.3875
                    Surrogate loss: -0.0050
             Mean action noise std: 0.69
                       Mean reward: 2640.88
               Mean episode length: 53.39
                  Mean reward/step: 57.67
       Mean episode length/episode: 7.08
            Mean episode successes: 4.8442
Mean episode consecutive_successes: 12.4431
--------------------------------------------------------------------------------
                   Total timesteps: 62652416
                    Iteration time: 8.49s
                        Total time: 39073.96s
                               ETA: 982744.9s

################################################################################
                    [1m Learning iteration 3824/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.210s, learning 0.157s)
               Value function loss: 167945.8629
                    Surrogate loss: 0.0185
             Mean action noise std: 0.69
                       Mean reward: 2373.89
               Mean episode length: 54.69
                  Mean reward/step: 58.23
       Mean episode length/episode: 7.10
            Mean episode successes: 4.9448
Mean episode consecutive_successes: 12.3853
--------------------------------------------------------------------------------
                   Total timesteps: 62668800
                    Iteration time: 8.37s
                        Total time: 39082.33s
                               ETA: 982688.1s

################################################################################
                    [1m Learning iteration 3825/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.295s, learning 0.213s)
               Value function loss: 157137.8375
                    Surrogate loss: 0.0030
             Mean action noise std: 0.69
                       Mean reward: 3433.66
               Mean episode length: 57.32
                  Mean reward/step: 58.00
       Mean episode length/episode: 7.07
            Mean episode successes: 5.0947
Mean episode consecutive_successes: 12.3737
--------------------------------------------------------------------------------
                   Total timesteps: 62685184
                    Iteration time: 8.51s
                        Total time: 39090.84s
                               ETA: 982634.9s

################################################################################
                    [1m Learning iteration 3826/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.986s, learning 0.171s)
               Value function loss: 168018.8477
                    Surrogate loss: -0.0132
             Mean action noise std: 0.69
                       Mean reward: 3173.11
               Mean episode length: 53.36
                  Mean reward/step: 56.05
       Mean episode length/episode: 7.13
            Mean episode successes: 4.7661
Mean episode consecutive_successes: 12.5373
--------------------------------------------------------------------------------
                   Total timesteps: 62701568
                    Iteration time: 8.16s
                        Total time: 39098.99s
                               ETA: 982572.9s

################################################################################
                    [1m Learning iteration 3827/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.388s, learning 0.195s)
               Value function loss: 173596.0902
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3446.28
               Mean episode length: 58.02
                  Mean reward/step: 52.83
       Mean episode length/episode: 7.11
            Mean episode successes: 4.4585
Mean episode consecutive_successes: 12.6468
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 8.58s
                        Total time: 39107.58s
                               ETA: 982521.7s

################################################################################
                    [1m Learning iteration 3828/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.196s, learning 0.179s)
               Value function loss: 171660.3551
                    Surrogate loss: -0.0120
             Mean action noise std: 0.69
                       Mean reward: 2926.81
               Mean episode length: 54.67
                  Mean reward/step: 53.76
       Mean episode length/episode: 7.13
            Mean episode successes: 4.4146
Mean episode consecutive_successes: 12.6229
--------------------------------------------------------------------------------
                   Total timesteps: 62734336
                    Iteration time: 8.37s
                        Total time: 39115.95s
                               ETA: 982465.2s

################################################################################
                    [1m Learning iteration 3829/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.199s, learning 0.171s)
               Value function loss: 174527.7746
                    Surrogate loss: 0.0451
             Mean action noise std: 0.69
                       Mean reward: 3000.79
               Mean episode length: 53.12
                  Mean reward/step: 55.57
       Mean episode length/episode: 7.16
            Mean episode successes: 4.5527
Mean episode consecutive_successes: 12.5711
--------------------------------------------------------------------------------
                   Total timesteps: 62750720
                    Iteration time: 8.37s
                        Total time: 39124.32s
                               ETA: 982408.6s

################################################################################
                    [1m Learning iteration 3830/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.186s, learning 0.163s)
               Value function loss: 153719.2023
                    Surrogate loss: 0.0053
             Mean action noise std: 0.69
                       Mean reward: 2949.75
               Mean episode length: 55.64
                  Mean reward/step: 58.97
       Mean episode length/episode: 7.07
            Mean episode successes: 4.7651
Mean episode consecutive_successes: 12.5009
--------------------------------------------------------------------------------
                   Total timesteps: 62767104
                    Iteration time: 8.35s
                        Total time: 39132.67s
                               ETA: 982351.6s

################################################################################
                    [1m Learning iteration 3831/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.230s, learning 0.190s)
               Value function loss: 172082.2270
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 2641.29
               Mean episode length: 54.24
                  Mean reward/step: 58.35
       Mean episode length/episode: 7.09
            Mean episode successes: 5.0244
Mean episode consecutive_successes: 12.3409
--------------------------------------------------------------------------------
                   Total timesteps: 62783488
                    Iteration time: 8.42s
                        Total time: 39141.09s
                               ETA: 982296.3s

################################################################################
                    [1m Learning iteration 3832/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.202s, learning 0.171s)
               Value function loss: 183752.6824
                    Surrogate loss: -0.0126
             Mean action noise std: 0.69
                       Mean reward: 4043.39
               Mean episode length: 60.91
                  Mean reward/step: 58.39
       Mean episode length/episode: 7.18
            Mean episode successes: 5.1299
Mean episode consecutive_successes: 12.4720
--------------------------------------------------------------------------------
                   Total timesteps: 62799872
                    Iteration time: 8.37s
                        Total time: 39149.46s
                               ETA: 982239.9s

################################################################################
                    [1m Learning iteration 3833/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.214s, learning 0.172s)
               Value function loss: 230758.0363
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 3536.17
               Mean episode length: 57.59
                  Mean reward/step: 56.49
       Mean episode length/episode: 7.02
            Mean episode successes: 4.8052
Mean episode consecutive_successes: 12.5475
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 8.39s
                        Total time: 39157.85s
                               ETA: 982183.8s

################################################################################
                    [1m Learning iteration 3834/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.935s, learning 0.178s)
               Value function loss: 213236.6055
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 3376.00
               Mean episode length: 57.11
                  Mean reward/step: 55.42
       Mean episode length/episode: 7.12
            Mean episode successes: 4.8179
Mean episode consecutive_successes: 12.5216
--------------------------------------------------------------------------------
                   Total timesteps: 62832640
                    Iteration time: 8.11s
                        Total time: 39165.96s
                               ETA: 982121.0s

################################################################################
                    [1m Learning iteration 3835/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.049s, learning 0.178s)
               Value function loss: 213114.7344
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2699.93
               Mean episode length: 52.63
                  Mean reward/step: 55.10
       Mean episode length/episode: 7.06
            Mean episode successes: 4.8555
Mean episode consecutive_successes: 12.4316
--------------------------------------------------------------------------------
                   Total timesteps: 62849024
                    Iteration time: 8.23s
                        Total time: 39174.19s
                               ETA: 982060.9s

################################################################################
                    [1m Learning iteration 3836/100000 [0m                    

                       Computation: 1110 steps/s (collection: 14.585s, learning 0.162s)
               Value function loss: 236035.6781
                    Surrogate loss: -0.0116
             Mean action noise std: 0.69
                       Mean reward: 3077.18
               Mean episode length: 55.14
                  Mean reward/step: 54.23
       Mean episode length/episode: 7.09
            Mean episode successes: 4.8145
Mean episode consecutive_successes: 12.4050
--------------------------------------------------------------------------------
                   Total timesteps: 62865408
                    Iteration time: 14.75s
                        Total time: 39188.94s
                               ETA: 982164.4s

################################################################################
                    [1m Learning iteration 3837/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.403s, learning 0.193s)
               Value function loss: 209642.9555
                    Surrogate loss: 0.0021
             Mean action noise std: 0.69
                       Mean reward: 3012.22
               Mean episode length: 55.78
                  Mean reward/step: 55.31
       Mean episode length/episode: 7.11
            Mean episode successes: 4.7651
Mean episode consecutive_successes: 12.3574
--------------------------------------------------------------------------------
                   Total timesteps: 62881792
                    Iteration time: 16.60s
                        Total time: 39205.53s
                               ETA: 982314.1s

################################################################################
                    [1m Learning iteration 3838/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.984s, learning 0.183s)
               Value function loss: 181319.1273
                    Surrogate loss: 0.0041
             Mean action noise std: 0.69
                       Mean reward: 3482.06
               Mean episode length: 55.14
                  Mean reward/step: 56.55
       Mean episode length/episode: 7.15
            Mean episode successes: 4.8188
Mean episode consecutive_successes: 12.3901
--------------------------------------------------------------------------------
                   Total timesteps: 62898176
                    Iteration time: 16.17s
                        Total time: 39221.70s
                               ETA: 982453.0s

################################################################################
                    [1m Learning iteration 3839/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.846s, learning 0.201s)
               Value function loss: 180566.5988
                    Surrogate loss: -0.0079
             Mean action noise std: 0.69
                       Mean reward: 3375.10
               Mean episode length: 56.44
                  Mean reward/step: 56.62
       Mean episode length/episode: 7.11
            Mean episode successes: 4.7578
Mean episode consecutive_successes: 12.4697
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 16.05s
                        Total time: 39237.75s
                               ETA: 982588.8s

################################################################################
                    [1m Learning iteration 3840/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.790s, learning 0.190s)
               Value function loss: 182585.5039
                    Surrogate loss: -0.0128
             Mean action noise std: 0.69
                       Mean reward: 3202.68
               Mean episode length: 57.05
                  Mean reward/step: 54.06
       Mean episode length/episode: 7.05
            Mean episode successes: 4.7041
Mean episode consecutive_successes: 12.3942
--------------------------------------------------------------------------------
                   Total timesteps: 62930944
                    Iteration time: 15.98s
                        Total time: 39253.73s
                               ETA: 982722.8s

################################################################################
                    [1m Learning iteration 3841/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.013s, learning 0.160s)
               Value function loss: 190533.2910
                    Surrogate loss: -0.0135
             Mean action noise std: 0.69
                       Mean reward: 3048.50
               Mean episode length: 53.61
                  Mean reward/step: 55.13
       Mean episode length/episode: 7.18
            Mean episode successes: 4.8115
Mean episode consecutive_successes: 12.3678
--------------------------------------------------------------------------------
                   Total timesteps: 62947328
                    Iteration time: 16.17s
                        Total time: 39269.90s
                               ETA: 982861.6s

/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-1c9gtazo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandBlockStack_ppo_20221020041912
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/1c9gtazo
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:4
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=4, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=4, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:4', seed=None, sim_device='cuda:4', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandBlockStack', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_block_stack', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 250, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.2, 'orientation_scale': 0.02, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'block', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandBlockStack', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandBlockStack_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 9198
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:4
Sequential(
  (0): Linear(in_features=430, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=430, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 896 steps/s (collection: 15.732s, learning 2.538s)
               Value function loss: 12.2613
                    Surrogate loss: 0.0124
             Mean action noise std: 0.80
                  Mean reward/step: 0.64
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 18.27s
                        Total time: 18.27s
                               ETA: 1827051.4s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 1385 steps/s (collection: 11.629s, learning 0.196s)
               Value function loss: 3.4083
                    Surrogate loss: -0.0269
             Mean action noise std: 0.80
                  Mean reward/step: 0.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 11.83s
                        Total time: 30.10s
                               ETA: 1504787.7s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 1136 steps/s (collection: 14.216s, learning 0.197s)
               Value function loss: 1.9946
                    Surrogate loss: -0.0286
             Mean action noise std: 0.80
                  Mean reward/step: 0.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 14.41s
                        Total time: 44.51s
                               ETA: 1483617.4s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 1020 steps/s (collection: 15.887s, learning 0.176s)
               Value function loss: 2.7246
                    Surrogate loss: -0.0100
             Mean action noise std: 0.80
                  Mean reward/step: 0.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 16.06s
                        Total time: 60.57s
                               ETA: 1514255.3s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 995 steps/s (collection: 16.277s, learning 0.178s)
               Value function loss: 1.0137
                    Surrogate loss: -0.0099
             Mean action noise std: 0.80
                  Mean reward/step: 0.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 16.45s
                        Total time: 77.03s
                               ETA: 1540473.2s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 988 steps/s (collection: 16.417s, learning 0.163s)
               Value function loss: 0.6685
                    Surrogate loss: -0.0181
             Mean action noise std: 0.80
                  Mean reward/step: 0.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 16.58s
                        Total time: 93.61s
                               ETA: 1560031.9s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 925 steps/s (collection: 17.535s, learning 0.164s)
               Value function loss: 0.5763
                    Surrogate loss: -0.0197
             Mean action noise std: 0.80
                  Mean reward/step: 0.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 17.70s
                        Total time: 111.31s
                               ETA: 1589990.3s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 950 steps/s (collection: 17.073s, learning 0.168s)
               Value function loss: 0.5151
                    Surrogate loss: -0.0176
             Mean action noise std: 0.80
                  Mean reward/step: 0.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 17.24s
                        Total time: 128.55s
                               ETA: 1606717.7s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 953 steps/s (collection: 17.008s, learning 0.171s)
               Value function loss: 0.6864
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                  Mean reward/step: 0.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 17.18s
                        Total time: 145.73s
                               ETA: 1619038.2s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 895 steps/s (collection: 17.433s, learning 0.872s)
               Value function loss: 0.9458
                    Surrogate loss: -0.0191
             Mean action noise std: 0.80
                  Mean reward/step: 0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 18.31s
                        Total time: 164.03s
                               ETA: 1640159.5s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 925 steps/s (collection: 17.528s, learning 0.180s)
               Value function loss: 0.8071
                    Surrogate loss: 0.0100
             Mean action noise std: 0.80
                  Mean reward/step: 0.65
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 17.71s
                        Total time: 181.74s
                               ETA: 1652000.9s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 951 steps/s (collection: 17.050s, learning 0.173s)
               Value function loss: 1.0330
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                  Mean reward/step: 0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 17.22s
                        Total time: 198.96s
                               ETA: 1657831.9s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.756s, learning 0.178s)
               Value function loss: 0.9972
                    Surrogate loss: 0.0174
             Mean action noise std: 0.80
                  Mean reward/step: 0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 16.93s
                        Total time: 215.89s
                               ETA: 1660530.4s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.978s, learning 0.181s)
               Value function loss: 0.8976
                    Surrogate loss: -0.0027
             Mean action noise std: 0.80
                  Mean reward/step: 0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 17.16s
                        Total time: 233.05s
                               ETA: 1664450.3s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 927 steps/s (collection: 17.489s, learning 0.177s)
               Value function loss: 0.7434
                    Surrogate loss: 0.0261
             Mean action noise std: 0.80
                  Mean reward/step: 0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 17.67s
                        Total time: 250.72s
                               ETA: 1671227.6s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 937 steps/s (collection: 17.258s, learning 0.226s)
               Value function loss: 0.7687
                    Surrogate loss: -0.0053
             Mean action noise std: 0.80
                       Mean reward: 71.66
               Mean episode length: 125.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 17.48s
                        Total time: 268.20s
                               ETA: 1676019.2s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 925 steps/s (collection: 17.509s, learning 0.191s)
               Value function loss: 0.5908
                    Surrogate loss: -0.0196
             Mean action noise std: 0.80
                       Mean reward: 71.66
               Mean episode length: 125.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 17.70s
                        Total time: 285.90s
                               ETA: 1681511.9s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.934s, learning 0.196s)
               Value function loss: 1.0986
                    Surrogate loss: -0.0149
             Mean action noise std: 0.80
                       Mean reward: 75.74
               Mean episode length: 129.75
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 17.13s
                        Total time: 303.03s
                               ETA: 1683229.4s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.652s, learning 0.170s)
               Value function loss: 1.6938
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: 78.67
               Mean episode length: 134.29
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 16.82s
                        Total time: 319.86s
                               ETA: 1683145.7s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.900s, learning 0.171s)
               Value function loss: 1.2378
                    Surrogate loss: -0.0008
             Mean action noise std: 0.80
                       Mean reward: 79.84
               Mean episode length: 136.10
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 17.07s
                        Total time: 336.93s
                               ETA: 1684311.8s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 961 steps/s (collection: 16.868s, learning 0.179s)
               Value function loss: 0.8427
                    Surrogate loss: -0.0086
             Mean action noise std: 0.80
                       Mean reward: 80.48
               Mean episode length: 137.08
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 17.05s
                        Total time: 353.97s
                               ETA: 1685248.9s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 941 steps/s (collection: 17.232s, learning 0.163s)
               Value function loss: 0.7719
                    Surrogate loss: -0.0079
             Mean action noise std: 0.80
                       Mean reward: 80.87
               Mean episode length: 137.69
                  Mean reward/step: 0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 17.39s
                        Total time: 371.37s
                               ETA: 1687682.0s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.800s, learning 0.177s)
               Value function loss: 1.2921
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 81.14
               Mean episode length: 138.11
                  Mean reward/step: 0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 16.98s
                        Total time: 388.34s
                               ETA: 1688082.6s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.574s, learning 0.231s)
               Value function loss: 1.1957
                    Surrogate loss: 0.0056
             Mean action noise std: 0.80
                       Mean reward: 81.34
               Mean episode length: 138.41
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 16.80s
                        Total time: 405.15s
                               ETA: 1687733.5s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.930s, learning 0.197s)
               Value function loss: 0.8988
                    Surrogate loss: -0.0209
             Mean action noise std: 0.80
                       Mean reward: 81.49
               Mean episode length: 138.64
                  Mean reward/step: 0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 17.13s
                        Total time: 422.28s
                               ETA: 1688701.9s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 950 steps/s (collection: 17.060s, learning 0.180s)
               Value function loss: 0.6487
                    Surrogate loss: -0.0117
             Mean action noise std: 0.80
                       Mean reward: 81.60
               Mean episode length: 138.82
                  Mean reward/step: 0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 17.24s
                        Total time: 439.52s
                               ETA: 1690025.5s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.158s, learning 0.168s)
               Value function loss: 0.4221
                    Surrogate loss: -0.0137
             Mean action noise std: 0.80
                       Mean reward: 81.70
               Mean episode length: 138.97
                  Mean reward/step: 1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 17.33s
                        Total time: 456.84s
                               ETA: 1691568.1s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.714s, learning 0.177s)
               Value function loss: 0.4539
                    Surrogate loss: -0.0099
             Mean action noise std: 0.80
                       Mean reward: 81.78
               Mean episode length: 139.09
                  Mean reward/step: 1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 16.89s
                        Total time: 473.73s
                               ETA: 1691446.4s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.830s, learning 0.173s)
               Value function loss: 0.3678
                    Surrogate loss: -0.0149
             Mean action noise std: 0.80
                       Mean reward: 81.84
               Mean episode length: 139.19
                  Mean reward/step: 1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 17.00s
                        Total time: 490.74s
                               ETA: 1691719.0s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 915 steps/s (collection: 17.680s, learning 0.210s)
               Value function loss: 0.2860
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: 81.90
               Mean episode length: 139.28
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 17.89s
                        Total time: 508.63s
                               ETA: 1694927.8s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 934 steps/s (collection: 17.341s, learning 0.184s)
               Value function loss: 0.2988
                    Surrogate loss: -0.0063
             Mean action noise std: 0.80
                       Mean reward: 81.95
               Mean episode length: 139.35
                  Mean reward/step: 1.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 17.52s
                        Total time: 526.15s
                               ETA: 1696749.9s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 952 steps/s (collection: 17.017s, learning 0.178s)
               Value function loss: 64.7811
                    Surrogate loss: 0.0137
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 17.19s
                        Total time: 543.34s
                               ETA: 1697426.7s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.957s, learning 0.207s)
               Value function loss: 0.9878
                    Surrogate loss: 0.1495
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 17.16s
                        Total time: 560.51s
                               ETA: 1697969.8s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 952 steps/s (collection: 17.028s, learning 0.177s)
               Value function loss: 0.3466
                    Surrogate loss: -0.0091
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 17.20s
                        Total time: 577.71s
                               ETA: 1698598.3s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 948 steps/s (collection: 17.097s, learning 0.179s)
               Value function loss: 0.1364
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 17.28s
                        Total time: 594.99s
                               ETA: 1699392.7s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 943 steps/s (collection: 17.194s, learning 0.174s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0170
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 17.37s
                        Total time: 612.36s
                               ETA: 1700398.7s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.799s, learning 0.170s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0294
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 16.97s
                        Total time: 629.33s
                               ETA: 1700272.6s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.760s, learning 0.173s)
               Value function loss: 0.0651
                    Surrogate loss: -0.0246
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 12.93s
                        Total time: 642.26s
                               ETA: 1689534.1s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.786s, learning 0.163s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0157
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 8.95s
                        Total time: 651.21s
                               ETA: 1669133.9s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.397s, learning 0.163s)
               Value function loss: 0.1904
                    Surrogate loss: -0.0161
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 8.56s
                        Total time: 659.77s
                               ETA: 1648780.5s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.687s, learning 0.176s)
               Value function loss: 0.2011
                    Surrogate loss: -0.0142
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 8.86s
                        Total time: 668.63s
                               ETA: 1630159.3s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.613s, learning 0.157s)
               Value function loss: 0.1890
                    Surrogate loss: -0.0157
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 8.77s
                        Total time: 677.40s
                               ETA: 1612204.2s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.530s, learning 0.164s)
               Value function loss: 0.2904
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                       Mean reward: 190.24
               Mean episode length: 249.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 8.69s
                        Total time: 686.10s
                               ETA: 1594904.3s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.800s, learning 0.234s)
               Value function loss: 0.2970
                    Surrogate loss: 0.0313
             Mean action noise std: 0.80
                       Mean reward: 189.06
               Mean episode length: 247.48
                  Mean reward/step: 1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 9.03s
                        Total time: 695.13s
                               ETA: 1579164.9s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.847s, learning 0.163s)
               Value function loss: 0.2264
                    Surrogate loss: 0.0101
             Mean action noise std: 0.80
                       Mean reward: 189.06
               Mean episode length: 247.48
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 9.01s
                        Total time: 704.14s
                               ETA: 1564071.5s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.515s, learning 0.185s)
               Value function loss: 0.3838
                    Surrogate loss: -0.0137
             Mean action noise std: 0.80
                       Mean reward: 189.06
               Mean episode length: 247.48
                  Mean reward/step: 1.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 8.70s
                        Total time: 712.84s
                               ETA: 1548960.5s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.669s, learning 0.186s)
               Value function loss: 0.4517
                    Surrogate loss: 0.0143
             Mean action noise std: 0.80
                       Mean reward: 189.35
               Mean episode length: 247.49
                  Mean reward/step: 1.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 8.86s
                        Total time: 721.70s
                               ETA: 1534821.5s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.993s, learning 0.158s)
               Value function loss: 0.2948
                    Surrogate loss: 0.0016
             Mean action noise std: 0.80
                       Mean reward: 189.35
               Mean episode length: 247.49
                  Mean reward/step: 1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 9.15s
                        Total time: 730.85s
                               ETA: 1521886.2s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.923s, learning 0.163s)
               Value function loss: 0.5622
                    Surrogate loss: 0.0133
             Mean action noise std: 0.80
                       Mean reward: 187.99
               Mean episode length: 245.30
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 9.09s
                        Total time: 739.94s
                               ETA: 1509346.7s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.661s, learning 0.163s)
               Value function loss: 0.8841
                    Surrogate loss: 0.0088
             Mean action noise std: 0.80
                       Mean reward: 187.44
               Mean episode length: 243.25
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 8.82s
                        Total time: 748.76s
                               ETA: 1496782.7s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.958s, learning 0.161s)
               Value function loss: 0.5395
                    Surrogate loss: -0.0076
             Mean action noise std: 0.80
                       Mean reward: 187.90
               Mean episode length: 243.26
                  Mean reward/step: 1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 9.12s
                        Total time: 757.88s
                               ETA: 1485291.5s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.818s, learning 0.164s)
               Value function loss: 0.5535
                    Surrogate loss: -0.0162
             Mean action noise std: 0.80
                       Mean reward: 187.10
               Mean episode length: 241.53
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 8.98s
                        Total time: 766.86s
                               ETA: 1473979.5s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.852s, learning 0.167s)
               Value function loss: 0.8738
                    Surrogate loss: -0.0141
             Mean action noise std: 0.80
                       Mean reward: 186.19
               Mean episode length: 239.24
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 9.02s
                        Total time: 775.88s
                               ETA: 1463163.0s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.609s, learning 0.159s)
               Value function loss: 0.8826
                    Surrogate loss: -0.0130
             Mean action noise std: 0.80
                       Mean reward: 185.49
               Mean episode length: 237.17
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 8.77s
                        Total time: 784.65s
                               ETA: 1452281.8s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.840s, learning 0.166s)
               Value function loss: 1.0588
                    Surrogate loss: -0.0105
             Mean action noise std: 0.80
                       Mean reward: 184.91
               Mean episode length: 234.73
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 9.01s
                        Total time: 793.65s
                               ETA: 1442227.8s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.766s, learning 0.172s)
               Value function loss: 0.9077
                    Surrogate loss: -0.0176
             Mean action noise std: 0.80
                       Mean reward: 184.95
               Mean episode length: 232.53
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 8.94s
                        Total time: 802.59s
                               ETA: 1432411.2s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.739s, learning 0.164s)
               Value function loss: 1.9798
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: 185.99
               Mean episode length: 228.49
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 8.90s
                        Total time: 811.50s
                               ETA: 1422878.4s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1764 steps/s (collection: 9.095s, learning 0.190s)
               Value function loss: 1.5828
                    Surrogate loss: -0.0062
             Mean action noise std: 0.80
                       Mean reward: 186.84
               Mean episode length: 226.24
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 9.28s
                        Total time: 820.78s
                               ETA: 1414330.8s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.805s, learning 0.166s)
               Value function loss: 1.7583
                    Surrogate loss: 0.0013
             Mean action noise std: 0.80
                       Mean reward: 188.57
               Mean episode length: 224.50
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 8.97s
                        Total time: 829.75s
                               ETA: 1405541.4s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.894s, learning 0.164s)
               Value function loss: 2.4633
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: 191.47
               Mean episode length: 222.61
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 9.06s
                        Total time: 838.81s
                               ETA: 1397189.4s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.613s, learning 0.162s)
               Value function loss: 3.2518
                    Surrogate loss: -0.0090
             Mean action noise std: 0.80
                       Mean reward: 194.15
               Mean episode length: 220.88
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 8.78s
                        Total time: 847.58s
                               ETA: 1388647.6s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1757 steps/s (collection: 9.158s, learning 0.163s)
               Value function loss: 5.0334
                    Surrogate loss: -0.0058
             Mean action noise std: 0.80
                       Mean reward: 201.56
               Mean episode length: 219.54
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 9.32s
                        Total time: 856.90s
                               ETA: 1381260.1s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1774 steps/s (collection: 9.075s, learning 0.157s)
               Value function loss: 145.0140
                    Surrogate loss: 0.0254
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 4.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 9.23s
                        Total time: 866.14s
                               ETA: 1373967.7s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.484s, learning 0.162s)
               Value function loss: 2.0100
                    Surrogate loss: 0.0015
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 8.65s
                        Total time: 874.78s
                               ETA: 1365986.8s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.637s, learning 0.169s)
               Value function loss: 0.2853
                    Surrogate loss: -0.0097
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 8.81s
                        Total time: 883.59s
                               ETA: 1358497.2s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.662s, learning 0.169s)
               Value function loss: 0.1908
                    Surrogate loss: -0.0122
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 8.83s
                        Total time: 892.42s
                               ETA: 1351273.1s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.811s, learning 0.163s)
               Value function loss: 0.1409
                    Surrogate loss: -0.0004
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 8.97s
                        Total time: 901.39s
                               ETA: 1344475.7s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.522s, learning 0.158s)
               Value function loss: 0.3927
                    Surrogate loss: 0.0137
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 8.68s
                        Total time: 910.07s
                               ETA: 1337447.1s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.718s, learning 0.164s)
               Value function loss: 0.6807
                    Surrogate loss: 0.0011
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 8.88s
                        Total time: 918.96s
                               ETA: 1330914.8s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.711s, learning 0.167s)
               Value function loss: 0.3571
                    Surrogate loss: -0.0193
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 8.88s
                        Total time: 927.83s
                               ETA: 1324562.9s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.451s, learning 0.163s)
               Value function loss: 0.2290
                    Surrogate loss: 0.0046
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 8.61s
                        Total time: 936.45s
                               ETA: 1318018.7s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.472s, learning 0.193s)
               Value function loss: 0.3571
                    Surrogate loss: -0.0036
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 8.67s
                        Total time: 945.11s
                               ETA: 1311726.4s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1788 steps/s (collection: 8.971s, learning 0.191s)
               Value function loss: 0.4532
                    Surrogate loss: 0.0103
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 9.16s
                        Total time: 954.28s
                               ETA: 1306286.6s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.771s, learning 0.215s)
               Value function loss: 0.3369
                    Surrogate loss: -0.0019
             Mean action noise std: 0.80
                       Mean reward: 245.44
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 8.99s
                        Total time: 963.26s
                               ETA: 1300756.4s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.710s, learning 0.170s)
               Value function loss: 0.7683
                    Surrogate loss: 0.0098
             Mean action noise std: 0.80
                       Mean reward: 244.17
               Mean episode length: 248.48
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 8.88s
                        Total time: 972.14s
                               ETA: 1295231.4s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.292s, learning 0.163s)
               Value function loss: 0.7887
                    Surrogate loss: 0.0006
             Mean action noise std: 0.80
                       Mean reward: 242.73
               Mean episode length: 247.07
                  Mean reward/step: 1.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 9.46s
                        Total time: 981.60s
                               ETA: 1290608.5s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.251s, learning 0.168s)
               Value function loss: 0.7971
                    Surrogate loss: 0.0008
             Mean action noise std: 0.80
                       Mean reward: 241.26
               Mean episode length: 245.71
                  Mean reward/step: 1.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 8.42s
                        Total time: 990.02s
                               ETA: 1284760.5s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.793s, learning 0.162s)
               Value function loss: 1.0770
                    Surrogate loss: 0.0137
             Mean action noise std: 0.80
                       Mean reward: 237.41
               Mean episode length: 241.97
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 8.95s
                        Total time: 998.97s
                               ETA: 1279748.4s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.619s, learning 0.221s)
               Value function loss: 2.1427
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 227.02
               Mean episode length: 232.08
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 8.84s
                        Total time: 1007.81s
                               ETA: 1274717.3s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.760s, learning 0.183s)
               Value function loss: 2.9549
                    Surrogate loss: 0.0047
             Mean action noise std: 0.80
                       Mean reward: 211.90
               Mean episode length: 216.70
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 8.94s
                        Total time: 1016.76s
                               ETA: 1269940.2s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.648s, learning 0.164s)
               Value function loss: 3.1447
                    Surrogate loss: -0.0003
             Mean action noise std: 0.80
                       Mean reward: 194.72
               Mean episode length: 198.69
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 8.81s
                        Total time: 1025.57s
                               ETA: 1265118.8s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.649s, learning 0.159s)
               Value function loss: 3.2995
                    Surrogate loss: -0.0055
             Mean action noise std: 0.80
                       Mean reward: 178.51
               Mean episode length: 182.64
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 8.81s
                        Total time: 1034.37s
                               ETA: 1260411.1s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.613s, learning 0.157s)
               Value function loss: 2.3435
                    Surrogate loss: -0.0082
             Mean action noise std: 0.80
                       Mean reward: 171.52
               Mean episode length: 175.37
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 8.77s
                        Total time: 1043.15s
                               ETA: 1255770.8s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.720s, learning 0.176s)
               Value function loss: 4.0101
                    Surrogate loss: 0.0148
             Mean action noise std: 0.80
                       Mean reward: 161.10
               Mean episode length: 165.95
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 8.90s
                        Total time: 1052.04s
                               ETA: 1251390.1s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.783s, learning 0.186s)
               Value function loss: 4.4882
                    Surrogate loss: -0.0024
             Mean action noise std: 0.80
                       Mean reward: 163.77
               Mean episode length: 172.16
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 8.97s
                        Total time: 1061.01s
                               ETA: 1247198.7s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.770s, learning 0.169s)
               Value function loss: 7.2478
                    Surrogate loss: 0.0055
             Mean action noise std: 0.80
                       Mean reward: 171.54
               Mean episode length: 184.25
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 8.94s
                        Total time: 1069.95s
                               ETA: 1243069.8s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.834s, learning 0.163s)
               Value function loss: 10.2682
                    Surrogate loss: -0.0043
             Mean action noise std: 0.80
                       Mean reward: 180.89
               Mean episode length: 195.30
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 9.00s
                        Total time: 1078.95s
                               ETA: 1239102.0s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.336s, learning 0.157s)
               Value function loss: 11.7221
                    Surrogate loss: -0.0175
             Mean action noise std: 0.80
                       Mean reward: 187.73
               Mean episode length: 204.76
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 8.49s
                        Total time: 1087.44s
                               ETA: 1234651.9s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.938s, learning 0.212s)
               Value function loss: 7.6995
                    Surrogate loss: -0.0215
             Mean action noise std: 0.80
                       Mean reward: 190.58
               Mean episode length: 212.14
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 9.15s
                        Total time: 1096.59s
                               ETA: 1231039.7s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.581s, learning 0.346s)
               Value function loss: 5.4857
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                       Mean reward: 195.04
               Mean episode length: 217.63
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 8.93s
                        Total time: 1105.52s
                               ETA: 1227259.1s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.743s, learning 0.170s)
               Value function loss: 5.4044
                    Surrogate loss: -0.0103
             Mean action noise std: 0.80
                       Mean reward: 200.78
               Mean episode length: 224.85
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 8.91s
                        Total time: 1114.43s
                               ETA: 1223547.1s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.641s, learning 0.166s)
               Value function loss: 5.4579
                    Surrogate loss: -0.0135
             Mean action noise std: 0.80
                       Mean reward: 209.06
               Mean episode length: 234.30
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 8.81s
                        Total time: 1123.24s
                               ETA: 1219800.2s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.642s, learning 0.162s)
               Value function loss: 4.7930
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 216.38
               Mean episode length: 241.12
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 8.80s
                        Total time: 1132.04s
                               ETA: 1216129.6s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.830s, learning 0.159s)
               Value function loss: 68.5117
                    Surrogate loss: 0.0097
             Mean action noise std: 0.80
                       Mean reward: 232.47
               Mean episode length: 250.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 4.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 8.99s
                        Total time: 1141.03s
                               ETA: 1212733.2s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.343s, learning 0.164s)
               Value function loss: 2.5541
                    Surrogate loss: -0.0245
             Mean action noise std: 0.80
                       Mean reward: 232.47
               Mean episode length: 250.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 8.51s
                        Total time: 1149.54s
                               ETA: 1208902.4s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.764s, learning 0.169s)
               Value function loss: 1.0672
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 230.41
               Mean episode length: 247.97
                  Mean reward/step: 0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 8.93s
                        Total time: 1158.47s
                               ETA: 1205594.1s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.789s, learning 0.208s)
               Value function loss: 0.4663
                    Surrogate loss: -0.0184
             Mean action noise std: 0.80
                       Mean reward: 230.41
               Mean episode length: 247.97
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 9.00s
                        Total time: 1167.47s
                               ETA: 1202419.3s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.870s, learning 0.167s)
               Value function loss: 0.6260
                    Surrogate loss: -0.0154
             Mean action noise std: 0.80
                       Mean reward: 230.41
               Mean episode length: 247.97
                  Mean reward/step: 0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 9.04s
                        Total time: 1176.50s
                               ETA: 1199350.2s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.645s, learning 0.161s)
               Value function loss: 0.3693
                    Surrogate loss: 0.0493
             Mean action noise std: 0.80
                       Mean reward: 228.98
               Mean episode length: 246.55
                  Mean reward/step: 1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 8.81s
                        Total time: 1185.31s
                               ETA: 1196110.1s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.411s, learning 0.181s)
               Value function loss: 0.5062
                    Surrogate loss: -0.0044
             Mean action noise std: 0.80
                       Mean reward: 228.38
               Mean episode length: 245.67
                  Mean reward/step: 1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 8.59s
                        Total time: 1193.90s
                               ETA: 1192721.0s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.745s, learning 0.201s)
               Value function loss: 0.3992
                    Surrogate loss: -0.0030
             Mean action noise std: 0.80
                       Mean reward: 228.38
               Mean episode length: 245.67
                  Mean reward/step: 1.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 8.95s
                        Total time: 1202.85s
                               ETA: 1189748.8s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1787 steps/s (collection: 8.965s, learning 0.199s)
               Value function loss: 0.6598
                    Surrogate loss: -0.0008
             Mean action noise std: 0.80
                       Mean reward: 226.32
               Mean episode length: 243.59
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 9.16s
                        Total time: 1212.01s
                               ETA: 1187048.8s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.624s, learning 0.167s)
               Value function loss: 0.4883
                    Surrogate loss: -0.0119
             Mean action noise std: 0.80
                       Mean reward: 225.76
               Mean episode length: 243.31
                  Mean reward/step: 1.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 8.79s
                        Total time: 1220.81s
                               ETA: 1184039.4s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.886s, learning 0.234s)
               Value function loss: 0.7892
                    Surrogate loss: -0.0052
             Mean action noise std: 0.80
                       Mean reward: 222.94
               Mean episode length: 239.76
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 9.12s
                        Total time: 1229.93s
                               ETA: 1181403.4s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.783s, learning 0.194s)
               Value function loss: 0.5036
                    Surrogate loss: -0.0087
             Mean action noise std: 0.80
                       Mean reward: 222.61
               Mean episode length: 239.24
                  Mean reward/step: 1.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 8.98s
                        Total time: 1238.90s
                               ETA: 1178680.7s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.683s, learning 0.158s)
               Value function loss: 0.8338
                    Surrogate loss: -0.0088
             Mean action noise std: 0.80
                       Mean reward: 221.53
               Mean episode length: 238.23
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 8.84s
                        Total time: 1247.74s
                               ETA: 1175881.1s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.692s, learning 0.169s)
               Value function loss: 1.0372
                    Surrogate loss: 0.0086
             Mean action noise std: 0.80
                       Mean reward: 217.93
               Mean episode length: 234.73
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 8.86s
                        Total time: 1256.61s
                               ETA: 1173152.6s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.660s, learning 0.166s)
               Value function loss: 1.4855
                    Surrogate loss: -0.0081
             Mean action noise std: 0.80
                       Mean reward: 212.61
               Mean episode length: 230.99
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 8.83s
                        Total time: 1265.43s
                               ETA: 1170441.5s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.770s, learning 0.158s)
               Value function loss: 1.2634
                    Surrogate loss: -0.0123
             Mean action noise std: 0.80
                       Mean reward: 210.69
               Mean episode length: 229.63
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 8.93s
                        Total time: 1274.36s
                               ETA: 1167874.8s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.728s, learning 0.162s)
               Value function loss: 2.7229
                    Surrogate loss: -0.0114
             Mean action noise std: 0.80
                       Mean reward: 201.23
               Mean episode length: 219.77
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 8.89s
                        Total time: 1283.25s
                               ETA: 1165319.4s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.721s, learning 0.161s)
               Value function loss: 3.8579
                    Surrogate loss: -0.0151
             Mean action noise std: 0.80
                       Mean reward: 196.21
               Mean episode length: 209.56
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 8.88s
                        Total time: 1292.13s
                               ETA: 1162802.5s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.783s, learning 0.252s)
               Value function loss: 3.6362
                    Surrogate loss: -0.0103
             Mean action noise std: 0.80
                       Mean reward: 199.71
               Mean episode length: 212.08
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 9.04s
                        Total time: 1301.17s
                               ETA: 1160466.8s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.644s, learning 0.211s)
               Value function loss: 2.0758
                    Surrogate loss: -0.0079
             Mean action noise std: 0.80
                       Mean reward: 207.10
               Mean episode length: 216.35
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 8.86s
                        Total time: 1310.02s
                               ETA: 1158013.6s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.842s, learning 0.193s)
               Value function loss: 2.7294
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 213.66
               Mean episode length: 220.70
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 9.03s
                        Total time: 1319.06s
                               ETA: 1155760.3s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1786 steps/s (collection: 9.011s, learning 0.160s)
               Value function loss: 3.5463
                    Surrogate loss: -0.0151
             Mean action noise std: 0.80
                       Mean reward: 213.90
               Mean episode length: 222.06
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 9.17s
                        Total time: 1328.23s
                               ETA: 1153664.7s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.823s, learning 0.172s)
               Value function loss: 4.0956
                    Surrogate loss: -0.0061
             Mean action noise std: 0.80
                       Mean reward: 217.99
               Mean episode length: 224.43
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 8.99s
                        Total time: 1337.22s
                               ETA: 1151452.9s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.483s, learning 0.170s)
               Value function loss: 4.7431
                    Surrogate loss: -0.0069
             Mean action noise std: 0.80
                       Mean reward: 224.30
               Mean episode length: 228.73
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 8.65s
                        Total time: 1345.88s
                               ETA: 1148987.2s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.719s, learning 0.160s)
               Value function loss: 5.1059
                    Surrogate loss: -0.0063
             Mean action noise std: 0.80
                       Mean reward: 239.10
               Mean episode length: 237.59
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 8.88s
                        Total time: 1354.75s
                               ETA: 1146754.0s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.636s, learning 0.161s)
               Value function loss: 6.5825
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: 234.77
               Mean episode length: 233.99
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 8.80s
                        Total time: 1363.55s
                               ETA: 1144489.1s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1807 steps/s (collection: 8.909s, learning 0.156s)
               Value function loss: 7.0295
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                       Mean reward: 239.28
               Mean episode length: 241.14
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 9.07s
                        Total time: 1372.62s
                               ETA: 1142485.8s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.597s, learning 0.161s)
               Value function loss: 7.8441
                    Surrogate loss: -0.0004
             Mean action noise std: 0.80
                       Mean reward: 240.54
               Mean episode length: 241.39
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 8.76s
                        Total time: 1381.37s
                               ETA: 1140262.2s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.641s, learning 0.175s)
               Value function loss: 7.2264
                    Surrogate loss: -0.0144
             Mean action noise std: 0.80
                       Mean reward: 245.47
               Mean episode length: 242.23
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 8.82s
                        Total time: 1390.19s
                               ETA: 1138121.7s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.836s, learning 0.201s)
               Value function loss: 8.1078
                    Surrogate loss: -0.0148
             Mean action noise std: 0.80
                       Mean reward: 248.19
               Mean episode length: 245.83
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 9.04s
                        Total time: 1399.23s
                               ETA: 1136195.6s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.640s, learning 0.163s)
               Value function loss: 6.4493
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: 248.86
               Mean episode length: 246.57
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 8.80s
                        Total time: 1408.03s
                               ETA: 1134111.8s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.867s, learning 0.168s)
               Value function loss: 88.5084
                    Surrogate loss: 0.0205
             Mean action noise std: 0.80
                       Mean reward: 253.95
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 5.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 9.04s
                        Total time: 1417.07s
                               ETA: 1132247.1s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.557s, learning 0.163s)
               Value function loss: 0.8667
                    Surrogate loss: -0.0101
             Mean action noise std: 0.80
                       Mean reward: 252.57
               Mean episode length: 248.94
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 8.72s
                        Total time: 1425.79s
                               ETA: 1130161.6s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.637s, learning 0.171s)
               Value function loss: 0.3859
                    Surrogate loss: -0.0192
             Mean action noise std: 0.80
                       Mean reward: 250.77
               Mean episode length: 247.45
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 8.81s
                        Total time: 1434.59s
                               ETA: 1128178.6s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.616s, learning 0.188s)
               Value function loss: 0.2300
                    Surrogate loss: -0.0226
             Mean action noise std: 0.80
                       Mean reward: 251.03
               Mean episode length: 247.45
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 8.80s
                        Total time: 1443.40s
                               ETA: 1126222.7s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.312s, learning 0.169s)
               Value function loss: 0.2602
                    Surrogate loss: -0.0266
             Mean action noise std: 0.80
                       Mean reward: 251.03
               Mean episode length: 247.45
                  Mean reward/step: 0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 8.48s
                        Total time: 1451.88s
                               ETA: 1124047.2s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.919s, learning 0.166s)
               Value function loss: 0.2828
                    Surrogate loss: -0.0113
             Mean action noise std: 0.80
                       Mean reward: 251.43
               Mean episode length: 247.45
                  Mean reward/step: 1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 9.08s
                        Total time: 1460.96s
                               ETA: 1122368.5s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.447s, learning 0.170s)
               Value function loss: 0.4555
                    Surrogate loss: -0.0086
             Mean action noise std: 0.80
                       Mean reward: 249.81
               Mean episode length: 245.63
                  Mean reward/step: 1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 8.62s
                        Total time: 1469.58s
                               ETA: 1120358.7s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.566s, learning 0.172s)
               Value function loss: 0.5973
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 246.64
               Mean episode length: 242.26
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 8.74s
                        Total time: 1478.32s
                               ETA: 1118470.7s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.883s, learning 0.166s)
               Value function loss: 0.6170
                    Surrogate loss: -0.0060
             Mean action noise std: 0.80
                       Mean reward: 243.00
               Mean episode length: 239.82
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 9.05s
                        Total time: 1487.37s
                               ETA: 1116844.4s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.836s, learning 0.166s)
               Value function loss: 0.9301
                    Surrogate loss: -0.0050
             Mean action noise std: 0.80
                       Mean reward: 236.17
               Mean episode length: 233.05
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 9.00s
                        Total time: 1496.37s
                               ETA: 1115208.2s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.772s, learning 0.158s)
               Value function loss: 0.8211
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: 233.24
               Mean episode length: 230.59
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 8.93s
                        Total time: 1505.30s
                               ETA: 1113542.4s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.445s, learning 0.199s)
               Value function loss: 1.3886
                    Surrogate loss: -0.0108
             Mean action noise std: 0.80
                       Mean reward: 224.31
               Mean episode length: 220.70
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 8.64s
                        Total time: 1513.94s
                               ETA: 1111690.8s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.655s, learning 0.163s)
               Value function loss: 0.7763
                    Surrogate loss: -0.0135
             Mean action noise std: 0.80
                       Mean reward: 222.68
               Mean episode length: 218.96
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 8.82s
                        Total time: 1522.76s
                               ETA: 1109992.7s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.677s, learning 0.258s)
               Value function loss: 1.2468
                    Surrogate loss: -0.0166
             Mean action noise std: 0.80
                       Mean reward: 220.20
               Mean episode length: 216.02
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 8.93s
                        Total time: 1531.70s
                               ETA: 1108403.7s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.606s, learning 0.162s)
               Value function loss: 2.1200
                    Surrogate loss: -0.0153
             Mean action noise std: 0.80
                       Mean reward: 205.79
               Mean episode length: 204.39
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 8.77s
                        Total time: 1540.46s
                               ETA: 1106717.9s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.755s, learning 0.161s)
               Value function loss: 2.1778
                    Surrogate loss: -0.0107
             Mean action noise std: 0.80
                       Mean reward: 191.93
               Mean episode length: 191.46
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 8.92s
                        Total time: 1549.38s
                               ETA: 1105161.6s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.417s, learning 0.196s)
               Value function loss: 3.1158
                    Surrogate loss: -0.0038
             Mean action noise std: 0.80
                       Mean reward: 185.91
               Mean episode length: 185.04
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 8.61s
                        Total time: 1557.99s
                               ETA: 1103413.0s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.531s, learning 0.166s)
               Value function loss: 4.3018
                    Surrogate loss: -0.0013
             Mean action noise std: 0.80
                       Mean reward: 211.56
               Mean episode length: 206.87
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 8.70s
                        Total time: 1566.69s
                               ETA: 1101747.8s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1794 steps/s (collection: 8.940s, learning 0.188s)
               Value function loss: 3.8182
                    Surrogate loss: -0.0045
             Mean action noise std: 0.80
                       Mean reward: 231.28
               Mean episode length: 222.51
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 9.13s
                        Total time: 1575.82s
                               ETA: 1100406.3s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.734s, learning 0.233s)
               Value function loss: 3.9681
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 236.53
               Mean episode length: 228.23
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 8.97s
                        Total time: 1584.79s
                               ETA: 1098971.5s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.809s, learning 0.159s)
               Value function loss: 4.7034
                    Surrogate loss: -0.0075
             Mean action noise std: 0.80
                       Mean reward: 226.17
               Mean episode length: 222.57
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 8.97s
                        Total time: 1593.75s
                               ETA: 1097557.1s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.601s, learning 0.164s)
               Value function loss: 6.4138
                    Surrogate loss: -0.0066
             Mean action noise std: 0.80
                       Mean reward: 223.78
               Mean episode length: 221.62
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 8.76s
                        Total time: 1602.52s
                               ETA: 1096023.2s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1756 steps/s (collection: 9.097s, learning 0.231s)
               Value function loss: 8.1005
                    Surrogate loss: -0.0160
             Mean action noise std: 0.80
                       Mean reward: 231.34
               Mean episode length: 229.21
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 9.33s
                        Total time: 1611.85s
                               ETA: 1094892.9s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.702s, learning 0.167s)
               Value function loss: 6.2369
                    Surrogate loss: -0.0229
             Mean action noise std: 0.80
                       Mean reward: 237.77
               Mean episode length: 236.85
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 8.87s
                        Total time: 1620.71s
                               ETA: 1093467.7s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.441s, learning 0.211s)
               Value function loss: 7.1804
                    Surrogate loss: 0.0046
             Mean action noise std: 0.80
                       Mean reward: 238.92
               Mean episode length: 240.39
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 8.65s
                        Total time: 1629.37s
                               ETA: 1091916.3s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.521s, learning 0.158s)
               Value function loss: 6.2553
                    Surrogate loss: -0.0049
             Mean action noise std: 0.80
                       Mean reward: 230.62
               Mean episode length: 238.94
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 8.68s
                        Total time: 1638.05s
                               ETA: 1090403.3s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.552s, learning 0.171s)
               Value function loss: 7.1924
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 227.63
               Mean episode length: 240.99
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 8.72s
                        Total time: 1646.77s
                               ETA: 1088939.9s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.436s, learning 0.184s)
               Value function loss: 6.8958
                    Surrogate loss: -0.0142
             Mean action noise std: 0.80
                       Mean reward: 225.86
               Mean episode length: 242.03
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 8.62s
                        Total time: 1655.39s
                               ETA: 1087427.5s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.531s, learning 0.237s)
               Value function loss: 6.8732
                    Surrogate loss: -0.0091
             Mean action noise std: 0.80
                       Mean reward: 239.16
               Mean episode length: 245.19
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 8.77s
                        Total time: 1664.16s
                               ETA: 1086031.1s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.461s, learning 0.208s)
               Value function loss: 7.6862
                    Surrogate loss: -0.0154
             Mean action noise std: 0.80
                       Mean reward: 227.27
               Mean episode length: 243.92
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 8.67s
                        Total time: 1672.83s
                               ETA: 1084588.7s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1754 steps/s (collection: 9.157s, learning 0.183s)
               Value function loss: 6.0396
                    Surrogate loss: -0.0067
             Mean action noise std: 0.80
                       Mean reward: 231.05
               Mean episode length: 246.44
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 9.34s
                        Total time: 1682.17s
                               ETA: 1083596.8s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.714s, learning 0.195s)
               Value function loss: 8.6226
                    Surrogate loss: -0.0137
             Mean action noise std: 0.80
                       Mean reward: 226.70
               Mean episode length: 247.31
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 8.91s
                        Total time: 1691.07s
                               ETA: 1082341.5s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.747s, learning 0.213s)
               Value function loss: 16.1816
                    Surrogate loss: -0.0113
             Mean action noise std: 0.80
                       Mean reward: 232.39
               Mean episode length: 248.85
                  Mean reward/step: 0.79
       Mean episode length/episode: 5.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 8.96s
                        Total time: 1700.03s
                               ETA: 1081134.7s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.746s, learning 0.193s)
               Value function loss: 0.4384
                    Surrogate loss: -0.0245
             Mean action noise std: 0.80
                       Mean reward: 232.62
               Mean episode length: 248.85
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 8.94s
                        Total time: 1708.97s
                               ETA: 1079930.1s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.589s, learning 0.167s)
               Value function loss: 0.3937
                    Surrogate loss: -0.0277
             Mean action noise std: 0.80
                       Mean reward: 231.38
               Mean episode length: 248.39
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 8.76s
                        Total time: 1717.73s
                               ETA: 1078625.6s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.586s, learning 0.169s)
               Value function loss: 0.5046
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 229.09
               Mean episode length: 245.70
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 8.76s
                        Total time: 1726.48s
                               ETA: 1077336.7s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.804s, learning 0.184s)
               Value function loss: 0.6118
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 226.68
               Mean episode length: 243.65
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 8.99s
                        Total time: 1735.47s
                               ETA: 1076208.2s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.611s, learning 0.163s)
               Value function loss: 0.6175
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: 223.63
               Mean episode length: 241.72
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 8.77s
                        Total time: 1744.25s
                               ETA: 1074961.7s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.514s, learning 0.196s)
               Value function loss: 0.9518
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 221.70
               Mean episode length: 238.65
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 8.71s
                        Total time: 1752.96s
                               ETA: 1073690.9s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.793s, learning 0.168s)
               Value function loss: 0.7431
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 218.06
               Mean episode length: 234.43
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 8.96s
                        Total time: 1761.92s
                               ETA: 1072588.8s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.829s, learning 0.168s)
               Value function loss: 1.0865
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 213.89
               Mean episode length: 231.42
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 9.00s
                        Total time: 1770.91s
                               ETA: 1071521.5s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.461s, learning 0.211s)
               Value function loss: 1.2762
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 208.86
               Mean episode length: 225.65
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 8.67s
                        Total time: 1779.59s
                               ETA: 1070271.1s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.601s, learning 0.168s)
               Value function loss: 1.6345
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 201.40
               Mean episode length: 219.26
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 8.77s
                        Total time: 1788.36s
                               ETA: 1069093.7s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.459s, learning 0.162s)
               Value function loss: 1.6143
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 192.62
               Mean episode length: 210.39
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 8.62s
                        Total time: 1796.98s
                               ETA: 1067842.4s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1759 steps/s (collection: 9.052s, learning 0.260s)
               Value function loss: 1.7551
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 184.81
               Mean episode length: 204.57
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 9.31s
                        Total time: 1806.29s
                               ETA: 1067014.3s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.502s, learning 0.160s)
               Value function loss: 2.1430
                    Surrogate loss: -0.0019
             Mean action noise std: 0.79
                       Mean reward: 183.55
               Mean episode length: 203.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 8.66s
                        Total time: 1814.95s
                               ETA: 1065813.5s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.658s, learning 0.190s)
               Value function loss: 2.8124
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 189.90
               Mean episode length: 208.84
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 8.85s
                        Total time: 1823.80s
                               ETA: 1064735.5s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.522s, learning 0.169s)
               Value function loss: 2.7221
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 203.33
               Mean episode length: 218.14
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 8.69s
                        Total time: 1832.49s
                               ETA: 1063579.1s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.567s, learning 0.175s)
               Value function loss: 3.0278
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 217.22
               Mean episode length: 227.84
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 8.74s
                        Total time: 1841.23s
                               ETA: 1062465.1s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.609s, learning 0.265s)
               Value function loss: 4.2942
                    Surrogate loss: -0.0203
             Mean action noise std: 0.79
                       Mean reward: 225.92
               Mean episode length: 229.60
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 8.87s
                        Total time: 1850.11s
                               ETA: 1061439.8s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.861s, learning 0.170s)
               Value function loss: 3.2443
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 225.45
               Mean episode length: 229.75
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 9.03s
                        Total time: 1859.14s
                               ETA: 1060515.2s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.161s)
               Value function loss: 3.7339
                    Surrogate loss: -0.0026
             Mean action noise std: 0.79
                       Mean reward: 221.33
               Mean episode length: 227.05
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 8.72s
                        Total time: 1867.85s
                               ETA: 1059423.0s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.632s, learning 0.203s)
               Value function loss: 4.2843
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 222.90
               Mean episode length: 229.87
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 8.84s
                        Total time: 1876.69s
                               ETA: 1058410.0s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.414s, learning 0.167s)
               Value function loss: 5.2642
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 224.02
               Mean episode length: 231.62
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 8.58s
                        Total time: 1885.27s
                               ETA: 1057265.5s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.802s, learning 0.178s)
               Value function loss: 4.9153
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 228.98
               Mean episode length: 238.66
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 8.98s
                        Total time: 1894.25s
                               ETA: 1056356.2s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.802s, learning 0.157s)
               Value function loss: 5.8568
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 233.99
               Mean episode length: 242.72
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 8.96s
                        Total time: 1903.21s
                               ETA: 1055445.5s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.466s, learning 0.201s)
               Value function loss: 6.2801
                    Surrogate loss: 0.0095
             Mean action noise std: 0.79
                       Mean reward: 239.73
               Mean episode length: 243.56
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 8.67s
                        Total time: 1911.88s
                               ETA: 1054383.9s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.609s, learning 0.168s)
               Value function loss: 8.8009
                    Surrogate loss: 0.0005
             Mean action noise std: 0.79
                       Mean reward: 236.31
               Mean episode length: 240.90
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 8.78s
                        Total time: 1920.65s
                               ETA: 1053393.7s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1791 steps/s (collection: 8.977s, learning 0.170s)
               Value function loss: 5.8315
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 243.48
               Mean episode length: 246.01
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 9.15s
                        Total time: 1929.80s
                               ETA: 1052615.8s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.760s, learning 0.181s)
               Value function loss: 7.8523
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 243.29
               Mean episode length: 246.18
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 8.94s
                        Total time: 1938.74s
                               ETA: 1051734.7s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.575s, learning 0.261s)
               Value function loss: 7.1141
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 237.53
               Mean episode length: 245.27
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 8.84s
                        Total time: 1947.58s
                               ETA: 1050806.4s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.798s, learning 0.166s)
               Value function loss: 7.7768
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 242.69
               Mean episode length: 248.39
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 8.96s
                        Total time: 1956.54s
                               ETA: 1049956.7s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.447s, learning 0.178s)
               Value function loss: 9.1903
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 241.72
               Mean episode length: 245.94
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 8.62s
                        Total time: 1965.16s
                               ETA: 1048934.9s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.891s, learning 0.190s)
               Value function loss: 33.8049
                    Surrogate loss: -0.0018
             Mean action noise std: 0.79
                       Mean reward: 248.74
               Mean episode length: 248.36
                  Mean reward/step: 0.81
       Mean episode length/episode: 5.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 9.08s
                        Total time: 1974.24s
                               ETA: 1048166.3s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1811 steps/s (collection: 8.853s, learning 0.190s)
               Value function loss: 0.7338
                    Surrogate loss: -0.0184
             Mean action noise std: 0.79
                       Mean reward: 248.74
               Mean episode length: 248.36
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 9.04s
                        Total time: 1983.29s
                               ETA: 1047385.8s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.451s, learning 0.162s)
               Value function loss: 0.6507
                    Surrogate loss: -0.0262
             Mean action noise std: 0.79
                       Mean reward: 242.11
               Mean episode length: 243.50
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 8.61s
                        Total time: 1991.90s
                               ETA: 1046387.8s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1779 steps/s (collection: 9.041s, learning 0.169s)
               Value function loss: 0.6607
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 236.90
               Mean episode length: 239.33
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 9.21s
                        Total time: 2001.11s
                               ETA: 1045711.4s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.639s, learning 0.224s)
               Value function loss: 0.5124
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 234.76
               Mean episode length: 237.74
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 8.86s
                        Total time: 2009.97s
                               ETA: 1044861.8s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.881s, learning 0.159s)
               Value function loss: 1.1116
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: 228.53
               Mean episode length: 232.71
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 9.04s
                        Total time: 2019.01s
                               ETA: 1044112.7s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.914s, learning 0.169s)
               Value function loss: 1.0164
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 220.95
               Mean episode length: 225.11
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 9.08s
                        Total time: 2028.10s
                               ETA: 1043393.2s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.559s, learning 0.194s)
               Value function loss: 0.7156
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 216.93
               Mean episode length: 220.94
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 8.75s
                        Total time: 2036.85s
                               ETA: 1042512.5s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.458s, learning 0.173s)
               Value function loss: 0.8231
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 211.83
               Mean episode length: 217.07
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 8.63s
                        Total time: 2045.48s
                               ETA: 1041578.1s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.504s, learning 0.155s)
               Value function loss: 1.6314
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 198.54
               Mean episode length: 205.76
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 8.66s
                        Total time: 2054.14s
                               ETA: 1040667.7s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.648s, learning 0.303s)
               Value function loss: 1.0889
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 192.69
               Mean episode length: 202.15
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 8.95s
                        Total time: 2063.09s
                               ETA: 1039912.9s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.725s, learning 0.175s)
               Value function loss: 1.6299
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 188.62
               Mean episode length: 196.83
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 8.90s
                        Total time: 2071.99s
                               ETA: 1039140.2s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.164s)
               Value function loss: 1.5909
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 187.31
               Mean episode length: 196.75
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 8.72s
                        Total time: 2080.71s
                               ETA: 1038284.9s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.617s, learning 0.162s)
               Value function loss: 1.6926
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 194.37
               Mean episode length: 206.90
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 8.78s
                        Total time: 2089.49s
                               ETA: 1037468.1s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.766s, learning 0.249s)
               Value function loss: 1.8906
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 198.84
               Mean episode length: 212.76
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 9.01s
                        Total time: 2098.50s
                               ETA: 1036775.4s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.711s, learning 0.190s)
               Value function loss: 1.5951
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 200.85
               Mean episode length: 215.03
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 8.90s
                        Total time: 2107.40s
                               ETA: 1036033.5s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.720s, learning 0.162s)
               Value function loss: 2.1837
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 212.92
               Mean episode length: 230.84
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 8.88s
                        Total time: 2116.29s
                               ETA: 1035289.6s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.407s, learning 0.163s)
               Value function loss: 2.7478
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 219.28
               Mean episode length: 235.97
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 8.57s
                        Total time: 2124.86s
                               ETA: 1034401.0s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.174s)
               Value function loss: 2.7366
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 226.38
               Mean episode length: 243.56
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 8.67s
                        Total time: 2133.53s
                               ETA: 1033570.4s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.162s)
               Value function loss: 2.3132
                    Surrogate loss: -0.0165
             Mean action noise std: 0.79
                       Mean reward: 225.24
               Mean episode length: 243.67
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 8.72s
                        Total time: 2142.25s
                               ETA: 1032769.8s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.788s, learning 0.182s)
               Value function loss: 2.4096
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 222.29
               Mean episode length: 242.09
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 8.97s
                        Total time: 2151.22s
                               ETA: 1032098.1s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.659s, learning 0.161s)
               Value function loss: 3.3006
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 219.49
               Mean episode length: 241.48
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 8.82s
                        Total time: 2160.04s
                               ETA: 1031360.8s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.680s, learning 0.217s)
               Value function loss: 4.3542
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 218.73
               Mean episode length: 244.01
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 8.90s
                        Total time: 2168.93s
                               ETA: 1030667.0s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.837s, learning 0.198s)
               Value function loss: 3.8365
                    Surrogate loss: 0.0073
             Mean action noise std: 0.79
                       Mean reward: 216.51
               Mean episode length: 241.39
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 9.03s
                        Total time: 2177.97s
                               ETA: 1030045.0s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.624s, learning 0.159s)
               Value function loss: 4.5046
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 219.71
               Mean episode length: 244.58
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 8.78s
                        Total time: 2186.75s
                               ETA: 1029310.0s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.610s, learning 0.198s)
               Value function loss: 5.8500
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 215.06
               Mean episode length: 243.16
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 8.81s
                        Total time: 2195.56s
                               ETA: 1028593.8s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.597s, learning 0.187s)
               Value function loss: 4.9743
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 221.63
               Mean episode length: 248.11
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 8.78s
                        Total time: 2204.34s
                               ETA: 1027873.3s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1807 steps/s (collection: 8.896s, learning 0.169s)
               Value function loss: 8.0045
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 220.21
               Mean episode length: 246.57
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 9.06s
                        Total time: 2213.41s
                               ETA: 1027289.2s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.576s, learning 0.183s)
               Value function loss: 3.7674
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 222.52
               Mean episode length: 247.83
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 8.76s
                        Total time: 2222.17s
                               ETA: 1026569.6s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1794 steps/s (collection: 8.941s, learning 0.192s)
               Value function loss: 4.7590
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 219.62
               Mean episode length: 247.92
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 9.13s
                        Total time: 2231.30s
                               ETA: 1026028.1s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.635s, learning 0.163s)
               Value function loss: 3.7027
                    Surrogate loss: -0.0218
             Mean action noise std: 0.79
                       Mean reward: 218.55
               Mean episode length: 247.59
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 8.80s
                        Total time: 2240.10s
                               ETA: 1025338.1s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.891s, learning 0.169s)
               Value function loss: 21.2330
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: 219.06
               Mean episode length: 248.22
                  Mean reward/step: 0.77
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 9.06s
                        Total time: 2249.16s
                               ETA: 1024774.2s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.566s, learning 0.167s)
               Value function loss: 0.8434
                    Surrogate loss: -0.0213
             Mean action noise std: 0.79
                       Mean reward: 215.31
               Mean episode length: 243.75
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 8.73s
                        Total time: 2257.89s
                               ETA: 1024066.8s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.720s, learning 0.189s)
               Value function loss: 0.5056
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 212.55
               Mean episode length: 241.89
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 8.91s
                        Total time: 2266.80s
                               ETA: 1023444.9s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.534s, learning 0.172s)
               Value function loss: 0.5055
                    Surrogate loss: -0.0208
             Mean action noise std: 0.79
                       Mean reward: 210.98
               Mean episode length: 240.17
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 8.71s
                        Total time: 2275.51s
                               ETA: 1022737.7s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.491s, learning 0.163s)
               Value function loss: 0.4785
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 207.02
               Mean episode length: 236.60
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 8.65s
                        Total time: 2284.16s
                               ETA: 1022013.6s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.472s, learning 0.160s)
               Value function loss: 0.6481
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 205.04
               Mean episode length: 235.36
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 8.63s
                        Total time: 2292.79s
                               ETA: 1021285.7s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.788s, learning 0.166s)
               Value function loss: 0.6544
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 200.86
               Mean episode length: 232.79
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 8.95s
                        Total time: 2301.75s
                               ETA: 1020707.1s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.416s, learning 0.160s)
               Value function loss: 0.7230
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 197.93
               Mean episode length: 228.49
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 8.58s
                        Total time: 2310.32s
                               ETA: 1019966.8s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.646s, learning 0.202s)
               Value function loss: 0.8075
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 193.53
               Mean episode length: 225.69
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 8.85s
                        Total time: 2319.17s
                               ETA: 1019352.7s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.611s, learning 0.261s)
               Value function loss: 0.8706
                    Surrogate loss: -0.0182
             Mean action noise std: 0.79
                       Mean reward: 187.94
               Mean episode length: 220.44
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 8.87s
                        Total time: 2328.04s
                               ETA: 1018754.1s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.413s, learning 0.167s)
               Value function loss: 0.8884
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 190.69
               Mean episode length: 224.56
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 8.58s
                        Total time: 2336.62s
                               ETA: 1018033.0s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.748s, learning 0.192s)
               Value function loss: 1.2086
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 190.79
               Mean episode length: 225.84
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 8.94s
                        Total time: 2345.56s
                               ETA: 1017474.7s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.667s, learning 0.170s)
               Value function loss: 0.8501
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 192.05
               Mean episode length: 227.01
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 8.84s
                        Total time: 2354.40s
                               ETA: 1016876.8s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.750s, learning 0.164s)
               Value function loss: 1.3348
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 192.29
               Mean episode length: 227.47
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 8.91s
                        Total time: 2363.31s
                               ETA: 1016317.1s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.504s, learning 0.166s)
               Value function loss: 1.3699
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 191.82
               Mean episode length: 228.40
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 8.67s
                        Total time: 2371.99s
                               ETA: 1015657.6s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.665s, learning 0.161s)
               Value function loss: 1.3303
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 191.59
               Mean episode length: 232.10
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 8.83s
                        Total time: 2380.81s
                               ETA: 1015070.4s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.488s, learning 0.190s)
               Value function loss: 2.1189
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 187.16
               Mean episode length: 229.46
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 8.68s
                        Total time: 2389.49s
                               ETA: 1014425.3s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.782s, learning 0.190s)
               Value function loss: 2.1441
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 187.17
               Mean episode length: 235.01
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 8.97s
                        Total time: 2398.46s
                               ETA: 1013909.8s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.471s, learning 0.172s)
               Value function loss: 2.3870
                    Surrogate loss: 0.0051
             Mean action noise std: 0.79
                       Mean reward: 185.99
               Mean episode length: 234.52
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 8.64s
                        Total time: 2407.11s
                               ETA: 1013259.5s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.528s, learning 0.169s)
               Value function loss: 1.9823
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 186.18
               Mean episode length: 232.22
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 8.70s
                        Total time: 2415.80s
                               ETA: 1012637.6s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.872s, learning 0.163s)
               Value function loss: 2.0103
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 187.83
               Mean episode length: 234.14
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 9.03s
                        Total time: 2424.84s
                               ETA: 1012161.7s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.782s, learning 0.166s)
               Value function loss: 2.3846
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 189.38
               Mean episode length: 236.93
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 8.95s
                        Total time: 2433.79s
                               ETA: 1011653.9s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1784 steps/s (collection: 8.992s, learning 0.188s)
               Value function loss: 3.1336
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 182.59
               Mean episode length: 235.83
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 9.18s
                        Total time: 2442.97s
                               ETA: 1011246.4s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1775 steps/s (collection: 8.988s, learning 0.238s)
               Value function loss: 3.2900
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 182.03
               Mean episode length: 236.82
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 9.23s
                        Total time: 2452.19s
                               ETA: 1010860.9s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.845s, learning 0.258s)
               Value function loss: 3.4048
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 191.91
               Mean episode length: 240.63
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 9.10s
                        Total time: 2461.30s
                               ETA: 1010427.7s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.692s, learning 0.167s)
               Value function loss: 3.9443
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 188.35
               Mean episode length: 238.70
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 8.86s
                        Total time: 2470.16s
                               ETA: 1009898.6s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.642s, learning 0.162s)
               Value function loss: 3.0347
                    Surrogate loss: -0.0199
             Mean action noise std: 0.79
                       Mean reward: 194.87
               Mean episode length: 244.97
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 8.80s
                        Total time: 2478.96s
                               ETA: 1009351.2s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.826s, learning 0.163s)
               Value function loss: 3.7036
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 191.35
               Mean episode length: 244.77
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 8.99s
                        Total time: 2487.95s
                               ETA: 1008883.0s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.869s, learning 0.204s)
               Value function loss: 2.8373
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 192.36
               Mean episode length: 246.72
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 9.07s
                        Total time: 2497.02s
                               ETA: 1008452.3s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.611s, learning 0.262s)
               Value function loss: 3.5667
                    Surrogate loss: -0.0194
             Mean action noise std: 0.79
                       Mean reward: 196.38
               Mean episode length: 247.05
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 8.87s
                        Total time: 2505.89s
                               ETA: 1007945.1s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.629s, learning 0.196s)
               Value function loss: 2.6322
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 195.38
               Mean episode length: 247.16
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 8.82s
                        Total time: 2514.72s
                               ETA: 1007422.4s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.346s, learning 0.190s)
               Value function loss: 23.8573
                    Surrogate loss: 0.0246
             Mean action noise std: 0.79
                       Mean reward: 201.07
               Mean episode length: 248.59
                  Mean reward/step: 0.74
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 8.54s
                        Total time: 2523.25s
                               ETA: 1006788.4s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.835s, learning 0.266s)
               Value function loss: 2.0496
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 197.23
               Mean episode length: 245.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 9.10s
                        Total time: 2532.35s
                               ETA: 1006384.0s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.732s, learning 0.169s)
               Value function loss: 0.7677
                    Surrogate loss: -0.0159
             Mean action noise std: 0.79
                       Mean reward: 196.94
               Mean episode length: 245.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 8.90s
                        Total time: 2541.26s
                               ETA: 1005903.6s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.242s, learning 0.202s)
               Value function loss: 0.6364
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: 195.18
               Mean episode length: 243.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 8.44s
                        Total time: 2549.70s
                               ETA: 1005246.8s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.635s, learning 0.195s)
               Value function loss: 0.6748
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 189.54
               Mean episode length: 236.05
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 8.83s
                        Total time: 2558.53s
                               ETA: 1004746.5s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.629s, learning 0.168s)
               Value function loss: 0.9426
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 187.56
               Mean episode length: 234.98
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 8.80s
                        Total time: 2567.33s
                               ETA: 1004237.2s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.838s, learning 0.161s)
               Value function loss: 0.6389
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 185.69
               Mean episode length: 233.07
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 9.00s
                        Total time: 2576.32s
                               ETA: 1003810.6s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.759s, learning 0.160s)
               Value function loss: 0.8373
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 181.20
               Mean episode length: 231.57
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 8.92s
                        Total time: 2585.24s
                               ETA: 1003356.3s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.420s, learning 0.187s)
               Value function loss: 0.7707
                    Surrogate loss: -0.0017
             Mean action noise std: 0.79
                       Mean reward: 177.13
               Mean episode length: 229.35
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 8.61s
                        Total time: 2593.85s
                               ETA: 1002784.7s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.164s)
               Value function loss: 0.9859
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 171.72
               Mean episode length: 225.40
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 8.72s
                        Total time: 2602.57s
                               ETA: 1002260.4s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.789s, learning 0.162s)
               Value function loss: 0.9010
                    Surrogate loss: -0.0000
             Mean action noise std: 0.79
                       Mean reward: 174.57
               Mean episode length: 228.59
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 8.95s
                        Total time: 2611.52s
                               ETA: 1001829.2s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.612s, learning 0.168s)
               Value function loss: 0.9297
                    Surrogate loss: 0.0177
             Mean action noise std: 0.79
                       Mean reward: 179.04
               Mean episode length: 235.07
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 8.78s
                        Total time: 2620.30s
                               ETA: 1001336.0s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.893s, learning 0.189s)
               Value function loss: 0.7385
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 179.24
               Mean episode length: 236.81
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 9.08s
                        Total time: 2629.38s
                               ETA: 1000961.3s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.852s, learning 0.203s)
               Value function loss: 1.0961
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 179.69
               Mean episode length: 236.37
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 9.05s
                        Total time: 2638.44s
                               ETA: 1000579.1s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.678s, learning 0.161s)
               Value function loss: 1.2447
                    Surrogate loss: 0.0034
             Mean action noise std: 0.79
                       Mean reward: 182.49
               Mean episode length: 238.39
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 8.84s
                        Total time: 2647.28s
                               ETA: 1000118.5s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.863s, learning 0.157s)
               Value function loss: 1.1164
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 184.02
               Mean episode length: 240.29
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 9.02s
                        Total time: 2656.30s
                               ETA: 999729.3s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.525s, learning 0.163s)
               Value function loss: 1.4916
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 183.82
               Mean episode length: 237.83
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 8.69s
                        Total time: 2664.98s
                               ETA: 999218.8s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.732s, learning 0.172s)
               Value function loss: 1.5082
                    Surrogate loss: -0.0195
             Mean action noise std: 0.79
                       Mean reward: 186.63
               Mean episode length: 240.42
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 8.90s
                        Total time: 2673.89s
                               ETA: 998792.1s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.634s, learning 0.164s)
               Value function loss: 1.9036
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 187.58
               Mean episode length: 243.23
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 8.80s
                        Total time: 2682.69s
                               ETA: 998329.4s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.753s, learning 0.219s)
               Value function loss: 2.1994
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 185.05
               Mean episode length: 240.84
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 8.97s
                        Total time: 2691.66s
                               ETA: 997934.5s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.327s, learning 0.172s)
               Value function loss: 1.9693
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 183.07
               Mean episode length: 239.89
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 8.50s
                        Total time: 2700.16s
                               ETA: 997367.8s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1780 steps/s (collection: 9.010s, learning 0.191s)
               Value function loss: 2.0357
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 179.51
               Mean episode length: 238.49
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 9.20s
                        Total time: 2709.36s
                               ETA: 997063.5s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.705s, learning 0.158s)
               Value function loss: 2.5592
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 182.02
               Mean episode length: 238.17
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 8.86s
                        Total time: 2718.22s
                               ETA: 996637.4s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.535s, learning 0.164s)
               Value function loss: 2.9112
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 182.50
               Mean episode length: 237.21
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 8.70s
                        Total time: 2726.92s
                               ETA: 996154.9s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.501s, learning 0.161s)
               Value function loss: 3.8998
                    Surrogate loss: 0.0020
             Mean action noise std: 0.79
                       Mean reward: 184.41
               Mean episode length: 239.47
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 8.66s
                        Total time: 2735.58s
                               ETA: 995661.9s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.815s, learning 0.178s)
               Value function loss: 5.1309
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 182.08
               Mean episode length: 239.71
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 8.99s
                        Total time: 2744.57s
                               ETA: 995292.3s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.909s, learning 0.192s)
               Value function loss: 5.6685
                    Surrogate loss: 0.0126
             Mean action noise std: 0.79
                       Mean reward: 184.49
               Mean episode length: 243.20
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 9.10s
                        Total time: 2753.68s
                               ETA: 994964.8s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.737s, learning 0.217s)
               Value function loss: 5.2475
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 186.65
               Mean episode length: 242.59
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 8.95s
                        Total time: 2762.63s
                               ETA: 994586.5s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.551s, learning 0.162s)
               Value function loss: 4.7403
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 184.32
               Mean episode length: 242.70
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 8.71s
                        Total time: 2771.34s
                               ETA: 994124.5s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.935s, learning 0.165s)
               Value function loss: 3.8088
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 186.62
               Mean episode length: 244.46
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 9.10s
                        Total time: 2780.44s
                               ETA: 993803.8s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.464s, learning 0.169s)
               Value function loss: 3.0313
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 185.57
               Mean episode length: 245.24
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 8.63s
                        Total time: 2789.08s
                               ETA: 993319.4s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.939s, learning 0.161s)
               Value function loss: 3.7263
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 184.27
               Mean episode length: 248.70
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 9.10s
                        Total time: 2798.18s
                               ETA: 993003.9s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1788 steps/s (collection: 8.996s, learning 0.163s)
               Value function loss: 6.1989
                    Surrogate loss: -0.0209
             Mean action noise std: 0.79
                       Mean reward: 192.69
               Mean episode length: 246.31
                  Mean reward/step: 0.73
       Mean episode length/episode: 6.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 9.16s
                        Total time: 2807.34s
                               ETA: 992711.6s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.867s, learning 0.208s)
               Value function loss: 0.4777
                    Surrogate loss: -0.0255
             Mean action noise std: 0.79
                       Mean reward: 190.68
               Mean episode length: 243.96
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 9.07s
                        Total time: 2816.41s
                               ETA: 992391.4s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.341s, learning 0.207s)
               Value function loss: 0.6103
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 185.08
               Mean episode length: 237.85
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 8.55s
                        Total time: 2824.96s
                               ETA: 991888.5s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.142s, learning 0.206s)
               Value function loss: 0.5913
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 182.24
               Mean episode length: 235.75
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 8.35s
                        Total time: 2833.31s
                               ETA: 991319.1s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.914s, learning 0.187s)
               Value function loss: 0.7682
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 177.18
               Mean episode length: 232.55
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 9.10s
                        Total time: 2842.41s
                               ETA: 991016.2s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.918s, learning 0.187s)
               Value function loss: 0.8382
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 171.66
               Mean episode length: 227.25
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 9.10s
                        Total time: 2851.51s
                               ETA: 990716.6s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.597s, learning 0.266s)
               Value function loss: 0.9168
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 164.27
               Mean episode length: 219.18
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 8.86s
                        Total time: 2860.37s
                               ETA: 990335.2s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.404s, learning 0.193s)
               Value function loss: 1.1644
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 155.81
               Mean episode length: 211.48
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 8.60s
                        Total time: 2868.97s
                               ETA: 989864.9s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1758 steps/s (collection: 9.026s, learning 0.292s)
               Value function loss: 1.2431
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 152.09
               Mean episode length: 208.13
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 9.32s
                        Total time: 2878.29s
                               ETA: 989645.3s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.720s, learning 0.168s)
               Value function loss: 1.5673
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 142.81
               Mean episode length: 196.51
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 8.89s
                        Total time: 2887.18s
                               ETA: 989280.0s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1786 steps/s (collection: 8.861s, learning 0.307s)
               Value function loss: 1.1964
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 140.03
               Mean episode length: 197.83
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 9.17s
                        Total time: 2896.35s
                               ETA: 989013.0s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.626s, learning 0.157s)
               Value function loss: 1.1029
                    Surrogate loss: -0.0189
             Mean action noise std: 0.79
                       Mean reward: 139.81
               Mean episode length: 199.85
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 8.78s
                        Total time: 2905.13s
                               ETA: 988616.7s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.899s, learning 0.184s)
               Value function loss: 1.7311
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 144.69
               Mean episode length: 207.98
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 9.08s
                        Total time: 2914.21s
                               ETA: 988324.8s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.490s, learning 0.160s)
               Value function loss: 1.7827
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 148.88
               Mean episode length: 214.40
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 8.65s
                        Total time: 2922.86s
                               ETA: 987888.4s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.908s, learning 0.170s)
               Value function loss: 1.3708
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 153.00
               Mean episode length: 224.57
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 9.08s
                        Total time: 2931.94s
                               ETA: 987598.8s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.767s, learning 0.171s)
               Value function loss: 1.5681
                    Surrogate loss: 0.0105
             Mean action noise std: 0.79
                       Mean reward: 155.76
               Mean episode length: 230.16
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 8.94s
                        Total time: 2940.88s
                               ETA: 987264.3s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.328s, learning 0.166s)
               Value function loss: 1.8538
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 161.76
               Mean episode length: 231.24
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 8.49s
                        Total time: 2949.37s
                               ETA: 986783.2s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.772s, learning 0.165s)
               Value function loss: 2.3753
                    Surrogate loss: 0.0007
             Mean action noise std: 0.79
                       Mean reward: 165.58
               Mean episode length: 235.89
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 8.94s
                        Total time: 2958.31s
                               ETA: 986453.2s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.523s, learning 0.160s)
               Value function loss: 2.3833
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 159.26
               Mean episode length: 233.11
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 8.68s
                        Total time: 2966.99s
                               ETA: 986040.8s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.515s, learning 0.156s)
               Value function loss: 2.4872
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 162.08
               Mean episode length: 236.14
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 8.67s
                        Total time: 2975.66s
                               ETA: 985627.2s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.609s, learning 0.161s)
               Value function loss: 2.5381
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 163.01
               Mean episode length: 233.77
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 8.77s
                        Total time: 2984.44s
                               ETA: 985249.0s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.602s, learning 0.188s)
               Value function loss: 2.6555
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 161.99
               Mean episode length: 233.06
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 8.79s
                        Total time: 2993.23s
                               ETA: 984879.8s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.765s, learning 0.172s)
               Value function loss: 3.4185
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 164.32
               Mean episode length: 234.88
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 8.94s
                        Total time: 3002.16s
                               ETA: 984561.4s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.688s, learning 0.204s)
               Value function loss: 4.1046
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 166.00
               Mean episode length: 238.83
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 8.89s
                        Total time: 3011.06s
                               ETA: 984230.1s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1794 steps/s (collection: 8.868s, learning 0.262s)
               Value function loss: 4.8686
                    Surrogate loss: -0.0176
             Mean action noise std: 0.79
                       Mean reward: 167.83
               Mean episode length: 239.46
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 9.13s
                        Total time: 3020.19s
                               ETA: 983978.4s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.471s, learning 0.155s)
               Value function loss: 4.8814
                    Surrogate loss: -0.0167
             Mean action noise std: 0.79
                       Mean reward: 175.52
               Mean episode length: 248.24
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 8.63s
                        Total time: 3028.81s
                               ETA: 983564.7s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.802s, learning 0.171s)
               Value function loss: 4.4169
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 175.27
               Mean episode length: 245.43
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 8.97s
                        Total time: 3037.78s
                               ETA: 983265.8s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.530s, learning 0.163s)
               Value function loss: 5.0642
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 175.23
               Mean episode length: 246.21
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 8.69s
                        Total time: 3046.48s
                               ETA: 982878.8s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.381s, learning 0.218s)
               Value function loss: 4.5335
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 178.02
               Mean episode length: 247.31
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 8.60s
                        Total time: 3055.08s
                               ETA: 982463.8s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.755s, learning 0.171s)
               Value function loss: 4.2078
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 178.51
               Mean episode length: 243.25
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 8.93s
                        Total time: 3064.00s
                               ETA: 982156.1s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.399s, learning 0.276s)
               Value function loss: 3.6565
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 183.26
               Mean episode length: 249.36
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 8.68s
                        Total time: 3072.68s
                               ETA: 981770.1s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.603s, learning 0.166s)
               Value function loss: 11.5526
                    Surrogate loss: -0.0021
             Mean action noise std: 0.78
                       Mean reward: 189.43
               Mean episode length: 249.77
                  Mean reward/step: 0.79
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 8.77s
                        Total time: 3081.45s
                               ETA: 981416.7s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.282s, learning 0.169s)
               Value function loss: 0.8451
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 186.18
               Mean episode length: 246.74
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 8.45s
                        Total time: 3089.90s
                               ETA: 980964.3s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.670s, learning 0.179s)
               Value function loss: 0.7919
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 184.13
               Mean episode length: 243.65
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 8.85s
                        Total time: 3098.75s
                               ETA: 980640.5s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1791 steps/s (collection: 8.980s, learning 0.165s)
               Value function loss: 0.8192
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 180.54
               Mean episode length: 239.14
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 9.14s
                        Total time: 3107.89s
                               ETA: 980412.2s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.502s, learning 0.195s)
               Value function loss: 0.9559
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 173.56
               Mean episode length: 233.18
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 8.70s
                        Total time: 3116.59s
                               ETA: 980044.7s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.722s, learning 0.180s)
               Value function loss: 1.0285
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 162.87
               Mean episode length: 221.35
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 8.90s
                        Total time: 3125.49s
                               ETA: 979743.5s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.364s, learning 0.192s)
               Value function loss: 1.2833
                    Surrogate loss: -0.0009
             Mean action noise std: 0.78
                       Mean reward: 156.90
               Mean episode length: 214.39
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 8.56s
                        Total time: 3134.05s
                               ETA: 979336.1s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.444s, learning 0.212s)
               Value function loss: 1.0642
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 157.24
               Mean episode length: 214.27
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 8.66s
                        Total time: 3142.70s
                               ETA: 978961.9s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.433s, learning 0.203s)
               Value function loss: 1.6445
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 160.34
               Mean episode length: 215.73
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 8.64s
                        Total time: 3151.34s
                               ETA: 978584.1s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.346s, learning 0.242s)
               Value function loss: 2.3827
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 163.57
               Mean episode length: 218.54
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 8.59s
                        Total time: 3159.93s
                               ETA: 978193.9s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.392s, learning 0.191s)
               Value function loss: 2.3637
                    Surrogate loss: 0.0011
             Mean action noise std: 0.78
                       Mean reward: 163.13
               Mean episode length: 217.35
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 8.58s
                        Total time: 3168.51s
                               ETA: 977804.3s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.742s, learning 0.165s)
               Value function loss: 2.1754
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 170.19
               Mean episode length: 221.21
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 8.91s
                        Total time: 3177.42s
                               ETA: 977516.8s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.587s, learning 0.170s)
               Value function loss: 2.3666
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 169.32
               Mean episode length: 217.82
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 8.76s
                        Total time: 3186.17s
                               ETA: 977185.1s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.325s, learning 0.172s)
               Value function loss: 2.4307
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 167.61
               Mean episode length: 215.79
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 8.50s
                        Total time: 3194.67s
                               ETA: 976775.7s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.336s, learning 0.160s)
               Value function loss: 2.5492
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 168.71
               Mean episode length: 216.93
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 8.50s
                        Total time: 3203.17s
                               ETA: 976368.4s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.704s, learning 0.204s)
               Value function loss: 2.5457
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 158.79
               Mean episode length: 207.01
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 8.91s
                        Total time: 3212.07s
                               ETA: 976088.6s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.578s, learning 0.169s)
               Value function loss: 3.7804
                    Surrogate loss: 0.0017
             Mean action noise std: 0.78
                       Mean reward: 161.37
               Mean episode length: 209.40
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 8.75s
                        Total time: 3220.82s
                               ETA: 975761.9s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.426s, learning 0.175s)
               Value function loss: 2.9234
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 171.11
               Mean episode length: 216.66
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 8.60s
                        Total time: 3229.42s
                               ETA: 975393.0s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.174s)
               Value function loss: 3.4457
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: 177.87
               Mean episode length: 220.83
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 8.83s
                        Total time: 3238.25s
                               ETA: 975093.9s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.477s, learning 0.219s)
               Value function loss: 3.4541
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 172.01
               Mean episode length: 215.03
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 8.70s
                        Total time: 3246.94s
                               ETA: 974757.8s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.317s, learning 0.182s)
               Value function loss: 3.4695
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 168.90
               Mean episode length: 212.09
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 8.50s
                        Total time: 3255.44s
                               ETA: 974364.7s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.619s, learning 0.165s)
               Value function loss: 4.2454
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 166.30
               Mean episode length: 210.81
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 8.78s
                        Total time: 3264.23s
                               ETA: 974058.9s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.331s, learning 0.165s)
               Value function loss: 5.0364
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 174.77
               Mean episode length: 216.43
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 8.50s
                        Total time: 3272.72s
                               ETA: 973669.1s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.517s, learning 0.192s)
               Value function loss: 4.0755
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 172.78
               Mean episode length: 215.34
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 8.71s
                        Total time: 3281.43s
                               ETA: 973344.7s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.660s, learning 0.258s)
               Value function loss: 4.0787
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: 181.45
               Mean episode length: 221.73
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 8.92s
                        Total time: 3290.35s
                               ETA: 973084.0s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.862s, learning 0.166s)
               Value function loss: 4.3824
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 196.27
               Mean episode length: 233.10
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 9.03s
                        Total time: 3299.38s
                               ETA: 972857.2s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.276s, learning 0.169s)
               Value function loss: 4.7709
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 194.68
               Mean episode length: 232.46
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 8.44s
                        Total time: 3307.82s
                               ETA: 972460.4s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.488s, learning 0.271s)
               Value function loss: 2.4463
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 196.22
               Mean episode length: 232.30
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 8.76s
                        Total time: 3316.58s
                               ETA: 972157.9s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.619s, learning 0.237s)
               Value function loss: 2.2423
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 198.79
               Mean episode length: 234.79
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 8.86s
                        Total time: 3325.44s
                               ETA: 971885.3s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.565s, learning 0.171s)
               Value function loss: 2.6160
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 200.53
               Mean episode length: 237.51
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 8.74s
                        Total time: 3334.17s
                               ETA: 971579.5s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.501s, learning 0.232s)
               Value function loss: 2.1633
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 197.03
               Mean episode length: 236.79
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 8.73s
                        Total time: 3342.90s
                               ETA: 971274.6s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.477s, learning 0.165s)
               Value function loss: 4.6332
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 216.03
               Mean episode length: 247.86
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 8.64s
                        Total time: 3351.55s
                               ETA: 970944.9s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.651s, learning 0.168s)
               Value function loss: 0.7602
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 205.46
               Mean episode length: 240.29
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 8.82s
                        Total time: 3360.37s
                               ETA: 970668.2s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.120s, learning 0.158s)
               Value function loss: 0.8950
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 191.97
               Mean episode length: 232.14
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 8.28s
                        Total time: 3368.64s
                               ETA: 970237.5s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.763s, learning 0.264s)
               Value function loss: 0.9497
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 178.07
               Mean episode length: 221.85
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 9.03s
                        Total time: 3377.67s
                               ETA: 970023.9s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.511s, learning 0.198s)
               Value function loss: 0.9988
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 166.32
               Mean episode length: 212.53
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 8.71s
                        Total time: 3386.38s
                               ETA: 969720.6s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.458s, learning 0.170s)
               Value function loss: 1.6245
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 162.86
               Mean episode length: 210.41
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 8.63s
                        Total time: 3395.01s
                               ETA: 969395.7s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.371s, learning 0.189s)
               Value function loss: 1.0289
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 166.59
               Mean episode length: 215.18
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 8.56s
                        Total time: 3403.57s
                               ETA: 969053.5s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.811s, learning 0.183s)
               Value function loss: 1.2124
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 172.19
               Mean episode length: 220.57
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 8.99s
                        Total time: 3412.56s
                               ETA: 968836.4s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.590s, learning 0.189s)
               Value function loss: 0.9931
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 174.25
               Mean episode length: 224.55
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 8.78s
                        Total time: 3421.34s
                               ETA: 968559.7s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.340s, learning 0.178s)
               Value function loss: 1.0601
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 178.48
               Mean episode length: 229.29
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 8.52s
                        Total time: 3429.86s
                               ETA: 968210.6s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.620s, learning 0.182s)
               Value function loss: 1.2482
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 181.37
               Mean episode length: 231.71
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 8.80s
                        Total time: 3438.66s
                               ETA: 967943.5s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.492s, learning 0.195s)
               Value function loss: 1.5118
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 185.71
               Mean episode length: 232.87
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 8.69s
                        Total time: 3447.35s
                               ETA: 967645.6s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.402s, learning 0.166s)
               Value function loss: 1.5920
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 178.59
               Mean episode length: 228.57
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 8.57s
                        Total time: 3455.91s
                               ETA: 967316.1s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.266s, learning 0.157s)
               Value function loss: 1.2660
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 176.88
               Mean episode length: 230.30
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 8.42s
                        Total time: 3464.34s
                               ETA: 966947.7s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.714s, learning 0.189s)
               Value function loss: 1.5587
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 181.34
               Mean episode length: 234.88
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 8.90s
                        Total time: 3473.24s
                               ETA: 966715.0s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.729s, learning 0.214s)
               Value function loss: 1.4571
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 188.56
               Mean episode length: 240.36
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 8.94s
                        Total time: 3482.18s
                               ETA: 966494.5s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.821s, learning 0.187s)
               Value function loss: 2.2045
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 190.54
               Mean episode length: 240.48
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 9.01s
                        Total time: 3491.19s
                               ETA: 966293.3s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.191s)
               Value function loss: 2.0037
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 191.48
               Mean episode length: 242.37
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 8.79s
                        Total time: 3499.98s
                               ETA: 966033.6s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.617s, learning 0.164s)
               Value function loss: 1.9802
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 193.02
               Mean episode length: 244.42
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 8.78s
                        Total time: 3508.76s
                               ETA: 965772.2s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.352s, learning 0.203s)
               Value function loss: 2.9710
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 194.24
               Mean episode length: 244.77
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 8.56s
                        Total time: 3517.32s
                               ETA: 965450.3s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.164s)
               Value function loss: 3.2159
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 193.52
               Mean episode length: 241.71
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 8.47s
                        Total time: 3525.78s
                               ETA: 965106.1s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.651s, learning 0.176s)
               Value function loss: 3.1858
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 198.15
               Mean episode length: 246.12
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 8.83s
                        Total time: 3534.61s
                               ETA: 964861.8s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.251s, learning 0.158s)
               Value function loss: 3.9368
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 201.98
               Mean episode length: 248.77
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 8.41s
                        Total time: 3543.02s
                               ETA: 964505.0s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1791 steps/s (collection: 8.973s, learning 0.171s)
               Value function loss: 4.9689
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 198.78
               Mean episode length: 244.51
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 9.14s
                        Total time: 3552.16s
                               ETA: 964349.7s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.550s, learning 0.219s)
               Value function loss: 5.5386
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 198.86
               Mean episode length: 244.10
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 8.77s
                        Total time: 3560.93s
                               ETA: 964093.6s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.753s, learning 0.195s)
               Value function loss: 4.9743
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 206.36
               Mean episode length: 247.67
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 8.95s
                        Total time: 3569.88s
                               ETA: 963887.2s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1758 steps/s (collection: 9.153s, learning 0.164s)
               Value function loss: 3.9073
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 203.75
               Mean episode length: 246.43
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 9.32s
                        Total time: 3579.20s
                               ETA: 963781.4s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.530s, learning 0.231s)
               Value function loss: 4.8057
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 205.69
               Mean episode length: 246.29
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 8.76s
                        Total time: 3587.96s
                               ETA: 963526.5s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.386s, learning 0.190s)
               Value function loss: 4.2174
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 206.61
               Mean episode length: 245.87
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 8.58s
                        Total time: 3596.53s
                               ETA: 963223.5s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.630s, learning 0.214s)
               Value function loss: 4.4641
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 211.40
               Mean episode length: 247.92
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 8.84s
                        Total time: 3605.38s
                               ETA: 962993.7s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.487s, learning 0.194s)
               Value function loss: 3.9115
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 210.33
               Mean episode length: 247.69
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 8.68s
                        Total time: 3614.06s
                               ETA: 962721.7s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.620s, learning 0.159s)
               Value function loss: 10.2821
                    Surrogate loss: -0.0004
             Mean action noise std: 0.78
                       Mean reward: 214.65
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 8.78s
                        Total time: 3622.84s
                               ETA: 962477.2s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.571s, learning 0.224s)
               Value function loss: 1.6357
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 212.68
               Mean episode length: 247.32
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 8.80s
                        Total time: 3631.63s
                               ETA: 962238.1s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.602s, learning 0.210s)
               Value function loss: 1.5400
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 210.95
               Mean episode length: 245.17
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 8.81s
                        Total time: 3640.45s
                               ETA: 962004.6s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.606s, learning 0.167s)
               Value function loss: 1.7010
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 211.07
               Mean episode length: 243.34
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 8.77s
                        Total time: 3649.22s
                               ETA: 961762.0s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.483s, learning 0.161s)
               Value function loss: 1.9388
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 208.41
               Mean episode length: 239.08
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 8.64s
                        Total time: 3657.86s
                               ETA: 961486.7s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.522s, learning 0.201s)
               Value function loss: 2.1210
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 207.41
               Mean episode length: 238.05
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 8.72s
                        Total time: 3666.58s
                               ETA: 961233.7s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.816s, learning 0.190s)
               Value function loss: 1.7576
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 206.88
               Mean episode length: 236.63
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 9.01s
                        Total time: 3675.59s
                               ETA: 961055.9s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.181s, learning 0.208s)
               Value function loss: 1.7647
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 207.12
               Mean episode length: 237.79
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 8.39s
                        Total time: 3683.98s
                               ETA: 960718.0s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.676s, learning 0.179s)
               Value function loss: 1.9091
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 209.93
               Mean episode length: 237.85
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 8.85s
                        Total time: 3692.83s
                               ETA: 960503.1s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.842s, learning 0.165s)
               Value function loss: 1.7628
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 209.46
               Mean episode length: 239.23
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 9.01s
                        Total time: 3701.84s
                               ETA: 960328.8s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.714s, learning 0.210s)
               Value function loss: 2.0845
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 212.20
               Mean episode length: 239.44
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 8.92s
                        Total time: 3710.76s
                               ETA: 960133.9s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.477s, learning 0.213s)
               Value function loss: 2.4376
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 216.41
               Mean episode length: 243.08
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 8.69s
                        Total time: 3719.45s
                               ETA: 959879.4s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.586s, learning 0.216s)
               Value function loss: 3.4628
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 222.94
               Mean episode length: 247.25
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 8.80s
                        Total time: 3728.26s
                               ETA: 959655.2s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.693s, learning 0.165s)
               Value function loss: 2.3188
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 221.19
               Mean episode length: 247.25
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 8.86s
                        Total time: 3737.11s
                               ETA: 959446.4s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.172s)
               Value function loss: 3.3342
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 221.41
               Mean episode length: 249.41
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 8.67s
                        Total time: 3745.78s
                               ETA: 959190.5s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.873s, learning 0.165s)
               Value function loss: 3.0515
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 220.65
               Mean episode length: 247.91
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 9.04s
                        Total time: 3754.82s
                               ETA: 959029.8s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.495s, learning 0.166s)
               Value function loss: 4.1674
                    Surrogate loss: 0.0017
             Mean action noise std: 0.78
                       Mean reward: 221.86
               Mean episode length: 248.50
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 8.66s
                        Total time: 3763.48s
                               ETA: 958773.9s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.889s, learning 0.218s)
               Value function loss: 4.1897
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 223.66
               Mean episode length: 248.26
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 9.11s
                        Total time: 3772.59s
                               ETA: 958632.6s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.309s, learning 0.192s)
               Value function loss: 4.2392
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 222.07
               Mean episode length: 249.22
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 8.50s
                        Total time: 3781.09s
                               ETA: 958338.4s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.430s, learning 0.262s)
               Value function loss: 5.2789
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 216.68
               Mean episode length: 243.71
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 8.69s
                        Total time: 3789.78s
                               ETA: 958093.9s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.715s, learning 0.253s)
               Value function loss: 4.9925
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 220.24
               Mean episode length: 248.02
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 8.97s
                        Total time: 3798.75s
                               ETA: 957920.4s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.334s, learning 0.212s)
               Value function loss: 4.9975
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 218.06
               Mean episode length: 247.69
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 8.55s
                        Total time: 3807.30s
                               ETA: 957641.5s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.238s, learning 0.265s)
               Value function loss: 6.8867
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: 221.89
               Mean episode length: 248.29
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 8.50s
                        Total time: 3815.80s
                               ETA: 957353.1s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.734s, learning 0.169s)
               Value function loss: 4.6307
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 216.29
               Mean episode length: 243.79
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 8.90s
                        Total time: 3824.71s
                               ETA: 957166.1s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.175s)
               Value function loss: 7.4114
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 218.89
               Mean episode length: 242.88
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 8.56s
                        Total time: 3833.27s
                               ETA: 956894.9s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.306s, learning 0.201s)
               Value function loss: 6.7551
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 218.39
               Mean episode length: 245.99
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 8.51s
                        Total time: 3841.77s
                               ETA: 956611.4s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.299s, learning 0.253s)
               Value function loss: 4.4068
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 219.98
               Mean episode length: 242.83
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 8.55s
                        Total time: 3850.33s
                               ETA: 956340.3s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1807 steps/s (collection: 8.893s, learning 0.169s)
               Value function loss: 3.9213
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 224.02
               Mean episode length: 250.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 9.06s
                        Total time: 3859.39s
                               ETA: 956197.2s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.545s, learning 0.197s)
               Value function loss: 3.6900
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 217.87
               Mean episode length: 239.91
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 8.74s
                        Total time: 3868.13s
                               ETA: 955975.6s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.534s, learning 0.165s)
               Value function loss: 3.7399
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 215.76
               Mean episode length: 239.91
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 8.70s
                        Total time: 3876.83s
                               ETA: 955744.3s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.530s, learning 0.185s)
               Value function loss: 4.7715
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 223.65
               Mean episode length: 247.73
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 8.71s
                        Total time: 3885.55s
                               ETA: 955517.9s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.492s, learning 0.166s)
               Value function loss: 3.2114
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 216.23
               Mean episode length: 239.67
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 8.66s
                        Total time: 3894.20s
                               ETA: 955278.9s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.239s, learning 0.165s)
               Value function loss: 3.4049
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 224.61
               Mean episode length: 246.29
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 8.40s
                        Total time: 3902.61s
                               ETA: 954978.9s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.530s, learning 0.179s)
               Value function loss: 1.1341
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 224.72
               Mean episode length: 245.87
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 8.71s
                        Total time: 3911.32s
                               ETA: 954754.5s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.615s, learning 0.168s)
               Value function loss: 1.6050
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 221.11
               Mean episode length: 241.87
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 8.78s
                        Total time: 3920.10s
                               ETA: 954549.1s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.522s, learning 0.191s)
               Value function loss: 1.7644
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 216.64
               Mean episode length: 236.26
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 8.71s
                        Total time: 3928.81s
                               ETA: 954327.8s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.588s, learning 0.199s)
               Value function loss: 2.3731
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 217.33
               Mean episode length: 234.82
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 8.79s
                        Total time: 3937.60s
                               ETA: 954125.4s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.737s, learning 0.187s)
               Value function loss: 2.4651
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 218.73
               Mean episode length: 235.81
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 8.92s
                        Total time: 3946.52s
                               ETA: 953957.1s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.663s, learning 0.163s)
               Value function loss: 1.7924
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 213.66
               Mean episode length: 231.82
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 8.83s
                        Total time: 3955.35s
                               ETA: 953766.0s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.737s, learning 0.179s)
               Value function loss: 2.3312
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 213.04
               Mean episode length: 232.32
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 8.92s
                        Total time: 3964.27s
                               ETA: 953597.5s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.168s)
               Value function loss: 1.6243
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 214.31
               Mean episode length: 233.98
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 8.49s
                        Total time: 3972.75s
                               ETA: 953327.2s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1762 steps/s (collection: 9.088s, learning 0.208s)
               Value function loss: 1.9428
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 218.63
               Mean episode length: 238.06
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 9.30s
                        Total time: 3982.05s
                               ETA: 953251.2s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.600s, learning 0.272s)
               Value function loss: 2.3064
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 222.28
               Mean episode length: 240.22
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 8.87s
                        Total time: 3990.92s
                               ETA: 953074.5s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.579s, learning 0.159s)
               Value function loss: 3.5945
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 224.76
               Mean episode length: 243.44
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 8.74s
                        Total time: 3999.66s
                               ETA: 952866.7s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.650s, learning 0.164s)
               Value function loss: 3.1248
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 225.16
               Mean episode length: 247.12
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 8.81s
                        Total time: 4008.48s
                               ETA: 952677.9s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.667s, learning 0.172s)
               Value function loss: 3.0995
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 227.87
               Mean episode length: 247.22
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 8.84s
                        Total time: 4017.32s
                               ETA: 952496.0s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.616s, learning 0.184s)
               Value function loss: 3.2300
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 227.73
               Mean episode length: 245.83
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 8.80s
                        Total time: 4026.12s
                               ETA: 952305.5s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.164s, learning 0.204s)
               Value function loss: 3.3770
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 224.25
               Mean episode length: 243.85
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 8.37s
                        Total time: 4034.48s
                               ETA: 952013.8s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.924s, learning 0.173s)
               Value function loss: 4.5832
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 230.28
               Mean episode length: 248.63
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 9.10s
                        Total time: 4043.58s
                               ETA: 951895.1s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.346s, learning 0.194s)
               Value function loss: 4.5505
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 229.34
               Mean episode length: 247.66
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 8.54s
                        Total time: 4052.12s
                               ETA: 951646.0s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.425s, learning 0.188s)
               Value function loss: 4.8037
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 229.26
               Mean episode length: 247.45
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 8.61s
                        Total time: 4060.73s
                               ETA: 951415.4s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1811 steps/s (collection: 8.858s, learning 0.185s)
               Value function loss: 6.6929
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 229.86
               Mean episode length: 249.02
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 9.04s
                        Total time: 4069.78s
                               ETA: 951286.2s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.766s, learning 0.241s)
               Value function loss: 5.8439
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 230.27
               Mean episode length: 247.54
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 9.01s
                        Total time: 4078.78s
                               ETA: 951149.3s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.553s, learning 0.178s)
               Value function loss: 7.7529
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 226.46
               Mean episode length: 243.96
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 8.73s
                        Total time: 4087.51s
                               ETA: 950948.6s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.548s, learning 0.217s)
               Value function loss: 7.7040
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 230.68
               Mean episode length: 247.02
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 8.77s
                        Total time: 4096.28s
                               ETA: 950756.9s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.336s, learning 0.165s)
               Value function loss: 9.4660
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 229.10
               Mean episode length: 248.76
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 8.50s
                        Total time: 4104.78s
                               ETA: 950504.7s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.708s, learning 0.187s)
               Value function loss: 6.8458
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 231.86
               Mean episode length: 247.69
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 8.89s
                        Total time: 4113.67s
                               ETA: 950344.7s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.848s, learning 0.160s)
               Value function loss: 6.3385
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 233.22
               Mean episode length: 247.94
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 9.01s
                        Total time: 4122.68s
                               ETA: 950211.6s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.330s, learning 0.166s)
               Value function loss: 5.4916
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 232.04
               Mean episode length: 247.94
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 8.50s
                        Total time: 4131.18s
                               ETA: 949961.2s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.651s, learning 0.161s)
               Value function loss: 5.2561
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 228.09
               Mean episode length: 242.32
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 8.81s
                        Total time: 4139.99s
                               ETA: 949784.3s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.620s, learning 0.168s)
               Value function loss: 4.3848
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 225.14
               Mean episode length: 240.21
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 8.79s
                        Total time: 4148.78s
                               ETA: 949602.9s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.597s, learning 0.167s)
               Value function loss: 6.2035
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 229.71
               Mean episode length: 244.03
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 8.76s
                        Total time: 4157.54s
                               ETA: 949416.9s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.413s, learning 0.166s)
               Value function loss: 3.6957
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 225.18
               Mean episode length: 240.34
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 8.58s
                        Total time: 4166.12s
                               ETA: 949189.5s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.585s, learning 0.162s)
               Value function loss: 6.7793
                    Surrogate loss: -0.0012
             Mean action noise std: 0.78
                       Mean reward: 233.71
               Mean episode length: 248.07
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 8.75s
                        Total time: 4174.87s
                               ETA: 949001.2s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.671s, learning 0.167s)
               Value function loss: 2.0152
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 230.29
               Mean episode length: 244.62
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 8.84s
                        Total time: 4183.71s
                               ETA: 948834.3s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.581s, learning 0.168s)
               Value function loss: 1.8620
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 226.48
               Mean episode length: 241.05
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 8.75s
                        Total time: 4192.46s
                               ETA: 948648.0s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.361s, learning 0.162s)
               Value function loss: 2.4152
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 222.18
               Mean episode length: 237.16
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 8.52s
                        Total time: 4200.98s
                               ETA: 948411.6s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.518s, learning 0.165s)
               Value function loss: 2.5656
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 225.94
               Mean episode length: 242.29
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 8.68s
                        Total time: 4209.66s
                               ETA: 948212.2s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.478s, learning 0.205s)
               Value function loss: 2.6988
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 229.65
               Mean episode length: 246.61
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 8.68s
                        Total time: 4218.35s
                               ETA: 948013.6s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.163s)
               Value function loss: 2.6591
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 233.58
               Mean episode length: 248.63
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 8.64s
                        Total time: 4226.98s
                               ETA: 947806.0s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.694s, learning 0.215s)
               Value function loss: 2.5175
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 232.55
               Mean episode length: 248.16
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 8.91s
                        Total time: 4235.89s
                               ETA: 947659.9s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.480s, learning 0.178s)
               Value function loss: 2.2191
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 230.50
               Mean episode length: 246.28
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 8.66s
                        Total time: 4244.55s
                               ETA: 947458.3s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.398s, learning 0.309s)
               Value function loss: 2.7327
                    Surrogate loss: -0.0026
             Mean action noise std: 0.78
                       Mean reward: 229.05
               Mean episode length: 245.13
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 8.71s
                        Total time: 4253.26s
                               ETA: 947268.5s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.547s, learning 0.227s)
               Value function loss: 2.7756
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 230.15
               Mean episode length: 245.29
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 8.77s
                        Total time: 4262.03s
                               ETA: 947094.5s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.514s, learning 0.207s)
               Value function loss: 4.2174
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 232.97
               Mean episode length: 248.32
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 8.72s
                        Total time: 4270.75s
                               ETA: 946909.1s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.299s, learning 0.275s)
               Value function loss: 4.0334
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 231.29
               Mean episode length: 246.77
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 8.57s
                        Total time: 4279.33s
                               ETA: 946692.3s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.460s, learning 0.315s)
               Value function loss: 3.0127
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 231.02
               Mean episode length: 246.77
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 8.78s
                        Total time: 4288.10s
                               ETA: 946520.6s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.442s, learning 0.204s)
               Value function loss: 3.9940
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 234.45
               Mean episode length: 248.78
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 8.65s
                        Total time: 4296.75s
                               ETA: 946321.4s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1786 steps/s (collection: 8.909s, learning 0.262s)
               Value function loss: 3.6687
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 231.89
               Mean episode length: 245.17
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 9.17s
                        Total time: 4305.92s
                               ETA: 946238.2s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.447s, learning 0.165s)
               Value function loss: 6.1451
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 228.14
               Mean episode length: 245.07
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 8.61s
                        Total time: 4314.53s
                               ETA: 946032.9s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.683s, learning 0.177s)
               Value function loss: 5.4635
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 231.34
               Mean episode length: 247.93
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 8.86s
                        Total time: 4323.39s
                               ETA: 945882.7s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.767s, learning 0.169s)
               Value function loss: 4.6631
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 233.06
               Mean episode length: 250.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 8.94s
                        Total time: 4332.33s
                               ETA: 945749.7s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.393s, learning 0.162s)
               Value function loss: 6.0947
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 230.68
               Mean episode length: 247.91
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 8.55s
                        Total time: 4340.89s
                               ETA: 945534.1s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.612s, learning 0.170s)
               Value function loss: 5.4945
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 232.25
               Mean episode length: 247.79
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 8.78s
                        Total time: 4349.67s
                               ETA: 945368.7s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.794s, learning 0.160s)
               Value function loss: 6.4224
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 234.23
               Mean episode length: 250.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 8.95s
                        Total time: 4358.62s
                               ETA: 945241.3s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.807s, learning 0.188s)
               Value function loss: 6.3424
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 234.82
               Mean episode length: 248.06
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 9.00s
                        Total time: 4367.62s
                               ETA: 945123.5s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.524s, learning 0.164s)
               Value function loss: 6.6688
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 232.99
               Mean episode length: 250.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 8.69s
                        Total time: 4376.30s
                               ETA: 944939.8s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.501s, learning 0.190s)
               Value function loss: 7.0830
                    Surrogate loss: -0.0043
             Mean action noise std: 0.78
                       Mean reward: 234.12
               Mean episode length: 250.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 8.69s
                        Total time: 4384.99s
                               ETA: 944757.5s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.516s, learning 0.175s)
               Value function loss: 6.1903
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 233.78
               Mean episode length: 249.49
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 8.69s
                        Total time: 4393.69s
                               ETA: 944575.9s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.170s)
               Value function loss: 4.5710
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 231.57
               Mean episode length: 247.46
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 8.46s
                        Total time: 4402.14s
                               ETA: 944344.7s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.526s, learning 0.160s)
               Value function loss: 4.5827
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 227.28
               Mean episode length: 243.11
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 8.69s
                        Total time: 4410.83s
                               ETA: 944163.8s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.669s, learning 0.216s)
               Value function loss: 4.2083
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 227.02
               Mean episode length: 245.31
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 8.88s
                        Total time: 4419.71s
                               ETA: 944026.0s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.631s, learning 0.278s)
               Value function loss: 5.2287
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 227.50
               Mean episode length: 248.12
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 8.91s
                        Total time: 4428.62s
                               ETA: 943893.8s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.861s, learning 0.170s)
               Value function loss: 4.1522
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 224.29
               Mean episode length: 244.65
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 9.03s
                        Total time: 4437.65s
                               ETA: 943788.0s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.860s, learning 0.165s)
               Value function loss: 8.6472
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 229.66
               Mean episode length: 248.59
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 9.02s
                        Total time: 4446.68s
                               ETA: 943681.4s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.684s, learning 0.167s)
               Value function loss: 1.7936
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 225.57
               Mean episode length: 244.89
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 8.85s
                        Total time: 4455.53s
                               ETA: 943538.4s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.602s, learning 0.161s)
               Value function loss: 1.7682
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 220.09
               Mean episode length: 240.06
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 8.76s
                        Total time: 4464.29s
                               ETA: 943377.3s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.496s, learning 0.171s)
               Value function loss: 1.9735
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 217.80
               Mean episode length: 236.50
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 8.67s
                        Total time: 4472.96s
                               ETA: 943196.7s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.619s, learning 0.176s)
               Value function loss: 2.1063
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 222.28
               Mean episode length: 240.57
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 8.80s
                        Total time: 4481.75s
                               ETA: 943043.9s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.786s, learning 0.198s)
               Value function loss: 3.5559
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 221.76
               Mean episode length: 242.58
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 8.98s
                        Total time: 4490.74s
                               ETA: 942931.3s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.851s, learning 0.245s)
               Value function loss: 2.4946
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 217.43
               Mean episode length: 238.69
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 9.10s
                        Total time: 4499.83s
                               ETA: 942842.7s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.568s, learning 0.225s)
               Value function loss: 2.4472
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 216.30
               Mean episode length: 237.78
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 8.79s
                        Total time: 4508.63s
                               ETA: 942691.1s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.593s, learning 0.200s)
               Value function loss: 2.2774
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 214.75
               Mean episode length: 235.09
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 8.79s
                        Total time: 4517.42s
                               ETA: 942539.9s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.671s, learning 0.177s)
               Value function loss: 2.3398
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 214.24
               Mean episode length: 234.43
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 8.85s
                        Total time: 4526.27s
                               ETA: 942400.9s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.264s, learning 0.162s)
               Value function loss: 2.5640
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 211.12
               Mean episode length: 233.90
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 8.43s
                        Total time: 4534.69s
                               ETA: 942174.7s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.347s, learning 0.164s)
               Value function loss: 3.4975
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 217.42
               Mean episode length: 241.50
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 8.51s
                        Total time: 4543.20s
                               ETA: 941967.0s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.240s, learning 0.164s)
               Value function loss: 3.5647
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 223.53
               Mean episode length: 246.52
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 8.40s
                        Total time: 4551.61s
                               ETA: 941737.9s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.475s, learning 0.162s)
               Value function loss: 2.5876
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 220.45
               Mean episode length: 242.51
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 8.64s
                        Total time: 4560.24s
                               ETA: 941557.9s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.314s, learning 0.167s)
               Value function loss: 3.1736
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 219.34
               Mean episode length: 244.50
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 8.48s
                        Total time: 4568.72s
                               ETA: 941346.5s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.692s, learning 0.163s)
               Value function loss: 3.3531
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 218.32
               Mean episode length: 245.26
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 8.86s
                        Total time: 4577.58s
                               ETA: 941213.0s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.576s, learning 0.179s)
               Value function loss: 4.7806
                    Surrogate loss: 0.0017
             Mean action noise std: 0.78
                       Mean reward: 219.34
               Mean episode length: 247.27
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 8.75s
                        Total time: 4586.34s
                               ETA: 941059.3s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.647s, learning 0.206s)
               Value function loss: 4.8712
                    Surrogate loss: 0.0035
             Mean action noise std: 0.78
                       Mean reward: 220.23
               Mean episode length: 248.29
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 8.85s
                        Total time: 4595.19s
                               ETA: 940926.4s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.640s, learning 0.222s)
               Value function loss: 3.4867
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 215.79
               Mean episode length: 246.92
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 8.86s
                        Total time: 4604.05s
                               ETA: 940795.9s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.436s, learning 0.174s)
               Value function loss: 5.8158
                    Surrogate loss: 0.0067
             Mean action noise std: 0.78
                       Mean reward: 211.05
               Mean episode length: 248.96
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 8.61s
                        Total time: 4612.66s
                               ETA: 940614.3s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.759s, learning 0.163s)
               Value function loss: 5.1435
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 214.32
               Mean episode length: 249.36
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 8.92s
                        Total time: 4621.58s
                               ETA: 940496.9s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.292s, learning 0.182s)
               Value function loss: 5.0425
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 210.86
               Mean episode length: 247.70
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 8.47s
                        Total time: 4630.06s
                               ETA: 940289.0s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1778 steps/s (collection: 8.991s, learning 0.219s)
               Value function loss: 6.7447
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 209.90
               Mean episode length: 247.66
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 9.21s
                        Total time: 4639.27s
                               ETA: 940231.3s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.410s, learning 0.197s)
               Value function loss: 7.5635
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 209.05
               Mean episode length: 247.59
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 8.61s
                        Total time: 4647.88s
                               ETA: 940051.7s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.763s, learning 0.242s)
               Value function loss: 7.5114
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 207.12
               Mean episode length: 247.92
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 9.00s
                        Total time: 4656.88s
                               ETA: 939952.9s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1807 steps/s (collection: 8.773s, learning 0.290s)
               Value function loss: 7.7562
                    Surrogate loss: 0.0095
             Mean action noise std: 0.78
                       Mean reward: 207.98
               Mean episode length: 249.64
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 9.06s
                        Total time: 4665.94s
                               ETA: 939866.4s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.332s, learning 0.178s)
               Value function loss: 5.7118
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 209.15
               Mean episode length: 249.47
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 8.51s
                        Total time: 4674.45s
                               ETA: 939668.9s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.529s, learning 0.167s)
               Value function loss: 4.8856
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 205.00
               Mean episode length: 249.35
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 8.70s
                        Total time: 4683.15s
                               ETA: 939509.4s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.782s, learning 0.266s)
               Value function loss: 4.5259
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 208.72
               Mean episode length: 250.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 9.05s
                        Total time: 4692.20s
                               ETA: 939421.1s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.688s, learning 0.175s)
               Value function loss: 4.9678
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 205.24
               Mean episode length: 247.85
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 8.86s
                        Total time: 4701.06s
                               ETA: 939296.0s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.711s, learning 0.160s)
               Value function loss: 5.3916
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 198.35
               Mean episode length: 245.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 8.87s
                        Total time: 4709.93s
                               ETA: 939173.3s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.493s, learning 0.218s)
               Value function loss: 11.2659
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 204.18
               Mean episode length: 250.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 8.71s
                        Total time: 4718.64s
                               ETA: 939018.9s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.641s, learning 0.268s)
               Value function loss: 2.0135
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 197.61
               Mean episode length: 243.25
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 8.91s
                        Total time: 4727.55s
                               ETA: 938904.4s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.761s, learning 0.225s)
               Value function loss: 2.1368
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 193.73
               Mean episode length: 243.16
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 8.99s
                        Total time: 4736.53s
                               ETA: 938805.7s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.445s, learning 0.268s)
               Value function loss: 2.2917
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 190.49
               Mean episode length: 239.29
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 8.71s
                        Total time: 4745.25s
                               ETA: 938653.3s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.791s, learning 0.166s)
               Value function loss: 2.2939
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 190.52
               Mean episode length: 241.04
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 8.96s
                        Total time: 4754.20s
                               ETA: 938549.7s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.066s, learning 0.162s)
               Value function loss: 2.7513
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 188.88
               Mean episode length: 240.99
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 8.23s
                        Total time: 4762.43s
                               ETA: 938302.8s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.448s, learning 0.209s)
               Value function loss: 2.5609
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 189.37
               Mean episode length: 245.92
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 8.66s
                        Total time: 4771.09s
                               ETA: 938141.1s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.407s, learning 0.168s)
               Value function loss: 2.3532
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 188.88
               Mean episode length: 245.60
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 8.57s
                        Total time: 4779.66s
                               ETA: 937964.0s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.339s, learning 0.197s)
               Value function loss: 2.4927
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 187.70
               Mean episode length: 243.32
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 8.54s
                        Total time: 4788.20s
                               ETA: 937780.1s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.488s, learning 0.191s)
               Value function loss: 2.1806
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 189.24
               Mean episode length: 243.55
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 8.68s
                        Total time: 4796.88s
                               ETA: 937624.8s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.413s, learning 0.180s)
               Value function loss: 2.4337
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 188.40
               Mean episode length: 245.61
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 8.59s
                        Total time: 4805.47s
                               ETA: 937453.2s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.629s, learning 0.163s)
               Value function loss: 2.6021
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 188.70
               Mean episode length: 244.93
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 8.79s
                        Total time: 4814.26s
                               ETA: 937321.0s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.689s, learning 0.160s)
               Value function loss: 3.5191
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 194.09
               Mean episode length: 246.07
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 8.85s
                        Total time: 4823.11s
                               ETA: 937200.4s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.569s, learning 0.194s)
               Value function loss: 2.3659
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 195.22
               Mean episode length: 248.05
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 8.76s
                        Total time: 4831.88s
                               ETA: 937063.5s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.603s, learning 0.164s)
               Value function loss: 3.1362
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 194.19
               Mean episode length: 249.87
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 8.77s
                        Total time: 4840.64s
                               ETA: 936927.8s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.224s, learning 0.170s)
               Value function loss: 2.9845
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 191.72
               Mean episode length: 247.80
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 8.39s
                        Total time: 4849.04s
                               ETA: 936720.6s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.384s, learning 0.192s)
               Value function loss: 3.7616
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 191.59
               Mean episode length: 249.56
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 8.58s
                        Total time: 4857.61s
                               ETA: 936549.4s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.447s, learning 0.157s)
               Value function loss: 3.6472
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 195.15
               Mean episode length: 249.71
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 8.60s
                        Total time: 4866.22s
                               ETA: 936384.3s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.519s, learning 0.236s)
               Value function loss: 3.8353
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 188.71
               Mean episode length: 244.94
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 8.75s
                        Total time: 4874.97s
                               ETA: 936248.5s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.589s, learning 0.170s)
               Value function loss: 4.3176
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 190.67
               Mean episode length: 244.91
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 8.76s
                        Total time: 4883.73s
                               ETA: 936114.0s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1766 steps/s (collection: 9.092s, learning 0.185s)
               Value function loss: 4.3725
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 197.43
               Mean episode length: 249.77
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 9.28s
                        Total time: 4893.01s
                               ETA: 936079.2s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.630s, learning 0.160s)
               Value function loss: 4.2626
                    Surrogate loss: -0.0016
             Mean action noise std: 0.78
                       Mean reward: 198.31
               Mean episode length: 247.35
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 8.79s
                        Total time: 4901.80s
                               ETA: 935951.6s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.453s, learning 0.161s)
               Value function loss: 5.0504
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 192.53
               Mean episode length: 244.99
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 8.61s
                        Total time: 4910.41s
                               ETA: 935790.7s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.162s)
               Value function loss: 4.4795
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 196.82
               Mean episode length: 247.75
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 8.72s
                        Total time: 4919.13s
                               ETA: 935650.1s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.746s, learning 0.194s)
               Value function loss: 5.3227
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 198.67
               Mean episode length: 247.94
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 8.94s
                        Total time: 4928.07s
                               ETA: 935552.3s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.418s, learning 0.181s)
               Value function loss: 5.4074
                    Surrogate loss: -0.0023
             Mean action noise std: 0.78
                       Mean reward: 197.92
               Mean episode length: 247.47
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 8.60s
                        Total time: 4936.67s
                               ETA: 935390.2s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.419s, learning 0.218s)
               Value function loss: 4.1750
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 196.41
               Mean episode length: 246.04
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 8.64s
                        Total time: 4945.30s
                               ETA: 935235.7s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.570s, learning 0.173s)
               Value function loss: 3.8735
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 195.33
               Mean episode length: 246.32
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 8.74s
                        Total time: 4954.05s
                               ETA: 935101.9s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.359s, learning 0.174s)
               Value function loss: 3.6529
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 192.38
               Mean episode length: 238.25
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 8.53s
                        Total time: 4962.58s
                               ETA: 934929.1s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.669s, learning 0.167s)
               Value function loss: 3.8022
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 195.54
               Mean episode length: 244.13
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 8.84s
                        Total time: 4971.41s
                               ETA: 934813.9s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1785 steps/s (collection: 8.999s, learning 0.175s)
               Value function loss: 4.5410
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 197.30
               Mean episode length: 248.41
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 9.17s
                        Total time: 4980.59s
                               ETA: 934762.6s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.712s, learning 0.183s)
               Value function loss: 3.9794
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 192.75
               Mean episode length: 240.11
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 8.89s
                        Total time: 4989.48s
                               ETA: 934659.1s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.586s, learning 0.273s)
               Value function loss: 3.3737
                    Surrogate loss: -0.0215
             Mean action noise std: 0.78
                       Mean reward: 194.26
               Mean episode length: 240.76
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 8.86s
                        Total time: 4998.34s
                               ETA: 934549.2s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.353s, learning 0.209s)
               Value function loss: 1.8578
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 194.25
               Mean episode length: 240.20
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 8.56s
                        Total time: 5006.90s
                               ETA: 934384.2s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.604s, learning 0.179s)
               Value function loss: 2.0511
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 190.35
               Mean episode length: 233.68
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 8.78s
                        Total time: 5015.69s
                               ETA: 934260.9s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.705s, learning 0.161s)
               Value function loss: 2.2839
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 188.93
               Mean episode length: 236.84
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 8.87s
                        Total time: 5024.55s
                               ETA: 934153.7s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.927s, learning 0.191s)
               Value function loss: 2.5772
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 194.29
               Mean episode length: 242.89
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 9.12s
                        Total time: 5033.67s
                               ETA: 934093.7s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.706s, learning 0.162s)
               Value function loss: 3.3530
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 195.28
               Mean episode length: 241.82
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 8.87s
                        Total time: 5042.54s
                               ETA: 933987.4s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.842s, learning 0.161s)
               Value function loss: 3.0328
                    Surrogate loss: 0.0161
             Mean action noise std: 0.78
                       Mean reward: 189.61
               Mean episode length: 236.26
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 9.00s
                        Total time: 5051.54s
                               ETA: 933906.4s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.401s, learning 0.156s)
               Value function loss: 3.2189
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 190.35
               Mean episode length: 236.54
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 8.56s
                        Total time: 5060.10s
                               ETA: 933743.3s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.634s, learning 0.167s)
               Value function loss: 2.0456
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 191.47
               Mean episode length: 238.39
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 8.80s
                        Total time: 5068.90s
                               ETA: 933625.8s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.780s, learning 0.204s)
               Value function loss: 3.1766
                    Surrogate loss: 0.0048
             Mean action noise std: 0.78
                       Mean reward: 196.65
               Mean episode length: 240.96
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 8.98s
                        Total time: 5077.88s
                               ETA: 933542.2s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.589s, learning 0.191s)
               Value function loss: 3.4133
                    Surrogate loss: -0.0008
             Mean action noise std: 0.78
                       Mean reward: 195.98
               Mean episode length: 242.76
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 8.78s
                        Total time: 5086.66s
                               ETA: 933421.4s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.261s, learning 0.170s)
               Value function loss: 4.3252
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 191.93
               Mean episode length: 239.78
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 8.43s
                        Total time: 5095.09s
                               ETA: 933237.2s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.862s, learning 0.166s)
               Value function loss: 3.7892
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 189.62
               Mean episode length: 235.43
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 9.03s
                        Total time: 5104.12s
                               ETA: 933162.7s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.476s, learning 0.256s)
               Value function loss: 3.0815
                    Surrogate loss: -0.0025
             Mean action noise std: 0.78
                       Mean reward: 193.62
               Mean episode length: 239.60
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 8.73s
                        Total time: 5112.85s
                               ETA: 933034.5s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.253s, learning 0.223s)
               Value function loss: 3.1758
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 197.02
               Mean episode length: 242.34
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 8.48s
                        Total time: 5121.33s
                               ETA: 932860.2s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.213s, learning 0.219s)
               Value function loss: 3.8548
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 198.38
               Mean episode length: 242.70
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 8.43s
                        Total time: 5129.76s
                               ETA: 932678.4s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.707s, learning 0.162s)
               Value function loss: 4.4539
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 197.18
               Mean episode length: 243.73
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 8.87s
                        Total time: 5138.63s
                               ETA: 932576.8s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.806s, learning 0.176s)
               Value function loss: 3.3314
                    Surrogate loss: 0.0039
             Mean action noise std: 0.78
                       Mean reward: 206.23
               Mean episode length: 247.78
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 8.98s
                        Total time: 5147.61s
                               ETA: 932495.7s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.680s, learning 0.161s)
               Value function loss: 3.3587
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 204.88
               Mean episode length: 249.69
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 8.84s
                        Total time: 5156.45s
                               ETA: 932389.4s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.523s, learning 0.162s)
               Value function loss: 4.2052
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 206.25
               Mean episode length: 248.04
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 8.69s
                        Total time: 5165.14s
                               ETA: 932255.6s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.350s, learning 0.178s)
               Value function loss: 4.5117
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 206.90
               Mean episode length: 246.13
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 8.53s
                        Total time: 5173.66s
                               ETA: 932093.8s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.847s, learning 0.168s)
               Value function loss: 5.0840
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 206.14
               Mean episode length: 248.09
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 9.01s
                        Total time: 5182.68s
                               ETA: 932020.1s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.585s, learning 0.171s)
               Value function loss: 4.5305
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 201.40
               Mean episode length: 241.99
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 8.76s
                        Total time: 5191.44s
                               ETA: 931900.2s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1787 steps/s (collection: 8.959s, learning 0.207s)
               Value function loss: 5.5029
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 202.07
               Mean episode length: 245.63
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 9.17s
                        Total time: 5200.60s
                               ETA: 931854.2s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.524s, learning 0.206s)
               Value function loss: 5.4476
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 206.80
               Mean episode length: 244.24
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 8.73s
                        Total time: 5209.33s
                               ETA: 931730.4s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.565s, learning 0.228s)
               Value function loss: 4.7427
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 206.97
               Mean episode length: 244.90
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 8.79s
                        Total time: 5218.13s
                               ETA: 931618.1s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.471s, learning 0.206s)
               Value function loss: 4.4979
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 203.78
               Mean episode length: 242.33
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 8.68s
                        Total time: 5226.80s
                               ETA: 931485.4s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.841s, learning 0.176s)
               Value function loss: 5.0816
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 200.71
               Mean episode length: 236.54
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 9.02s
                        Total time: 5235.82s
                               ETA: 931413.7s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1793 steps/s (collection: 8.876s, learning 0.257s)
               Value function loss: 4.3313
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 199.55
               Mean episode length: 234.55
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 9.13s
                        Total time: 5244.95s
                               ETA: 931362.8s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.693s, learning 0.178s)
               Value function loss: 5.9933
                    Surrogate loss: 0.0065
             Mean action noise std: 0.78
                       Mean reward: 209.82
               Mean episode length: 248.11
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 8.87s
                        Total time: 5253.82s
                               ETA: 931265.7s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1789 steps/s (collection: 8.895s, learning 0.259s)
               Value function loss: 4.6575
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 206.49
               Mean episode length: 240.27
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 9.15s
                        Total time: 5262.98s
                               ETA: 931219.0s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.551s, learning 0.163s)
               Value function loss: 7.2753
                    Surrogate loss: 0.0040
             Mean action noise std: 0.78
                       Mean reward: 205.40
               Mean episode length: 236.57
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 8.71s
                        Total time: 5271.69s
                               ETA: 931094.6s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.428s, learning 0.162s)
               Value function loss: 2.1633
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 193.15
               Mean episode length: 225.13
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 8.59s
                        Total time: 5280.28s
                               ETA: 930948.9s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.884s, learning 0.163s)
               Value function loss: 2.0138
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 186.56
               Mean episode length: 219.01
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 9.05s
                        Total time: 5289.33s
                               ETA: 930884.2s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.803s, learning 0.190s)
               Value function loss: 2.3769
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 197.30
               Mean episode length: 228.60
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 8.99s
                        Total time: 5298.32s
                               ETA: 930810.1s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.482s, learning 0.156s)
               Value function loss: 2.4893
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 198.67
               Mean episode length: 228.65
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 8.64s
                        Total time: 5306.96s
                               ETA: 930673.9s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.487s, learning 0.183s)
               Value function loss: 3.6977
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 188.85
               Mean episode length: 217.48
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 8.67s
                        Total time: 5315.63s
                               ETA: 930544.0s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.466s, learning 0.179s)
               Value function loss: 3.0431
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 194.59
               Mean episode length: 221.10
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 8.64s
                        Total time: 5324.27s
                               ETA: 930409.9s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.784s, learning 0.167s)
               Value function loss: 3.7755
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 194.28
               Mean episode length: 217.02
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 8.95s
                        Total time: 5333.23s
                               ETA: 930329.7s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.321s, learning 0.169s)
               Value function loss: 2.0826
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 188.41
               Mean episode length: 209.99
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 8.49s
                        Total time: 5341.72s
                               ETA: 930169.5s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.466s, learning 0.164s)
               Value function loss: 2.7279
                    Surrogate loss: -0.0016
             Mean action noise std: 0.78
                       Mean reward: 203.74
               Mean episode length: 225.63
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 8.63s
                        Total time: 5350.35s
                               ETA: 930034.1s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.512s, learning 0.172s)
               Value function loss: 2.8210
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 215.04
               Mean episode length: 235.45
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 8.68s
                        Total time: 5359.03s
                               ETA: 929908.6s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.565s, learning 0.162s)
               Value function loss: 3.8265
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 224.04
               Mean episode length: 246.12
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 8.73s
                        Total time: 5367.76s
                               ETA: 929790.7s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.465s, learning 0.224s)
               Value function loss: 3.8271
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 221.28
               Mean episode length: 242.50
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 8.69s
                        Total time: 5376.44s
                               ETA: 929666.8s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.536s, learning 0.163s)
               Value function loss: 3.3148
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 221.82
               Mean episode length: 238.53
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 8.70s
                        Total time: 5385.14s
                               ETA: 929545.0s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.513s, learning 0.168s)
               Value function loss: 3.8383
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 214.08
               Mean episode length: 230.37
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 8.68s
                        Total time: 5393.83s
                               ETA: 929420.6s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.534s, learning 0.169s)
               Value function loss: 3.9996
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 215.15
               Mean episode length: 230.69
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 8.70s
                        Total time: 5402.53s
                               ETA: 929300.2s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.701s, learning 0.163s)
               Value function loss: 4.8682
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 232.17
               Mean episode length: 246.25
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 8.86s
                        Total time: 5411.39s
                               ETA: 929208.0s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.398s, learning 0.161s)
               Value function loss: 4.3833
                    Surrogate loss: -0.0042
             Mean action noise std: 0.78
                       Mean reward: 224.78
               Mean episode length: 236.19
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 8.56s
                        Total time: 5419.95s
                               ETA: 929063.7s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.731s, learning 0.243s)
               Value function loss: 4.1894
                    Surrogate loss: 0.0115
             Mean action noise std: 0.78
                       Mean reward: 215.53
               Mean episode length: 226.77
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 8.97s
                        Total time: 5428.93s
                               ETA: 928990.9s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.466s, learning 0.174s)
               Value function loss: 4.4511
                    Surrogate loss: 0.0040
             Mean action noise std: 0.78
                       Mean reward: 231.67
               Mean episode length: 240.35
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 8.64s
                        Total time: 5437.56s
                               ETA: 928861.3s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1797 steps/s (collection: 8.952s, learning 0.164s)
               Value function loss: 4.3444
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 237.09
               Mean episode length: 244.07
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 9.12s
                        Total time: 5446.68s
                               ETA: 928813.2s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.664s, learning 0.170s)
               Value function loss: 4.8595
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 240.10
               Mean episode length: 245.66
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 8.83s
                        Total time: 5455.51s
                               ETA: 928717.2s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.922s, learning 0.164s)
               Value function loss: 5.7085
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 235.57
               Mean episode length: 243.20
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 9.09s
                        Total time: 5464.60s
                               ETA: 928664.4s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.596s, learning 0.166s)
               Value function loss: 6.2192
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 234.97
               Mean episode length: 239.90
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 8.76s
                        Total time: 5473.36s
                               ETA: 928556.7s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.632s, learning 0.205s)
               Value function loss: 6.5935
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 234.86
               Mean episode length: 238.38
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 8.84s
                        Total time: 5482.20s
                               ETA: 928462.1s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.626s, learning 0.198s)
               Value function loss: 5.2270
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 239.90
               Mean episode length: 242.07
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 8.82s
                        Total time: 5491.02s
                               ETA: 928365.7s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.762s, learning 0.159s)
               Value function loss: 4.5694
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 236.97
               Mean episode length: 238.18
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 8.92s
                        Total time: 5499.94s
                               ETA: 928285.9s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.543s, learning 0.259s)
               Value function loss: 4.9836
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 229.42
               Mean episode length: 232.52
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 8.80s
                        Total time: 5508.74s
                               ETA: 928186.1s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.660s, learning 0.167s)
               Value function loss: 4.6175
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 227.34
               Mean episode length: 230.51
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 8.83s
                        Total time: 5517.57s
                               ETA: 928090.9s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.761s, learning 0.167s)
               Value function loss: 5.1391
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 235.64
               Mean episode length: 233.97
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 8.93s
                        Total time: 5526.50s
                               ETA: 928013.1s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.565s, learning 0.169s)
               Value function loss: 5.6311
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 231.88
               Mean episode length: 233.25
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 8.73s
                        Total time: 5535.23s
                               ETA: 927903.0s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.636s, learning 0.163s)
               Value function loss: 7.7415
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 243.57
               Mean episode length: 242.16
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 8.80s
                        Total time: 5544.03s
                               ETA: 927804.1s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.691s, learning 0.163s)
               Value function loss: 2.6572
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 228.44
               Mean episode length: 228.70
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 8.85s
                        Total time: 5552.89s
                               ETA: 927714.7s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.787s, learning 0.159s)
               Value function loss: 2.5029
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 223.34
               Mean episode length: 222.88
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 8.95s
                        Total time: 5561.83s
                               ETA: 927641.0s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.589s, learning 0.184s)
               Value function loss: 2.9474
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 225.89
               Mean episode length: 224.86
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 8.77s
                        Total time: 5570.61s
                               ETA: 927538.5s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.762s, learning 0.215s)
               Value function loss: 2.8349
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 218.83
               Mean episode length: 218.06
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 8.98s
                        Total time: 5579.58s
                               ETA: 927470.2s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.814s, learning 0.164s)
               Value function loss: 3.7133
                    Surrogate loss: -0.0004
             Mean action noise std: 0.78
                       Mean reward: 225.59
               Mean episode length: 223.63
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 8.98s
                        Total time: 5588.56s
                               ETA: 927402.5s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.677s, learning 0.163s)
               Value function loss: 3.3637
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 216.57
               Mean episode length: 215.54
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 8.84s
                        Total time: 5597.40s
                               ETA: 927312.0s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.946s, learning 0.174s)
               Value function loss: 2.9090
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 211.84
               Mean episode length: 211.58
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 9.12s
                        Total time: 5606.52s
                               ETA: 927268.1s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.547s, learning 0.165s)
               Value function loss: 2.1518
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 223.93
               Mean episode length: 221.38
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 8.71s
                        Total time: 5615.23s
                               ETA: 927156.9s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.690s, learning 0.166s)
               Value function loss: 2.6381
                    Surrogate loss: 0.0036
             Mean action noise std: 0.78
                       Mean reward: 236.48
               Mean episode length: 233.03
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 8.86s
                        Total time: 5624.09s
                               ETA: 927069.8s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.739s, learning 0.160s)
               Value function loss: 2.6720
                    Surrogate loss: 0.0019
             Mean action noise std: 0.78
                       Mean reward: 241.92
               Mean episode length: 236.53
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 8.90s
                        Total time: 5632.99s
                               ETA: 926990.1s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1761 steps/s (collection: 9.089s, learning 0.212s)
               Value function loss: 2.5777
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 237.19
               Mean episode length: 231.80
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 9.30s
                        Total time: 5642.29s
                               ETA: 926976.7s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.418s, learning 0.219s)
               Value function loss: 3.4503
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 231.27
               Mean episode length: 228.52
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 8.64s
                        Total time: 5650.93s
                               ETA: 926854.5s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.500s, learning 0.163s)
               Value function loss: 2.9158
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 229.71
               Mean episode length: 228.66
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 8.66s
                        Total time: 5659.59s
                               ETA: 926736.7s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.755s, learning 0.265s)
               Value function loss: 3.0922
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 233.50
               Mean episode length: 231.59
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 9.02s
                        Total time: 5668.61s
                               ETA: 926677.8s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.592s, learning 0.192s)
               Value function loss: 3.4332
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 238.21
               Mean episode length: 233.85
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 8.78s
                        Total time: 5677.39s
                               ETA: 926580.4s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.529s, learning 0.161s)
               Value function loss: 4.3395
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 236.39
               Mean episode length: 233.12
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 8.69s
                        Total time: 5686.08s
                               ETA: 926468.0s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.721s, learning 0.166s)
               Value function loss: 4.1717
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 244.65
               Mean episode length: 240.75
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 8.89s
                        Total time: 5694.97s
                               ETA: 926388.0s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.544s, learning 0.161s)
               Value function loss: 3.0799
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 247.23
               Mean episode length: 241.20
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 8.71s
                        Total time: 5703.68s
                               ETA: 926278.8s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.778s, learning 0.206s)
               Value function loss: 4.9109
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 252.95
               Mean episode length: 246.58
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 8.98s
                        Total time: 5712.66s
                               ETA: 926215.1s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.388s, learning 0.166s)
               Value function loss: 4.5639
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 249.69
               Mean episode length: 244.33
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 8.55s
                        Total time: 5721.21s
                               ETA: 926082.1s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.297s, learning 0.189s)
               Value function loss: 4.7548
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 247.67
               Mean episode length: 241.59
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 8.49s
                        Total time: 5729.70s
                               ETA: 925938.3s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.618s, learning 0.241s)
               Value function loss: 5.9143
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 250.81
               Mean episode length: 246.18
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 8.86s
                        Total time: 5738.56s
                               ETA: 925855.1s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.500s, learning 0.192s)
               Value function loss: 5.8647
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 249.83
               Mean episode length: 244.57
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 8.69s
                        Total time: 5747.25s
                               ETA: 925745.4s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.640s, learning 0.167s)
               Value function loss: 6.0081
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 250.82
               Mean episode length: 244.37
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 8.81s
                        Total time: 5756.06s
                               ETA: 925654.4s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.794s, learning 0.162s)
               Value function loss: 5.8786
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 253.16
               Mean episode length: 245.01
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 8.96s
                        Total time: 5765.01s
                               ETA: 925587.5s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.467s, learning 0.160s)
               Value function loss: 4.7320
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 251.30
               Mean episode length: 242.11
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 8.63s
                        Total time: 5773.64s
                               ETA: 925468.2s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.783s, learning 0.167s)
               Value function loss: 4.4417
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 252.64
               Mean episode length: 242.68
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 8.95s
                        Total time: 5782.59s
                               ETA: 925400.8s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.691s, learning 0.164s)
               Value function loss: 4.8552
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 254.00
               Mean episode length: 244.74
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 8.85s
                        Total time: 5791.45s
                               ETA: 925318.5s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.355s, learning 0.196s)
               Value function loss: 4.5747
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 250.44
               Mean episode length: 240.74
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 8.55s
                        Total time: 5800.00s
                               ETA: 925187.9s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1795 steps/s (collection: 8.850s, learning 0.275s)
               Value function loss: 4.9024
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 258.38
               Mean episode length: 248.06
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 9.12s
                        Total time: 5809.12s
                               ETA: 925149.2s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.471s, learning 0.162s)
               Value function loss: 7.7370
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 260.83
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 8.63s
                        Total time: 5817.75s
                               ETA: 925032.3s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.437s, learning 0.224s)
               Value function loss: 2.3260
                    Surrogate loss: -0.0032
             Mean action noise std: 0.78
                       Mean reward: 259.90
               Mean episode length: 249.96
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 8.66s
                        Total time: 5826.42s
                               ETA: 924920.2s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.542s, learning 0.166s)
               Value function loss: 1.8958
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 257.17
               Mean episode length: 245.04
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 8.71s
                        Total time: 5835.12s
                               ETA: 924815.9s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.581s, learning 0.167s)
               Value function loss: 2.1605
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 249.77
               Mean episode length: 239.74
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 8.75s
                        Total time: 5843.87s
                               ETA: 924718.2s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.601s, learning 0.162s)
               Value function loss: 2.0096
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 252.89
               Mean episode length: 243.48
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 8.76s
                        Total time: 5852.63s
                               ETA: 924623.2s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.615s, learning 0.198s)
               Value function loss: 2.6789
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 256.89
               Mean episode length: 246.66
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 8.81s
                        Total time: 5861.45s
                               ETA: 924536.3s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.832s, learning 0.162s)
               Value function loss: 2.9824
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 257.16
               Mean episode length: 245.52
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 8.99s
                        Total time: 5870.44s
                               ETA: 924478.2s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.644s, learning 0.166s)
               Value function loss: 2.8107
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 253.95
               Mean episode length: 241.59
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 8.81s
                        Total time: 5879.25s
                               ETA: 924391.3s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.299s, learning 0.184s)
               Value function loss: 2.5502
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 250.81
               Mean episode length: 240.06
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 8.48s
                        Total time: 5887.73s
                               ETA: 924253.3s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.801s, learning 0.157s)
               Value function loss: 2.3339
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 251.60
               Mean episode length: 242.06
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 8.96s
                        Total time: 5896.69s
                               ETA: 924190.2s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.678s, learning 0.183s)
               Value function loss: 1.9931
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 256.76
               Mean episode length: 247.02
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 8.86s
                        Total time: 5905.55s
                               ETA: 924112.1s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.758s, learning 0.162s)
               Value function loss: 2.4520
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 260.76
               Mean episode length: 248.91
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 8.92s
                        Total time: 5914.47s
                               ETA: 924043.3s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.662s, learning 0.164s)
               Value function loss: 3.8751
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 261.73
               Mean episode length: 248.73
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 8.83s
                        Total time: 5923.30s
                               ETA: 923960.2s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.443s, learning 0.159s)
               Value function loss: 2.7303
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 262.65
               Mean episode length: 248.21
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 8.60s
                        Total time: 5931.90s
                               ETA: 923842.4s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.469s, learning 0.167s)
               Value function loss: 2.2870
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 261.44
               Mean episode length: 248.22
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 8.64s
                        Total time: 5940.54s
                               ETA: 923730.2s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.207s, learning 0.190s)
               Value function loss: 2.7967
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 261.39
               Mean episode length: 248.74
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 8.40s
                        Total time: 5948.93s
                               ETA: 923581.2s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.487s, learning 0.162s)
               Value function loss: 3.4679
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 266.14
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 8.65s
                        Total time: 5957.58s
                               ETA: 923471.8s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.715s, learning 0.260s)
               Value function loss: 3.2457
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 261.19
               Mean episode length: 248.47
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 8.97s
                        Total time: 5966.56s
                               ETA: 923413.0s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.377s, learning 0.162s)
               Value function loss: 3.5596
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 260.92
               Mean episode length: 248.47
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 8.54s
                        Total time: 5975.10s
                               ETA: 923287.1s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.173s, learning 0.162s)
               Value function loss: 3.8151
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 267.80
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 8.34s
                        Total time: 5983.43s
                               ETA: 923130.1s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.659s, learning 0.167s)
               Value function loss: 4.2382
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 264.89
               Mean episode length: 249.86
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 8.83s
                        Total time: 5992.26s
                               ETA: 923049.2s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.427s, learning 0.159s)
               Value function loss: 3.8195
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 266.46
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 8.59s
                        Total time: 6000.84s
                               ETA: 922931.5s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.514s, learning 0.174s)
               Value function loss: 5.7006
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 267.98
               Mean episode length: 249.56
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 8.69s
                        Total time: 6009.53s
                               ETA: 922829.9s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.698s, learning 0.194s)
               Value function loss: 5.4270
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 270.35
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 8.89s
                        Total time: 6018.42s
                               ETA: 922759.8s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.747s, learning 0.172s)
               Value function loss: 5.1318
                    Surrogate loss: -0.0022
             Mean action noise std: 0.78
                       Mean reward: 268.15
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 8.92s
                        Total time: 6027.34s
                               ETA: 922694.1s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.273s, learning 0.158s)
               Value function loss: 5.4062
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 270.54
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 8.43s
                        Total time: 6035.77s
                               ETA: 922553.9s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.419s, learning 0.173s)
               Value function loss: 4.7250
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 270.04
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 8.59s
                        Total time: 6044.37s
                               ETA: 922438.8s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.484s, learning 0.171s)
               Value function loss: 4.7513
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 274.02
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 8.66s
                        Total time: 6053.02s
                               ETA: 922333.6s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.576s, learning 0.160s)
               Value function loss: 4.5159
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 271.78
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 8.74s
                        Total time: 6061.76s
                               ETA: 922240.9s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.758s, learning 0.190s)
               Value function loss: 3.8992
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 272.63
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 8.95s
                        Total time: 6070.70s
                               ETA: 922180.7s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.758s, learning 0.157s)
               Value function loss: 4.8289
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 271.16
               Mean episode length: 248.92
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 8.92s
                        Total time: 6079.62s
                               ETA: 922115.7s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.684s, learning 0.159s)
               Value function loss: 3.8665
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 272.18
               Mean episode length: 248.92
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 8.84s
                        Total time: 6088.46s
                               ETA: 922040.0s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1779 steps/s (collection: 9.030s, learning 0.176s)
               Value function loss: 4.1000
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 270.36
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 9.21s
                        Total time: 6097.67s
                               ETA: 922019.3s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.741s, learning 0.260s)
               Value function loss: 1.6406
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 270.47
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 9.00s
                        Total time: 6106.67s
                               ETA: 921967.7s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.375s, learning 0.158s)
               Value function loss: 2.2013
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 271.79
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 8.53s
                        Total time: 6115.20s
                               ETA: 921845.6s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.491s, learning 0.215s)
               Value function loss: 2.6022
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 271.35
               Mean episode length: 249.33
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 8.71s
                        Total time: 6123.91s
                               ETA: 921750.1s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.840s, learning 0.237s)
               Value function loss: 2.5616
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 270.08
               Mean episode length: 249.33
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 9.08s
                        Total time: 6132.98s
                               ETA: 921710.5s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.779s, learning 0.189s)
               Value function loss: 3.3943
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 271.53
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 8.97s
                        Total time: 6141.95s
                               ETA: 921654.6s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.532s, learning 0.197s)
               Value function loss: 2.7828
                    Surrogate loss: -0.0031
             Mean action noise std: 0.77
                       Mean reward: 272.53
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 8.73s
                        Total time: 6150.68s
                               ETA: 921563.2s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.458s, learning 0.216s)
               Value function loss: 3.4109
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 273.72
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 8.67s
                        Total time: 6159.36s
                               ETA: 921463.7s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.239s, learning 0.213s)
               Value function loss: 1.8878
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 273.21
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 8.45s
                        Total time: 6167.81s
                               ETA: 921331.4s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.631s, learning 0.185s)
               Value function loss: 2.4612
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 275.00
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 8.82s
                        Total time: 6176.62s
                               ETA: 921253.6s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.668s, learning 0.170s)
               Value function loss: 2.6792
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 274.24
               Mean episode length: 248.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 8.84s
                        Total time: 6185.46s
                               ETA: 921179.4s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.872s, learning 0.164s)
               Value function loss: 3.5997
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 273.68
               Mean episode length: 248.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 9.04s
                        Total time: 6194.50s
                               ETA: 921134.8s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.434s, learning 0.159s)
               Value function loss: 3.7230
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 273.64
               Mean episode length: 248.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 8.59s
                        Total time: 6203.09s
                               ETA: 921024.5s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.525s, learning 0.157s)
               Value function loss: 2.7982
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 273.46
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 8.68s
                        Total time: 6211.77s
                               ETA: 920927.7s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.710s, learning 0.160s)
               Value function loss: 3.8968
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 271.76
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 8.87s
                        Total time: 6220.64s
                               ETA: 920858.9s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1734 steps/s (collection: 9.278s, learning 0.169s)
               Value function loss: 4.1485
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 270.93
               Mean episode length: 249.43
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 9.45s
                        Total time: 6230.09s
                               ETA: 920875.8s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.404s, learning 0.160s)
               Value function loss: 4.2350
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 270.76
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 8.56s
                        Total time: 6238.65s
                               ETA: 920762.2s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1806 steps/s (collection: 8.833s, learning 0.234s)
               Value function loss: 3.1390
                    Surrogate loss: 0.0017
             Mean action noise std: 0.77
                       Mean reward: 271.66
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 9.07s
                        Total time: 6247.72s
                               ETA: 920723.0s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.464s, learning 0.276s)
               Value function loss: 2.9436
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 268.20
               Mean episode length: 247.82
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 8.74s
                        Total time: 6256.46s
                               ETA: 920635.8s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.792s, learning 0.187s)
               Value function loss: 3.5168
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: 267.78
               Mean episode length: 247.82
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 8.98s
                        Total time: 6265.44s
                               ETA: 920584.0s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.758s, learning 0.171s)
               Value function loss: 3.6455
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 268.90
               Mean episode length: 249.48
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 8.93s
                        Total time: 6274.37s
                               ETA: 920524.8s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.547s, learning 0.213s)
               Value function loss: 3.9020
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 269.43
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 8.76s
                        Total time: 6283.13s
                               ETA: 920441.2s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.572s, learning 0.190s)
               Value function loss: 4.0120
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 271.06
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 8.76s
                        Total time: 6291.89s
                               ETA: 920358.0s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.756s, learning 0.264s)
               Value function loss: 5.3480
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 267.86
               Mean episode length: 248.10
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 9.02s
                        Total time: 6300.91s
                               ETA: 920312.8s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.603s, learning 0.191s)
               Value function loss: 5.6887
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: 266.66
               Mean episode length: 248.35
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 8.79s
                        Total time: 6309.70s
                               ETA: 920234.6s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.065s, learning 0.176s)
               Value function loss: 4.9756
                    Surrogate loss: 0.0021
             Mean action noise std: 0.77
                       Mean reward: 269.57
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 8.24s
                        Total time: 6317.94s
                               ETA: 920076.2s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.258s, learning 0.161s)
               Value function loss: 5.1410
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 271.06
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 8.42s
                        Total time: 6326.36s
                               ETA: 919944.2s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.262s, learning 0.168s)
               Value function loss: 5.1715
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 270.49
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 8.43s
                        Total time: 6334.79s
                               ETA: 919813.9s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1755 steps/s (collection: 9.170s, learning 0.165s)
               Value function loss: 4.4001
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 271.39
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 9.34s
                        Total time: 6344.13s
                               ETA: 919815.4s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.670s, learning 0.182s)
               Value function loss: 5.4798
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 269.13
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 8.85s
                        Total time: 6352.98s
                               ETA: 919746.7s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.458s, learning 0.235s)
               Value function loss: 4.3956
                    Surrogate loss: 0.0005
             Mean action noise std: 0.77
                       Mean reward: 265.34
               Mean episode length: 248.01
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 8.69s
                        Total time: 6361.67s
                               ETA: 919655.3s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.521s, learning 0.171s)
               Value function loss: 6.5231
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 271.57
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 8.69s
                        Total time: 6370.36s
                               ETA: 919564.0s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.497s, learning 0.154s)
               Value function loss: 2.7415
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 270.10
               Mean episode length: 249.75
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 8.65s
                        Total time: 6379.02s
                               ETA: 919467.1s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.787s, learning 0.240s)
               Value function loss: 3.0383
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 270.30
               Mean episode length: 249.75
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 9.03s
                        Total time: 6388.04s
                               ETA: 919424.5s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1783 steps/s (collection: 8.876s, learning 0.312s)
               Value function loss: 3.0908
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 270.09
               Mean episode length: 249.75
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 9.19s
                        Total time: 6397.23s
                               ETA: 919405.3s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.652s, learning 0.163s)
               Value function loss: 2.4149
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 267.48
               Mean episode length: 248.16
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 8.82s
                        Total time: 6406.05s
                               ETA: 919332.5s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.468s, learning 0.161s)
               Value function loss: 4.0868
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 267.74
               Mean episode length: 248.16
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 8.63s
                        Total time: 6414.68s
                               ETA: 919233.2s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.406s, learning 0.157s)
               Value function loss: 3.7732
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 270.67
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 8.56s
                        Total time: 6423.24s
                               ETA: 919124.7s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.748s, learning 0.168s)
               Value function loss: 3.9242
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 268.41
               Mean episode length: 249.07
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 8.92s
                        Total time: 6432.16s
                               ETA: 919067.1s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.312s, learning 0.158s)
               Value function loss: 2.1017
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 269.46
               Mean episode length: 249.07
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 8.47s
                        Total time: 6440.62s
                               ETA: 918945.8s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.372s, learning 0.162s)
               Value function loss: 2.4320
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 268.70
               Mean episode length: 249.07
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 8.53s
                        Total time: 6449.16s
                               ETA: 918834.0s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.608s, learning 0.170s)
               Value function loss: 2.7073
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 268.82
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 8.78s
                        Total time: 6457.94s
                               ETA: 918757.2s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.495s, learning 0.219s)
               Value function loss: 2.9636
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 266.62
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 8.71s
                        Total time: 6466.65s
                               ETA: 918671.6s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1782 steps/s (collection: 9.027s, learning 0.163s)
               Value function loss: 4.5247
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 263.12
               Mean episode length: 246.71
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 9.19s
                        Total time: 6475.84s
                               ETA: 918653.6s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.711s, learning 0.160s)
               Value function loss: 2.5672
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 264.11
               Mean episode length: 246.71
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 8.87s
                        Total time: 6484.71s
                               ETA: 918590.4s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.693s, learning 0.185s)
               Value function loss: 4.2249
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 263.87
               Mean episode length: 246.21
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 8.88s
                        Total time: 6493.59s
                               ETA: 918528.5s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.916s, learning 0.169s)
               Value function loss: 3.5688
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 261.84
               Mean episode length: 244.17
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 9.08s
                        Total time: 6502.68s
                               ETA: 918495.9s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.748s, learning 0.169s)
               Value function loss: 3.8320
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 267.02
               Mean episode length: 249.52
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 8.92s
                        Total time: 6511.59s
                               ETA: 918439.7s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.571s, learning 0.162s)
               Value function loss: 3.6536
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 265.29
               Mean episode length: 248.98
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 8.73s
                        Total time: 6520.33s
                               ETA: 918357.8s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.585s, learning 0.162s)
               Value function loss: 3.8121
                    Surrogate loss: 0.0054
             Mean action noise std: 0.77
                       Mean reward: 266.43
               Mean episode length: 249.46
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 8.75s
                        Total time: 6529.07s
                               ETA: 918278.0s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.818s, learning 0.164s)
               Value function loss: 4.9616
                    Surrogate loss: -0.0002
             Mean action noise std: 0.77
                       Mean reward: 265.72
               Mean episode length: 249.72
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 8.98s
                        Total time: 6538.05s
                               ETA: 918231.4s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.582s, learning 0.162s)
               Value function loss: 4.3474
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 267.22
               Mean episode length: 249.72
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 8.74s
                        Total time: 6546.80s
                               ETA: 918151.5s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1775 steps/s (collection: 9.050s, learning 0.175s)
               Value function loss: 5.7304
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: 268.04
               Mean episode length: 248.92
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 9.23s
                        Total time: 6556.02s
                               ETA: 918139.2s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.681s, learning 0.206s)
               Value function loss: 6.7278
                    Surrogate loss: 0.0049
             Mean action noise std: 0.77
                       Mean reward: 269.72
               Mean episode length: 249.93
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 8.89s
                        Total time: 6564.91s
                               ETA: 918079.7s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.693s, learning 0.256s)
               Value function loss: 6.1180
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 266.75
               Mean episode length: 248.65
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 8.95s
                        Total time: 6573.86s
                               ETA: 918029.0s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.912s, learning 0.166s)
               Value function loss: 6.1408
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 266.80
               Mean episode length: 249.50
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 9.08s
                        Total time: 6582.94s
                               ETA: 917996.5s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.761s, learning 0.174s)
               Value function loss: 4.8317
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 266.88
               Mean episode length: 249.55
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 8.93s
                        Total time: 6591.87s
                               ETA: 917943.9s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.577s, learning 0.197s)
               Value function loss: 5.0706
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 264.33
               Mean episode length: 248.02
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 8.77s
                        Total time: 6600.65s
                               ETA: 917869.2s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.689s, learning 0.221s)
               Value function loss: 4.9467
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 265.46
               Mean episode length: 248.47
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 8.91s
                        Total time: 6609.56s
                               ETA: 917813.5s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.874s, learning 0.164s)
               Value function loss: 4.6216
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 267.80
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 9.04s
                        Total time: 6618.60s
                               ETA: 917775.6s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.573s, learning 0.208s)
               Value function loss: 4.6208
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 266.44
               Mean episode length: 249.88
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 8.78s
                        Total time: 6627.38s
                               ETA: 917702.2s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.786s, learning 0.181s)
               Value function loss: 5.5032
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 266.47
               Mean episode length: 248.14
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 8.97s
                        Total time: 6636.34s
                               ETA: 917654.8s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.694s, learning 0.167s)
               Value function loss: 8.5211
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 269.32
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 8.86s
                        Total time: 6645.21s
                               ETA: 917592.9s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.805s, learning 0.159s)
               Value function loss: 2.7953
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 268.09
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 8.96s
                        Total time: 6654.17s
                               ETA: 917545.3s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.766s, learning 0.182s)
               Value function loss: 1.7227
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 269.40
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 8.95s
                        Total time: 6663.12s
                               ETA: 917495.5s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.499s, learning 0.161s)
               Value function loss: 2.7206
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 268.24
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 8.66s
                        Total time: 6671.78s
                               ETA: 917406.4s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.595s, learning 0.161s)
               Value function loss: 2.5858
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 269.70
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 8.76s
                        Total time: 6680.53s
                               ETA: 917330.6s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.549s, learning 0.166s)
               Value function loss: 3.2042
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 272.20
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 8.71s
                        Total time: 6689.25s
                               ETA: 917249.3s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.687s, learning 0.182s)
               Value function loss: 3.5884
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 271.31
               Mean episode length: 249.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 8.87s
                        Total time: 6698.12s
                               ETA: 917189.4s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.445s, learning 0.161s)
               Value function loss: 3.1644
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 272.08
               Mean episode length: 249.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 8.61s
                        Total time: 6706.72s
                               ETA: 917093.5s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.796s, learning 0.171s)
               Value function loss: 2.3617
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 271.16
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 8.97s
                        Total time: 6715.69s
                               ETA: 917047.2s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.592s, learning 0.182s)
               Value function loss: 2.8307
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 268.36
               Mean episode length: 247.10
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 8.77s
                        Total time: 6724.46s
                               ETA: 916974.9s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.472s, learning 0.159s)
               Value function loss: 3.2225
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 268.88
               Mean episode length: 247.10
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 8.63s
                        Total time: 6733.09s
                               ETA: 916883.1s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.754s, learning 0.193s)
               Value function loss: 3.5329
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 271.08
               Mean episode length: 247.74
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 8.95s
                        Total time: 6742.04s
                               ETA: 916834.6s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.639s, learning 0.169s)
               Value function loss: 4.7807
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 269.53
               Mean episode length: 247.16
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 8.81s
                        Total time: 6750.85s
                               ETA: 916767.3s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.437s, learning 0.241s)
               Value function loss: 3.6400
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 267.83
               Mean episode length: 246.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 8.68s
                        Total time: 6759.53s
                               ETA: 916682.5s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.437s, learning 0.224s)
               Value function loss: 3.4408
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 270.60
               Mean episode length: 248.14
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 8.66s
                        Total time: 6768.19s
                               ETA: 916595.7s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.611s, learning 0.213s)
               Value function loss: 4.1350
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 272.95
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 8.82s
                        Total time: 6777.01s
                               ETA: 916531.0s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.769s, learning 0.177s)
               Value function loss: 5.7370
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 273.62
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 8.95s
                        Total time: 6785.96s
                               ETA: 916483.1s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.274s, learning 0.171s)
               Value function loss: 5.8434
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 273.27
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 8.45s
                        Total time: 6794.41s
                               ETA: 916367.7s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.620s, learning 0.171s)
               Value function loss: 4.9686
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 271.66
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 8.79s
                        Total time: 6803.20s
                               ETA: 916299.1s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.349s, learning 0.166s)
               Value function loss: 7.6227
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 266.59
               Mean episode length: 246.89
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 8.52s
                        Total time: 6811.71s
                               ETA: 916193.7s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.593s, learning 0.174s)
               Value function loss: 6.0925
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 267.96
               Mean episode length: 248.44
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 8.77s
                        Total time: 6820.48s
                               ETA: 916122.2s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.837s, learning 0.162s)
               Value function loss: 6.5071
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 269.55
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 9.00s
                        Total time: 6829.48s
                               ETA: 916082.0s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.500s, learning 0.171s)
               Value function loss: 9.2705
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 270.26
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 8.67s
                        Total time: 6838.15s
                               ETA: 915998.0s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.678s, learning 0.261s)
               Value function loss: 8.8208
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 270.13
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 8.94s
                        Total time: 6847.09s
                               ETA: 915950.1s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.761s, learning 0.167s)
               Value function loss: 8.6816
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 272.71
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 8.93s
                        Total time: 6856.01s
                               ETA: 915900.8s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.659s, learning 0.168s)
               Value function loss: 8.3724
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 270.36
               Mean episode length: 248.93
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 8.83s
                        Total time: 6864.84s
                               ETA: 915838.2s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.923s, learning 0.159s)
               Value function loss: 6.6647
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 268.74
               Mean episode length: 249.45
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 9.08s
                        Total time: 6873.92s
                               ETA: 915809.8s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.725s, learning 0.168s)
               Value function loss: 6.5053
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 271.37
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 8.89s
                        Total time: 6882.82s
                               ETA: 915756.1s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.565s, learning 0.168s)
               Value function loss: 6.9683
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 270.83
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 8.73s
                        Total time: 6891.55s
                               ETA: 915681.4s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1806 steps/s (collection: 8.903s, learning 0.167s)
               Value function loss: 6.0876
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 270.77
               Mean episode length: 249.91
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 9.07s
                        Total time: 6900.62s
                               ETA: 915651.4s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.829s, learning 0.222s)
               Value function loss: 5.8048
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 271.74
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 9.05s
                        Total time: 6909.67s
                               ETA: 915619.1s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.166s)
               Value function loss: 10.9840
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 270.76
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 8.48s
                        Total time: 6918.15s
                               ETA: 915511.2s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1778 steps/s (collection: 9.056s, learning 0.157s)
               Value function loss: 2.5678
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 272.15
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 9.21s
                        Total time: 6927.36s
                               ETA: 915500.5s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.711s, learning 0.162s)
               Value function loss: 2.3323
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 270.43
               Mean episode length: 248.53
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 8.87s
                        Total time: 6936.24s
                               ETA: 915444.9s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.514s, learning 0.156s)
               Value function loss: 3.1486
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 268.18
               Mean episode length: 247.52
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 8.67s
                        Total time: 6944.91s
                               ETA: 915362.8s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.519s, learning 0.163s)
               Value function loss: 2.4159
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 267.44
               Mean episode length: 247.52
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 8.68s
                        Total time: 6953.59s
                               ETA: 915282.4s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.736s, learning 0.192s)
               Value function loss: 3.3833
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 271.00
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 8.93s
                        Total time: 6962.52s
                               ETA: 915234.5s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.980s, learning 0.169s)
               Value function loss: 4.0012
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 270.48
               Mean episode length: 249.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 9.15s
                        Total time: 6971.67s
                               ETA: 915215.7s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1789 steps/s (collection: 8.989s, learning 0.166s)
               Value function loss: 3.3556
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 269.53
               Mean episode length: 249.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 9.16s
                        Total time: 6980.82s
                               ETA: 915197.8s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.742s, learning 0.179s)
               Value function loss: 3.3169
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 265.48
               Mean episode length: 247.66
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 8.92s
                        Total time: 6989.74s
                               ETA: 915149.2s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.242s, learning 0.166s)
               Value function loss: 3.1118
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 266.09
               Mean episode length: 248.32
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 8.41s
                        Total time: 6998.15s
                               ETA: 915033.7s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.434s, learning 0.164s)
               Value function loss: 2.9493
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 265.76
               Mean episode length: 247.34
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 8.60s
                        Total time: 7006.75s
                               ETA: 914943.2s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.690s, learning 0.170s)
               Value function loss: 3.1683
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 267.96
               Mean episode length: 247.83
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 8.86s
                        Total time: 7015.61s
                               ETA: 914887.0s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.504s, learning 0.271s)
               Value function loss: 4.8319
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 268.36
               Mean episode length: 249.01
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 8.77s
                        Total time: 7024.38s
                               ETA: 914820.0s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.644s, learning 0.189s)
               Value function loss: 3.8958
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 270.11
               Mean episode length: 249.20
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 8.83s
                        Total time: 7033.22s
                               ETA: 914760.7s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.781s, learning 0.173s)
               Value function loss: 3.1105
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 269.63
               Mean episode length: 248.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 8.95s
                        Total time: 7042.17s
                               ETA: 914717.2s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.625s, learning 0.209s)
               Value function loss: 4.1862
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 268.43
               Mean episode length: 248.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 8.83s
                        Total time: 7051.00s
                               ETA: 914658.2s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.394s, learning 0.193s)
               Value function loss: 4.2749
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 268.38
               Mean episode length: 248.28
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 8.59s
                        Total time: 7059.59s
                               ETA: 914567.4s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1788 steps/s (collection: 8.994s, learning 0.165s)
               Value function loss: 4.5417
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 267.69
               Mean episode length: 248.47
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 9.16s
                        Total time: 7068.75s
                               ETA: 914550.7s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.670s, learning 0.163s)
               Value function loss: 3.8706
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 268.84
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 8.83s
                        Total time: 7077.58s
                               ETA: 914492.0s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.798s, learning 0.172s)
               Value function loss: 4.0654
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 268.98
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 8.97s
                        Total time: 7086.55s
                               ETA: 914451.1s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.581s, learning 0.158s)
               Value function loss: 4.2569
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 268.78
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 8.74s
                        Total time: 7095.29s
                               ETA: 914380.6s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.516s, learning 0.201s)
               Value function loss: 4.3929
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 268.21
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 8.72s
                        Total time: 7104.01s
                               ETA: 914307.3s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.651s, learning 0.266s)
               Value function loss: 5.7348
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 263.61
               Mean episode length: 248.39
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 8.92s
                        Total time: 7112.93s
                               ETA: 914259.9s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.599s, learning 0.160s)
               Value function loss: 5.6793
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 264.64
               Mean episode length: 248.13
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 8.76s
                        Total time: 7121.69s
                               ETA: 914192.4s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.518s, learning 0.174s)
               Value function loss: 6.2811
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 264.48
               Mean episode length: 246.90
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 8.69s
                        Total time: 7130.38s
                               ETA: 914116.4s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.200s, learning 0.165s)
               Value function loss: 6.1888
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 268.72
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 8.37s
                        Total time: 7138.74s
                               ETA: 913998.8s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.232s, learning 0.172s)
               Value function loss: 4.8916
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 267.09
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 8.40s
                        Total time: 7147.15s
                               ETA: 913886.3s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.833s, learning 0.159s)
               Value function loss: 5.1113
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 266.65
               Mean episode length: 248.74
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 8.99s
                        Total time: 7156.14s
                               ETA: 913849.2s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.659s, learning 0.205s)
               Value function loss: 4.6125
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 266.04
               Mean episode length: 248.74
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 8.86s
                        Total time: 7165.00s
                               ETA: 913795.9s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.538s, learning 0.181s)
               Value function loss: 4.1004
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 264.77
               Mean episode length: 249.07
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 8.72s
                        Total time: 7173.72s
                               ETA: 913724.1s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.470s, learning 0.165s)
               Value function loss: 4.9579
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 266.09
               Mean episode length: 248.25
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 8.63s
                        Total time: 7182.36s
                               ETA: 913641.9s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.391s, learning 0.216s)
               Value function loss: 4.4883
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 265.30
               Mean episode length: 246.47
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 8.61s
                        Total time: 7190.96s
                               ETA: 913556.4s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.455s, learning 0.176s)
               Value function loss: 4.6426
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 272.34
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 8.63s
                        Total time: 7199.60s
                               ETA: 913474.1s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.468s, learning 0.171s)
               Value function loss: 1.9758
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 272.18
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 8.64s
                        Total time: 7208.23s
                               ETA: 913392.9s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.475s, learning 0.169s)
               Value function loss: 2.6074
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 271.08
               Mean episode length: 249.76
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 8.64s
                        Total time: 7216.88s
                               ETA: 913312.5s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.753s, learning 0.163s)
               Value function loss: 2.7032
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 269.12
               Mean episode length: 247.93
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 8.92s
                        Total time: 7225.80s
                               ETA: 913266.9s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.367s, learning 0.237s)
               Value function loss: 2.6716
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 269.40
               Mean episode length: 248.17
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 8.60s
                        Total time: 7234.40s
                               ETA: 913181.8s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.227s, learning 0.170s)
               Value function loss: 3.6143
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 265.92
               Mean episode length: 246.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 8.40s
                        Total time: 7242.80s
                               ETA: 913070.8s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.484s, learning 0.168s)
               Value function loss: 3.4314
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 263.12
               Mean episode length: 245.40
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 8.65s
                        Total time: 7251.45s
                               ETA: 912992.2s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.627s, learning 0.160s)
               Value function loss: 3.4349
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 267.12
               Mean episode length: 247.10
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 8.79s
                        Total time: 7260.23s
                               ETA: 912930.7s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.167s, learning 0.162s)
               Value function loss: 2.2263
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 264.95
               Mean episode length: 245.64
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 8.33s
                        Total time: 7268.56s
                               ETA: 912811.9s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.550s, learning 0.167s)
               Value function loss: 2.5475
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 266.74
               Mean episode length: 248.47
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 8.72s
                        Total time: 7277.28s
                               ETA: 912742.0s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.543s, learning 0.242s)
               Value function loss: 3.1581
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 265.10
               Mean episode length: 248.54
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 8.79s
                        Total time: 7286.07s
                               ETA: 912680.8s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.888s, learning 0.171s)
               Value function loss: 4.2847
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 265.35
               Mean episode length: 246.90
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 9.06s
                        Total time: 7295.12s
                               ETA: 912654.0s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.454s, learning 0.163s)
               Value function loss: 4.1874
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 268.02
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 8.62s
                        Total time: 7303.74s
                               ETA: 912572.0s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.565s, learning 0.181s)
               Value function loss: 3.0692
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 267.44
               Mean episode length: 248.55
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 8.75s
                        Total time: 7312.49s
                               ETA: 912506.3s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.160s)
               Value function loss: 4.2567
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 267.15
               Mean episode length: 246.15
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 8.31s
                        Total time: 7320.79s
                               ETA: 912386.0s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.416s, learning 0.188s)
               Value function loss: 3.8154
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 265.36
               Mean episode length: 245.67
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 8.60s
                        Total time: 7329.40s
                               ETA: 912303.0s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.587s, learning 0.171s)
               Value function loss: 4.8574
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 268.46
               Mean episode length: 248.96
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 8.76s
                        Total time: 7338.15s
                               ETA: 912239.3s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.461s, learning 0.178s)
               Value function loss: 4.6905
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 264.96
               Mean episode length: 245.21
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 8.64s
                        Total time: 7346.79s
                               ETA: 912161.0s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.929s, learning 0.158s)
               Value function loss: 4.8134
                    Surrogate loss: -0.0017
             Mean action noise std: 0.77
                       Mean reward: 262.50
               Mean episode length: 245.04
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 9.09s
                        Total time: 7355.88s
                               ETA: 912138.4s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.386s, learning 0.187s)
               Value function loss: 4.6635
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 265.29
               Mean episode length: 248.90
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 8.57s
                        Total time: 7364.45s
                               ETA: 912052.2s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.547s, learning 0.190s)
               Value function loss: 5.1832
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 266.68
               Mean episode length: 248.59
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 8.74s
                        Total time: 7373.19s
                               ETA: 911986.5s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.816s, learning 0.241s)
               Value function loss: 5.4945
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 266.32
               Mean episode length: 247.17
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 9.06s
                        Total time: 7382.25s
                               ETA: 911960.4s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.587s, learning 0.160s)
               Value function loss: 5.7204
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 259.53
               Mean episode length: 244.50
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 8.75s
                        Total time: 7390.99s
                               ETA: 911896.2s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.742s, learning 0.286s)
               Value function loss: 6.2914
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 268.83
               Mean episode length: 248.97
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 9.03s
                        Total time: 7400.02s
                               ETA: 911866.7s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.691s, learning 0.164s)
               Value function loss: 6.5393
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 267.67
               Mean episode length: 248.46
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 8.85s
                        Total time: 7408.88s
                               ETA: 911816.0s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.529s, learning 0.284s)
               Value function loss: 5.1540
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 262.97
               Mean episode length: 247.68
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 8.81s
                        Total time: 7417.69s
                               ETA: 911760.2s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.103s, learning 0.210s)
               Value function loss: 5.4163
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 259.65
               Mean episode length: 242.10
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 8.31s
                        Total time: 7426.00s
                               ETA: 911643.1s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1782 steps/s (collection: 8.917s, learning 0.277s)
               Value function loss: 5.0356
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 267.53
               Mean episode length: 248.38
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 9.19s
                        Total time: 7435.20s
                               ETA: 911634.3s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1774 steps/s (collection: 9.044s, learning 0.187s)
               Value function loss: 4.2346
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 266.09
               Mean episode length: 246.59
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 9.23s
                        Total time: 7444.43s
                               ETA: 911630.1s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.753s, learning 0.168s)
               Value function loss: 5.4735
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 263.96
               Mean episode length: 246.20
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 8.92s
                        Total time: 7453.35s
                               ETA: 911588.0s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.525s, learning 0.175s)
               Value function loss: 5.0111
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 251.12
               Mean episode length: 234.52
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 8.70s
                        Total time: 7462.05s
                               ETA: 911518.8s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.543s, learning 0.160s)
               Value function loss: 6.0076
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 268.04
               Mean episode length: 248.64
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 8.70s
                        Total time: 7470.75s
                               ETA: 911450.2s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.773s, learning 0.263s)
               Value function loss: 2.3794
                    Surrogate loss: 0.0073
             Mean action noise std: 0.77
                       Mean reward: 264.07
               Mean episode length: 246.21
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 9.04s
                        Total time: 7479.79s
                               ETA: 911422.4s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.683s, learning 0.173s)
               Value function loss: 2.9803
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 258.14
               Mean episode length: 240.61
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 8.86s
                        Total time: 7488.65s
                               ETA: 911372.7s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.268s, learning 0.170s)
               Value function loss: 3.4166
                    Surrogate loss: -0.0026
             Mean action noise std: 0.77
                       Mean reward: 256.04
               Mean episode length: 240.97
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 8.44s
                        Total time: 7497.08s
                               ETA: 911272.2s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.620s, learning 0.185s)
               Value function loss: 2.4521
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 257.87
               Mean episode length: 242.49
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 8.81s
                        Total time: 7505.89s
                               ETA: 911216.6s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.376s, learning 0.211s)
               Value function loss: 3.9564
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 259.33
               Mean episode length: 241.14
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 8.59s
                        Total time: 7514.47s
                               ETA: 911134.7s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.597s, learning 0.161s)
               Value function loss: 3.1595
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 265.27
               Mean episode length: 245.93
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 8.76s
                        Total time: 7523.23s
                               ETA: 911073.6s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.498s, learning 0.317s)
               Value function loss: 3.3749
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 261.40
               Mean episode length: 244.30
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 8.82s
                        Total time: 7532.05s
                               ETA: 911019.6s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.760s, learning 0.157s)
               Value function loss: 2.5170
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 253.14
               Mean episode length: 237.19
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 8.92s
                        Total time: 7540.96s
                               ETA: 910977.9s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.363s, learning 0.160s)
               Value function loss: 2.7478
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 255.21
               Mean episode length: 237.34
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 8.52s
                        Total time: 7549.49s
                               ETA: 910888.8s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1770 steps/s (collection: 9.086s, learning 0.167s)
               Value function loss: 3.1063
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 251.29
               Mean episode length: 234.26
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 9.25s
                        Total time: 7558.74s
                               ETA: 910887.8s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.095s, learning 0.173s)
               Value function loss: 3.3647
                    Surrogate loss: 0.0058
             Mean action noise std: 0.77
                       Mean reward: 252.59
               Mean episode length: 239.58
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 8.27s
                        Total time: 7567.01s
                               ETA: 910768.3s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.818s, learning 0.159s)
               Value function loss: 4.1926
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 261.55
               Mean episode length: 247.42
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 8.98s
                        Total time: 7575.98s
                               ETA: 910734.3s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.920s, learning 0.155s)
               Value function loss: 2.7914
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 256.08
               Mean episode length: 243.12
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 9.08s
                        Total time: 7585.06s
                               ETA: 910712.2s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.508s, learning 0.159s)
               Value function loss: 3.6191
                    Surrogate loss: -0.0038
             Mean action noise std: 0.77
                       Mean reward: 247.31
               Mean episode length: 236.67
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 8.67s
                        Total time: 7593.73s
                               ETA: 910641.2s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.345s, learning 0.191s)
               Value function loss: 3.4426
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 250.06
               Mean episode length: 237.23
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 8.54s
                        Total time: 7602.26s
                               ETA: 910554.5s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.609s, learning 0.203s)
               Value function loss: 4.4847
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 251.87
               Mean episode length: 238.73
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 8.81s
                        Total time: 7611.07s
                               ETA: 910501.0s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.609s, learning 0.167s)
               Value function loss: 3.9027
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 249.12
               Mean episode length: 238.09
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 8.78s
                        Total time: 7619.85s
                               ETA: 910443.4s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.464s, learning 0.164s)
               Value function loss: 3.7226
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 253.39
               Mean episode length: 241.21
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 8.63s
                        Total time: 7628.48s
                               ETA: 910368.3s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.719s, learning 0.192s)
               Value function loss: 4.6825
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 256.23
               Mean episode length: 242.47
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 8.91s
                        Total time: 7637.39s
                               ETA: 910327.1s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.440s, learning 0.238s)
               Value function loss: 3.9628
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 252.28
               Mean episode length: 240.81
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 8.68s
                        Total time: 7646.07s
                               ETA: 910258.3s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.357s, learning 0.288s)
               Value function loss: 4.1283
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 262.39
               Mean episode length: 249.60
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 8.64s
                        Total time: 7654.71s
                               ETA: 910185.6s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.655s, learning 0.169s)
               Value function loss: 4.7412
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 255.86
               Mean episode length: 244.19
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 8.82s
                        Total time: 7663.54s
                               ETA: 910134.3s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.768s, learning 0.170s)
               Value function loss: 5.1055
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 261.55
               Mean episode length: 247.33
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 8.94s
                        Total time: 7672.47s
                               ETA: 910096.7s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1750 steps/s (collection: 9.030s, learning 0.330s)
               Value function loss: 5.8258
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 261.92
               Mean episode length: 247.49
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 9.36s
                        Total time: 7681.83s
                               ETA: 910109.2s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.692s, learning 0.165s)
               Value function loss: 4.5686
                    Surrogate loss: 0.0009
             Mean action noise std: 0.77
                       Mean reward: 261.77
               Mean episode length: 249.21
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 8.86s
                        Total time: 7690.69s
                               ETA: 910062.0s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.495s, learning 0.176s)
               Value function loss: 4.0765
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 256.92
               Mean episode length: 244.23
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 8.67s
                        Total time: 7699.36s
                               ETA: 909993.0s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.728s, learning 0.177s)
               Value function loss: 4.5109
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 256.13
               Mean episode length: 247.12
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 8.91s
                        Total time: 7708.27s
                               ETA: 909951.7s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.278s, learning 0.176s)
               Value function loss: 4.4605
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 245.97
               Mean episode length: 236.28
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 8.45s
                        Total time: 7716.72s
                               ETA: 909857.4s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.415s, learning 0.203s)
               Value function loss: 5.1250
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 245.64
               Mean episode length: 236.83
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 8.62s
                        Total time: 7725.34s
                               ETA: 909782.7s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.295s, learning 0.178s)
               Value function loss: 5.3043
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 252.53
               Mean episode length: 245.16
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 8.47s
                        Total time: 7733.81s
                               ETA: 909691.0s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.406s, learning 0.184s)
               Value function loss: 8.7258
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 256.16
               Mean episode length: 246.28
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 8.59s
                        Total time: 7742.40s
                               ETA: 909613.2s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.633s, learning 0.182s)
               Value function loss: 2.9083
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 253.39
               Mean episode length: 242.91
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 8.82s
                        Total time: 7751.22s
                               ETA: 909561.9s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1795 steps/s (collection: 8.927s, learning 0.199s)
               Value function loss: 3.0298
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 240.90
               Mean episode length: 233.91
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 9.13s
                        Total time: 7760.34s
                               ETA: 909547.2s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.342s, learning 0.229s)
               Value function loss: 2.6369
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 240.94
               Mean episode length: 232.51
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 8.57s
                        Total time: 7768.92s
                               ETA: 909467.6s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.568s, learning 0.218s)
               Value function loss: 2.2129
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 239.08
               Mean episode length: 230.88
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 8.79s
                        Total time: 7777.70s
                               ETA: 909413.1s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.747s, learning 0.221s)
               Value function loss: 2.9937
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 249.23
               Mean episode length: 240.30
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 8.97s
                        Total time: 7786.67s
                               ETA: 909380.2s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.405s, learning 0.169s)
               Value function loss: 3.2207
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 253.58
               Mean episode length: 242.51
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 8.57s
                        Total time: 7795.24s
                               ETA: 909301.4s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.550s, learning 0.172s)
               Value function loss: 3.7753
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 244.11
               Mean episode length: 240.09
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 8.72s
                        Total time: 7803.97s
                               ETA: 909239.9s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.599s, learning 0.226s)
               Value function loss: 3.1262
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 238.11
               Mean episode length: 236.54
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 8.82s
                        Total time: 7812.79s
                               ETA: 909190.5s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.776s, learning 0.206s)
               Value function loss: 2.6466
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 235.71
               Mean episode length: 231.70
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 8.98s
                        Total time: 7821.77s
                               ETA: 909159.5s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.577s, learning 0.164s)
               Value function loss: 2.7249
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 244.89
               Mean episode length: 241.72
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 8.74s
                        Total time: 7830.51s
                               ETA: 909100.5s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.432s, learning 0.174s)
               Value function loss: 2.9163
                    Surrogate loss: 0.0033
             Mean action noise std: 0.77
                       Mean reward: 249.25
               Mean episode length: 246.37
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 8.61s
                        Total time: 7839.12s
                               ETA: 909026.0s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.457s, learning 0.183s)
               Value function loss: 3.3414
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 249.17
               Mean episode length: 240.94
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 8.64s
                        Total time: 7847.76s
                               ETA: 908955.6s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.359s, learning 0.167s)
               Value function loss: 3.5534
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 245.75
               Mean episode length: 238.44
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 8.53s
                        Total time: 7856.28s
                               ETA: 908872.2s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.189s, learning 0.201s)
               Value function loss: 3.1152
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 244.66
               Mean episode length: 239.01
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 8.39s
                        Total time: 7864.67s
                               ETA: 908773.3s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.200s, learning 0.167s)
               Value function loss: 3.4452
                    Surrogate loss: 0.0002
             Mean action noise std: 0.77
                       Mean reward: 244.81
               Mean episode length: 239.59
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 8.37s
                        Total time: 7873.04s
                               ETA: 908671.9s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.655s, learning 0.194s)
               Value function loss: 4.1403
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 250.73
               Mean episode length: 242.52
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 8.85s
                        Total time: 7881.89s
                               ETA: 908626.3s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.721s, learning 0.245s)
               Value function loss: 4.0585
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 252.16
               Mean episode length: 244.03
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 8.97s
                        Total time: 7890.86s
                               ETA: 908594.2s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.518s, learning 0.213s)
               Value function loss: 3.5154
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 245.77
               Mean episode length: 243.36
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 8.73s
                        Total time: 7899.59s
                               ETA: 908535.0s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.569s, learning 0.179s)
               Value function loss: 5.0422
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 245.36
               Mean episode length: 238.06
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 8.75s
                        Total time: 7908.34s
                               ETA: 908478.0s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.730s, learning 0.184s)
               Value function loss: 4.5019
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 250.18
               Mean episode length: 239.24
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 8.91s
                        Total time: 7917.25s
                               ETA: 908440.2s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.440s, learning 0.190s)
               Value function loss: 3.8240
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 249.86
               Mean episode length: 242.08
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 8.63s
                        Total time: 7925.88s
                               ETA: 908369.8s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.551s, learning 0.169s)
               Value function loss: 4.4499
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 254.90
               Mean episode length: 244.08
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 8.72s
                        Total time: 7934.60s
                               ETA: 908310.0s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.640s, learning 0.192s)
               Value function loss: 4.6885
                    Surrogate loss: -0.0055
             Mean action noise std: 0.77
                       Mean reward: 247.07
               Mean episode length: 240.67
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 8.83s
                        Total time: 7943.43s
                               ETA: 908263.1s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.869s, learning 0.159s)
               Value function loss: 4.5822
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 253.73
               Mean episode length: 240.23
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 9.03s
                        Total time: 7952.46s
                               ETA: 908238.6s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.374s, learning 0.170s)
               Value function loss: 4.2474
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: 253.51
               Mean episode length: 241.31
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 8.54s
                        Total time: 7961.00s
                               ETA: 908159.0s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.361s, learning 0.158s)
               Value function loss: 4.1568
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 254.90
               Mean episode length: 244.06
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 8.52s
                        Total time: 7969.52s
                               ETA: 908076.6s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.622s, learning 0.185s)
               Value function loss: 4.0565
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 254.01
               Mean episode length: 242.18
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 8.81s
                        Total time: 7978.33s
                               ETA: 908027.2s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.832s, learning 0.176s)
               Value function loss: 4.6660
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 259.34
               Mean episode length: 246.38
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 9.01s
                        Total time: 7987.34s
                               ETA: 908000.8s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.822s, learning 0.179s)
               Value function loss: 4.9256
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 253.08
               Mean episode length: 239.12
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 9.00s
                        Total time: 7996.34s
                               ETA: 907973.6s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.554s, learning 0.167s)
               Value function loss: 4.8060
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 256.29
               Mean episode length: 240.78
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 8.72s
                        Total time: 8005.06s
                               ETA: 907914.7s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.487s, learning 0.169s)
               Value function loss: 8.3199
                    Surrogate loss: 0.0125
             Mean action noise std: 0.77
                       Mean reward: 264.93
               Mean episode length: 246.20
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 8.66s
                        Total time: 8013.71s
                               ETA: 907848.5s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.743s, learning 0.221s)
               Value function loss: 2.4921
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 262.45
               Mean episode length: 244.56
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 8.96s
                        Total time: 8022.68s
                               ETA: 907817.3s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.588s, learning 0.163s)
               Value function loss: 2.8096
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 252.81
               Mean episode length: 236.88
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 8.75s
                        Total time: 8031.43s
                               ETA: 907762.1s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.565s, learning 0.348s)
               Value function loss: 3.0022
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 256.05
               Mean episode length: 239.72
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 8.91s
                        Total time: 8040.34s
                               ETA: 907725.3s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.746s, learning 0.179s)
               Value function loss: 2.9446
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 252.94
               Mean episode length: 239.45
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 8.93s
                        Total time: 8049.27s
                               ETA: 907690.0s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.230s, learning 0.165s)
               Value function loss: 3.7037
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 251.27
               Mean episode length: 238.32
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 8.40s
                        Total time: 8057.66s
                               ETA: 907595.0s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.743s, learning 0.175s)
               Value function loss: 5.4056
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 259.53
               Mean episode length: 240.70
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 8.92s
                        Total time: 8066.58s
                               ETA: 907559.0s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.687s, learning 0.159s)
               Value function loss: 7.7632
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 260.20
               Mean episode length: 240.49
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 8.85s
                        Total time: 8075.43s
                               ETA: 907515.0s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.359s, learning 0.166s)
               Value function loss: 5.6343
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 255.86
               Mean episode length: 241.01
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 8.53s
                        Total time: 8083.95s
                               ETA: 907435.0s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.662s, learning 0.165s)
               Value function loss: 4.2997
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 252.04
               Mean episode length: 238.84
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 8.83s
                        Total time: 8092.78s
                               ETA: 907389.0s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.461s, learning 0.209s)
               Value function loss: 3.5575
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 254.59
               Mean episode length: 239.73
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 8.67s
                        Total time: 8101.45s
                               ETA: 907325.6s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.704s, learning 0.247s)
               Value function loss: 3.6716
                    Surrogate loss: 0.0020
             Mean action noise std: 0.77
                       Mean reward: 258.98
               Mean episode length: 242.65
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 8.95s
                        Total time: 8110.40s
                               ETA: 907293.7s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.806s, learning 0.163s)
               Value function loss: 5.3697
                    Surrogate loss: -0.0038
             Mean action noise std: 0.77
                       Mean reward: 258.77
               Mean episode length: 242.41
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 8.97s
                        Total time: 8119.37s
                               ETA: 907263.9s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.817s, learning 0.168s)
               Value function loss: 4.1739
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 263.01
               Mean episode length: 246.66
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 8.99s
                        Total time: 8128.35s
                               ETA: 907236.0s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.424s, learning 0.170s)
               Value function loss: 3.5038
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 266.34
               Mean episode length: 248.88
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 8.59s
                        Total time: 8136.95s
                               ETA: 907164.5s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.607s, learning 0.165s)
               Value function loss: 4.4036
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 258.66
               Mean episode length: 243.17
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 8.77s
                        Total time: 8145.72s
                               ETA: 907112.9s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.494s, learning 0.171s)
               Value function loss: 4.6406
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 260.58
               Mean episode length: 243.79
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 8.67s
                        Total time: 8154.39s
                               ETA: 907049.5s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.446s, learning 0.173s)
               Value function loss: 6.5470
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 246.98
               Mean episode length: 238.82
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 8.62s
                        Total time: 8163.01s
                               ETA: 906981.3s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.347s, learning 0.193s)
               Value function loss: 6.4574
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 251.03
               Mean episode length: 237.44
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 8.54s
                        Total time: 8171.55s
                               ETA: 906904.3s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.481s, learning 0.159s)
               Value function loss: 6.1747
                    Surrogate loss: -0.0015
             Mean action noise std: 0.77
                       Mean reward: 260.11
               Mean episode length: 247.46
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 8.64s
                        Total time: 8180.19s
                               ETA: 906838.6s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1793 steps/s (collection: 8.977s, learning 0.159s)
               Value function loss: 6.6293
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 262.80
               Mean episode length: 247.60
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 9.14s
                        Total time: 8189.32s
                               ETA: 906827.9s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.762s, learning 0.160s)
               Value function loss: 5.6360
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 265.92
               Mean episode length: 246.11
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 8.92s
                        Total time: 8198.24s
                               ETA: 906793.6s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.752s, learning 0.217s)
               Value function loss: 7.2806
                    Surrogate loss: -0.0025
             Mean action noise std: 0.77
                       Mean reward: 264.97
               Mean episode length: 247.75
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 8.97s
                        Total time: 8207.21s
                               ETA: 906764.4s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.457s, learning 0.165s)
               Value function loss: 8.2819
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 265.49
               Mean episode length: 248.27
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 8.62s
                        Total time: 8215.84s
                               ETA: 906697.2s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.481s, learning 0.167s)
               Value function loss: 7.5164
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 264.61
               Mean episode length: 247.26
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 8.65s
                        Total time: 8224.48s
                               ETA: 906632.7s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.595s, learning 0.172s)
               Value function loss: 7.4413
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 260.50
               Mean episode length: 247.88
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 8.77s
                        Total time: 8233.25s
                               ETA: 906581.6s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.454s, learning 0.235s)
               Value function loss: 5.3304
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 258.30
               Mean episode length: 241.95
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 8.69s
                        Total time: 8241.94s
                               ETA: 906522.0s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.670s, learning 0.199s)
               Value function loss: 5.4595
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 260.71
               Mean episode length: 245.23
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 8.87s
                        Total time: 8250.81s
                               ETA: 906482.2s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.705s, learning 0.228s)
               Value function loss: 5.9920
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 268.67
               Mean episode length: 249.56
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 8.93s
                        Total time: 8259.74s
                               ETA: 906449.6s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.919s, learning 0.185s)
               Value function loss: 5.2222
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 261.88
               Mean episode length: 247.61
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 9.10s
                        Total time: 8268.85s
                               ETA: 906435.7s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.706s, learning 0.164s)
               Value function loss: 6.4156
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 264.16
               Mean episode length: 247.58
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 8.87s
                        Total time: 8277.72s
                               ETA: 906396.2s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.748s, learning 0.162s)
               Value function loss: 4.3885
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 268.90
               Mean episode length: 247.11
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 8.91s
                        Total time: 8286.63s
                               ETA: 906361.1s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.539s, learning 0.245s)
               Value function loss: 5.0812
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 271.37
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 8.78s
                        Total time: 8295.41s
                               ETA: 906312.5s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.839s, learning 0.171s)
               Value function loss: 3.0000
                    Surrogate loss: 0.0117
             Mean action noise std: 0.77
                       Mean reward: 267.28
               Mean episode length: 246.13
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 9.01s
                        Total time: 8304.42s
                               ETA: 906288.4s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.504s, learning 0.165s)
               Value function loss: 3.7793
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 266.01
               Mean episode length: 244.13
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 8.67s
                        Total time: 8313.09s
                               ETA: 906227.3s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.673s, learning 0.166s)
               Value function loss: 3.9853
                    Surrogate loss: -0.0012
             Mean action noise std: 0.77
                       Mean reward: 265.33
               Mean episode length: 247.84
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 8.84s
                        Total time: 8321.93s
                               ETA: 906184.8s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.050s, learning 0.168s)
               Value function loss: 4.1940
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 264.61
               Mean episode length: 248.75
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 8.22s
                        Total time: 8330.15s
                               ETA: 906074.9s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.623s, learning 0.219s)
               Value function loss: 4.1455
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 256.77
               Mean episode length: 240.51
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 8.84s
                        Total time: 8338.99s
                               ETA: 906032.9s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.382s, learning 0.165s)
               Value function loss: 3.8524
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 253.10
               Mean episode length: 239.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 8.55s
                        Total time: 8347.54s
                               ETA: 905959.1s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.512s, learning 0.202s)
               Value function loss: 4.5982
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 261.59
               Mean episode length: 245.07
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 8.71s
                        Total time: 8356.25s
                               ETA: 905903.5s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.334s, learning 0.214s)
               Value function loss: 3.1327
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 266.14
               Mean episode length: 245.45
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 8.55s
                        Total time: 8364.80s
                               ETA: 905829.9s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.515s, learning 0.165s)
               Value function loss: 3.2114
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 265.60
               Mean episode length: 242.94
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 8.68s
                        Total time: 8373.48s
                               ETA: 905770.9s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1781 steps/s (collection: 9.030s, learning 0.165s)
               Value function loss: 3.5622
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 269.62
               Mean episode length: 247.46
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 9.19s
                        Total time: 8382.67s
                               ETA: 905767.5s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.661s, learning 0.158s)
               Value function loss: 4.8081
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 268.71
               Mean episode length: 247.85
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 8.82s
                        Total time: 8391.49s
                               ETA: 905723.5s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.393s, learning 0.164s)
               Value function loss: 4.2965
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 267.61
               Mean episode length: 247.59
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 8.56s
                        Total time: 8400.05s
                               ETA: 905651.3s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.763s, learning 0.249s)
               Value function loss: 3.4553
                    Surrogate loss: 0.0022
             Mean action noise std: 0.77
                       Mean reward: 266.94
               Mean episode length: 247.91
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 9.01s
                        Total time: 8409.06s
                               ETA: 905628.3s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.371s, learning 0.209s)
               Value function loss: 5.0873
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 266.90
               Mean episode length: 248.09
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 8.58s
                        Total time: 8417.64s
                               ETA: 905559.0s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1784 steps/s (collection: 8.995s, learning 0.187s)
               Value function loss: 5.0376
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 266.41
               Mean episode length: 247.31
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 9.18s
                        Total time: 8426.82s
                               ETA: 905554.4s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.778s, learning 0.176s)
               Value function loss: 5.7823
                    Surrogate loss: 0.0009
             Mean action noise std: 0.77
                       Mean reward: 273.92
               Mean episode length: 249.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 8.95s
                        Total time: 8435.78s
                               ETA: 905525.3s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.298s, learning 0.166s)
               Value function loss: 4.7985
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 266.31
               Mean episode length: 247.62
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 8.46s
                        Total time: 8444.24s
                               ETA: 905443.7s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.831s, learning 0.161s)
               Value function loss: 5.3979
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 268.34
               Mean episode length: 247.23
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 8.99s
                        Total time: 8453.23s
                               ETA: 905418.9s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.718s, learning 0.168s)
               Value function loss: 7.1453
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 271.86
               Mean episode length: 248.48
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 8.89s
                        Total time: 8462.12s
                               ETA: 905382.8s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.061s, learning 0.160s)
               Value function loss: 6.1630
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 269.95
               Mean episode length: 248.69
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 8.22s
                        Total time: 8470.34s
                               ETA: 905275.6s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1749 steps/s (collection: 9.201s, learning 0.167s)
               Value function loss: 6.7127
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 266.73
               Mean episode length: 248.25
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 9.37s
                        Total time: 8479.71s
                               ETA: 905291.0s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.652s, learning 0.175s)
               Value function loss: 8.3840
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 264.84
               Mean episode length: 245.58
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 8.83s
                        Total time: 8488.54s
                               ETA: 905248.8s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.379s, learning 0.212s)
               Value function loss: 12.3042
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 271.40
               Mean episode length: 249.12
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 8.59s
                        Total time: 8497.13s
                               ETA: 905181.4s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.706s, learning 0.186s)
               Value function loss: 10.1903
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 262.51
               Mean episode length: 245.72
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 8.89s
                        Total time: 8506.02s
                               ETA: 905146.3s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.291s, learning 0.209s)
               Value function loss: 10.2282
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 267.07
               Mean episode length: 246.94
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 8.50s
                        Total time: 8514.52s
                               ETA: 905069.4s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1146 steps/s (collection: 14.098s, learning 0.192s)
               Value function loss: 7.5637
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 272.54
               Mean episode length: 248.35
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 14.29s
                        Total time: 8528.81s
                               ETA: 905607.6s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 937 steps/s (collection: 17.318s, learning 0.167s)
               Value function loss: 8.3909
                    Surrogate loss: 0.0078
             Mean action noise std: 0.77
                       Mean reward: 272.86
               Mean episode length: 249.07
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 17.48s
                        Total time: 8546.29s
                               ETA: 906483.4s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.577s, learning 0.166s)
               Value function loss: 7.5250
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 274.89
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 16.74s
                        Total time: 8563.03s
                               ETA: 907278.7s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.648s, learning 0.185s)
               Value function loss: 8.4606
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 276.30
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 16.83s
                        Total time: 8579.87s
                               ETA: 908081.8s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.837s, learning 0.168s)
               Value function loss: 6.0431
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 275.44
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 17.00s
                        Total time: 8596.87s
                               ETA: 908901.4s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 942 steps/s (collection: 17.219s, learning 0.172s)
               Value function loss: 9.0550
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 276.89
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 17.39s
                        Total time: 8614.26s
                               ETA: 909759.9s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.482s, learning 0.185s)
               Value function loss: 3.4642
                    Surrogate loss: -0.0040
             Mean action noise std: 0.77
                       Mean reward: 273.99
               Mean episode length: 249.16
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 16.67s
                        Total time: 8630.93s
                               ETA: 910540.2s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.452s, learning 0.181s)
               Value function loss: 5.3477
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 272.50
               Mean episode length: 245.37
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 16.63s
                        Total time: 8647.56s
                               ETA: 911315.2s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.921s, learning 0.201s)
               Value function loss: 5.8603
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 275.01
               Mean episode length: 246.17
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 17.12s
                        Total time: 8664.69s
                               ETA: 912140.1s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.932s, learning 0.173s)
               Value function loss: 4.0861
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 276.86
               Mean episode length: 248.58
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 17.11s
                        Total time: 8681.79s
                               ETA: 912961.3s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.579s, learning 0.180s)
               Value function loss: 5.8216
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 280.33
               Mean episode length: 248.36
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 16.76s
                        Total time: 8698.55s
                               ETA: 913744.4s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.707s, learning 0.213s)
               Value function loss: 5.7205
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 284.54
               Mean episode length: 249.94
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 16.92s
                        Total time: 8715.47s
                               ETA: 914542.7s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.814s, learning 0.162s)
               Value function loss: 6.2350
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 277.47
               Mean episode length: 245.27
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 16.98s
                        Total time: 8732.45s
                               ETA: 915345.2s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.565s, learning 0.159s)
               Value function loss: 3.6758
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 276.88
               Mean episode length: 245.27
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 16.72s
                        Total time: 8749.17s
                               ETA: 916119.5s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.812s, learning 0.217s)
               Value function loss: 4.3778
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 277.45
               Mean episode length: 248.02
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 17.03s
                        Total time: 8766.20s
                               ETA: 916924.1s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.695s, learning 0.160s)
               Value function loss: 4.4365
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 274.48
               Mean episode length: 246.94
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 16.86s
                        Total time: 8783.05s
                               ETA: 917708.8s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.684s, learning 0.230s)
               Value function loss: 5.3968
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 272.77
               Mean episode length: 246.88
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 16.91s
                        Total time: 8799.97s
                               ETA: 918497.9s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.068s, learning 0.263s)
               Value function loss: 6.2596
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 277.08
               Mean episode length: 249.78
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 17.33s
                        Total time: 8817.30s
                               ETA: 919328.8s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 944 steps/s (collection: 17.169s, learning 0.186s)
               Value function loss: 4.5205
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 280.23
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 17.35s
                        Total time: 8834.65s
                               ETA: 920160.4s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 952 steps/s (collection: 17.047s, learning 0.161s)
               Value function loss: 6.2130
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 279.01
               Mean episode length: 247.15
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 17.21s
                        Total time: 8851.86s
                               ETA: 920974.9s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.470s, learning 0.190s)
               Value function loss: 5.6126
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 274.99
               Mean episode length: 245.30
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 16.66s
                        Total time: 8868.52s
                               ETA: 921730.8s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.542s, learning 0.267s)
               Value function loss: 7.3505
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 278.29
               Mean episode length: 249.49
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 16.81s
                        Total time: 8885.33s
                               ETA: 922500.4s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.030s, learning 0.296s)
               Value function loss: 6.7520
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 278.19
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 17.33s
                        Total time: 8902.66s
                               ETA: 923322.1s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 964 steps/s (collection: 16.791s, learning 0.196s)
               Value function loss: 6.4686
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 275.05
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 16.99s
                        Total time: 8919.64s
                               ETA: 924106.8s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.669s, learning 0.164s)
               Value function loss: 7.5081
                    Surrogate loss: 0.0013
             Mean action noise std: 0.77
                       Mean reward: 270.92
               Mean episode length: 246.46
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 16.83s
                        Total time: 8936.48s
                               ETA: 924874.0s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 940 steps/s (collection: 17.216s, learning 0.207s)
               Value function loss: 7.2673
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 272.82
               Mean episode length: 246.78
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 17.42s
                        Total time: 8953.90s
                               ETA: 925700.5s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.193s, learning 0.183s)
               Value function loss: 6.6608
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 276.67
               Mean episode length: 247.94
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 16.38s
                        Total time: 8970.28s
                               ETA: 926417.2s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 940 steps/s (collection: 17.163s, learning 0.254s)
               Value function loss: 8.4677
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 277.84
               Mean episode length: 247.25
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 17.42s
                        Total time: 8987.69s
                               ETA: 927239.7s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.669s, learning 0.170s)
               Value function loss: 9.5106
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 280.91
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 16.84s
                        Total time: 9004.53s
                               ETA: 928000.9s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.660s, learning 0.164s)
               Value function loss: 9.7503
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: 279.14
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 16.82s
                        Total time: 9021.36s
                               ETA: 928758.9s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.736s, learning 0.167s)
               Value function loss: 8.6040
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 280.55
               Mean episode length: 248.62
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 16.90s
                        Total time: 9038.26s
                               ETA: 929523.4s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.299s, learning 0.166s)
               Value function loss: 6.5337
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 275.62
               Mean episode length: 245.63
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 16.47s
                        Total time: 9054.72s
                               ETA: 930241.4s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 951 steps/s (collection: 17.034s, learning 0.188s)
               Value function loss: 7.4960
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 278.01
               Mean episode length: 246.19
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 17.22s
                        Total time: 9071.95s
                               ETA: 931035.5s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.604s, learning 0.257s)
               Value function loss: 6.8560
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 281.64
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 16.86s
                        Total time: 9088.81s
                               ETA: 931790.8s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.383s, learning 0.200s)
               Value function loss: 7.7675
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 285.46
               Mean episode length: 249.82
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 16.58s
                        Total time: 9105.39s
                               ETA: 932516.1s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.505s, learning 0.208s)
               Value function loss: 6.3047
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 281.80
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 16.71s
                        Total time: 9122.10s
                               ETA: 933253.3s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.385s, learning 0.166s)
               Value function loss: 11.4819
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 281.45
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 16.55s
                        Total time: 9138.65s
                               ETA: 933972.3s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.777s, learning 0.168s)
               Value function loss: 3.3397
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 281.88
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 16.95s
                        Total time: 9155.60s
                               ETA: 934730.0s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.324s, learning 0.163s)
               Value function loss: 3.9049
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 282.65
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 8.49s
                        Total time: 9164.09s
                               ETA: 934623.5s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.328s, learning 0.168s)
               Value function loss: 4.0524
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 277.76
               Mean episode length: 247.96
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 8.50s
                        Total time: 9172.58s
                               ETA: 934518.1s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.254s, learning 0.156s)
               Value function loss: 4.1368
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 273.53
               Mean episode length: 244.10
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 8.41s
                        Total time: 9180.99s
                               ETA: 934404.1s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.369s, learning 0.161s)
               Value function loss: 5.3905
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 269.86
               Mean episode length: 241.14
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 8.53s
                        Total time: 9189.52s
                               ETA: 934302.6s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.825s, learning 0.173s)
               Value function loss: 5.5057
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 268.85
               Mean episode length: 241.68
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 9.00s
                        Total time: 9198.52s
                               ETA: 934248.8s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.450s, learning 0.174s)
               Value function loss: 5.4757
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 273.20
               Mean episode length: 244.61
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 8.62s
                        Total time: 9207.14s
                               ETA: 934157.1s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.483s, learning 0.214s)
               Value function loss: 4.1135
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 279.85
               Mean episode length: 247.62
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 8.70s
                        Total time: 9215.84s
                               ETA: 934073.1s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.354s, learning 0.160s)
               Value function loss: 4.3012
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 279.55
               Mean episode length: 248.20
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 8.51s
                        Total time: 9224.36s
                               ETA: 933970.7s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.576s, learning 0.172s)
               Value function loss: 4.6673
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 277.56
               Mean episode length: 247.36
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 8.75s
                        Total time: 9233.10s
                               ETA: 933892.1s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.626s, learning 0.167s)
               Value function loss: 4.5408
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 279.34
               Mean episode length: 248.56
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 8.79s
                        Total time: 9241.90s
                               ETA: 933818.1s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.596s, learning 0.163s)
               Value function loss: 5.8167
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: 276.84
               Mean episode length: 248.83
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 8.76s
                        Total time: 9250.65s
                               ETA: 933740.9s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.482s, learning 0.165s)
               Value function loss: 4.3354
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 276.74
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 8.65s
                        Total time: 9259.30s
                               ETA: 933652.6s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.124s, learning 0.188s)
               Value function loss: 4.7614
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 277.63
               Mean episode length: 249.94
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 8.31s
                        Total time: 9267.61s
                               ETA: 933530.7s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.364s, learning 0.182s)
               Value function loss: 5.0765
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 277.93
               Mean episode length: 247.63
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 8.55s
                        Total time: 9276.16s
                               ETA: 933432.6s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.512s, learning 0.193s)
               Value function loss: 7.0189
                    Surrogate loss: -0.0028
             Mean action noise std: 0.77
                       Mean reward: 278.53
               Mean episode length: 247.76
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 8.71s
                        Total time: 9284.87s
                               ETA: 933350.6s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.741s, learning 0.168s)
               Value function loss: 6.6986
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 283.90
               Mean episode length: 249.89
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 8.91s
                        Total time: 9293.78s
                               ETA: 933289.2s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.919s, learning 0.177s)
               Value function loss: 4.9533
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 277.21
               Mean episode length: 246.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 9.10s
                        Total time: 9302.87s
                               ETA: 933246.7s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.649s, learning 0.180s)
               Value function loss: 7.8874
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 283.60
               Mean episode length: 249.59
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 8.83s
                        Total time: 9311.70s
                               ETA: 933177.4s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.600s, learning 0.291s)
               Value function loss: 6.8629
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 278.48
               Mean episode length: 247.36
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 8.89s
                        Total time: 9320.59s
                               ETA: 933114.6s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.417s, learning 0.202s)
               Value function loss: 6.5437
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 276.74
               Mean episode length: 247.24
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 8.62s
                        Total time: 9329.21s
                               ETA: 933024.7s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.804s, learning 0.167s)
               Value function loss: 7.6781
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 283.47
               Mean episode length: 249.54
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 8.97s
                        Total time: 9338.18s
                               ETA: 932970.0s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.565s, learning 0.202s)
               Value function loss: 9.0852
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 275.39
               Mean episode length: 246.03
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 8.77s
                        Total time: 9346.95s
                               ETA: 932895.1s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.633s, learning 0.174s)
               Value function loss: 8.3507
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 280.18
               Mean episode length: 249.10
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 8.81s
                        Total time: 9355.75s
                               ETA: 932824.3s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.417s, learning 0.168s)
               Value function loss: 7.9437
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 283.15
               Mean episode length: 249.82
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 8.59s
                        Total time: 9364.34s
                               ETA: 932731.6s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.367s, learning 0.178s)
               Value function loss: 6.3462
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 282.97
               Mean episode length: 249.39
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 8.55s
                        Total time: 9372.89s
                               ETA: 932635.1s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.841s, learning 0.176s)
               Value function loss: 6.5790
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 280.40
               Mean episode length: 246.27
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 9.02s
                        Total time: 9381.90s
                               ETA: 932585.6s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.522s, learning 0.163s)
               Value function loss: 7.3370
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 277.12
               Mean episode length: 247.82
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 8.68s
                        Total time: 9390.59s
                               ETA: 932503.2s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.650s, learning 0.172s)
               Value function loss: 7.5331
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 280.18
               Mean episode length: 249.85
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 8.82s
                        Total time: 9399.41s
                               ETA: 932434.5s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.819s, learning 0.205s)
               Value function loss: 7.2961
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 282.52
               Mean episode length: 247.97
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 9.02s
                        Total time: 9408.43s
                               ETA: 932386.0s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.700s, learning 0.215s)
               Value function loss: 12.6031
                    Surrogate loss: -0.0007
             Mean action noise std: 0.77
                       Mean reward: 283.06
               Mean episode length: 249.95
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 8.91s
                        Total time: 9417.35s
                               ETA: 932326.8s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.854s, learning 0.193s)
               Value function loss: 3.0325
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 279.53
               Mean episode length: 247.69
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 9.05s
                        Total time: 9426.39s
                               ETA: 932280.8s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.268s, learning 0.179s)
               Value function loss: 3.4596
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 274.39
               Mean episode length: 245.41
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 8.45s
                        Total time: 9434.84s
                               ETA: 932175.6s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.763s, learning 0.163s)
               Value function loss: 3.8229
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 275.56
               Mean episode length: 244.73
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 8.93s
                        Total time: 9443.77s
                               ETA: 932117.8s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.813s, learning 0.171s)
               Value function loss: 3.7414
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 273.94
               Mean episode length: 241.97
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 8.98s
                        Total time: 9452.75s
                               ETA: 932065.7s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.172s)
               Value function loss: 4.3887
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 274.03
               Mean episode length: 243.26
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 8.67s
                        Total time: 9461.42s
                               ETA: 931983.0s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.459s, learning 0.181s)
               Value function loss: 4.9407
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 276.86
               Mean episode length: 244.89
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 8.64s
                        Total time: 9470.06s
                               ETA: 931897.4s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.599s, learning 0.269s)
               Value function loss: 4.0655
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 282.06
               Mean episode length: 248.55
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 8.87s
                        Total time: 9478.93s
                               ETA: 931834.3s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.444s, learning 0.165s)
               Value function loss: 4.8465
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 281.97
               Mean episode length: 248.30
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 8.61s
                        Total time: 9487.54s
                               ETA: 931745.9s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.562s, learning 0.182s)
               Value function loss: 4.1491
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 281.53
               Mean episode length: 248.06
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 8.74s
                        Total time: 9496.28s
                               ETA: 931670.9s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.753s, learning 0.162s)
               Value function loss: 3.4742
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 284.48
               Mean episode length: 248.41
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 8.92s
                        Total time: 9505.20s
                               ETA: 931612.9s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.826s, learning 0.175s)
               Value function loss: 3.6207
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 283.49
               Mean episode length: 248.32
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 9.00s
                        Total time: 9514.20s
                               ETA: 931563.4s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.548s, learning 0.167s)
               Value function loss: 5.2453
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 281.01
               Mean episode length: 247.47
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 8.72s
                        Total time: 9522.91s
                               ETA: 931485.9s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.487s, learning 0.227s)
               Value function loss: 4.6406
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 283.39
               Mean episode length: 249.12
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 8.71s
                        Total time: 9531.63s
                               ETA: 931408.5s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.608s, learning 0.210s)
               Value function loss: 3.7198
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 284.47
               Mean episode length: 248.59
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 8.82s
                        Total time: 9540.45s
                               ETA: 931341.3s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.902s, learning 0.217s)
               Value function loss: 6.5252
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 275.28
               Mean episode length: 240.70
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 9.12s
                        Total time: 9549.57s
                               ETA: 931303.7s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.695s, learning 0.176s)
               Value function loss: 5.9640
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 273.23
               Mean episode length: 238.24
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 8.87s
                        Total time: 9558.44s
                               ETA: 931241.9s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.543s, learning 0.203s)
               Value function loss: 6.8044
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 277.29
               Mean episode length: 242.78
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 8.75s
                        Total time: 9567.18s
                               ETA: 931168.0s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.531s, learning 0.198s)
               Value function loss: 6.5616
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 280.39
               Mean episode length: 244.79
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 8.73s
                        Total time: 9575.91s
                               ETA: 931092.7s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.240s, learning 0.215s)
               Value function loss: 6.3146
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 280.61
               Mean episode length: 248.52
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 8.45s
                        Total time: 9584.37s
                               ETA: 930990.8s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.551s, learning 0.186s)
               Value function loss: 7.3141
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 282.14
               Mean episode length: 248.79
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 8.74s
                        Total time: 9593.10s
                               ETA: 930916.5s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.602s, learning 0.191s)
               Value function loss: 6.9824
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 281.59
               Mean episode length: 247.45
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 8.79s
                        Total time: 9601.90s
                               ETA: 930847.8s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.918s, learning 0.184s)
               Value function loss: 7.5880
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 283.16
               Mean episode length: 248.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 8.10s
                        Total time: 9610.00s
                               ETA: 930712.3s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.489s, learning 0.198s)
               Value function loss: 9.3924
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 287.80
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 8.69s
                        Total time: 9618.69s
                               ETA: 930633.6s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.758s, learning 0.190s)
               Value function loss: 7.3076
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 286.44
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 8.95s
                        Total time: 9627.63s
                               ETA: 930580.3s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.707s, learning 0.165s)
               Value function loss: 8.4271
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 285.28
               Mean episode length: 248.96
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 8.87s
                        Total time: 9636.50s
                               ETA: 930519.7s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.296s, learning 0.224s)
               Value function loss: 6.7875
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 284.77
               Mean episode length: 249.04
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 8.52s
                        Total time: 9645.03s
                               ETA: 930425.3s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.705s, learning 0.206s)
               Value function loss: 7.0149
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 284.27
               Mean episode length: 247.34
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 8.91s
                        Total time: 9653.94s
                               ETA: 930368.7s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.959s, learning 0.180s)
               Value function loss: 6.5026
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 284.03
               Mean episode length: 249.89
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 9.14s
                        Total time: 9663.08s
                               ETA: 930334.2s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.760s, learning 0.163s)
               Value function loss: 5.8134
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 284.54
               Mean episode length: 249.30
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 8.92s
                        Total time: 9672.00s
                               ETA: 930279.0s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.624s, learning 0.186s)
               Value function loss: 7.4598
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 285.45
               Mean episode length: 249.88
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 8.81s
                        Total time: 9680.81s
                               ETA: 930212.9s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.806s, learning 0.287s)
               Value function loss: 4.8731
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 284.94
               Mean episode length: 249.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 9.09s
                        Total time: 9689.90s
                               ETA: 930174.2s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.609s, learning 0.192s)
               Value function loss: 4.9786
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 285.61
               Mean episode length: 247.65
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 8.80s
                        Total time: 9698.70s
                               ETA: 930107.5s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.664s, learning 0.197s)
               Value function loss: 2.7607
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 283.86
               Mean episode length: 246.46
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 8.86s
                        Total time: 9707.56s
                               ETA: 930046.6s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.632s, learning 0.170s)
               Value function loss: 3.6612
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 281.58
               Mean episode length: 245.08
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 8.80s
                        Total time: 9716.37s
                               ETA: 929980.2s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.580s, learning 0.162s)
               Value function loss: 4.6187
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 279.58
               Mean episode length: 244.24
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 8.74s
                        Total time: 9725.11s
                               ETA: 929908.2s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.586s, learning 0.177s)
               Value function loss: 4.8227
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 280.60
               Mean episode length: 244.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 8.76s
                        Total time: 9733.87s
                               ETA: 929838.3s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.904s, learning 0.182s)
               Value function loss: 4.6289
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 286.36
               Mean episode length: 248.38
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 9.09s
                        Total time: 9742.96s
                               ETA: 929799.3s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.911s, learning 0.195s)
               Value function loss: 4.6299
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 286.68
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 9.11s
                        Total time: 9752.06s
                               ETA: 929762.4s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.930s, learning 0.171s)
               Value function loss: 5.7366
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 280.91
               Mean episode length: 245.21
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 9.10s
                        Total time: 9761.16s
                               ETA: 929725.0s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.953s, learning 0.195s)
               Value function loss: 4.3731
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 272.97
               Mean episode length: 240.85
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 9.15s
                        Total time: 9770.31s
                               ETA: 929692.1s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.054s, learning 0.160s)
               Value function loss: 3.6201
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 270.27
               Mean episode length: 241.33
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 9.21s
                        Total time: 9779.53s
                               ETA: 929665.5s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.558s, learning 0.162s)
               Value function loss: 4.2026
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 272.31
               Mean episode length: 243.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 8.72s
                        Total time: 9788.24s
                               ETA: 929592.0s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.754s, learning 0.193s)
               Value function loss: 5.5169
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 280.57
               Mean episode length: 246.57
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 8.95s
                        Total time: 9797.19s
                               ETA: 929540.2s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.553s, learning 0.294s)
               Value function loss: 4.8282
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 283.67
               Mean episode length: 247.36
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 8.85s
                        Total time: 9806.04s
                               ETA: 929479.1s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.853s, learning 0.163s)
               Value function loss: 3.8581
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 282.75
               Mean episode length: 245.75
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 9.02s
                        Total time: 9815.06s
                               ETA: 929434.1s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.698s, learning 0.165s)
               Value function loss: 6.2933
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 279.42
               Mean episode length: 247.32
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 8.86s
                        Total time: 9823.92s
                               ETA: 929374.6s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.678s, learning 0.197s)
               Value function loss: 6.0038
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 281.13
               Mean episode length: 246.99
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 8.88s
                        Total time: 9832.79s
                               ETA: 929316.4s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.438s, learning 0.263s)
               Value function loss: 7.1505
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 283.42
               Mean episode length: 248.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 8.70s
                        Total time: 9841.50s
                               ETA: 929241.9s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.830s, learning 0.159s)
               Value function loss: 6.7761
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 276.31
               Mean episode length: 241.30
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 8.99s
                        Total time: 9850.48s
                               ETA: 929194.6s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.506s, learning 0.180s)
               Value function loss: 5.7711
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 276.03
               Mean episode length: 243.48
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 8.69s
                        Total time: 9859.17s
                               ETA: 929118.9s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.706s, learning 0.179s)
               Value function loss: 7.9851
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 279.39
               Mean episode length: 244.63
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 8.89s
                        Total time: 9868.06s
                               ETA: 929062.0s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.170s)
               Value function loss: 7.0981
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 276.46
               Mean episode length: 242.82
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 8.43s
                        Total time: 9876.49s
                               ETA: 928962.4s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.570s, learning 0.189s)
               Value function loss: 6.4344
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 274.73
               Mean episode length: 244.71
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 8.76s
                        Total time: 9885.25s
                               ETA: 928893.9s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.609s, learning 0.186s)
               Value function loss: 8.6101
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 284.44
               Mean episode length: 248.78
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 8.79s
                        Total time: 9894.04s
                               ETA: 928828.8s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.726s, learning 0.175s)
               Value function loss: 8.7429
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 286.48
               Mean episode length: 248.88
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 8.90s
                        Total time: 9902.94s
                               ETA: 928773.9s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.446s, learning 0.171s)
               Value function loss: 8.9189
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 286.07
               Mean episode length: 248.95
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 8.62s
                        Total time: 9911.56s
                               ETA: 928692.4s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.431s, learning 0.174s)
               Value function loss: 8.7390
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 283.90
               Mean episode length: 248.10
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 8.60s
                        Total time: 9920.16s
                               ETA: 928609.9s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.501s, learning 0.178s)
               Value function loss: 6.9110
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 285.08
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 8.68s
                        Total time: 9928.84s
                               ETA: 928534.5s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.630s, learning 0.171s)
               Value function loss: 6.7688
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 277.80
               Mean episode length: 242.15
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 8.80s
                        Total time: 9937.64s
                               ETA: 928470.7s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.537s, learning 0.171s)
               Value function loss: 5.7975
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 277.08
               Mean episode length: 242.15
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 8.71s
                        Total time: 9946.35s
                               ETA: 928398.3s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.438s, learning 0.174s)
               Value function loss: 7.5801
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 287.47
               Mean episode length: 249.83
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 8.61s
                        Total time: 9954.96s
                               ETA: 928316.9s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.515s, learning 0.184s)
               Value function loss: 4.8872
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 285.27
               Mean episode length: 248.88
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 8.70s
                        Total time: 9963.66s
                               ETA: 928243.8s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.779s, learning 0.203s)
               Value function loss: 6.5559
                    Surrogate loss: -0.0014
             Mean action noise std: 0.77
                       Mean reward: 286.31
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 8.98s
                        Total time: 9972.65s
                               ETA: 928197.2s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.411s, learning 0.296s)
               Value function loss: 2.7796
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 284.71
               Mean episode length: 248.50
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 8.71s
                        Total time: 9981.35s
                               ETA: 928125.1s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.736s, learning 0.257s)
               Value function loss: 4.4174
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 280.18
               Mean episode length: 246.33
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 8.99s
                        Total time: 9990.35s
                               ETA: 928079.6s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.279s, learning 0.240s)
               Value function loss: 5.3418
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 277.50
               Mean episode length: 244.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 8.52s
                        Total time: 9998.86s
                               ETA: 927990.3s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.584s, learning 0.200s)
               Value function loss: 4.0627
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 276.68
               Mean episode length: 243.40
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 8.78s
                        Total time: 10007.65s
                               ETA: 927925.7s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.411s, learning 0.196s)
               Value function loss: 5.0460
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 278.17
               Mean episode length: 245.90
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 8.61s
                        Total time: 10016.26s
                               ETA: 927844.8s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.765s, learning 0.178s)
               Value function loss: 5.1856
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 276.39
               Mean episode length: 242.71
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 8.94s
                        Total time: 10025.20s
                               ETA: 927795.1s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.407s, learning 0.172s)
               Value function loss: 6.0700
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 277.85
               Mean episode length: 242.36
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 8.58s
                        Total time: 10033.78s
                               ETA: 927711.9s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.622s, learning 0.180s)
               Value function loss: 3.2134
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 283.78
               Mean episode length: 246.60
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 8.80s
                        Total time: 10042.58s
                               ETA: 927649.3s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.361s, learning 0.206s)
               Value function loss: 3.5314
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 285.66
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 8.57s
                        Total time: 10051.15s
                               ETA: 927565.1s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.772s, learning 0.178s)
               Value function loss: 3.9997
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 280.72
               Mean episode length: 247.75
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 8.95s
                        Total time: 10060.10s
                               ETA: 927516.4s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.467s, learning 0.195s)
               Value function loss: 5.2400
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 281.73
               Mean episode length: 247.75
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 8.66s
                        Total time: 10068.76s
                               ETA: 927441.2s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.810s, learning 0.205s)
               Value function loss: 5.2744
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 282.32
               Mean episode length: 247.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 9.02s
                        Total time: 10077.77s
                               ETA: 927398.8s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.541s, learning 0.266s)
               Value function loss: 4.2612
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 277.09
               Mean episode length: 244.23
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 8.81s
                        Total time: 10086.58s
                               ETA: 927337.3s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.564s, learning 0.166s)
               Value function loss: 6.1105
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 283.03
               Mean episode length: 247.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 8.73s
                        Total time: 10095.31s
                               ETA: 927268.8s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.488s, learning 0.271s)
               Value function loss: 6.0086
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 286.58
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 8.76s
                        Total time: 10104.07s
                               ETA: 927203.0s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.761s, learning 0.243s)
               Value function loss: 6.7170
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 285.35
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 9.00s
                        Total time: 10113.07s
                               ETA: 927159.8s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.236s, learning 0.175s)
               Value function loss: 7.0284
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 282.59
               Mean episode length: 248.14
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 8.41s
                        Total time: 10121.48s
                               ETA: 927062.4s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.724s, learning 0.228s)
               Value function loss: 6.0014
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 282.06
               Mean episode length: 246.14
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 8.95s
                        Total time: 10130.44s
                               ETA: 927014.6s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.671s, learning 0.194s)
               Value function loss: 7.0846
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 279.64
               Mean episode length: 244.48
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 8.86s
                        Total time: 10139.30s
                               ETA: 926958.9s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.488s, learning 0.221s)
               Value function loss: 7.1877
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 280.70
               Mean episode length: 243.87
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 8.71s
                        Total time: 10148.01s
                               ETA: 926889.1s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.604s, learning 0.227s)
               Value function loss: 6.3913
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 286.61
               Mean episode length: 247.64
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 8.83s
                        Total time: 10156.84s
                               ETA: 926830.5s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.712s, learning 0.173s)
               Value function loss: 7.4579
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 284.40
               Mean episode length: 248.16
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 8.89s
                        Total time: 10165.73s
                               ETA: 926777.0s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.509s, learning 0.233s)
               Value function loss: 8.1222
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 285.02
               Mean episode length: 248.16
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 8.74s
                        Total time: 10174.47s
                               ETA: 926710.5s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.691s, learning 0.265s)
               Value function loss: 9.1835
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 291.21
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 8.96s
                        Total time: 10183.42s
                               ETA: 926663.5s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.805s, learning 0.213s)
               Value function loss: 7.4012
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 284.71
               Mean episode length: 247.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 9.02s
                        Total time: 10192.44s
                               ETA: 926622.3s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.414s, learning 0.166s)
               Value function loss: 6.6229
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 283.50
               Mean episode length: 245.06
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 8.58s
                        Total time: 10201.02s
                               ETA: 926541.4s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.561s, learning 0.180s)
               Value function loss: 7.2021
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 288.13
               Mean episode length: 247.18
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 8.74s
                        Total time: 10209.76s
                               ETA: 926475.1s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.209s, learning 0.168s)
               Value function loss: 5.3374
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 283.27
               Mean episode length: 244.53
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 8.38s
                        Total time: 10218.14s
                               ETA: 926376.1s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.430s, learning 0.167s)
               Value function loss: 6.7026
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 285.01
               Mean episode length: 247.54
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 8.60s
                        Total time: 10226.74s
                               ETA: 926297.1s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.202s, learning 0.178s)
               Value function loss: 5.1599
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 288.84
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 8.38s
                        Total time: 10235.12s
                               ETA: 926198.6s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.294s, learning 0.195s)
               Value function loss: 9.5429
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 285.23
               Mean episode length: 247.43
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 8.49s
                        Total time: 10243.61s
                               ETA: 926110.1s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.418s, learning 0.234s)
               Value function loss: 3.0321
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 281.18
               Mean episode length: 245.38
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 8.65s
                        Total time: 10252.26s
                               ETA: 926036.5s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.981s, learning 0.181s)
               Value function loss: 3.5276
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 277.43
               Mean episode length: 245.30
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 9.16s
                        Total time: 10261.42s
                               ETA: 926009.1s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.433s, learning 0.180s)
               Value function loss: 3.2033
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 282.93
               Mean episode length: 249.92
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 8.61s
                        Total time: 10270.03s
                               ETA: 925932.1s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.866s, learning 0.167s)
               Value function loss: 3.7692
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 284.17
               Mean episode length: 248.31
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 9.03s
                        Total time: 10279.07s
                               ETA: 925893.1s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.425s, learning 0.177s)
               Value function loss: 4.9039
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 282.13
               Mean episode length: 247.72
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 8.60s
                        Total time: 10287.67s
                               ETA: 925815.3s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.431s, learning 0.195s)
               Value function loss: 4.8366
                    Surrogate loss: -0.0031
             Mean action noise std: 0.77
                       Mean reward: 284.26
               Mean episode length: 249.41
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 8.63s
                        Total time: 10296.29s
                               ETA: 925739.9s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.933s, learning 0.169s)
               Value function loss: 5.3679
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 287.66
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 9.10s
                        Total time: 10305.40s
                               ETA: 925707.3s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.368s, learning 0.170s)
               Value function loss: 3.8209
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 286.14
               Mean episode length: 248.37
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 8.54s
                        Total time: 10313.94s
                               ETA: 925624.2s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.336s, learning 0.168s)
               Value function loss: 3.7849
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 284.18
               Mean episode length: 247.05
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 8.50s
                        Total time: 10322.44s
                               ETA: 925538.2s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.414s, learning 0.200s)
               Value function loss: 3.9819
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 285.60
               Mean episode length: 248.68
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 8.61s
                        Total time: 10331.05s
                               ETA: 925462.1s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.657s, learning 0.194s)
               Value function loss: 3.2769
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 285.92
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 8.85s
                        Total time: 10339.90s
                               ETA: 925407.4s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.821s, learning 0.175s)
               Value function loss: 5.0291
                    Surrogate loss: -0.0025
             Mean action noise std: 0.77
                       Mean reward: 278.77
               Mean episode length: 244.87
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 9.00s
                        Total time: 10348.90s
                               ETA: 925365.8s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.336s, learning 0.249s)
               Value function loss: 4.5575
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 282.85
               Mean episode length: 245.23
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 8.58s
                        Total time: 10357.49s
                               ETA: 925287.4s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.631s, learning 0.169s)
               Value function loss: 4.5316
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 288.06
               Mean episode length: 248.78
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 8.80s
                        Total time: 10366.29s
                               ETA: 925228.4s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.567s, learning 0.191s)
               Value function loss: 6.2524
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 282.69
               Mean episode length: 247.93
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 8.76s
                        Total time: 10375.04s
                               ETA: 925165.7s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.708s, learning 0.168s)
               Value function loss: 5.9801
                    Surrogate loss: -0.0006
             Mean action noise std: 0.77
                       Mean reward: 278.57
               Mean episode length: 246.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 8.88s
                        Total time: 10383.92s
                               ETA: 925113.7s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.612s, learning 0.177s)
               Value function loss: 7.0030
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 283.00
               Mean episode length: 248.93
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 8.79s
                        Total time: 10392.71s
                               ETA: 925054.0s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.676s, learning 0.177s)
               Value function loss: 5.0773
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 285.40
               Mean episode length: 249.42
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 8.85s
                        Total time: 10401.56s
                               ETA: 925000.1s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.460s, learning 0.171s)
               Value function loss: 7.9027
                    Surrogate loss: -0.0011
             Mean action noise std: 0.77
                       Mean reward: 285.99
               Mean episode length: 247.72
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 8.63s
                        Total time: 10410.19s
                               ETA: 924926.5s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.791s, learning 0.169s)
               Value function loss: 7.2627
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 284.31
               Mean episode length: 248.04
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 8.96s
                        Total time: 10419.15s
                               ETA: 924882.3s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.483s, learning 0.168s)
               Value function loss: 5.9014
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: 284.44
               Mean episode length: 249.82
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 8.65s
                        Total time: 10427.81s
                               ETA: 924810.7s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.639s, learning 0.196s)
               Value function loss: 5.9490
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 285.17
               Mean episode length: 249.94
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 8.84s
                        Total time: 10436.64s
                               ETA: 924755.5s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.475s, learning 0.189s)
               Value function loss: 7.6596
                    Surrogate loss: 0.0024
             Mean action noise std: 0.77
                       Mean reward: 280.46
               Mean episode length: 247.75
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 8.66s
                        Total time: 10445.30s
                               ETA: 924685.3s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.553s, learning 0.215s)
               Value function loss: 6.9219
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 281.40
               Mean episode length: 248.98
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 8.77s
                        Total time: 10454.07s
                               ETA: 924624.4s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.187s, learning 0.216s)
               Value function loss: 6.6470
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 278.84
               Mean episode length: 245.87
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 8.40s
                        Total time: 10462.48s
                               ETA: 924531.4s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.408s, learning 0.210s)
               Value function loss: 6.1886
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 281.21
               Mean episode length: 248.74
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 8.62s
                        Total time: 10471.09s
                               ETA: 924457.4s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.677s, learning 0.193s)
               Value function loss: 5.6784
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 276.89
               Mean episode length: 247.01
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 8.87s
                        Total time: 10479.97s
                               ETA: 924405.8s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.591s, learning 0.175s)
               Value function loss: 5.7072
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 281.18
               Mean episode length: 249.44
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 8.77s
                        Total time: 10488.73s
                               ETA: 924345.2s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.485s, learning 0.189s)
               Value function loss: 6.7023
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 279.94
               Mean episode length: 248.39
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 8.67s
                        Total time: 10497.41s
                               ETA: 924276.5s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.647s, learning 0.196s)
               Value function loss: 5.1165
                    Surrogate loss: 0.0005
             Mean action noise std: 0.77
                       Mean reward: 279.65
               Mean episode length: 248.29
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 8.84s
                        Total time: 10506.25s
                               ETA: 924222.7s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.522s, learning 0.182s)
               Value function loss: 7.6536
                    Surrogate loss: -0.0031
             Mean action noise std: 0.77
                       Mean reward: 280.71
               Mean episode length: 249.69
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 8.70s
                        Total time: 10514.95s
                               ETA: 924156.8s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.457s, learning 0.182s)
               Value function loss: 2.2131
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 278.14
               Mean episode length: 249.56
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 8.64s
                        Total time: 10523.59s
                               ETA: 924085.4s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.267s, learning 0.176s)
               Value function loss: 2.8956
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 270.92
               Mean episode length: 245.89
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 8.44s
                        Total time: 10532.04s
                               ETA: 923996.9s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.651s, learning 0.202s)
               Value function loss: 2.8055
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 270.40
               Mean episode length: 246.33
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 8.85s
                        Total time: 10540.89s
                               ETA: 923944.3s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.319s, learning 0.175s)
               Value function loss: 3.5477
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 271.66
               Mean episode length: 246.64
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 8.49s
                        Total time: 10549.38s
                               ETA: 923860.5s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.175s)
               Value function loss: 3.0700
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 272.15
               Mean episode length: 245.14
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 8.83s
                        Total time: 10558.21s
                               ETA: 923805.9s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1785 steps/s (collection: 8.957s, learning 0.219s)
               Value function loss: 3.5627
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 274.18
               Mean episode length: 248.27
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 9.18s
                        Total time: 10567.38s
                               ETA: 923781.9s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.241s, learning 0.172s)
               Value function loss: 3.6525
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 270.75
               Mean episode length: 244.77
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 8.41s
                        Total time: 10575.80s
                               ETA: 923691.3s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.322s, learning 0.266s)
               Value function loss: 3.8394
                    Surrogate loss: -0.0028
             Mean action noise std: 0.77
                       Mean reward: 272.07
               Mean episode length: 244.93
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 8.59s
                        Total time: 10584.39s
                               ETA: 923616.1s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.577s, learning 0.170s)
               Value function loss: 3.5213
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 278.70
               Mean episode length: 248.43
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 8.75s
                        Total time: 10593.13s
                               ETA: 923554.9s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.711s, learning 0.181s)
               Value function loss: 3.0424
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 277.40
               Mean episode length: 247.23
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 8.89s
                        Total time: 10602.02s
                               ETA: 923506.4s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.719s, learning 0.194s)
               Value function loss: 2.9386
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 274.54
               Mean episode length: 247.83
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 8.91s
                        Total time: 10610.94s
                               ETA: 923459.8s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.513s, learning 0.216s)
               Value function loss: 4.2091
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 272.45
               Mean episode length: 246.50
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 8.73s
                        Total time: 10619.67s
                               ETA: 923397.3s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.355s, learning 0.216s)
               Value function loss: 3.2820
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 270.35
               Mean episode length: 247.12
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 8.57s
                        Total time: 10628.24s
                               ETA: 923321.1s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.334s, learning 0.169s)
               Value function loss: 2.7751
                    Surrogate loss: 0.0012
             Mean action noise std: 0.77
                       Mean reward: 270.67
               Mean episode length: 246.61
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 8.50s
                        Total time: 10636.74s
                               ETA: 923239.2s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.648s, learning 0.179s)
               Value function loss: 4.1421
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 276.56
               Mean episode length: 248.75
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 8.83s
                        Total time: 10645.57s
                               ETA: 923185.5s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.745s, learning 0.175s)
               Value function loss: 4.5978
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 275.07
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 8.92s
                        Total time: 10654.49s
                               ETA: 923139.9s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.456s, learning 0.216s)
               Value function loss: 5.3432
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 273.87
               Mean episode length: 248.56
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 8.67s
                        Total time: 10663.16s
                               ETA: 923073.0s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.555s, learning 0.186s)
               Value function loss: 4.8848
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 274.03
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 8.74s
                        Total time: 10671.90s
                               ETA: 923012.0s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.984s, learning 0.177s)
               Value function loss: 4.9940
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 274.91
               Mean episode length: 248.72
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 9.16s
                        Total time: 10681.06s
                               ETA: 922987.6s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.660s, learning 0.188s)
               Value function loss: 5.4497
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 274.22
               Mean episode length: 246.59
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 8.85s
                        Total time: 10689.91s
                               ETA: 922936.1s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.431s, learning 0.177s)
               Value function loss: 4.1354
                    Surrogate loss: -0.0026
             Mean action noise std: 0.77
                       Mean reward: 275.09
               Mean episode length: 248.39
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 8.61s
                        Total time: 10698.52s
                               ETA: 922863.9s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.481s, learning 0.186s)
               Value function loss: 5.3195
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 272.65
               Mean episode length: 249.19
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 8.67s
                        Total time: 10707.18s
                               ETA: 922796.9s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.513s, learning 0.174s)
               Value function loss: 5.7655
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 270.99
               Mean episode length: 249.94
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 8.69s
                        Total time: 10715.87s
                               ETA: 922731.8s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.496s, learning 0.171s)
               Value function loss: 5.8304
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 268.42
               Mean episode length: 248.03
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 8.67s
                        Total time: 10724.54s
                               ETA: 922665.0s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.523s, learning 0.180s)
               Value function loss: 6.6682
                    Surrogate loss: 0.0008
             Mean action noise std: 0.77
                       Mean reward: 274.56
               Mean episode length: 247.19
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 8.70s
                        Total time: 10733.24s
                               ETA: 922601.5s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.551s, learning 0.175s)
               Value function loss: 5.6866
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 276.97
               Mean episode length: 249.46
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 8.73s
                        Total time: 10741.97s
                               ETA: 922540.0s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.690s, learning 0.245s)
               Value function loss: 5.8829
                    Surrogate loss: -0.0040
             Mean action noise std: 0.77
                       Mean reward: 277.78
               Mean episode length: 249.45
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 8.93s
                        Total time: 10750.90s
                               ETA: 922496.5s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.500s, learning 0.176s)
               Value function loss: 5.6014
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: 277.10
               Mean episode length: 248.22
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 8.68s
                        Total time: 10759.58s
                               ETA: 922430.9s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.725s, learning 0.231s)
               Value function loss: 5.1448
                    Surrogate loss: -0.0016
             Mean action noise std: 0.77
                       Mean reward: 275.16
               Mean episode length: 248.22
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 8.96s
                        Total time: 10768.54s
                               ETA: 922389.5s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.435s, learning 0.213s)
               Value function loss: 5.7921
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 272.96
               Mean episode length: 244.64
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 8.65s
                        Total time: 10777.18s
                               ETA: 922321.7s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.691s, learning 0.225s)
               Value function loss: 4.0162
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 273.27
               Mean episode length: 248.61
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 8.92s
                        Total time: 10786.10s
                               ETA: 922276.9s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.373s, learning 0.167s)
               Value function loss: 4.4034
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 276.68
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 8.54s
                        Total time: 10794.64s
                               ETA: 922200.0s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.829s, learning 0.169s)
               Value function loss: 2.3812
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 277.09
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 9.00s
                        Total time: 10803.64s
                               ETA: 922162.3s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.384s, learning 0.181s)
               Value function loss: 2.9269
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 276.56
               Mean episode length: 249.55
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 8.57s
                        Total time: 10812.20s
                               ETA: 922087.7s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.757s, learning 0.179s)
               Value function loss: 4.4904
                    Surrogate loss: 0.0032
             Mean action noise std: 0.77
                       Mean reward: 276.86
               Mean episode length: 249.55
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 8.94s
                        Total time: 10821.14s
                               ETA: 922044.9s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.615s, learning 0.233s)
               Value function loss: 4.4716
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 274.39
               Mean episode length: 247.32
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 8.85s
                        Total time: 10829.99s
                               ETA: 921994.7s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.827s, learning 0.176s)
               Value function loss: 4.8248
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 266.79
               Mean episode length: 244.64
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 9.00s
                        Total time: 10838.99s
                               ETA: 921957.8s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.356s, learning 0.252s)
               Value function loss: 4.3078
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 266.73
               Mean episode length: 244.21
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 8.61s
                        Total time: 10847.60s
                               ETA: 921887.2s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.554s, learning 0.195s)
               Value function loss: 5.8379
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 272.69
               Mean episode length: 247.38
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 8.75s
                        Total time: 10856.35s
                               ETA: 921828.8s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.778s, learning 0.336s)
               Value function loss: 4.1933
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 270.13
               Mean episode length: 245.62
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 9.11s
                        Total time: 10865.46s
                               ETA: 921801.4s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.305s, learning 0.270s)
               Value function loss: 3.3049
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 267.83
               Mean episode length: 246.02
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 8.57s
                        Total time: 10874.03s
                               ETA: 921728.3s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.573s, learning 0.249s)
               Value function loss: 4.4471
                    Surrogate loss: 0.0027
             Mean action noise std: 0.77
                       Mean reward: 269.94
               Mean episode length: 245.06
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 8.82s
                        Total time: 10882.86s
                               ETA: 921676.3s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.592s, learning 0.194s)
               Value function loss: 4.4742
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 270.14
               Mean episode length: 244.30
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 8.79s
                        Total time: 10891.64s
                               ETA: 921621.2s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.417s, learning 0.200s)
               Value function loss: 4.1925
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 271.80
               Mean episode length: 246.35
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 8.62s
                        Total time: 10900.26s
                               ETA: 921552.1s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.491s, learning 0.167s)
               Value function loss: 3.3105
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 273.19
               Mean episode length: 247.50
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 8.66s
                        Total time: 10908.92s
                               ETA: 921486.4s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.447s, learning 0.177s)
               Value function loss: 4.8124
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 273.37
               Mean episode length: 248.69
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 8.62s
                        Total time: 10917.54s
                               ETA: 921418.0s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.634s, learning 0.185s)
               Value function loss: 5.5904
                    Surrogate loss: -0.0028
             Mean action noise std: 0.77
                       Mean reward: 269.20
               Mean episode length: 244.49
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 8.82s
                        Total time: 10926.36s
                               ETA: 921366.2s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.469s, learning 0.174s)
               Value function loss: 6.5547
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 277.41
               Mean episode length: 248.85
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 8.64s
                        Total time: 10935.00s
                               ETA: 921299.6s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.548s, learning 0.248s)
               Value function loss: 5.7329
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 277.90
               Mean episode length: 247.80
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 8.80s
                        Total time: 10943.80s
                               ETA: 921246.0s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.414s, learning 0.231s)
               Value function loss: 5.7265
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 278.57
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 8.65s
                        Total time: 10952.44s
                               ETA: 921179.8s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.533s, learning 0.350s)
               Value function loss: 7.8724
                    Surrogate loss: -0.0006
             Mean action noise std: 0.77
                       Mean reward: 276.64
               Mean episode length: 247.46
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 8.88s
                        Total time: 10961.33s
                               ETA: 921133.7s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.522s, learning 0.252s)
               Value function loss: 6.7919
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 276.82
               Mean episode length: 248.32
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 8.77s
                        Total time: 10970.10s
                               ETA: 921078.4s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.523s, learning 0.203s)
               Value function loss: 5.7200
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 275.23
               Mean episode length: 248.76
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 8.73s
                        Total time: 10978.83s
                               ETA: 921019.2s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.811s, learning 0.176s)
               Value function loss: 7.6035
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 279.44
               Mean episode length: 248.56
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 8.99s
                        Total time: 10987.81s
                               ETA: 920982.0s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.484s, learning 0.189s)
               Value function loss: 6.9374
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 274.44
               Mean episode length: 248.38
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 8.67s
                        Total time: 10996.49s
                               ETA: 920918.6s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.222s, learning 0.244s)
               Value function loss: 6.9930
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 274.26
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 8.47s
                        Total time: 11004.96s
                               ETA: 920838.0s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.669s, learning 0.240s)
               Value function loss: 7.2831
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 278.69
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 8.91s
                        Total time: 11013.86s
                               ETA: 920794.5s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.356s, learning 0.166s)
               Value function loss: 6.3966
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 276.23
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 8.52s
                        Total time: 11022.39s
                               ETA: 920718.7s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.563s, learning 0.196s)
               Value function loss: 6.1987
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 274.15
               Mean episode length: 248.19
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 8.76s
                        Total time: 11031.15s
                               ETA: 920662.8s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.776s, learning 0.173s)
               Value function loss: 5.0239
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 275.09
               Mean episode length: 249.18
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 8.95s
                        Total time: 11040.09s
                               ETA: 920622.7s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.570s, learning 0.174s)
               Value function loss: 7.3409
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 279.72
               Mean episode length: 249.46
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 8.74s
                        Total time: 11048.84s
                               ETA: 920565.7s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.629s, learning 0.168s)
               Value function loss: 5.3284
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 278.35
               Mean episode length: 249.49
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 8.80s
                        Total time: 11057.64s
                               ETA: 920513.2s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.605s, learning 0.208s)
               Value function loss: 7.0045
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 278.40
               Mean episode length: 249.87
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 8.81s
                        Total time: 11066.45s
                               ETA: 920462.1s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.899s, learning 0.174s)
               Value function loss: 2.9014
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 276.29
               Mean episode length: 247.89
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 9.07s
                        Total time: 11075.52s
                               ETA: 920432.6s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.588s, learning 0.178s)
               Value function loss: 4.4620
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 273.00
               Mean episode length: 246.06
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 8.77s
                        Total time: 11084.29s
                               ETA: 920377.8s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.700s, learning 0.270s)
               Value function loss: 4.5758
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 275.04
               Mean episode length: 246.87
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 8.97s
                        Total time: 11093.26s
                               ETA: 920339.8s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.300s, learning 0.177s)
               Value function loss: 4.3565
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 274.81
               Mean episode length: 246.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 8.48s
                        Total time: 11101.73s
                               ETA: 920261.1s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.609s, learning 0.177s)
               Value function loss: 4.6504
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 273.31
               Mean episode length: 247.72
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 8.79s
                        Total time: 11110.52s
                               ETA: 920208.1s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.522s, learning 0.183s)
               Value function loss: 4.9164
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 277.49
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 8.71s
                        Total time: 11119.23s
                               ETA: 920148.5s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.289s, learning 0.182s)
               Value function loss: 5.7917
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 279.87
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 8.47s
                        Total time: 11127.70s
                               ETA: 920069.6s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.600s, learning 0.182s)
               Value function loss: 3.4864
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 276.83
               Mean episode length: 248.35
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 8.78s
                        Total time: 11136.48s
                               ETA: 920016.6s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.435s, learning 0.174s)
               Value function loss: 3.9346
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: 277.13
               Mean episode length: 248.35
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 8.61s
                        Total time: 11145.09s
                               ETA: 919949.3s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.722s, learning 0.228s)
               Value function loss: 4.5767
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 281.91
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 8.95s
                        Total time: 11154.04s
                               ETA: 919910.2s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.536s, learning 0.173s)
               Value function loss: 5.7161
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 279.85
               Mean episode length: 248.25
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 8.71s
                        Total time: 11162.75s
                               ETA: 919851.3s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.572s, learning 0.272s)
               Value function loss: 5.3511
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 280.58
               Mean episode length: 248.25
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 8.84s
                        Total time: 11171.59s
                               ETA: 919803.7s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.596s, learning 0.181s)
               Value function loss: 4.4101
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 284.62
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 8.78s
                        Total time: 11180.37s
                               ETA: 919750.6s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.323s, learning 0.176s)
               Value function loss: 5.5369
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 280.26
               Mean episode length: 248.48
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 8.50s
                        Total time: 11188.87s
                               ETA: 919674.7s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.282s, learning 0.179s)
               Value function loss: 6.5767
                    Surrogate loss: -0.0055
             Mean action noise std: 0.77
                       Mean reward: 277.82
               Mean episode length: 248.70
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 8.46s
                        Total time: 11197.33s
                               ETA: 919595.8s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.450s, learning 0.184s)
               Value function loss: 7.5949
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 278.03
               Mean episode length: 247.69
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 8.63s
                        Total time: 11205.96s
                               ETA: 919531.2s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.055s, learning 0.283s)
               Value function loss: 6.6980
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 286.65
               Mean episode length: 249.59
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 9.34s
                        Total time: 11215.30s
                               ETA: 919524.4s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.433s, learning 0.313s)
               Value function loss: 5.8831
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 285.04
               Mean episode length: 249.82
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 8.75s
                        Total time: 11224.05s
                               ETA: 919469.1s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.469s, learning 0.220s)
               Value function loss: 7.5230
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 282.56
               Mean episode length: 248.60
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 8.69s
                        Total time: 11232.74s
                               ETA: 919409.3s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.497s, learning 0.299s)
               Value function loss: 6.9757
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 283.64
               Mean episode length: 248.60
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 8.80s
                        Total time: 11241.53s
                               ETA: 919358.3s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.582s, learning 0.175s)
               Value function loss: 6.3542
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 282.31
               Mean episode length: 248.41
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 8.76s
                        Total time: 11250.29s
                               ETA: 919304.1s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.668s, learning 0.211s)
               Value function loss: 7.0116
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: 280.55
               Mean episode length: 247.90
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 8.88s
                        Total time: 11259.17s
                               ETA: 919260.1s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.440s, learning 0.169s)
               Value function loss: 8.4803
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 285.58
               Mean episode length: 247.77
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 8.61s
                        Total time: 11267.78s
                               ETA: 919194.0s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.451s, learning 0.179s)
               Value function loss: 8.0043
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 281.47
               Mean episode length: 248.28
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 8.63s
                        Total time: 11276.41s
                               ETA: 919129.7s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.276s, learning 0.168s)
               Value function loss: 7.4970
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 281.56
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 8.44s
                        Total time: 11284.85s
                               ETA: 919050.4s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.476s, learning 0.196s)
               Value function loss: 6.7736
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 279.71
               Mean episode length: 247.71
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 8.67s
                        Total time: 11293.53s
                               ETA: 918989.7s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.469s, learning 0.269s)
               Value function loss: 7.2380
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 285.89
               Mean episode length: 249.98
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 8.74s
                        Total time: 11302.26s
                               ETA: 918934.5s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.505s, learning 0.169s)
               Value function loss: 5.6137
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 283.46
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 8.67s
                        Total time: 11310.94s
                               ETA: 918874.1s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.840s, learning 0.231s)
               Value function loss: 6.9513
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 280.60
               Mean episode length: 246.74
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 9.07s
                        Total time: 11320.01s
                               ETA: 918846.1s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.657s, learning 0.176s)
               Value function loss: 4.8008
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 282.27
               Mean episode length: 248.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 8.83s
                        Total time: 11328.84s
                               ETA: 918798.8s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.687s, learning 0.173s)
               Value function loss: 8.4940
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 283.11
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 8.86s
                        Total time: 11337.70s
                               ETA: 918753.7s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.470s, learning 0.272s)
               Value function loss: 3.4183
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 283.33
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 8.74s
                        Total time: 11346.44s
                               ETA: 918699.2s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.664s, learning 0.267s)
               Value function loss: 3.7465
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 283.23
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 8.93s
                        Total time: 11355.37s
                               ETA: 918660.0s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.550s, learning 0.268s)
               Value function loss: 3.6309
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 283.47
               Mean episode length: 249.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 8.82s
                        Total time: 11364.19s
                               ETA: 918611.8s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.427s, learning 0.232s)
               Value function loss: 4.4459
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 281.97
               Mean episode length: 249.10
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 8.66s
                        Total time: 11372.85s
                               ETA: 918550.7s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.733s, learning 0.204s)
               Value function loss: 5.5115
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 285.97
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 8.94s
                        Total time: 11381.79s
                               ETA: 918512.2s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.543s, learning 0.208s)
               Value function loss: 4.5087
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 285.28
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 8.75s
                        Total time: 11390.54s
                               ETA: 918458.8s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.666s, learning 0.166s)
               Value function loss: 5.7407
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 283.85
               Mean episode length: 248.08
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 8.83s
                        Total time: 11399.37s
                               ETA: 918412.0s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.955s, learning 0.178s)
               Value function loss: 4.4318
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 281.70
               Mean episode length: 245.12
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 9.13s
                        Total time: 11408.51s
                               ETA: 918389.3s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.399s, learning 0.167s)
               Value function loss: 3.5793
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 281.56
               Mean episode length: 246.31
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 8.57s
                        Total time: 11417.07s
                               ETA: 918321.1s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.430s, learning 0.222s)
               Value function loss: 4.0086
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 275.31
               Mean episode length: 243.88
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 8.65s
                        Total time: 11425.72s
                               ETA: 918260.0s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.475s, learning 0.169s)
               Value function loss: 4.5024
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 276.99
               Mean episode length: 244.39
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 8.64s
                        Total time: 11434.37s
                               ETA: 918198.3s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.752s, learning 0.201s)
               Value function loss: 4.3898
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 279.68
               Mean episode length: 248.84
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 8.95s
                        Total time: 11443.32s
                               ETA: 918161.5s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.673s, learning 0.174s)
               Value function loss: 4.2662
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 281.28
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 8.85s
                        Total time: 11452.17s
                               ETA: 918116.2s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.581s, learning 0.205s)
               Value function loss: 4.7892
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 282.03
               Mean episode length: 248.41
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 8.79s
                        Total time: 11460.95s
                               ETA: 918066.1s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.761s, learning 0.174s)
               Value function loss: 5.2689
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 283.05
               Mean episode length: 249.68
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 8.94s
                        Total time: 11469.89s
                               ETA: 918028.0s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.660s, learning 0.183s)
               Value function loss: 5.8241
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 285.59
               Mean episode length: 249.73
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 8.84s
                        Total time: 11478.73s
                               ETA: 917982.5s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.732s, learning 0.181s)
               Value function loss: 6.9090
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 284.68
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 8.91s
                        Total time: 11487.64s
                               ETA: 917942.6s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.607s, learning 0.191s)
               Value function loss: 4.9534
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 281.79
               Mean episode length: 249.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 8.80s
                        Total time: 11496.44s
                               ETA: 917893.7s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.639s, learning 0.169s)
               Value function loss: 7.1841
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 285.38
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 8.81s
                        Total time: 11505.25s
                               ETA: 917845.7s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.593s, learning 0.172s)
               Value function loss: 6.9245
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 286.26
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 8.76s
                        Total time: 11514.01s
                               ETA: 917794.2s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.773s, learning 0.234s)
               Value function loss: 5.4443
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 284.75
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 9.01s
                        Total time: 11523.02s
                               ETA: 917762.2s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.741s, learning 0.194s)
               Value function loss: 6.1480
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 284.87
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 8.94s
                        Total time: 11531.96s
                               ETA: 917724.4s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.626s, learning 0.165s)
               Value function loss: 7.6042
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 285.44
               Mean episode length: 248.53
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 8.79s
                        Total time: 11540.75s
                               ETA: 917675.3s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.844s, learning 0.165s)
               Value function loss: 6.2389
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 284.67
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 9.01s
                        Total time: 11549.76s
                               ETA: 917643.5s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.701s, learning 0.170s)
               Value function loss: 6.2371
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 279.14
               Mean episode length: 249.77
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 8.87s
                        Total time: 11558.63s
                               ETA: 917600.7s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.525s, learning 0.167s)
               Value function loss: 6.1470
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 279.80
               Mean episode length: 248.35
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 8.69s
                        Total time: 11567.32s
                               ETA: 917543.9s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.817s, learning 0.199s)
               Value function loss: 5.2795
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 277.79
               Mean episode length: 247.63
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 9.02s
                        Total time: 11576.33s
                               ETA: 917512.8s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.613s, learning 0.173s)
               Value function loss: 6.2021
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 276.19
               Mean episode length: 246.26
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 8.79s
                        Total time: 11585.12s
                               ETA: 917463.5s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.655s, learning 0.273s)
               Value function loss: 6.4483
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 267.05
               Mean episode length: 237.72
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 8.93s
                        Total time: 11594.05s
                               ETA: 917425.5s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.284s, learning 0.209s)
               Value function loss: 5.5213
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 280.13
               Mean episode length: 248.59
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 8.49s
                        Total time: 11602.54s
                               ETA: 917353.1s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.675s, learning 0.197s)
               Value function loss: 9.0845
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 280.85
               Mean episode length: 246.44
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 8.87s
                        Total time: 11611.41s
                               ETA: 917310.9s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.535s, learning 0.202s)
               Value function loss: 3.4072
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 275.74
               Mean episode length: 242.50
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 8.74s
                        Total time: 11620.15s
                               ETA: 917258.0s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.506s, learning 0.174s)
               Value function loss: 3.3501
                    Surrogate loss: 0.0040
             Mean action noise std: 0.77
                       Mean reward: 277.17
               Mean episode length: 244.07
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 8.68s
                        Total time: 11628.83s
                               ETA: 917200.7s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.607s, learning 0.180s)
               Value function loss: 2.8765
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 281.43
               Mean episode length: 247.93
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 8.79s
                        Total time: 11637.62s
                               ETA: 917152.0s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.576s, learning 0.180s)
               Value function loss: 3.8767
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 285.66
               Mean episode length: 249.88
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 8.76s
                        Total time: 11646.37s
                               ETA: 917100.8s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.477s, learning 0.176s)
               Value function loss: 4.7237
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 278.85
               Mean episode length: 247.69
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 8.65s
                        Total time: 11655.03s
                               ETA: 917041.6s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.633s, learning 0.255s)
               Value function loss: 4.5711
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 278.91
               Mean episode length: 246.58
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 8.89s
                        Total time: 11663.91s
                               ETA: 917001.0s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.548s, learning 0.183s)
               Value function loss: 4.7309
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 280.04
               Mean episode length: 246.10
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 8.73s
                        Total time: 11672.65s
                               ETA: 916948.1s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.494s, learning 0.200s)
               Value function loss: 4.3693
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 277.06
               Mean episode length: 245.16
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 8.69s
                        Total time: 11681.34s
                               ETA: 916892.4s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.825s, learning 0.181s)
               Value function loss: 4.0288
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 273.69
               Mean episode length: 244.20
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 9.01s
                        Total time: 11690.35s
                               ETA: 916861.1s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.837s, learning 0.169s)
               Value function loss: 4.1914
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 267.45
               Mean episode length: 239.86
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 9.01s
                        Total time: 11699.35s
                               ETA: 916830.0s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.735s, learning 0.242s)
               Value function loss: 3.7092
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 268.52
               Mean episode length: 241.63
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 8.98s
                        Total time: 11708.33s
                               ETA: 916796.5s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.434s, learning 0.246s)
               Value function loss: 5.1634
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 280.84
               Mean episode length: 249.79
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 8.68s
                        Total time: 11717.01s
                               ETA: 916740.0s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.577s, learning 0.172s)
               Value function loss: 4.0359
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 281.19
               Mean episode length: 249.84
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 8.75s
                        Total time: 11725.76s
                               ETA: 916688.8s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.351s, learning 0.187s)
               Value function loss: 3.9052
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 274.01
               Mean episode length: 245.29
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 8.54s
                        Total time: 11734.30s
                               ETA: 916621.2s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.588s, learning 0.217s)
               Value function loss: 4.9347
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 277.05
               Mean episode length: 247.19
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 8.81s
                        Total time: 11743.10s
                               ETA: 916574.6s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.775s, learning 0.184s)
               Value function loss: 4.8652
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 275.69
               Mean episode length: 249.77
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 8.96s
                        Total time: 11752.06s
                               ETA: 916540.1s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.545s, learning 0.174s)
               Value function loss: 5.9822
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 276.47
               Mean episode length: 247.84
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 8.72s
                        Total time: 11760.78s
                               ETA: 916486.9s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.795s, learning 0.176s)
               Value function loss: 5.9547
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 276.59
               Mean episode length: 246.39
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 8.97s
                        Total time: 11769.75s
                               ETA: 916453.3s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.821s, learning 0.200s)
               Value function loss: 6.2605
                    Surrogate loss: 0.0000
             Mean action noise std: 0.77
                       Mean reward: 275.77
               Mean episode length: 247.29
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 9.02s
                        Total time: 11778.77s
                               ETA: 916423.7s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.585s, learning 0.177s)
               Value function loss: 7.5436
                    Surrogate loss: 0.0030
             Mean action noise std: 0.77
                       Mean reward: 278.78
               Mean episode length: 249.02
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 8.76s
                        Total time: 11787.53s
                               ETA: 916374.0s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.388s, learning 0.175s)
               Value function loss: 5.7928
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 280.16
               Mean episode length: 248.10
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 8.56s
                        Total time: 11796.10s
                               ETA: 916308.9s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.752s, learning 0.169s)
               Value function loss: 5.9096
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 280.13
               Mean episode length: 247.69
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 8.92s
                        Total time: 11805.02s
                               ETA: 916271.6s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.657s, learning 0.167s)
               Value function loss: 7.8011
                    Surrogate loss: 0.0032
             Mean action noise std: 0.77
                       Mean reward: 277.06
               Mean episode length: 249.78
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 8.82s
                        Total time: 11813.84s
                               ETA: 916226.9s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.359s, learning 0.197s)
               Value function loss: 6.7023
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 279.07
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 8.56s
                        Total time: 11822.40s
                               ETA: 916161.6s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.487s, learning 0.171s)
               Value function loss: 7.6308
                    Surrogate loss: 0.0062
             Mean action noise std: 0.77
                       Mean reward: 273.83
               Mean episode length: 246.48
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 8.66s
                        Total time: 11831.06s
                               ETA: 916104.2s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.606s, learning 0.268s)
               Value function loss: 6.5635
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 277.16
               Mean episode length: 249.44
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 8.87s
                        Total time: 11839.93s
                               ETA: 916063.6s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.594s, learning 0.181s)
               Value function loss: 7.0634
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 273.06
               Mean episode length: 247.84
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 8.77s
                        Total time: 11848.71s
                               ETA: 916015.3s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.806s, learning 0.182s)
               Value function loss: 6.0701
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 274.77
               Mean episode length: 248.43
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 8.99s
                        Total time: 11857.69s
                               ETA: 915983.7s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.753s, learning 0.206s)
               Value function loss: 5.6054
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 279.69
               Mean episode length: 248.43
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 8.96s
                        Total time: 11866.65s
                               ETA: 915949.7s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.437s, learning 0.218s)
               Value function loss: 6.4427
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 271.67
               Mean episode length: 246.94
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 8.66s
                        Total time: 11875.31s
                               ETA: 915892.4s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.740s, learning 0.187s)
               Value function loss: 5.0388
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 264.35
               Mean episode length: 243.39
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 8.93s
                        Total time: 11884.24s
                               ETA: 915856.1s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.820s, learning 0.273s)
               Value function loss: 4.7309
                    Surrogate loss: 0.0024
             Mean action noise std: 0.77
                       Mean reward: 270.87
               Mean episode length: 247.97
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 9.09s
                        Total time: 11893.33s
                               ETA: 915832.7s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1782 steps/s (collection: 8.944s, learning 0.248s)
               Value function loss: 2.6804
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 261.20
               Mean episode length: 242.86
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 9.19s
                        Total time: 11902.52s
                               ETA: 915816.8s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.750s, learning 0.180s)
               Value function loss: 3.9695
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 257.02
               Mean episode length: 238.39
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 8.93s
                        Total time: 11911.45s
                               ETA: 915780.9s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.537s, learning 0.185s)
               Value function loss: 4.9903
                    Surrogate loss: -0.0001
             Mean action noise std: 0.77
                       Mean reward: 263.76
               Mean episode length: 241.70
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 8.72s
                        Total time: 11920.17s
                               ETA: 915728.9s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.683s, learning 0.211s)
               Value function loss: 4.7446
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: 266.53
               Mean episode length: 245.25
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 8.89s
                        Total time: 11929.07s
                               ETA: 915690.3s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.564s, learning 0.241s)
               Value function loss: 4.5648
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 273.62
               Mean episode length: 248.57
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 8.81s
                        Total time: 11937.87s
                               ETA: 915644.9s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.675s, learning 0.225s)
               Value function loss: 4.3300
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 270.39
               Mean episode length: 246.72
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 8.90s
                        Total time: 11946.77s
                               ETA: 915606.8s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.163s, learning 0.197s)
               Value function loss: 5.7826
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 261.14
               Mean episode length: 241.35
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 8.36s
                        Total time: 11955.13s
                               ETA: 915527.3s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.553s, learning 0.228s)
               Value function loss: 4.2309
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 259.86
               Mean episode length: 239.48
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 8.78s
                        Total time: 11963.91s
                               ETA: 915480.3s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.536s, learning 0.207s)
               Value function loss: 3.8449
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 264.09
               Mean episode length: 241.98
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 8.74s
                        Total time: 11972.65s
                               ETA: 915430.4s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.337s, learning 0.200s)
               Value function loss: 5.0097
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: 275.86
               Mean episode length: 249.52
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 8.54s
                        Total time: 11981.19s
                               ETA: 915364.8s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.458s, learning 0.210s)
               Value function loss: 4.6633
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 273.52
               Mean episode length: 249.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 8.67s
                        Total time: 11989.86s
                               ETA: 915309.4s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.689s, learning 0.207s)
               Value function loss: 4.8323
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 267.38
               Mean episode length: 247.55
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 8.90s
                        Total time: 11998.75s
                               ETA: 915271.3s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.879s, learning 0.189s)
               Value function loss: 3.7694
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 265.27
               Mean episode length: 246.65
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 9.07s
                        Total time: 12007.82s
                               ETA: 915246.5s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.788s, learning 0.203s)
               Value function loss: 4.7946
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 264.17
               Mean episode length: 242.64
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 8.99s
                        Total time: 12016.81s
                               ETA: 915215.8s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.320s, learning 0.255s)
               Value function loss: 5.3941
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 263.28
               Mean episode length: 243.54
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 8.57s
                        Total time: 12025.39s
                               ETA: 915153.4s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.761s, learning 0.173s)
               Value function loss: 6.1692
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 266.79
               Mean episode length: 247.28
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 8.93s
                        Total time: 12034.32s
                               ETA: 915118.5s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.736s, learning 0.195s)
               Value function loss: 5.2212
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: 263.13
               Mean episode length: 244.89
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 8.93s
                        Total time: 12043.25s
                               ETA: 915083.4s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.291s, learning 0.168s)
               Value function loss: 5.7642
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 260.07
               Mean episode length: 240.86
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 8.46s
                        Total time: 12051.71s
                               ETA: 915012.5s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.474s, learning 0.196s)
               Value function loss: 5.9009
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 271.80
               Mean episode length: 246.89
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 8.67s
                        Total time: 12060.38s
                               ETA: 914957.7s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.492s, learning 0.195s)
               Value function loss: 5.5482
                    Surrogate loss: -0.0001
             Mean action noise std: 0.77
                       Mean reward: 271.79
               Mean episode length: 246.99
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 8.69s
                        Total time: 12069.07s
                               ETA: 914904.2s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.470s, learning 0.176s)
               Value function loss: 5.0091
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 270.88
               Mean episode length: 247.10
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 8.65s
                        Total time: 12077.72s
                               ETA: 914847.7s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.662s, learning 0.186s)
               Value function loss: 5.5669
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 270.88
               Mean episode length: 248.56
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 8.85s
                        Total time: 12086.57s
                               ETA: 914806.5s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.784s, learning 0.231s)
               Value function loss: 6.0732
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 269.56
               Mean episode length: 244.79
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 9.02s
                        Total time: 12095.58s
                               ETA: 914778.1s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.553s, learning 0.240s)
               Value function loss: 6.5866
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 271.95
               Mean episode length: 248.04
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 8.79s
                        Total time: 12104.37s
                               ETA: 914732.9s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.720s, learning 0.224s)
               Value function loss: 6.3936
                    Surrogate loss: 0.0012
             Mean action noise std: 0.77
                       Mean reward: 266.54
               Mean episode length: 246.95
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 8.94s
                        Total time: 12113.32s
                               ETA: 914699.2s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.690s, learning 0.179s)
               Value function loss: 5.6580
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 269.06
               Mean episode length: 247.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 8.87s
                        Total time: 12122.19s
                               ETA: 914659.8s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.834s, learning 0.170s)
               Value function loss: 6.6715
                    Surrogate loss: -0.0055
             Mean action noise std: 0.77
                       Mean reward: 272.41
               Mean episode length: 246.64
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 9.00s
                        Total time: 12131.19s
                               ETA: 914630.6s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.448s, learning 0.178s)
               Value function loss: 4.8185
                    Surrogate loss: 0.0062
             Mean action noise std: 0.77
                       Mean reward: 266.95
               Mean episode length: 244.84
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 8.63s
                        Total time: 12139.82s
                               ETA: 914572.9s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.509s, learning 0.198s)
               Value function loss: 6.6950
                    Surrogate loss: -0.0012
             Mean action noise std: 0.77
                       Mean reward: 271.66
               Mean episode length: 248.46
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 8.71s
                        Total time: 12148.52s
                               ETA: 914521.5s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.712s, learning 0.205s)
               Value function loss: 4.9621
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 266.89
               Mean episode length: 244.25
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 8.92s
                        Total time: 12157.44s
                               ETA: 914486.0s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.661s, learning 0.181s)
               Value function loss: 6.4584
                    Surrogate loss: -0.0040
             Mean action noise std: 0.77
                       Mean reward: 278.25
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 8.84s
                        Total time: 12166.28s
                               ETA: 914444.8s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.557s, learning 0.211s)
               Value function loss: 2.6910
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 277.68
               Mean episode length: 249.74
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 8.77s
                        Total time: 12175.05s
                               ETA: 914398.1s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.006s, learning 0.188s)
               Value function loss: 3.5500
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 270.67
               Mean episode length: 247.60
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 8.19s
                        Total time: 12183.24s
                               ETA: 914308.4s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.474s, learning 0.200s)
               Value function loss: 4.5004
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 267.96
               Mean episode length: 244.36
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 8.67s
                        Total time: 12191.92s
                               ETA: 914254.8s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.211s, learning 0.167s)
               Value function loss: 3.8292
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 272.35
               Mean episode length: 244.65
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 8.38s
                        Total time: 12200.29s
                               ETA: 914179.1s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.490s, learning 0.201s)
               Value function loss: 4.3124
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 274.82
               Mean episode length: 246.47
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 8.69s
                        Total time: 12208.98s
                               ETA: 914126.9s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.685s, learning 0.167s)
               Value function loss: 4.6998
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 272.66
               Mean episode length: 244.60
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 8.85s
                        Total time: 12217.84s
                               ETA: 914086.9s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.832s, learning 0.182s)
               Value function loss: 5.9812
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 264.67
               Mean episode length: 237.85
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 9.01s
                        Total time: 12226.85s
                               ETA: 914059.0s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.101s, learning 0.183s)
               Value function loss: 3.5882
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 267.11
               Mean episode length: 240.88
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 8.28s
                        Total time: 12235.13s
                               ETA: 913976.6s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.751s, learning 0.203s)
               Value function loss: 3.4765
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 271.07
               Mean episode length: 246.12
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 8.95s
                        Total time: 12244.09s
                               ETA: 913944.4s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.666s, learning 0.169s)
               Value function loss: 4.3030
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 270.20
               Mean episode length: 245.10
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 8.83s
                        Total time: 12252.92s
                               ETA: 913903.2s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.649s, learning 0.178s)
               Value function loss: 4.4958
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 268.97
               Mean episode length: 244.08
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 8.83s
                        Total time: 12261.75s
                               ETA: 913861.6s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.551s, learning 0.178s)
               Value function loss: 4.4192
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 267.14
               Mean episode length: 247.84
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 8.73s
                        Total time: 12270.48s
                               ETA: 913812.7s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.362s, learning 0.195s)
               Value function loss: 3.7001
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: 269.56
               Mean episode length: 248.21
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 8.56s
                        Total time: 12279.04s
                               ETA: 913751.0s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.176s)
               Value function loss: 4.4887
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 270.61
               Mean episode length: 246.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 8.83s
                        Total time: 12287.86s
                               ETA: 913709.5s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.174s)
               Value function loss: 5.1233
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 274.67
               Mean episode length: 247.77
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 8.83s
                        Total time: 12296.69s
                               ETA: 913668.0s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.612s, learning 0.205s)
               Value function loss: 5.6717
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 269.31
               Mean episode length: 245.48
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 8.82s
                        Total time: 12305.51s
                               ETA: 913625.9s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.535s, learning 0.204s)
               Value function loss: 5.3281
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 267.27
               Mean episode length: 246.80
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 8.74s
                        Total time: 12314.24s
                               ETA: 913578.0s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.647s, learning 0.179s)
               Value function loss: 4.6213
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 270.28
               Mean episode length: 247.02
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 8.83s
                        Total time: 12323.07s
                               ETA: 913536.6s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.223s, learning 0.206s)
               Value function loss: 5.3298
                    Surrogate loss: 0.0064
             Mean action noise std: 0.77
                       Mean reward: 265.52
               Mean episode length: 245.29
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 8.43s
                        Total time: 12331.50s
                               ETA: 913466.0s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.341s, learning 0.198s)
               Value function loss: 4.8743
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 271.23
               Mean episode length: 247.94
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 8.54s
                        Total time: 12340.04s
                               ETA: 913403.5s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.644s, learning 0.220s)
               Value function loss: 4.6359
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 268.68
               Mean episode length: 244.93
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 8.86s
                        Total time: 12348.90s
                               ETA: 913365.1s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.509s, learning 0.192s)
               Value function loss: 4.8371
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 264.60
               Mean episode length: 243.71
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 8.70s
                        Total time: 12357.60s
                               ETA: 913314.7s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.668s, learning 0.201s)
               Value function loss: 5.9710
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 273.28
               Mean episode length: 246.15
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 8.87s
                        Total time: 12366.47s
                               ETA: 913276.8s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.567s, learning 0.206s)
               Value function loss: 7.0744
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 271.61
               Mean episode length: 245.72
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 8.77s
                        Total time: 12375.24s
                               ETA: 913231.9s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.203s, learning 0.167s)
               Value function loss: 6.2352
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: 273.46
               Mean episode length: 247.84
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 8.37s
                        Total time: 12383.61s
                               ETA: 913157.3s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.697s, learning 0.206s)
               Value function loss: 5.4798
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 272.37
               Mean episode length: 248.62
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 8.90s
                        Total time: 12392.52s
                               ETA: 913122.0s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.492s, learning 0.225s)
               Value function loss: 5.6265
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 271.91
               Mean episode length: 247.79
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 8.72s
                        Total time: 12401.23s
                               ETA: 913073.1s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.132s, learning 0.196s)
               Value function loss: 4.9999
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 268.43
               Mean episode length: 246.48
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 8.33s
                        Total time: 12409.56s
                               ETA: 912995.7s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.509s, learning 0.180s)
               Value function loss: 5.4127
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 266.42
               Mean episode length: 246.99
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 8.69s
                        Total time: 12418.25s
                               ETA: 912944.9s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.638s, learning 0.189s)
               Value function loss: 5.2700
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 267.42
               Mean episode length: 243.06
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 8.83s
                        Total time: 12427.08s
                               ETA: 912904.4s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.513s, learning 0.179s)
               Value function loss: 7.6319
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 275.31
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 8.69s
                        Total time: 12435.77s
                               ETA: 912854.0s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.840s, learning 0.171s)
               Value function loss: 3.5016
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 272.30
               Mean episode length: 246.31
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 9.01s
                        Total time: 12444.78s
                               ETA: 912827.0s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.594s, learning 0.192s)
               Value function loss: 4.3600
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 261.85
               Mean episode length: 236.82
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 8.79s
                        Total time: 12453.57s
                               ETA: 912783.6s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.673s, learning 0.176s)
               Value function loss: 2.9869
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 260.01
               Mean episode length: 235.40
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 8.85s
                        Total time: 12462.42s
                               ETA: 912744.8s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.412s, learning 0.176s)
               Value function loss: 3.7556
                    Surrogate loss: -0.0055
             Mean action noise std: 0.77
                       Mean reward: 270.56
               Mean episode length: 247.08
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 8.59s
                        Total time: 12471.00s
                               ETA: 912686.9s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.153s, learning 0.174s)
               Value function loss: 4.3542
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 267.01
               Mean episode length: 246.23
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 8.33s
                        Total time: 12479.33s
                               ETA: 912610.0s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.800s, learning 0.176s)
               Value function loss: 4.3776
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 268.20
               Mean episode length: 246.80
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 8.98s
                        Total time: 12488.31s
                               ETA: 912580.7s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.696s, learning 0.186s)
               Value function loss: 4.8445
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 271.41
               Mean episode length: 247.74
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 8.88s
                        Total time: 12497.19s
                               ETA: 912544.5s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.225s, learning 0.190s)
               Value function loss: 4.1301
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 265.81
               Mean episode length: 244.03
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 8.42s
                        Total time: 12505.60s
                               ETA: 912474.3s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.681s, learning 0.231s)
               Value function loss: 3.2852
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 263.48
               Mean episode length: 240.33
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 8.91s
                        Total time: 12514.51s
                               ETA: 912440.4s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.619s, learning 0.231s)
               Value function loss: 3.8478
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 263.33
               Mean episode length: 239.14
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 8.85s
                        Total time: 12523.36s
                               ETA: 912402.0s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.514s, learning 0.171s)
               Value function loss: 3.2344
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 262.55
               Mean episode length: 242.61
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 8.68s
                        Total time: 12532.05s
                               ETA: 912351.6s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.511s, learning 0.204s)
               Value function loss: 3.5904
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 263.44
               Mean episode length: 245.10
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 8.71s
                        Total time: 12540.76s
                               ETA: 912303.5s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.282s, learning 0.171s)
               Value function loss: 4.1770
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 268.10
               Mean episode length: 245.97
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 8.45s
                        Total time: 12549.22s
                               ETA: 912236.5s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.729s, learning 0.213s)
               Value function loss: 3.7293
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 266.77
               Mean episode length: 242.91
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 8.94s
                        Total time: 12558.16s
                               ETA: 912205.0s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.462s, learning 0.170s)
               Value function loss: 5.0081
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 267.15
               Mean episode length: 242.85
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 8.63s
                        Total time: 12566.79s
                               ETA: 912151.2s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.747s, learning 0.191s)
               Value function loss: 5.3403
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 263.39
               Mean episode length: 241.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 8.94s
                        Total time: 12575.73s
                               ETA: 912119.5s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.151s, learning 0.172s)
               Value function loss: 5.7995
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 268.70
               Mean episode length: 246.82
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 8.32s
                        Total time: 12584.05s
                               ETA: 912043.3s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.103s, learning 0.166s)
               Value function loss: 4.2299
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 268.36
               Mean episode length: 245.28
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 8.27s
                        Total time: 12592.32s
                               ETA: 911963.2s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.784s, learning 0.175s)
               Value function loss: 6.6104
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 271.07
               Mean episode length: 245.96
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 8.96s
                        Total time: 12601.28s
                               ETA: 911933.3s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.495s, learning 0.177s)
               Value function loss: 5.5374
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 274.15
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 8.67s
                        Total time: 12609.95s
                               ETA: 911882.5s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.746s, learning 0.180s)
               Value function loss: 4.9526
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 272.26
               Mean episode length: 246.67
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 8.93s
                        Total time: 12618.88s
                               ETA: 911850.2s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.366s, learning 0.178s)
               Value function loss: 5.3611
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 271.96
               Mean episode length: 244.66
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 8.54s
                        Total time: 12627.42s
                               ETA: 911790.4s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.338s, learning 0.181s)
               Value function loss: 7.1663
                    Surrogate loss: 0.0023
             Mean action noise std: 0.77
                       Mean reward: 275.04
               Mean episode length: 248.73
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 8.52s
                        Total time: 12635.94s
                               ETA: 911728.8s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.487s, learning 0.216s)
               Value function loss: 6.0664
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 271.25
               Mean episode length: 246.23
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 8.70s
                        Total time: 12644.64s
                               ETA: 911680.6s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.690s, learning 0.189s)
               Value function loss: 6.2786
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 275.33
               Mean episode length: 248.23
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 8.88s
                        Total time: 12653.52s
                               ETA: 911645.1s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.921s, learning 0.171s)
               Value function loss: 5.7310
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 268.11
               Mean episode length: 246.72
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 9.09s
                        Total time: 12662.61s
                               ETA: 911625.0s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.472s, learning 0.177s)
               Value function loss: 5.5503
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 269.44
               Mean episode length: 247.61
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 8.65s
                        Total time: 12671.26s
                               ETA: 911573.1s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.377s, learning 0.185s)
               Value function loss: 5.8974
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 274.26
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 8.56s
                        Total time: 12679.83s
                               ETA: 911515.0s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.726s, learning 0.184s)
               Value function loss: 6.7515
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 268.22
               Mean episode length: 244.11
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 8.91s
                        Total time: 12688.74s
                               ETA: 911481.9s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.501s, learning 0.193s)
               Value function loss: 6.1161
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 269.45
               Mean episode length: 243.57
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 8.69s
                        Total time: 12697.43s
                               ETA: 911433.4s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.345s, learning 0.206s)
               Value function loss: 9.0236
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 276.17
               Mean episode length: 247.62
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 8.55s
                        Total time: 12705.98s
                               ETA: 911374.6s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.651s, learning 0.190s)
               Value function loss: 3.0019
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 275.41
               Mean episode length: 246.51
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 8.84s
                        Total time: 12714.82s
                               ETA: 911336.8s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.590s, learning 0.183s)
               Value function loss: 4.0283
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 270.95
               Mean episode length: 245.18
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 8.77s
                        Total time: 12723.60s
                               ETA: 911294.1s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.343s, learning 0.199s)
               Value function loss: 3.0031
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 274.46
               Mean episode length: 246.75
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 8.54s
                        Total time: 12732.14s
                               ETA: 911234.9s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.763s, learning 0.183s)
               Value function loss: 4.0565
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 275.75
               Mean episode length: 246.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 8.95s
                        Total time: 12741.08s
                               ETA: 911204.6s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.766s, learning 0.293s)
               Value function loss: 4.8376
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 271.90
               Mean episode length: 247.09
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 9.06s
                        Total time: 12750.14s
                               ETA: 911182.4s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.344s, learning 0.250s)
               Value function loss: 4.5667
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 272.23
               Mean episode length: 246.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 8.59s
                        Total time: 12758.74s
                               ETA: 911127.1s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.537s, learning 0.315s)
               Value function loss: 4.7221
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 272.67
               Mean episode length: 247.26
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 8.85s
                        Total time: 12767.59s
                               ETA: 911090.4s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.339s, learning 0.221s)
               Value function loss: 4.1145
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 274.31
               Mean episode length: 247.19
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 8.56s
                        Total time: 12776.15s
                               ETA: 911032.7s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.570s, learning 0.243s)
               Value function loss: 3.8915
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 277.17
               Mean episode length: 249.23
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 8.81s
                        Total time: 12784.96s
                               ETA: 910993.1s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.512s, learning 0.174s)
               Value function loss: 4.5073
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 275.14
               Mean episode length: 247.59
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 8.69s
                        Total time: 12793.65s
                               ETA: 910944.6s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.206s, learning 0.185s)
               Value function loss: 3.8124
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 271.54
               Mean episode length: 247.59
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 8.39s
                        Total time: 12802.04s
                               ETA: 910875.2s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.736s, learning 0.205s)
               Value function loss: 4.5905
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 275.38
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 8.94s
                        Total time: 12810.98s
                               ETA: 910844.9s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.376s, learning 0.243s)
               Value function loss: 4.0788
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 275.90
               Mean episode length: 247.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 8.62s
                        Total time: 12819.60s
                               ETA: 910791.8s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.331s, learning 0.212s)
               Value function loss: 4.2514
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 274.41
               Mean episode length: 244.95
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 8.54s
                        Total time: 12828.14s
                               ETA: 910733.4s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.548s, learning 0.190s)
               Value function loss: 5.6444
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 276.24
               Mean episode length: 247.16
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 8.74s
                        Total time: 12836.88s
                               ETA: 910688.9s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.773s, learning 0.167s)
               Value function loss: 5.0420
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 278.88
               Mean episode length: 248.72
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 8.94s
                        Total time: 12845.82s
                               ETA: 910658.7s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.544s, learning 0.223s)
               Value function loss: 6.2163
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 277.76
               Mean episode length: 247.70
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 8.77s
                        Total time: 12854.59s
                               ETA: 910616.3s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.646s, learning 0.250s)
               Value function loss: 5.2983
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 277.59
               Mean episode length: 246.33
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 8.90s
                        Total time: 12863.48s
                               ETA: 910583.1s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.069s, learning 0.222s)
               Value function loss: 5.8055
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 274.09
               Mean episode length: 246.10
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 9.29s
                        Total time: 12872.77s
                               ETA: 910577.9s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.399s, learning 0.236s)
               Value function loss: 5.5770
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 274.28
               Mean episode length: 248.60
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 8.63s
                        Total time: 12881.41s
                               ETA: 910526.3s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.227s, learning 0.210s)
               Value function loss: 5.5261
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 278.07
               Mean episode length: 248.55
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 8.44s
                        Total time: 12889.84s
                               ETA: 910460.7s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.341s, learning 0.186s)
               Value function loss: 5.2729
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 270.91
               Mean episode length: 245.18
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 8.53s
                        Total time: 12898.37s
                               ETA: 910401.7s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.767s, learning 0.169s)
               Value function loss: 6.4366
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 272.83
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 8.94s
                        Total time: 12907.31s
                               ETA: 910371.5s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.338s, learning 0.168s)
               Value function loss: 5.8201
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 272.56
               Mean episode length: 249.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 8.51s
                        Total time: 12915.81s
                               ETA: 910311.0s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.674s, learning 0.179s)
               Value function loss: 7.1519
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 276.12
               Mean episode length: 248.93
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 8.85s
                        Total time: 12924.67s
                               ETA: 910275.1s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.353s, learning 0.167s)
               Value function loss: 5.6673
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 278.87
               Mean episode length: 249.25
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 8.52s
                        Total time: 12933.19s
                               ETA: 910215.7s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.385s, learning 0.185s)
               Value function loss: 5.4111
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 277.51
               Mean episode length: 246.19
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 8.57s
                        Total time: 12941.76s
                               ETA: 910159.9s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.552s, learning 0.172s)
               Value function loss: 5.7574
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 276.87
               Mean episode length: 244.83
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 8.72s
                        Total time: 12950.48s
                               ETA: 910115.1s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.624s, learning 0.292s)
               Value function loss: 4.7383
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 281.83
               Mean episode length: 249.03
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 8.92s
                        Total time: 12959.40s
                               ETA: 910083.8s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.465s, learning 0.193s)
               Value function loss: 5.5949
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 275.92
               Mean episode length: 248.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 8.66s
                        Total time: 12968.05s
                               ETA: 910034.4s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.381s, learning 0.177s)
               Value function loss: 4.7028
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 275.42
               Mean episode length: 246.47
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 8.56s
                        Total time: 12976.61s
                               ETA: 909978.1s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.611s, learning 0.172s)
               Value function loss: 3.9691
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 279.72
               Mean episode length: 248.31
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 8.78s
                        Total time: 12985.40s
                               ETA: 909937.5s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.475s, learning 0.184s)
               Value function loss: 3.2080
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 276.58
               Mean episode length: 246.12
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 8.66s
                        Total time: 12994.06s
                               ETA: 909888.4s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.369s, learning 0.167s)
               Value function loss: 3.2778
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 275.58
               Mean episode length: 244.02
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 8.54s
                        Total time: 13002.59s
                               ETA: 909830.7s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.953s, learning 0.172s)
               Value function loss: 3.8035
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 279.56
               Mean episode length: 247.01
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 9.12s
                        Total time: 13011.72s
                               ETA: 909814.3s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.614s, learning 0.182s)
               Value function loss: 4.2758
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 278.20
               Mean episode length: 248.04
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 8.80s
                        Total time: 13020.51s
                               ETA: 909774.9s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.502s, learning 0.197s)
               Value function loss: 4.3684
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 279.18
               Mean episode length: 249.79
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 8.70s
                        Total time: 13029.21s
                               ETA: 909728.8s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.472s, learning 0.211s)
               Value function loss: 4.3526
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 280.73
               Mean episode length: 249.03
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 8.68s
                        Total time: 13037.90s
                               ETA: 909681.5s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.454s, learning 0.171s)
               Value function loss: 4.6871
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 280.92
               Mean episode length: 247.79
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 8.62s
                        Total time: 13046.52s
                               ETA: 909630.3s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.461s, learning 0.197s)
               Value function loss: 3.4652
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 276.73
               Mean episode length: 247.15
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 8.66s
                        Total time: 13055.18s
                               ETA: 909581.5s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.543s, learning 0.274s)
               Value function loss: 3.7632
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 272.46
               Mean episode length: 246.82
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 8.82s
                        Total time: 13064.00s
                               ETA: 909543.8s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.616s, learning 0.186s)
               Value function loss: 3.3298
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 278.85
               Mean episode length: 248.43
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 8.80s
                        Total time: 13072.80s
                               ETA: 909505.1s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.711s, learning 0.193s)
               Value function loss: 4.0486
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 280.16
               Mean episode length: 249.01
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 8.90s
                        Total time: 13081.70s
                               ETA: 909473.4s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.778s, learning 0.229s)
               Value function loss: 3.7702
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 279.06
               Mean episode length: 248.11
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 9.01s
                        Total time: 13090.71s
                               ETA: 909449.1s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.432s, learning 0.177s)
               Value function loss: 3.2218
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 276.92
               Mean episode length: 246.95
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 8.61s
                        Total time: 13099.32s
                               ETA: 909397.1s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.440s, learning 0.219s)
               Value function loss: 3.7683
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 273.77
               Mean episode length: 246.35
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 8.66s
                        Total time: 13107.98s
                               ETA: 909348.6s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.061s, learning 0.179s)
               Value function loss: 4.3206
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 275.28
               Mean episode length: 248.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 8.24s
                        Total time: 13116.22s
                               ETA: 909271.1s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.464s, learning 0.222s)
               Value function loss: 5.0088
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 276.26
               Mean episode length: 249.35
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 8.69s
                        Total time: 13124.90s
                               ETA: 909224.6s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.493s, learning 0.183s)
               Value function loss: 3.9663
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 274.79
               Mean episode length: 247.38
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 8.68s
                        Total time: 13133.58s
                               ETA: 909177.5s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.475s, learning 0.182s)
               Value function loss: 4.8050
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: 275.51
               Mean episode length: 247.73
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 8.66s
                        Total time: 13142.24s
                               ETA: 909129.1s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.685s, learning 0.212s)
               Value function loss: 4.8785
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 275.90
               Mean episode length: 248.29
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 8.90s
                        Total time: 13151.13s
                               ETA: 909097.4s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.469s, learning 0.219s)
               Value function loss: 4.8902
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 272.98
               Mean episode length: 246.56
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 8.69s
                        Total time: 13159.82s
                               ETA: 909051.3s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.289s, learning 0.251s)
               Value function loss: 4.7394
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 269.36
               Mean episode length: 244.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 8.54s
                        Total time: 13168.36s
                               ETA: 908995.0s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.334s, learning 0.168s)
               Value function loss: 5.4813
                    Surrogate loss: -0.0036
             Mean action noise std: 0.77
                       Mean reward: 278.74
               Mean episode length: 248.22
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 8.50s
                        Total time: 13176.86s
                               ETA: 908936.1s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.656s, learning 0.168s)
               Value function loss: 4.5755
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 276.26
               Mean episode length: 247.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 8.82s
                        Total time: 13185.69s
                               ETA: 908899.5s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.508s, learning 0.219s)
               Value function loss: 5.9952
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 274.59
               Mean episode length: 247.84
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 8.73s
                        Total time: 13194.41s
                               ETA: 908856.3s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.518s, learning 0.174s)
               Value function loss: 5.1765
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 274.00
               Mean episode length: 246.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 8.69s
                        Total time: 13203.10s
                               ETA: 908810.6s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.364s, learning 0.174s)
               Value function loss: 5.4320
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 274.76
               Mean episode length: 247.02
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 8.54s
                        Total time: 13211.64s
                               ETA: 908754.6s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.580s, learning 0.258s)
               Value function loss: 5.3228
                    Surrogate loss: -0.0016
             Mean action noise std: 0.77
                       Mean reward: 274.59
               Mean episode length: 246.56
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 8.84s
                        Total time: 13220.48s
                               ETA: 908719.1s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.470s, learning 0.174s)
               Value function loss: 3.8438
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 274.29
               Mean episode length: 246.21
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 8.64s
                        Total time: 13229.13s
                               ETA: 908670.4s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.422s, learning 0.189s)
               Value function loss: 5.4865
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 275.68
               Mean episode length: 246.92
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 8.61s
                        Total time: 13237.74s
                               ETA: 908619.4s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.442s, learning 0.201s)
               Value function loss: 3.7254
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 277.16
               Mean episode length: 248.52
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 8.64s
                        Total time: 13246.38s
                               ETA: 908570.8s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.522s, learning 0.183s)
               Value function loss: 4.3060
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 274.93
               Mean episode length: 248.82
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 8.70s
                        Total time: 13255.08s
                               ETA: 908526.3s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.705s, learning 0.174s)
               Value function loss: 3.3444
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 269.20
               Mean episode length: 243.88
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 8.88s
                        Total time: 13263.96s
                               ETA: 908493.9s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.995s, learning 0.173s)
               Value function loss: 3.4945
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 270.34
               Mean episode length: 243.61
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 9.17s
                        Total time: 13273.13s
                               ETA: 908481.3s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.469s, learning 0.183s)
               Value function loss: 3.0310
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 272.85
               Mean episode length: 245.67
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 8.65s
                        Total time: 13281.78s
                               ETA: 908433.4s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.573s, learning 0.188s)
               Value function loss: 2.9908
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 273.40
               Mean episode length: 243.83
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 8.76s
                        Total time: 13290.54s
                               ETA: 908393.0s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.377s, learning 0.174s)
               Value function loss: 3.5950
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 277.07
               Mean episode length: 247.07
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 8.55s
                        Total time: 13299.09s
                               ETA: 908338.3s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.370s, learning 0.186s)
               Value function loss: 3.9853
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 276.88
               Mean episode length: 246.70
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 8.56s
                        Total time: 13307.65s
                               ETA: 908284.1s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.589s, learning 0.170s)
               Value function loss: 3.9317
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 278.67
               Mean episode length: 249.34
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 8.76s
                        Total time: 13316.41s
                               ETA: 908243.7s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.354s, learning 0.177s)
               Value function loss: 2.8735
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 274.38
               Mean episode length: 245.12
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 8.53s
                        Total time: 13324.94s
                               ETA: 908187.8s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.471s, learning 0.179s)
               Value function loss: 3.7559
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 267.88
               Mean episode length: 240.50
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 8.65s
                        Total time: 13333.59s
                               ETA: 908140.1s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.539s, learning 0.191s)
               Value function loss: 3.0050
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 268.63
               Mean episode length: 244.67
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 8.73s
                        Total time: 13342.32s
                               ETA: 908098.0s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.448s, learning 0.214s)
               Value function loss: 3.1388
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 274.81
               Mean episode length: 249.29
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 8.66s
                        Total time: 13350.98s
                               ETA: 908051.2s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.550s, learning 0.214s)
               Value function loss: 2.8150
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 272.05
               Mean episode length: 248.93
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 8.76s
                        Total time: 13359.75s
                               ETA: 908011.4s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1781 steps/s (collection: 9.034s, learning 0.165s)
               Value function loss: 3.3135
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 269.83
               Mean episode length: 246.58
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 9.20s
                        Total time: 13368.95s
                               ETA: 908001.2s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.503s, learning 0.169s)
               Value function loss: 3.0488
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 273.94
               Mean episode length: 247.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 8.67s
                        Total time: 13377.62s
                               ETA: 907955.2s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.386s, learning 0.179s)
               Value function loss: 4.0617
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 273.06
               Mean episode length: 245.26
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 8.56s
                        Total time: 13386.18s
                               ETA: 907902.0s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.727s, learning 0.198s)
               Value function loss: 4.2748
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 272.11
               Mean episode length: 246.14
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 8.93s
                        Total time: 13395.11s
                               ETA: 907873.3s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.562s, learning 0.172s)
               Value function loss: 3.5963
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 269.95
               Mean episode length: 245.35
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 8.73s
                        Total time: 13403.84s
                               ETA: 907831.7s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.600s, learning 0.166s)
               Value function loss: 3.7577
                    Surrogate loss: -0.0008
             Mean action noise std: 0.77
                       Mean reward: 275.02
               Mean episode length: 249.27
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 8.77s
                        Total time: 13412.61s
                               ETA: 907792.3s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.565s, learning 0.219s)
               Value function loss: 3.7536
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 276.16
               Mean episode length: 246.62
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 8.78s
                        Total time: 13421.39s
                               ETA: 907754.1s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.538s, learning 0.218s)
               Value function loss: 3.4386
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 274.25
               Mean episode length: 245.52
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 8.76s
                        Total time: 13430.15s
                               ETA: 907714.1s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.422s, learning 0.168s)
               Value function loss: 3.8420
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 274.39
               Mean episode length: 247.21
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 8.59s
                        Total time: 13438.74s
                               ETA: 907663.0s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.517s, learning 0.207s)
               Value function loss: 4.3129
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 270.93
               Mean episode length: 248.87
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 8.72s
                        Total time: 13447.46s
                               ETA: 907621.0s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.767s, learning 0.168s)
               Value function loss: 4.2408
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 272.05
               Mean episode length: 249.16
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 8.93s
                        Total time: 13456.40s
                               ETA: 907593.2s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.620s, learning 0.168s)
               Value function loss: 4.8230
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 272.30
               Mean episode length: 248.25
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 8.79s
                        Total time: 13465.19s
                               ETA: 907555.5s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.398s, learning 0.165s)
               Value function loss: 4.2046
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 271.46
               Mean episode length: 249.88
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 8.56s
                        Total time: 13473.75s
                               ETA: 907502.7s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.661s, learning 0.171s)
               Value function loss: 4.2648
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 271.30
               Mean episode length: 248.35
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 8.83s
                        Total time: 13482.58s
                               ETA: 907468.1s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.741s, learning 0.194s)
               Value function loss: 3.9154
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 275.44
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 8.94s
                        Total time: 13491.52s
                               ETA: 907440.5s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.701s, learning 0.167s)
               Value function loss: 3.8576
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 275.02
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 8.87s
                        Total time: 13500.39s
                               ETA: 907408.3s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.441s, learning 0.259s)
               Value function loss: 4.5644
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 272.52
               Mean episode length: 247.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 8.70s
                        Total time: 13509.09s
                               ETA: 907364.9s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.167s)
               Value function loss: 4.0736
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 274.55
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 8.46s
                        Total time: 13517.54s
                               ETA: 907305.2s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.429s, learning 0.184s)
               Value function loss: 5.6799
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 274.50
               Mean episode length: 246.86
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 8.61s
                        Total time: 13526.16s
                               ETA: 907256.1s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.432s, learning 0.176s)
               Value function loss: 2.6344
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 264.71
               Mean episode length: 244.68
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 8.61s
                        Total time: 13534.76s
                               ETA: 907206.7s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.208s, learning 0.190s)
               Value function loss: 3.3954
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 267.06
               Mean episode length: 247.23
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 8.40s
                        Total time: 13543.16s
                               ETA: 907143.3s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.482s, learning 0.180s)
               Value function loss: 2.5149
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 274.20
               Mean episode length: 248.23
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 8.66s
                        Total time: 13551.82s
                               ETA: 907097.6s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.498s, learning 0.192s)
               Value function loss: 2.7017
                    Surrogate loss: 0.0024
             Mean action noise std: 0.76
                       Mean reward: 270.40
               Mean episode length: 247.29
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 8.69s
                        Total time: 13560.51s
                               ETA: 907053.9s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.445s, learning 0.289s)
               Value function loss: 3.3559
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 264.16
               Mean episode length: 247.40
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 8.73s
                        Total time: 13569.25s
                               ETA: 907013.2s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.650s, learning 0.182s)
               Value function loss: 2.8912
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 265.09
               Mean episode length: 249.59
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 8.83s
                        Total time: 13578.08s
                               ETA: 906979.0s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.630s, learning 0.167s)
               Value function loss: 3.5480
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 263.48
               Mean episode length: 248.05
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 8.80s
                        Total time: 13586.88s
                               ETA: 906942.6s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.445s, learning 0.213s)
               Value function loss: 2.8852
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 257.22
               Mean episode length: 245.54
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 8.66s
                        Total time: 13595.54s
                               ETA: 906896.9s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.934s, learning 0.166s)
               Value function loss: 2.8145
                    Surrogate loss: 0.0050
             Mean action noise std: 0.76
                       Mean reward: 253.56
               Mean episode length: 243.42
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 9.10s
                        Total time: 13604.64s
                               ETA: 906880.7s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.569s, learning 0.218s)
               Value function loss: 2.8166
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 259.28
               Mean episode length: 246.56
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 8.79s
                        Total time: 13613.42s
                               ETA: 906843.6s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.374s, learning 0.191s)
               Value function loss: 2.4840
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 259.24
               Mean episode length: 248.16
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 8.57s
                        Total time: 13621.99s
                               ETA: 906791.9s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.013s, learning 0.192s)
               Value function loss: 3.2475
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 265.19
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 9.21s
                        Total time: 13631.19s
                               ETA: 906782.8s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.528s, learning 0.172s)
               Value function loss: 2.7771
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 264.78
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 8.70s
                        Total time: 13639.90s
                               ETA: 906740.1s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.596s, learning 0.191s)
               Value function loss: 2.6506
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 263.98
               Mean episode length: 249.87
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 8.79s
                        Total time: 13648.68s
                               ETA: 906703.2s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.444s, learning 0.169s)
               Value function loss: 3.6026
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 266.28
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 8.61s
                        Total time: 13657.29s
                               ETA: 906654.8s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.311s, learning 0.178s)
               Value function loss: 3.5144
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 261.65
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 8.49s
                        Total time: 13665.78s
                               ETA: 906598.2s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.409s, learning 0.169s)
               Value function loss: 5.0619
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 266.87
               Mean episode length: 249.20
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 8.58s
                        Total time: 13674.36s
                               ETA: 906547.6s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.503s, learning 0.261s)
               Value function loss: 3.1265
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 263.43
               Mean episode length: 247.50
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 8.76s
                        Total time: 13683.13s
                               ETA: 906509.4s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.815s, learning 0.193s)
               Value function loss: 4.7631
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 262.58
               Mean episode length: 248.85
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 9.01s
                        Total time: 13692.13s
                               ETA: 906487.3s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.389s, learning 0.175s)
               Value function loss: 3.8243
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 262.55
               Mean episode length: 246.49
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 8.56s
                        Total time: 13700.70s
                               ETA: 906435.9s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.365s, learning 0.170s)
               Value function loss: 4.4476
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 266.12
               Mean episode length: 248.89
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 8.54s
                        Total time: 13709.23s
                               ETA: 906382.7s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.377s, learning 0.182s)
               Value function loss: 3.9019
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 266.61
               Mean episode length: 247.05
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 8.56s
                        Total time: 13717.79s
                               ETA: 906331.0s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.658s, learning 0.175s)
               Value function loss: 5.2769
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 260.95
               Mean episode length: 244.11
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 8.83s
                        Total time: 13726.62s
                               ETA: 906297.6s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.207s, learning 0.260s)
               Value function loss: 4.2318
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 258.37
               Mean episode length: 244.57
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 8.47s
                        Total time: 13735.09s
                               ETA: 906239.9s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.338s, learning 0.218s)
               Value function loss: 4.1089
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 265.76
               Mean episode length: 246.60
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 8.56s
                        Total time: 13743.65s
                               ETA: 906188.3s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.709s, learning 0.172s)
               Value function loss: 4.3099
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 265.19
               Mean episode length: 249.65
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 8.88s
                        Total time: 13752.53s
                               ETA: 906158.1s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.364s, learning 0.223s)
               Value function loss: 3.6492
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 264.22
               Mean episode length: 248.76
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 8.59s
                        Total time: 13761.11s
                               ETA: 906108.7s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.454s, learning 0.278s)
               Value function loss: 4.2040
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 268.62
               Mean episode length: 248.02
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 8.73s
                        Total time: 13769.85s
                               ETA: 906068.7s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.737s, learning 0.181s)
               Value function loss: 5.3501
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 266.87
               Mean episode length: 246.27
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 8.92s
                        Total time: 13778.76s
                               ETA: 906041.1s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.880s, learning 0.191s)
               Value function loss: 5.1269
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 262.40
               Mean episode length: 244.30
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 9.07s
                        Total time: 13787.83s
                               ETA: 906023.5s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.514s, learning 0.263s)
               Value function loss: 6.3814
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 270.35
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 8.78s
                        Total time: 13796.61s
                               ETA: 905986.7s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.268s, learning 0.267s)
               Value function loss: 2.4710
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 270.14
               Mean episode length: 248.66
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 8.54s
                        Total time: 13805.15s
                               ETA: 905934.0s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.580s, learning 0.177s)
               Value function loss: 3.6580
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 264.00
               Mean episode length: 242.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 8.76s
                        Total time: 13813.90s
                               ETA: 905896.0s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.610s, learning 0.173s)
               Value function loss: 2.7751
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 269.81
               Mean episode length: 246.59
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 8.78s
                        Total time: 13822.69s
                               ETA: 905859.6s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.491s, learning 0.265s)
               Value function loss: 3.5055
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 267.11
               Mean episode length: 242.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 8.76s
                        Total time: 13831.44s
                               ETA: 905821.6s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.512s, learning 0.197s)
               Value function loss: 3.4713
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 264.82
               Mean episode length: 242.94
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 8.71s
                        Total time: 13840.15s
                               ETA: 905780.5s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.499s, learning 0.168s)
               Value function loss: 3.9901
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 266.65
               Mean episode length: 244.13
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 8.67s
                        Total time: 13848.82s
                               ETA: 905736.8s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.015s, learning 0.166s)
               Value function loss: 3.5250
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 265.00
               Mean episode length: 245.41
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 8.18s
                        Total time: 13857.00s
                               ETA: 905661.3s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.421s, learning 0.240s)
               Value function loss: 3.9875
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 257.44
               Mean episode length: 239.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 8.66s
                        Total time: 13865.66s
                               ETA: 905617.2s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.281s, learning 0.273s)
               Value function loss: 3.3775
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 254.34
               Mean episode length: 233.71
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 8.55s
                        Total time: 13874.22s
                               ETA: 905566.1s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.310s, learning 0.178s)
               Value function loss: 4.0109
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 262.43
               Mean episode length: 239.77
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 8.49s
                        Total time: 13882.70s
                               ETA: 905510.8s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.433s, learning 0.254s)
               Value function loss: 3.1666
                    Surrogate loss: 0.0022
             Mean action noise std: 0.76
                       Mean reward: 255.00
               Mean episode length: 233.95
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 8.69s
                        Total time: 13891.39s
                               ETA: 905468.6s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.381s, learning 0.220s)
               Value function loss: 3.5067
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 260.76
               Mean episode length: 237.90
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 8.60s
                        Total time: 13899.99s
                               ETA: 905420.8s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.539s, learning 0.210s)
               Value function loss: 3.3325
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 273.56
               Mean episode length: 248.09
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 8.75s
                        Total time: 13908.74s
                               ETA: 905382.6s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.559s, learning 0.209s)
               Value function loss: 3.1802
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 266.56
               Mean episode length: 243.15
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 8.77s
                        Total time: 13917.51s
                               ETA: 905345.8s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.533s, learning 0.183s)
               Value function loss: 3.6022
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 260.40
               Mean episode length: 237.61
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 8.72s
                        Total time: 13926.22s
                               ETA: 905305.6s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.485s, learning 0.185s)
               Value function loss: 4.5991
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 263.68
               Mean episode length: 241.25
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 8.67s
                        Total time: 13934.89s
                               ETA: 905262.5s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.596s, learning 0.168s)
               Value function loss: 5.2205
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 263.82
               Mean episode length: 242.23
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 8.76s
                        Total time: 13943.66s
                               ETA: 905225.6s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.639s, learning 0.173s)
               Value function loss: 3.8691
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 263.47
               Mean episode length: 243.57
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 8.81s
                        Total time: 13952.47s
                               ETA: 905191.8s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.443s, learning 0.216s)
               Value function loss: 4.5614
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 272.62
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 8.66s
                        Total time: 13961.13s
                               ETA: 905148.1s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.509s, learning 0.259s)
               Value function loss: 4.8025
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 263.86
               Mean episode length: 243.47
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 8.77s
                        Total time: 13969.90s
                               ETA: 905111.5s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.189s, learning 0.176s)
               Value function loss: 4.0063
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 262.91
               Mean episode length: 242.51
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 8.37s
                        Total time: 13978.26s
                               ETA: 905048.9s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.846s, learning 0.168s)
               Value function loss: 3.8677
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 263.75
               Mean episode length: 243.11
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 9.01s
                        Total time: 13987.28s
                               ETA: 905028.2s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.533s, learning 0.236s)
               Value function loss: 5.3104
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 267.03
               Mean episode length: 245.57
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 8.77s
                        Total time: 13996.04s
                               ETA: 904991.8s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.647s, learning 0.175s)
               Value function loss: 4.7225
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 266.60
               Mean episode length: 244.47
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 8.82s
                        Total time: 14004.87s
                               ETA: 904958.8s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.451s, learning 0.183s)
               Value function loss: 5.5120
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 265.54
               Mean episode length: 243.85
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 8.63s
                        Total time: 14013.50s
                               ETA: 904913.8s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.370s, learning 0.178s)
               Value function loss: 4.2180
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 268.24
               Mean episode length: 245.36
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 8.55s
                        Total time: 14022.05s
                               ETA: 904863.2s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.654s, learning 0.188s)
               Value function loss: 4.5753
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 266.38
               Mean episode length: 246.13
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 8.84s
                        Total time: 14030.89s
                               ETA: 904831.6s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.579s, learning 0.195s)
               Value function loss: 3.7438
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 273.01
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 8.77s
                        Total time: 14039.66s
                               ETA: 904795.7s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.411s, learning 0.181s)
               Value function loss: 4.3667
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 271.12
               Mean episode length: 248.32
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 8.59s
                        Total time: 14048.26s
                               ETA: 904748.1s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.518s, learning 0.170s)
               Value function loss: 4.4455
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 268.00
               Mean episode length: 244.77
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 8.69s
                        Total time: 14056.94s
                               ETA: 904706.8s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.859s, learning 0.179s)
               Value function loss: 3.4951
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 266.96
               Mean episode length: 244.82
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 9.04s
                        Total time: 14065.98s
                               ETA: 904688.0s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.712s, learning 0.213s)
               Value function loss: 3.7933
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 259.99
               Mean episode length: 238.52
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 8.93s
                        Total time: 14074.91s
                               ETA: 904662.0s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.847s, learning 0.251s)
               Value function loss: 2.6586
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 254.21
               Mean episode length: 233.01
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 9.10s
                        Total time: 14084.01s
                               ETA: 904647.0s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.301s, learning 0.273s)
               Value function loss: 2.8998
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 259.89
               Mean episode length: 238.62
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 8.57s
                        Total time: 14092.58s
                               ETA: 904598.4s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.487s, learning 0.237s)
               Value function loss: 3.2994
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 262.95
               Mean episode length: 238.66
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 8.72s
                        Total time: 14101.30s
                               ETA: 904559.6s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.688s, learning 0.194s)
               Value function loss: 3.4239
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 264.35
               Mean episode length: 239.09
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 8.88s
                        Total time: 14110.19s
                               ETA: 904530.9s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.500s, learning 0.170s)
               Value function loss: 3.5076
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 265.21
               Mean episode length: 239.40
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 8.67s
                        Total time: 14118.86s
                               ETA: 904488.6s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.945s, learning 0.177s)
               Value function loss: 3.5028
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 266.68
               Mean episode length: 242.09
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 9.12s
                        Total time: 14127.98s
                               ETA: 904475.3s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.378s, learning 0.197s)
               Value function loss: 4.3501
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 266.57
               Mean episode length: 238.35
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 8.57s
                        Total time: 14136.55s
                               ETA: 904427.0s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.517s, learning 0.180s)
               Value function loss: 3.8418
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 265.62
               Mean episode length: 237.49
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 8.70s
                        Total time: 14145.25s
                               ETA: 904386.6s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.550s, learning 0.176s)
               Value function loss: 4.2767
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 263.28
               Mean episode length: 237.47
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 8.73s
                        Total time: 14153.98s
                               ETA: 904348.1s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.342s, learning 0.176s)
               Value function loss: 3.3241
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 273.98
               Mean episode length: 244.37
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 8.52s
                        Total time: 14162.49s
                               ETA: 904296.3s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.426s, learning 0.179s)
               Value function loss: 3.1869
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 271.66
               Mean episode length: 243.41
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 8.61s
                        Total time: 14171.10s
                               ETA: 904250.2s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.413s, learning 0.170s)
               Value function loss: 3.3885
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 272.45
               Mean episode length: 245.38
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 8.58s
                        Total time: 14179.68s
                               ETA: 904202.7s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.529s, learning 0.253s)
               Value function loss: 3.0964
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 273.92
               Mean episode length: 246.16
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 8.78s
                        Total time: 14188.47s
                               ETA: 904168.0s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.656s, learning 0.368s)
               Value function loss: 3.5268
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 271.23
               Mean episode length: 245.79
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 9.02s
                        Total time: 14197.49s
                               ETA: 904148.7s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.584s, learning 0.248s)
               Value function loss: 4.6173
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 268.66
               Mean episode length: 242.35
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 8.83s
                        Total time: 14206.32s
                               ETA: 904117.1s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.808s, learning 0.211s)
               Value function loss: 5.4370
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 271.68
               Mean episode length: 242.26
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 9.02s
                        Total time: 14215.34s
                               ETA: 904097.5s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.971s, learning 0.179s)
               Value function loss: 4.7842
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 270.05
               Mean episode length: 244.13
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 9.15s
                        Total time: 14224.49s
                               ETA: 904086.2s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.440s, learning 0.202s)
               Value function loss: 4.3490
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 264.17
               Mean episode length: 241.48
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 8.64s
                        Total time: 14233.13s
                               ETA: 904042.6s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.281s, learning 0.192s)
               Value function loss: 4.9087
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 272.98
               Mean episode length: 247.35
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 8.47s
                        Total time: 14241.61s
                               ETA: 903988.4s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.336s, learning 0.194s)
               Value function loss: 4.8595
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 277.09
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 8.53s
                        Total time: 14250.14s
                               ETA: 903937.9s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.480s, learning 0.181s)
               Value function loss: 4.8653
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 272.70
               Mean episode length: 245.76
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 8.66s
                        Total time: 14258.80s
                               ETA: 903895.7s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.411s, learning 0.333s)
               Value function loss: 4.9466
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 278.03
               Mean episode length: 245.12
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 8.74s
                        Total time: 14267.54s
                               ETA: 903858.7s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.797s, learning 0.191s)
               Value function loss: 5.5581
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 274.19
               Mean episode length: 248.37
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 8.99s
                        Total time: 14276.53s
                               ETA: 903837.3s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.613s, learning 0.176s)
               Value function loss: 5.8441
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 274.07
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 8.79s
                        Total time: 14285.32s
                               ETA: 903803.4s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.520s, learning 0.203s)
               Value function loss: 5.1955
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 273.33
               Mean episode length: 244.84
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 8.72s
                        Total time: 14294.04s
                               ETA: 903765.2s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.605s, learning 0.212s)
               Value function loss: 4.6544
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 266.18
               Mean episode length: 239.45
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 8.82s
                        Total time: 14302.86s
                               ETA: 903733.1s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.505s, learning 0.198s)
               Value function loss: 4.5892
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 273.08
               Mean episode length: 244.57
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 8.70s
                        Total time: 14311.56s
                               ETA: 903693.8s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.754s, learning 0.193s)
               Value function loss: 4.6208
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 265.30
               Mean episode length: 241.05
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 8.95s
                        Total time: 14320.51s
                               ETA: 903669.9s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.715s, learning 0.178s)
               Value function loss: 5.1577
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 272.49
               Mean episode length: 246.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 8.89s
                        Total time: 14329.40s
                               ETA: 903642.6s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.421s, learning 0.187s)
               Value function loss: 4.4055
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 277.17
               Mean episode length: 247.48
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 8.61s
                        Total time: 14338.01s
                               ETA: 903597.4s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.745s, learning 0.343s)
               Value function loss: 4.4339
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 277.25
               Mean episode length: 248.06
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 9.09s
                        Total time: 14347.10s
                               ETA: 903582.5s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.559s, learning 0.357s)
               Value function loss: 3.5443
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 275.63
               Mean episode length: 246.45
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 8.92s
                        Total time: 14356.01s
                               ETA: 903556.7s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.668s, learning 0.199s)
               Value function loss: 3.9645
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 268.63
               Mean episode length: 239.14
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 8.87s
                        Total time: 14364.88s
                               ETA: 903527.9s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.626s, learning 0.200s)
               Value function loss: 3.3179
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 264.42
               Mean episode length: 235.36
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 8.83s
                        Total time: 14373.70s
                               ETA: 903496.5s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.828s, learning 0.263s)
               Value function loss: 3.0370
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 275.45
               Mean episode length: 246.53
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 9.09s
                        Total time: 14382.79s
                               ETA: 903481.8s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.843s, learning 0.203s)
               Value function loss: 3.7027
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 270.84
               Mean episode length: 245.37
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 9.05s
                        Total time: 14391.84s
                               ETA: 903464.3s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.610s, learning 0.251s)
               Value function loss: 3.2245
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 275.25
               Mean episode length: 247.04
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 8.86s
                        Total time: 14400.70s
                               ETA: 903435.2s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.694s, learning 0.209s)
               Value function loss: 4.1283
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 277.30
               Mean episode length: 248.32
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 8.90s
                        Total time: 14409.60s
                               ETA: 903408.7s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.591s, learning 0.196s)
               Value function loss: 3.2865
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 275.94
               Mean episode length: 246.66
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 8.79s
                        Total time: 14418.39s
                               ETA: 903375.0s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.440s, learning 0.173s)
               Value function loss: 4.0724
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 270.16
               Mean episode length: 241.44
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 8.61s
                        Total time: 14427.00s
                               ETA: 903330.5s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.625s, learning 0.181s)
               Value function loss: 2.9248
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 270.87
               Mean episode length: 242.49
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 8.81s
                        Total time: 14435.81s
                               ETA: 903298.0s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.670s, learning 0.273s)
               Value function loss: 3.2475
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 275.08
               Mean episode length: 245.27
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 8.94s
                        Total time: 14444.75s
                               ETA: 903274.2s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.472s, learning 0.255s)
               Value function loss: 3.7330
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 274.39
               Mean episode length: 244.10
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 8.73s
                        Total time: 14453.48s
                               ETA: 903236.9s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.524s, learning 0.227s)
               Value function loss: 3.6651
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 273.25
               Mean episode length: 246.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 8.75s
                        Total time: 14462.23s
                               ETA: 903201.0s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.436s, learning 0.204s)
               Value function loss: 3.9542
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 270.31
               Mean episode length: 243.29
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 8.64s
                        Total time: 14470.87s
                               ETA: 903158.3s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.311s, learning 0.176s)
               Value function loss: 3.6026
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 272.05
               Mean episode length: 244.54
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 8.49s
                        Total time: 14479.35s
                               ETA: 903106.2s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.658s, learning 0.328s)
               Value function loss: 4.8640
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 273.24
               Mean episode length: 246.48
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 8.99s
                        Total time: 14488.34s
                               ETA: 903085.2s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.334s, learning 0.214s)
               Value function loss: 5.1472
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 275.77
               Mean episode length: 248.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 8.55s
                        Total time: 14496.89s
                               ETA: 903036.9s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.606s, learning 0.228s)
               Value function loss: 3.7642
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 273.64
               Mean episode length: 246.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 8.83s
                        Total time: 14505.72s
                               ETA: 903006.4s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.584s, learning 0.232s)
               Value function loss: 4.5854
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 272.81
               Mean episode length: 247.76
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 8.82s
                        Total time: 14514.54s
                               ETA: 902974.9s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.678s, learning 0.197s)
               Value function loss: 3.8096
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 274.28
               Mean episode length: 246.94
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 8.87s
                        Total time: 14523.41s
                               ETA: 902947.1s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1779 steps/s (collection: 8.964s, learning 0.246s)
               Value function loss: 4.1604
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 277.28
               Mean episode length: 249.84
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 9.21s
                        Total time: 14532.62s
                               ETA: 902940.1s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.719s, learning 0.224s)
               Value function loss: 4.3373
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 275.50
               Mean episode length: 246.93
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 8.94s
                        Total time: 14541.57s
                               ETA: 902916.5s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.614s, learning 0.243s)
               Value function loss: 4.8913
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 277.76
               Mean episode length: 249.97
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 8.86s
                        Total time: 14550.42s
                               ETA: 902887.7s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.458s, learning 0.186s)
               Value function loss: 5.3649
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 274.46
               Mean episode length: 246.54
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 8.64s
                        Total time: 14559.07s
                               ETA: 902845.6s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.631s, learning 0.225s)
               Value function loss: 4.4210
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 273.84
               Mean episode length: 246.38
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 8.86s
                        Total time: 14567.92s
                               ETA: 902816.8s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.527s, learning 0.181s)
               Value function loss: 4.5215
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 273.56
               Mean episode length: 247.08
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 8.71s
                        Total time: 14576.63s
                               ETA: 902778.7s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.873s, learning 0.261s)
               Value function loss: 4.4057
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 271.41
               Mean episode length: 248.03
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 9.13s
                        Total time: 14585.76s
                               ETA: 902767.1s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.685s, learning 0.201s)
               Value function loss: 4.3318
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 269.64
               Mean episode length: 246.25
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 8.89s
                        Total time: 14594.65s
                               ETA: 902740.1s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.592s, learning 0.205s)
               Value function loss: 4.9923
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 271.47
               Mean episode length: 247.87
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 8.80s
                        Total time: 14603.45s
                               ETA: 902707.7s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.210s, learning 0.208s)
               Value function loss: 5.2191
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 261.64
               Mean episode length: 240.22
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 8.42s
                        Total time: 14611.86s
                               ETA: 902651.8s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.764s, learning 0.238s)
               Value function loss: 5.2197
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 263.73
               Mean episode length: 239.21
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 9.00s
                        Total time: 14620.87s
                               ETA: 902632.1s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.716s, learning 0.168s)
               Value function loss: 3.0811
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 273.47
               Mean episode length: 248.21
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 8.88s
                        Total time: 14629.75s
                               ETA: 902605.1s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1782 steps/s (collection: 8.864s, learning 0.328s)
               Value function loss: 3.3829
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 268.14
               Mean episode length: 247.80
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 9.19s
                        Total time: 14638.94s
                               ETA: 902597.1s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1764 steps/s (collection: 9.101s, learning 0.182s)
               Value function loss: 2.4801
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 269.48
               Mean episode length: 249.59
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 9.28s
                        Total time: 14648.22s
                               ETA: 902594.8s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.257s, learning 0.202s)
               Value function loss: 3.3033
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 271.66
               Mean episode length: 247.99
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 8.46s
                        Total time: 14656.68s
                               ETA: 902541.7s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.431s, learning 0.219s)
               Value function loss: 3.8260
                    Surrogate loss: -0.0000
             Mean action noise std: 0.76
                       Mean reward: 271.55
               Mean episode length: 244.92
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 8.65s
                        Total time: 14665.33s
                               ETA: 902500.5s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.835s, learning 0.201s)
               Value function loss: 3.6956
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 265.41
               Mean episode length: 240.03
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 9.04s
                        Total time: 14674.37s
                               ETA: 902482.9s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.519s, learning 0.185s)
               Value function loss: 3.6703
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 269.18
               Mean episode length: 243.02
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 8.70s
                        Total time: 14683.07s
                               ETA: 902445.0s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.285s, learning 0.173s)
               Value function loss: 3.6546
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 268.60
               Mean episode length: 244.91
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 8.46s
                        Total time: 14691.53s
                               ETA: 902392.0s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.281s, learning 0.177s)
               Value function loss: 3.3553
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 270.23
               Mean episode length: 246.23
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 8.46s
                        Total time: 14699.99s
                               ETA: 902339.2s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.539s, learning 0.186s)
               Value function loss: 3.7124
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 267.04
               Mean episode length: 242.93
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 8.73s
                        Total time: 14708.72s
                               ETA: 902302.7s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.373s, learning 0.174s)
               Value function loss: 3.3962
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 270.15
               Mean episode length: 246.34
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 8.55s
                        Total time: 14717.26s
                               ETA: 902255.3s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.641s, learning 0.175s)
               Value function loss: 3.5188
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 269.07
               Mean episode length: 242.22
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 8.82s
                        Total time: 14726.08s
                               ETA: 902224.5s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.565s, learning 0.227s)
               Value function loss: 3.3661
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 266.18
               Mean episode length: 242.27
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 8.79s
                        Total time: 14734.87s
                               ETA: 902192.2s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.555s, learning 0.245s)
               Value function loss: 3.2096
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 268.93
               Mean episode length: 243.46
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 8.80s
                        Total time: 14743.67s
                               ETA: 902160.5s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.581s, learning 0.187s)
               Value function loss: 3.9571
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 269.79
               Mean episode length: 244.03
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 8.77s
                        Total time: 14752.44s
                               ETA: 902126.8s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.505s, learning 0.241s)
               Value function loss: 4.2923
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 272.03
               Mean episode length: 248.96
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 8.75s
                        Total time: 14761.19s
                               ETA: 902091.8s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.722s, learning 0.191s)
               Value function loss: 5.1265
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 271.60
               Mean episode length: 247.85
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 8.91s
                        Total time: 14770.10s
                               ETA: 902067.0s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.404s, learning 0.203s)
               Value function loss: 4.3134
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 271.39
               Mean episode length: 243.71
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 8.61s
                        Total time: 14778.71s
                               ETA: 902023.6s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.608s, learning 0.224s)
               Value function loss: 4.6278
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 274.25
               Mean episode length: 244.29
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 8.83s
                        Total time: 14787.54s
                               ETA: 901994.0s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.718s, learning 0.189s)
               Value function loss: 3.9266
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 277.41
               Mean episode length: 247.60
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 8.91s
                        Total time: 14796.45s
                               ETA: 901968.9s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.546s, learning 0.216s)
               Value function loss: 4.5769
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 276.54
               Mean episode length: 247.11
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 8.76s
                        Total time: 14805.21s
                               ETA: 901935.1s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.835s, learning 0.181s)
               Value function loss: 4.1874
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 281.85
               Mean episode length: 246.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 9.02s
                        Total time: 14814.22s
                               ETA: 901916.7s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.652s, learning 0.199s)
               Value function loss: 5.2716
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 280.75
               Mean episode length: 246.82
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 8.85s
                        Total time: 14823.08s
                               ETA: 901888.3s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.557s, learning 0.219s)
               Value function loss: 5.1347
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 279.85
               Mean episode length: 246.77
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 8.78s
                        Total time: 14831.85s
                               ETA: 901855.4s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.508s, learning 0.266s)
               Value function loss: 4.7918
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 278.17
               Mean episode length: 248.47
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 8.77s
                        Total time: 14840.62s
                               ETA: 901822.3s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.418s, learning 0.216s)
               Value function loss: 5.2885
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 270.82
               Mean episode length: 241.80
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 8.63s
                        Total time: 14849.26s
                               ETA: 901780.8s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.537s, learning 0.304s)
               Value function loss: 4.0658
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 275.96
               Mean episode length: 245.58
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 8.84s
                        Total time: 14858.10s
                               ETA: 901751.9s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.158s, learning 0.210s)
               Value function loss: 4.7677
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 283.07
               Mean episode length: 249.57
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 8.37s
                        Total time: 14866.47s
                               ETA: 901694.3s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.736s, learning 0.229s)
               Value function loss: 5.3021
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 283.98
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 8.96s
                        Total time: 14875.43s
                               ETA: 901672.9s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.490s, learning 0.177s)
               Value function loss: 5.0964
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 277.71
               Mean episode length: 247.22
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 8.67s
                        Total time: 14884.10s
                               ETA: 901633.6s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.544s, learning 0.175s)
               Value function loss: 6.6589
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 284.24
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 8.72s
                        Total time: 14892.82s
                               ETA: 901597.4s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.844s, learning 0.197s)
               Value function loss: 3.7358
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 280.74
               Mean episode length: 248.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 9.04s
                        Total time: 14901.86s
                               ETA: 901580.7s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.539s, learning 0.183s)
               Value function loss: 4.1577
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 277.70
               Mean episode length: 244.02
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 8.72s
                        Total time: 14910.58s
                               ETA: 901544.8s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.509s, learning 0.170s)
               Value function loss: 2.4949
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 278.73
               Mean episode length: 245.36
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 8.68s
                        Total time: 14919.26s
                               ETA: 901506.3s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.540s, learning 0.167s)
               Value function loss: 3.8210
                    Surrogate loss: -0.0005
             Mean action noise std: 0.76
                       Mean reward: 284.15
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 8.71s
                        Total time: 14927.97s
                               ETA: 901469.5s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.520s, learning 0.173s)
               Value function loss: 4.2993
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 284.86
               Mean episode length: 247.34
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 8.69s
                        Total time: 14936.66s
                               ETA: 901431.9s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.493s, learning 0.265s)
               Value function loss: 4.1537
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 279.47
               Mean episode length: 240.72
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 8.76s
                        Total time: 14945.41s
                               ETA: 901398.2s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.568s, learning 0.212s)
               Value function loss: 3.6853
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 266.76
               Mean episode length: 240.92
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 8.78s
                        Total time: 14954.20s
                               ETA: 901366.0s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.556s, learning 0.204s)
               Value function loss: 4.0210
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 276.12
               Mean episode length: 246.64
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 8.76s
                        Total time: 14962.96s
                               ETA: 901332.5s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.737s, learning 0.171s)
               Value function loss: 3.9444
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 277.37
               Mean episode length: 246.95
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 8.91s
                        Total time: 14971.86s
                               ETA: 901308.0s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.694s, learning 0.189s)
               Value function loss: 4.7119
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 275.30
               Mean episode length: 245.40
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 8.88s
                        Total time: 14980.75s
                               ETA: 901282.0s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.556s, learning 0.179s)
               Value function loss: 3.8124
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 280.25
               Mean episode length: 247.75
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 8.74s
                        Total time: 14989.48s
                               ETA: 901247.2s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.624s, learning 0.215s)
               Value function loss: 3.7632
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 281.25
               Mean episode length: 245.89
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 8.84s
                        Total time: 14998.32s
                               ETA: 901218.6s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.491s, learning 0.196s)
               Value function loss: 3.9574
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 282.46
               Mean episode length: 245.97
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 8.69s
                        Total time: 15007.01s
                               ETA: 901180.8s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.662s, learning 0.168s)
               Value function loss: 4.1187
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 289.12
               Mean episode length: 249.53
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 8.83s
                        Total time: 15015.84s
                               ETA: 901151.8s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.590s, learning 0.172s)
               Value function loss: 4.4427
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 289.99
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 8.76s
                        Total time: 15024.60s
                               ETA: 901118.7s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.419s, learning 0.220s)
               Value function loss: 4.9162
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 284.13
               Mean episode length: 244.66
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 8.64s
                        Total time: 15033.24s
                               ETA: 901078.2s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.683s, learning 0.173s)
               Value function loss: 6.5656
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 289.09
               Mean episode length: 247.57
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 8.86s
                        Total time: 15042.09s
                               ETA: 901050.7s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.780s, learning 0.265s)
               Value function loss: 5.2129
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 285.98
               Mean episode length: 247.25
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 9.05s
                        Total time: 15051.14s
                               ETA: 901034.7s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.213s, learning 0.168s)
               Value function loss: 5.3469
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 288.36
               Mean episode length: 248.31
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 8.38s
                        Total time: 15059.52s
                               ETA: 900978.8s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.545s, learning 0.178s)
               Value function loss: 5.6122
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 290.07
               Mean episode length: 247.01
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 8.72s
                        Total time: 15068.24s
                               ETA: 900943.5s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.438s, learning 0.181s)
               Value function loss: 5.2864
                    Surrogate loss: 0.0061
             Mean action noise std: 0.76
                       Mean reward: 291.44
               Mean episode length: 247.72
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 8.62s
                        Total time: 15076.86s
                               ETA: 900902.0s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.470s, learning 0.223s)
               Value function loss: 5.2314
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 282.45
               Mean episode length: 245.12
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 8.69s
                        Total time: 15085.56s
                               ETA: 900865.0s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.719s, learning 0.251s)
               Value function loss: 6.9485
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 280.48
               Mean episode length: 243.06
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 8.97s
                        Total time: 15094.53s
                               ETA: 900844.6s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.752s, learning 0.229s)
               Value function loss: 6.2621
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 286.31
               Mean episode length: 246.50
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 8.98s
                        Total time: 15103.51s
                               ETA: 900824.7s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.379s, learning 0.170s)
               Value function loss: 6.2760
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 287.61
               Mean episode length: 248.28
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 8.55s
                        Total time: 15112.05s
                               ETA: 900779.2s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.653s, learning 0.197s)
               Value function loss: 5.9062
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 286.77
               Mean episode length: 248.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 8.85s
                        Total time: 15120.90s
                               ETA: 900751.6s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.596s, learning 0.166s)
               Value function loss: 5.2093
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 283.83
               Mean episode length: 246.72
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 8.76s
                        Total time: 15129.67s
                               ETA: 900718.8s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.695s, learning 0.179s)
               Value function loss: 5.1498
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 286.20
               Mean episode length: 248.38
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 8.87s
                        Total time: 15138.54s
                               ETA: 900692.8s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.606s, learning 0.174s)
               Value function loss: 5.5673
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 282.77
               Mean episode length: 244.95
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 8.78s
                        Total time: 15147.32s
                               ETA: 900661.1s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.409s, learning 0.165s)
               Value function loss: 6.2032
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 289.62
               Mean episode length: 246.92
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 8.57s
                        Total time: 15155.89s
                               ETA: 900617.3s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.323s, learning 0.206s)
               Value function loss: 4.5199
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 290.95
               Mean episode length: 248.39
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 8.53s
                        Total time: 15164.42s
                               ETA: 900570.8s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.584s, learning 0.182s)
               Value function loss: 4.5439
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 288.54
               Mean episode length: 246.88
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 8.77s
                        Total time: 15173.19s
                               ETA: 900538.5s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.462s, learning 0.179s)
               Value function loss: 4.3533
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 282.18
               Mean episode length: 245.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 8.64s
                        Total time: 15181.83s
                               ETA: 900498.6s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.575s, learning 0.180s)
               Value function loss: 3.4192
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 282.91
               Mean episode length: 243.24
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 8.76s
                        Total time: 15190.59s
                               ETA: 900465.7s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.438s, learning 0.172s)
               Value function loss: 3.9483
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 282.01
               Mean episode length: 242.70
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 8.61s
                        Total time: 15199.20s
                               ETA: 900424.2s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.623s, learning 0.208s)
               Value function loss: 4.4935
                    Surrogate loss: -0.0045
             Mean action noise std: 0.76
                       Mean reward: 285.48
               Mean episode length: 245.15
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 8.83s
                        Total time: 15208.03s
                               ETA: 900395.8s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.527s, learning 0.200s)
               Value function loss: 5.8427
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 286.12
               Mean episode length: 243.09
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 8.73s
                        Total time: 15216.75s
                               ETA: 900361.3s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.376s, learning 0.170s)
               Value function loss: 5.1361
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 278.59
               Mean episode length: 238.51
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 8.55s
                        Total time: 15225.30s
                               ETA: 900316.1s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.387s, learning 0.182s)
               Value function loss: 5.4724
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 287.49
               Mean episode length: 243.37
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 8.57s
                        Total time: 15233.87s
                               ETA: 900272.3s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.685s, learning 0.229s)
               Value function loss: 4.2097
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 295.31
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 8.91s
                        Total time: 15242.78s
                               ETA: 900248.9s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.401s, learning 0.184s)
               Value function loss: 4.9509
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 288.00
               Mean episode length: 244.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 8.58s
                        Total time: 15251.37s
                               ETA: 900206.1s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.316s, learning 0.274s)
               Value function loss: 4.2665
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 286.72
               Mean episode length: 241.98
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 8.59s
                        Total time: 15259.96s
                               ETA: 900163.7s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.560s, learning 0.204s)
               Value function loss: 4.1679
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 289.97
               Mean episode length: 243.53
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 8.76s
                        Total time: 15268.72s
                               ETA: 900131.5s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.639s, learning 0.170s)
               Value function loss: 3.3699
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 292.68
               Mean episode length: 246.73
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 8.81s
                        Total time: 15277.53s
                               ETA: 900102.1s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.611s, learning 0.171s)
               Value function loss: 3.9804
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 291.35
               Mean episode length: 247.55
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 8.78s
                        Total time: 15286.31s
                               ETA: 900071.0s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.080s, learning 0.183s)
               Value function loss: 4.5000
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 282.95
               Mean episode length: 241.63
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 8.26s
                        Total time: 15294.58s
                               ETA: 900009.5s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.441s, learning 0.189s)
               Value function loss: 5.3148
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 283.13
               Mean episode length: 240.87
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 8.63s
                        Total time: 15303.21s
                               ETA: 899969.6s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.751s, learning 0.176s)
               Value function loss: 5.6963
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 295.45
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 8.93s
                        Total time: 15312.14s
                               ETA: 899947.2s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.501s, learning 0.213s)
               Value function loss: 4.8316
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 295.64
               Mean episode length: 247.06
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 8.71s
                        Total time: 15320.85s
                               ETA: 899912.3s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.384s, learning 0.173s)
               Value function loss: 5.3218
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 293.50
               Mean episode length: 249.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 8.56s
                        Total time: 15329.41s
                               ETA: 899868.2s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.561s, learning 0.193s)
               Value function loss: 5.3759
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 293.82
               Mean episode length: 249.99
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 8.75s
                        Total time: 15338.16s
                               ETA: 899835.7s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.689s, learning 0.168s)
               Value function loss: 5.0185
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 291.32
               Mean episode length: 246.61
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 8.86s
                        Total time: 15347.02s
                               ETA: 899809.3s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.735s, learning 0.165s)
               Value function loss: 5.4526
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 292.62
               Mean episode length: 246.72
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 8.90s
                        Total time: 15355.92s
                               ETA: 899785.4s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.452s, learning 0.182s)
               Value function loss: 5.2709
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 297.45
               Mean episode length: 248.38
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 8.63s
                        Total time: 15364.55s
                               ETA: 899746.0s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.558s, learning 0.174s)
               Value function loss: 5.5727
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 295.56
               Mean episode length: 249.93
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 8.73s
                        Total time: 15373.28s
                               ETA: 899712.3s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.281s, learning 0.176s)
               Value function loss: 6.4761
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 289.01
               Mean episode length: 248.16
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 8.46s
                        Total time: 15381.74s
                               ETA: 899662.5s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.201s, learning 0.170s)
               Value function loss: 5.2436
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 297.89
               Mean episode length: 249.67
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 8.37s
                        Total time: 15390.11s
                               ETA: 899607.8s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.805s, learning 0.172s)
               Value function loss: 6.4575
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 291.40
               Mean episode length: 243.32
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 8.98s
                        Total time: 15399.09s
                               ETA: 899588.5s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.848s, learning 0.170s)
               Value function loss: 5.2302
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 298.92
               Mean episode length: 248.77
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 9.02s
                        Total time: 15408.10s
                               ETA: 899571.6s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.714s, learning 0.170s)
               Value function loss: 5.3125
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 300.44
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 8.88s
                        Total time: 15416.99s
                               ETA: 899547.0s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.499s, learning 0.180s)
               Value function loss: 8.0864
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 297.14
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 8.68s
                        Total time: 15425.67s
                               ETA: 899510.4s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.410s, learning 0.167s)
               Value function loss: 5.1730
                    Surrogate loss: 0.0009
             Mean action noise std: 0.76
                       Mean reward: 300.03
               Mean episode length: 248.35
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 8.58s
                        Total time: 15434.25s
                               ETA: 899468.0s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.332s, learning 0.176s)
               Value function loss: 5.6013
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 296.59
               Mean episode length: 248.06
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 8.51s
                        Total time: 15442.75s
                               ETA: 899421.5s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.379s, learning 0.182s)
               Value function loss: 4.4001
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 291.86
               Mean episode length: 244.73
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 8.56s
                        Total time: 15451.32s
                               ETA: 899378.2s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.461s, learning 0.251s)
               Value function loss: 3.7628
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 288.25
               Mean episode length: 245.01
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 8.71s
                        Total time: 15460.03s
                               ETA: 899343.7s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.191s, learning 0.174s)
               Value function loss: 3.9186
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 284.31
               Mean episode length: 245.36
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 8.37s
                        Total time: 15468.39s
                               ETA: 899289.0s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.457s, learning 0.168s)
               Value function loss: 3.4952
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 283.52
               Mean episode length: 243.17
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 8.62s
                        Total time: 15477.02s
                               ETA: 899249.5s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.299s, learning 0.204s)
               Value function loss: 5.3461
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 292.57
               Mean episode length: 247.10
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 8.50s
                        Total time: 15485.52s
                               ETA: 899203.0s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.688s, learning 0.193s)
               Value function loss: 4.2405
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 294.23
               Mean episode length: 248.50
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 8.88s
                        Total time: 15494.40s
                               ETA: 899178.4s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.465s, learning 0.231s)
               Value function loss: 6.0595
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 293.83
               Mean episode length: 247.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 8.70s
                        Total time: 15503.10s
                               ETA: 899143.1s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.582s, learning 0.201s)
               Value function loss: 3.7604
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 290.33
               Mean episode length: 246.77
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 8.78s
                        Total time: 15511.88s
                               ETA: 899112.9s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.687s, learning 0.173s)
               Value function loss: 4.4915
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 297.57
               Mean episode length: 248.25
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 8.86s
                        Total time: 15520.74s
                               ETA: 899087.2s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.559s, learning 0.180s)
               Value function loss: 4.6150
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 296.05
               Mean episode length: 247.57
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 8.74s
                        Total time: 15529.48s
                               ETA: 899054.5s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.666s, learning 0.178s)
               Value function loss: 3.9606
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 291.46
               Mean episode length: 245.11
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 8.84s
                        Total time: 15538.32s
                               ETA: 899027.9s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.426s, learning 0.172s)
               Value function loss: 4.3084
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 297.63
               Mean episode length: 247.54
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 8.60s
                        Total time: 15546.92s
                               ETA: 898987.1s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.341s, learning 0.174s)
               Value function loss: 4.3768
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 296.90
               Mean episode length: 247.97
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 8.52s
                        Total time: 15555.44s
                               ETA: 898941.5s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.715s, learning 0.194s)
               Value function loss: 4.5062
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 289.81
               Mean episode length: 246.29
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 8.91s
                        Total time: 15564.35s
                               ETA: 898918.7s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.640s, learning 0.177s)
               Value function loss: 5.0192
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 292.34
               Mean episode length: 246.64
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 8.82s
                        Total time: 15573.16s
                               ETA: 898890.7s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.297s, learning 0.176s)
               Value function loss: 5.4849
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 293.78
               Mean episode length: 246.64
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 8.47s
                        Total time: 15581.64s
                               ETA: 898842.8s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.691s, learning 0.182s)
               Value function loss: 5.4265
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 301.19
               Mean episode length: 248.52
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 8.87s
                        Total time: 15590.51s
                               ETA: 898818.0s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.384s, learning 0.190s)
               Value function loss: 4.5174
                    Surrogate loss: 0.0016
             Mean action noise std: 0.76
                       Mean reward: 295.32
               Mean episode length: 248.36
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 8.57s
                        Total time: 15599.08s
                               ETA: 898776.0s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.426s, learning 0.249s)
               Value function loss: 5.5222
                    Surrogate loss: 0.0021
             Mean action noise std: 0.76
                       Mean reward: 293.63
               Mean episode length: 246.32
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 8.68s
                        Total time: 15607.76s
                               ETA: 898739.9s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.481s, learning 0.170s)
               Value function loss: 4.9447
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 290.27
               Mean episode length: 245.43
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 8.65s
                        Total time: 15616.41s
                               ETA: 898702.4s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.434s, learning 0.174s)
               Value function loss: 5.0407
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 298.19
               Mean episode length: 248.31
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 8.61s
                        Total time: 15625.02s
                               ETA: 898662.5s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.523s, learning 0.218s)
               Value function loss: 4.6508
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 291.93
               Mean episode length: 246.69
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 8.74s
                        Total time: 15633.76s
                               ETA: 898630.3s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.493s, learning 0.232s)
               Value function loss: 5.8508
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 301.23
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 8.72s
                        Total time: 15642.48s
                               ETA: 898597.1s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.779s, learning 0.182s)
               Value function loss: 6.4574
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 295.51
               Mean episode length: 245.64
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 8.96s
                        Total time: 15651.44s
                               ETA: 898577.6s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.411s, learning 0.235s)
               Value function loss: 5.4178
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 296.77
               Mean episode length: 248.38
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 8.65s
                        Total time: 15660.09s
                               ETA: 898540.0s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.905s, learning 0.176s)
               Value function loss: 5.3330
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 291.15
               Mean episode length: 245.10
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 9.08s
                        Total time: 15669.17s
                               ETA: 898527.4s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.745s, learning 0.217s)
               Value function loss: 5.6813
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 296.95
               Mean episode length: 246.82
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 8.96s
                        Total time: 15678.13s
                               ETA: 898507.9s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.419s, learning 0.181s)
               Value function loss: 4.8599
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 301.00
               Mean episode length: 248.42
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 8.60s
                        Total time: 15686.73s
                               ETA: 898467.7s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.381s, learning 0.183s)
               Value function loss: 5.5060
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 294.09
               Mean episode length: 244.89
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 8.56s
                        Total time: 15695.30s
                               ETA: 898425.5s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.472s, learning 0.187s)
               Value function loss: 5.1134
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 297.82
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 8.66s
                        Total time: 15703.96s
                               ETA: 898388.8s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.270s, learning 0.182s)
               Value function loss: 5.7593
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 304.34
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 8.45s
                        Total time: 15712.41s
                               ETA: 898340.2s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.955s, learning 0.181s)
               Value function loss: 5.3713
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 293.59
               Mean episode length: 246.64
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 9.14s
                        Total time: 15721.54s
                               ETA: 898330.8s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.541s, learning 0.201s)
               Value function loss: 4.4551
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 285.84
               Mean episode length: 243.47
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 8.74s
                        Total time: 15730.28s
                               ETA: 898298.9s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.775s, learning 0.190s)
               Value function loss: 3.0276
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 291.64
               Mean episode length: 246.83
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 8.97s
                        Total time: 15739.25s
                               ETA: 898279.8s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.632s, learning 0.225s)
               Value function loss: 4.2215
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 301.59
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 8.86s
                        Total time: 15748.11s
                               ETA: 898254.5s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.542s, learning 0.169s)
               Value function loss: 5.8072
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 296.89
               Mean episode length: 246.49
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 8.71s
                        Total time: 15756.82s
                               ETA: 898220.9s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.848s, learning 0.174s)
               Value function loss: 4.6677
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 292.21
               Mean episode length: 243.36
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 9.02s
                        Total time: 15765.84s
                               ETA: 898205.1s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.805s, learning 0.167s)
               Value function loss: 5.2844
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 298.39
               Mean episode length: 248.44
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 8.97s
                        Total time: 15774.81s
                               ETA: 898186.4s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.483s, learning 0.171s)
               Value function loss: 4.9941
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 290.43
               Mean episode length: 244.42
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 8.65s
                        Total time: 15783.47s
                               ETA: 898149.6s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.739s, learning 0.194s)
               Value function loss: 3.6924
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 289.15
               Mean episode length: 242.20
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 8.93s
                        Total time: 15792.40s
                               ETA: 898128.7s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.476s, learning 0.200s)
               Value function loss: 4.0506
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 290.08
               Mean episode length: 243.64
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 8.68s
                        Total time: 15801.08s
                               ETA: 898093.3s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.584s, learning 0.173s)
               Value function loss: 3.5780
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 294.60
               Mean episode length: 247.42
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 8.76s
                        Total time: 15809.83s
                               ETA: 898062.5s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.583s, learning 0.183s)
               Value function loss: 4.0500
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 295.46
               Mean episode length: 247.26
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 8.77s
                        Total time: 15818.60s
                               ETA: 898032.2s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.827s, learning 0.332s)
               Value function loss: 4.0272
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 295.17
               Mean episode length: 247.44
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 9.16s
                        Total time: 15827.76s
                               ETA: 898024.2s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1791 steps/s (collection: 8.926s, learning 0.218s)
               Value function loss: 4.2022
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 290.35
               Mean episode length: 246.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 9.14s
                        Total time: 15836.90s
                               ETA: 898015.4s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.698s, learning 0.222s)
               Value function loss: 4.4660
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 286.30
               Mean episode length: 242.54
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 8.92s
                        Total time: 15845.82s
                               ETA: 897993.9s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.540s, learning 0.194s)
               Value function loss: 4.9633
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 296.28
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 8.73s
                        Total time: 15854.56s
                               ETA: 897961.8s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.912s, learning 0.181s)
               Value function loss: 6.2977
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 292.95
               Mean episode length: 248.21
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 9.09s
                        Total time: 15863.65s
                               ETA: 897950.2s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.840s, learning 0.310s)
               Value function loss: 4.5109
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 288.20
               Mean episode length: 246.41
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 9.15s
                        Total time: 15872.80s
                               ETA: 897941.7s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.768s, learning 0.178s)
               Value function loss: 5.7109
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 293.33
               Mean episode length: 247.09
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 8.95s
                        Total time: 15881.75s
                               ETA: 897921.7s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.684s, learning 0.190s)
               Value function loss: 5.3397
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: 292.75
               Mean episode length: 248.24
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 8.87s
                        Total time: 15890.62s
                               ETA: 897897.7s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.291s, learning 0.206s)
               Value function loss: 5.4387
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 293.64
               Mean episode length: 246.80
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 8.50s
                        Total time: 15899.12s
                               ETA: 897852.4s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.738s, learning 0.274s)
               Value function loss: 5.5036
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 294.19
               Mean episode length: 244.93
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 9.01s
                        Total time: 15908.13s
                               ETA: 897836.1s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.353s, learning 0.181s)
               Value function loss: 6.5474
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 297.02
               Mean episode length: 247.88
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 8.53s
                        Total time: 15916.66s
                               ETA: 897793.0s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.327s, learning 0.176s)
               Value function loss: 6.3181
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 300.93
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 8.50s
                        Total time: 15925.17s
                               ETA: 897748.1s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.350s, learning 0.169s)
               Value function loss: 5.4793
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 294.89
               Mean episode length: 246.68
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 8.52s
                        Total time: 15933.68s
                               ETA: 897704.2s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.372s, learning 0.179s)
               Value function loss: 4.9198
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 296.24
               Mean episode length: 246.67
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 8.55s
                        Total time: 15942.24s
                               ETA: 897662.1s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.580s, learning 0.183s)
               Value function loss: 3.9166
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 296.85
               Mean episode length: 246.23
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 8.76s
                        Total time: 15951.00s
                               ETA: 897632.0s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.639s, learning 0.208s)
               Value function loss: 5.4331
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 287.95
               Mean episode length: 244.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 8.85s
                        Total time: 15959.85s
                               ETA: 897606.6s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.806s, learning 0.179s)
               Value function loss: 5.6678
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 297.08
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 8.98s
                        Total time: 15968.83s
                               ETA: 897589.0s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.548s, learning 0.206s)
               Value function loss: 5.5644
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 292.98
               Mean episode length: 247.84
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 8.75s
                        Total time: 15977.58s
                               ETA: 897558.4s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.758s, learning 0.227s)
               Value function loss: 7.1537
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 295.97
               Mean episode length: 248.16
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 8.99s
                        Total time: 15986.57s
                               ETA: 897540.8s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.631s, learning 0.173s)
               Value function loss: 4.1319
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 289.75
               Mean episode length: 245.50
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 8.80s
                        Total time: 15995.37s
                               ETA: 897513.2s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.707s, learning 0.196s)
               Value function loss: 5.0246
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 285.45
               Mean episode length: 243.75
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 8.90s
                        Total time: 16004.28s
                               ETA: 897491.0s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.325s, learning 0.179s)
               Value function loss: 3.0572
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 288.88
               Mean episode length: 244.18
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 8.50s
                        Total time: 16012.78s
                               ETA: 897446.5s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.657s, learning 0.184s)
               Value function loss: 5.0598
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 290.89
               Mean episode length: 246.24
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 8.84s
                        Total time: 16021.62s
                               ETA: 897420.9s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.317s, learning 0.172s)
               Value function loss: 4.9838
                    Surrogate loss: 0.0044
             Mean action noise std: 0.76
                       Mean reward: 289.99
               Mean episode length: 246.56
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 8.49s
                        Total time: 16030.11s
                               ETA: 897375.7s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.758s, learning 0.211s)
               Value function loss: 4.6811
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 295.61
               Mean episode length: 248.82
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 8.97s
                        Total time: 16039.08s
                               ETA: 897357.3s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.376s, learning 0.219s)
               Value function loss: 4.7822
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 290.44
               Mean episode length: 247.03
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 8.59s
                        Total time: 16047.68s
                               ETA: 897318.0s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.258s, learning 0.200s)
               Value function loss: 5.0035
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 292.74
               Mean episode length: 247.82
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 8.46s
                        Total time: 16056.13s
                               ETA: 897271.2s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.733s, learning 0.227s)
               Value function loss: 4.4766
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 293.50
               Mean episode length: 246.86
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 8.96s
                        Total time: 16065.09s
                               ETA: 897252.4s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.476s, learning 0.171s)
               Value function loss: 4.7509
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 286.28
               Mean episode length: 243.80
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 8.65s
                        Total time: 16073.74s
                               ETA: 897216.1s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.500s, learning 0.255s)
               Value function loss: 3.8381
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 292.25
               Mean episode length: 248.09
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 8.75s
                        Total time: 16082.50s
                               ETA: 897185.9s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.631s, learning 0.176s)
               Value function loss: 4.3192
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 288.19
               Mean episode length: 246.26
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 8.81s
                        Total time: 16091.30s
                               ETA: 897158.6s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.652s, learning 0.269s)
               Value function loss: 4.0615
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 285.14
               Mean episode length: 243.18
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 8.92s
                        Total time: 16100.22s
                               ETA: 897137.7s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.613s, learning 0.168s)
               Value function loss: 4.2014
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 290.76
               Mean episode length: 244.91
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 8.78s
                        Total time: 16109.01s
                               ETA: 897109.1s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.837s, learning 0.194s)
               Value function loss: 4.6379
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 292.47
               Mean episode length: 247.99
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 9.03s
                        Total time: 16118.04s
                               ETA: 897094.3s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.504s, learning 0.188s)
               Value function loss: 5.2337
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 286.36
               Mean episode length: 247.05
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 8.69s
                        Total time: 16126.73s
                               ETA: 897060.7s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.660s, learning 0.172s)
               Value function loss: 6.4442
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 284.95
               Mean episode length: 244.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 8.83s
                        Total time: 16135.56s
                               ETA: 897034.9s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.623s, learning 0.182s)
               Value function loss: 5.9923
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 284.13
               Mean episode length: 247.39
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 8.81s
                        Total time: 16144.37s
                               ETA: 897007.6s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.539s, learning 0.204s)
               Value function loss: 5.8870
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 284.54
               Mean episode length: 247.98
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 8.74s
                        Total time: 16153.11s
                               ETA: 896976.9s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.941s, learning 0.199s)
               Value function loss: 6.2104
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 286.37
               Mean episode length: 247.98
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 9.14s
                        Total time: 16162.25s
                               ETA: 896968.2s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.437s, learning 0.187s)
               Value function loss: 5.4770
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 286.45
               Mean episode length: 246.09
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 8.62s
                        Total time: 16170.87s
                               ETA: 896931.0s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.192s, learning 0.194s)
               Value function loss: 6.1610
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 290.23
               Mean episode length: 247.36
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 9.39s
                        Total time: 16180.26s
                               ETA: 896936.0s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.268s, learning 0.175s)
               Value function loss: 7.3282
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 287.93
               Mean episode length: 245.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 8.44s
                        Total time: 16188.70s
                               ETA: 896888.7s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.286s, learning 0.192s)
               Value function loss: 5.9679
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 284.62
               Mean episode length: 246.05
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 8.48s
                        Total time: 16197.18s
                               ETA: 896843.4s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.565s, learning 0.201s)
               Value function loss: 6.2395
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 288.56
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 8.77s
                        Total time: 16205.94s
                               ETA: 896814.1s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.379s, learning 0.213s)
               Value function loss: 5.8645
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 287.56
               Mean episode length: 248.03
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 8.59s
                        Total time: 16214.54s
                               ETA: 896775.3s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.721s, learning 0.215s)
               Value function loss: 5.2430
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 283.43
               Mean episode length: 245.21
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 8.94s
                        Total time: 16223.47s
                               ETA: 896755.4s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.832s, learning 0.176s)
               Value function loss: 5.2307
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 275.85
               Mean episode length: 241.22
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 9.01s
                        Total time: 16232.48s
                               ETA: 896739.6s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.635s, learning 0.207s)
               Value function loss: 5.5729
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 283.06
               Mean episode length: 246.01
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 8.84s
                        Total time: 16241.32s
                               ETA: 896714.5s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.529s, learning 0.192s)
               Value function loss: 6.1795
                    Surrogate loss: 0.0025
             Mean action noise std: 0.76
                       Mean reward: 284.82
               Mean episode length: 248.33
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 8.72s
                        Total time: 16250.04s
                               ETA: 896682.9s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.731s, learning 0.187s)
               Value function loss: 5.4218
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 284.14
               Mean episode length: 246.96
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 8.92s
                        Total time: 16258.96s
                               ETA: 896662.1s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.461s, learning 0.164s)
               Value function loss: 4.6484
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 277.48
               Mean episode length: 240.76
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 8.62s
                        Total time: 16267.59s
                               ETA: 896625.2s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.725s, learning 0.176s)
               Value function loss: 4.3428
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 273.54
               Mean episode length: 240.52
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 8.90s
                        Total time: 16276.49s
                               ETA: 896603.5s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.645s, learning 0.183s)
               Value function loss: 4.0436
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 278.80
               Mean episode length: 243.21
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 8.83s
                        Total time: 16285.32s
                               ETA: 896577.8s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.750s, learning 0.264s)
               Value function loss: 5.3053
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 276.11
               Mean episode length: 238.74
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 9.01s
                        Total time: 16294.33s
                               ETA: 896562.4s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.470s, learning 0.180s)
               Value function loss: 4.2263
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 279.15
               Mean episode length: 243.67
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 8.65s
                        Total time: 16302.98s
                               ETA: 896526.9s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.402s, learning 0.175s)
               Value function loss: 5.9184
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 282.67
               Mean episode length: 244.26
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 8.58s
                        Total time: 16311.56s
                               ETA: 896487.5s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.532s, learning 0.189s)
               Value function loss: 4.2062
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 278.31
               Mean episode length: 241.40
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 8.72s
                        Total time: 16320.28s
                               ETA: 896456.0s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.842s, learning 0.176s)
               Value function loss: 5.8647
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 281.51
               Mean episode length: 245.15
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 9.02s
                        Total time: 16329.30s
                               ETA: 896440.9s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.239s, learning 0.196s)
               Value function loss: 5.0580
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 278.97
               Mean episode length: 245.25
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 8.43s
                        Total time: 16337.73s
                               ETA: 896393.7s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.816s, learning 0.180s)
               Value function loss: 5.1837
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 278.41
               Mean episode length: 246.94
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 9.00s
                        Total time: 16346.73s
                               ETA: 896377.4s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.562s, learning 0.203s)
               Value function loss: 4.2886
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 276.51
               Mean episode length: 242.35
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 8.77s
                        Total time: 16355.49s
                               ETA: 896348.5s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.531s, learning 0.187s)
               Value function loss: 4.6981
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 271.85
               Mean episode length: 239.12
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 8.72s
                        Total time: 16364.21s
                               ETA: 896316.9s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.446s, learning 0.238s)
               Value function loss: 3.8470
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 278.81
               Mean episode length: 243.67
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 8.68s
                        Total time: 16372.89s
                               ETA: 896283.6s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.212s, learning 0.197s)
               Value function loss: 4.6091
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 282.99
               Mean episode length: 245.55
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 8.41s
                        Total time: 16381.30s
                               ETA: 896235.2s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.465s, learning 0.174s)
               Value function loss: 4.3714
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 275.95
               Mean episode length: 242.70
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 8.64s
                        Total time: 16389.94s
                               ETA: 896199.4s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.421s, learning 0.182s)
               Value function loss: 4.6365
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 279.10
               Mean episode length: 245.79
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 8.60s
                        Total time: 16398.54s
                               ETA: 896161.7s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.804s, learning 0.207s)
               Value function loss: 5.5649
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 282.44
               Mean episode length: 248.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 9.01s
                        Total time: 16407.55s
                               ETA: 896146.3s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.586s, learning 0.174s)
               Value function loss: 4.9960
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 286.25
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 8.76s
                        Total time: 16416.32s
                               ETA: 896117.3s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.636s, learning 0.210s)
               Value function loss: 5.8215
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 273.95
               Mean episode length: 240.98
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 8.85s
                        Total time: 16425.16s
                               ETA: 896092.9s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.562s, learning 0.206s)
               Value function loss: 5.5863
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 274.74
               Mean episode length: 243.69
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 8.77s
                        Total time: 16433.93s
                               ETA: 896064.3s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.949s, learning 0.178s)
               Value function loss: 5.0808
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 282.55
               Mean episode length: 247.54
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 9.13s
                        Total time: 16443.06s
                               ETA: 896055.3s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.682s, learning 0.180s)
               Value function loss: 4.6981
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 280.02
               Mean episode length: 249.52
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 8.86s
                        Total time: 16451.92s
                               ETA: 896031.8s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.447s, learning 0.261s)
               Value function loss: 5.6991
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 274.41
               Mean episode length: 246.86
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 8.71s
                        Total time: 16460.63s
                               ETA: 896000.0s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.338s, learning 0.224s)
               Value function loss: 5.1404
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 275.23
               Mean episode length: 246.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 8.56s
                        Total time: 16469.19s
                               ETA: 895960.3s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.534s, learning 0.248s)
               Value function loss: 5.9792
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 276.20
               Mean episode length: 247.79
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 8.78s
                        Total time: 16477.97s
                               ETA: 895932.6s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.626s, learning 0.305s)
               Value function loss: 4.9376
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 281.45
               Mean episode length: 249.94
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 8.93s
                        Total time: 16486.90s
                               ETA: 895913.0s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.798s, learning 0.252s)
               Value function loss: 5.1078
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 281.62
               Mean episode length: 249.94
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 9.05s
                        Total time: 16495.95s
                               ETA: 895899.8s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.521s, learning 0.179s)
               Value function loss: 4.6480
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 281.58
               Mean episode length: 247.27
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 8.70s
                        Total time: 16504.65s
                               ETA: 895867.7s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.715s, learning 0.226s)
               Value function loss: 4.8917
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 280.44
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 8.94s
                        Total time: 16513.59s
                               ETA: 895848.7s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.348s, learning 0.191s)
               Value function loss: 5.1134
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 277.60
               Mean episode length: 246.42
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 8.54s
                        Total time: 16522.13s
                               ETA: 895807.8s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.514s, learning 0.180s)
               Value function loss: 5.1288
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 280.73
               Mean episode length: 248.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 8.69s
                        Total time: 16530.82s
                               ETA: 895775.4s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.532s, learning 0.191s)
               Value function loss: 5.1826
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 278.00
               Mean episode length: 247.38
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 8.72s
                        Total time: 16539.55s
                               ETA: 895744.7s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.889s, learning 0.173s)
               Value function loss: 4.8728
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 270.27
               Mean episode length: 243.43
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 9.06s
                        Total time: 16548.61s
                               ETA: 895732.3s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.547s, learning 0.227s)
               Value function loss: 3.7714
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 268.59
               Mean episode length: 246.05
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 8.77s
                        Total time: 16557.38s
                               ETA: 895704.3s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.710s, learning 0.297s)
               Value function loss: 4.5430
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 272.25
               Mean episode length: 248.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 9.01s
                        Total time: 16566.39s
                               ETA: 895688.9s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.704s, learning 0.173s)
               Value function loss: 3.3224
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 270.81
               Mean episode length: 244.23
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 8.88s
                        Total time: 16575.27s
                               ETA: 895666.5s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.485s, learning 0.176s)
               Value function loss: 5.8136
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 269.32
               Mean episode length: 244.20
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 8.66s
                        Total time: 16583.93s
                               ETA: 895632.5s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.685s, learning 0.199s)
               Value function loss: 4.4730
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 269.54
               Mean episode length: 244.62
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 8.88s
                        Total time: 16592.81s
                               ETA: 895610.5s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.263s, learning 0.178s)
               Value function loss: 5.4493
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 272.54
               Mean episode length: 246.75
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 8.44s
                        Total time: 16601.25s
                               ETA: 895564.6s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.204s, learning 0.206s)
               Value function loss: 3.7082
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 266.92
               Mean episode length: 243.97
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 8.41s
                        Total time: 16609.66s
                               ETA: 895517.2s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1764 steps/s (collection: 9.084s, learning 0.200s)
               Value function loss: 4.7894
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 267.34
               Mean episode length: 242.01
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 9.28s
                        Total time: 16618.95s
                               ETA: 895516.8s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.752s, learning 0.196s)
               Value function loss: 4.5121
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 278.06
               Mean episode length: 249.58
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 8.95s
                        Total time: 16627.89s
                               ETA: 895498.3s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.668s, learning 0.180s)
               Value function loss: 3.8110
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 274.66
               Mean episode length: 247.78
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 8.85s
                        Total time: 16636.74s
                               ETA: 895474.5s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.594s, learning 0.229s)
               Value function loss: 3.6616
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 275.07
               Mean episode length: 247.94
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 8.82s
                        Total time: 16645.56s
                               ETA: 895449.3s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.284s, learning 0.204s)
               Value function loss: 4.0679
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 265.49
               Mean episode length: 241.68
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 8.49s
                        Total time: 16654.05s
                               ETA: 895406.1s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.666s, learning 0.204s)
               Value function loss: 4.1699
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 268.80
               Mean episode length: 242.45
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 8.87s
                        Total time: 16662.92s
                               ETA: 895383.5s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.477s, learning 0.211s)
               Value function loss: 4.2473
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 271.07
               Mean episode length: 244.09
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 8.69s
                        Total time: 16671.61s
                               ETA: 895351.2s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.864s, learning 0.189s)
               Value function loss: 5.3296
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 263.99
               Mean episode length: 240.82
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 9.05s
                        Total time: 16680.66s
                               ETA: 895338.5s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.211s, learning 0.199s)
               Value function loss: 5.7020
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 271.28
               Mean episode length: 244.39
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 8.41s
                        Total time: 16689.07s
                               ETA: 895291.3s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.753s, learning 0.199s)
               Value function loss: 5.3340
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 271.26
               Mean episode length: 246.48
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 8.95s
                        Total time: 16698.03s
                               ETA: 895273.2s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.457s, learning 0.181s)
               Value function loss: 5.2136
                    Surrogate loss: -0.0001
             Mean action noise std: 0.76
                       Mean reward: 272.10
               Mean episode length: 244.09
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 8.64s
                        Total time: 16706.66s
                               ETA: 895238.2s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.535s, learning 0.177s)
               Value function loss: 4.2174
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 276.94
               Mean episode length: 247.68
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 8.71s
                        Total time: 16715.38s
                               ETA: 895207.3s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.640s, learning 0.216s)
               Value function loss: 4.5047
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 277.76
               Mean episode length: 249.97
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 8.86s
                        Total time: 16724.23s
                               ETA: 895184.1s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.826s, learning 0.194s)
               Value function loss: 4.5192
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 272.44
               Mean episode length: 247.84
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 9.02s
                        Total time: 16733.25s
                               ETA: 895169.6s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.713s, learning 0.191s)
               Value function loss: 5.3365
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 269.56
               Mean episode length: 244.29
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 8.90s
                        Total time: 16742.16s
                               ETA: 895149.1s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.499s, learning 0.209s)
               Value function loss: 6.6640
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 273.20
               Mean episode length: 248.25
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 8.71s
                        Total time: 16750.86s
                               ETA: 895118.0s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.349s, learning 0.247s)
               Value function loss: 5.4337
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 268.52
               Mean episode length: 245.99
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 8.60s
                        Total time: 16759.46s
                               ETA: 895081.0s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.535s, learning 0.190s)
               Value function loss: 4.9983
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 272.97
               Mean episode length: 248.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 8.72s
                        Total time: 16768.18s
                               ETA: 895050.9s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.771s, learning 0.192s)
               Value function loss: 5.7587
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 271.24
               Mean episode length: 248.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 8.96s
                        Total time: 16777.15s
                               ETA: 895033.5s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.371s, learning 0.179s)
               Value function loss: 5.6617
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 273.41
               Mean episode length: 249.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 8.55s
                        Total time: 16785.70s
                               ETA: 894994.0s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.302s, learning 0.189s)
               Value function loss: 5.5208
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 272.95
               Mean episode length: 247.05
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 8.49s
                        Total time: 16794.19s
                               ETA: 894951.5s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.262s, learning 0.210s)
               Value function loss: 6.6956
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 274.78
               Mean episode length: 248.36
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 8.47s
                        Total time: 16802.66s
                               ETA: 894908.0s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.427s, learning 0.185s)
               Value function loss: 6.7715
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 276.56
               Mean episode length: 249.17
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 8.61s
                        Total time: 16811.27s
                               ETA: 894872.0s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.089s, learning 0.205s)
               Value function loss: 6.2368
                    Surrogate loss: 0.0129
             Mean action noise std: 0.76
                       Mean reward: 271.08
               Mean episode length: 246.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 16.29s
                        Total time: 16827.57s
                               ETA: 895244.8s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.637s, learning 0.233s)
               Value function loss: 4.7420
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 263.74
               Mean episode length: 240.71
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 16.87s
                        Total time: 16844.44s
                               ETA: 895647.7s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.794s, learning 0.213s)
               Value function loss: 3.5351
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 264.98
               Mean episode length: 242.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 17.01s
                        Total time: 16861.44s
                               ETA: 896057.4s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.022s, learning 0.223s)
               Value function loss: 4.8326
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 273.43
               Mean episode length: 248.52
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 17.25s
                        Total time: 16878.69s
                               ETA: 896479.4s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.365s, learning 0.171s)
               Value function loss: 5.6805
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 272.35
               Mean episode length: 246.50
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 16.54s
                        Total time: 16895.22s
                               ETA: 896863.2s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.565s, learning 0.210s)
               Value function loss: 5.0144
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 265.99
               Mean episode length: 241.69
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 16.78s
                        Total time: 16912.00s
                               ETA: 897259.3s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.270s, learning 0.206s)
               Value function loss: 4.5265
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 268.76
               Mean episode length: 244.60
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 16.48s
                        Total time: 16928.47s
                               ETA: 897639.0s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.236s, learning 0.218s)
               Value function loss: 5.2905
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 269.93
               Mean episode length: 241.92
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 16.45s
                        Total time: 16944.93s
                               ETA: 898017.2s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.337s, learning 0.320s)
               Value function loss: 4.7432
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 265.56
               Mean episode length: 241.35
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 16.66s
                        Total time: 16961.59s
                               ETA: 898405.7s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.877s, learning 0.272s)
               Value function loss: 3.9374
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 268.56
               Mean episode length: 246.83
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 17.15s
                        Total time: 16978.74s
                               ETA: 898819.9s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.605s, learning 0.205s)
               Value function loss: 3.7885
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 268.79
               Mean episode length: 244.42
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 16.81s
                        Total time: 16995.55s
                               ETA: 899215.6s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.404s, learning 0.224s)
               Value function loss: 3.9200
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 274.48
               Mean episode length: 248.50
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 16.63s
                        Total time: 17012.17s
                               ETA: 899601.2s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.671s, learning 0.216s)
               Value function loss: 3.5014
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 276.42
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 16.89s
                        Total time: 17029.06s
                               ETA: 900000.1s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.779s, learning 0.198s)
               Value function loss: 3.7152
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 274.50
               Mean episode length: 248.01
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 16.98s
                        Total time: 17046.04s
                               ETA: 900403.3s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.527s, learning 0.195s)
               Value function loss: 5.0583
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 272.81
               Mean episode length: 244.61
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 16.72s
                        Total time: 17062.76s
                               ETA: 900792.6s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.780s, learning 0.171s)
               Value function loss: 6.4418
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 268.53
               Mean episode length: 246.77
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 16.95s
                        Total time: 17079.71s
                               ETA: 901193.6s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.912s, learning 0.184s)
               Value function loss: 6.8423
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 271.10
               Mean episode length: 249.39
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 17.10s
                        Total time: 17096.81s
                               ETA: 901601.7s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.679s, learning 0.202s)
               Value function loss: 5.3743
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 270.51
               Mean episode length: 246.19
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 16.88s
                        Total time: 17113.69s
                               ETA: 901998.1s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.624s, learning 0.187s)
               Value function loss: 5.8713
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 271.53
               Mean episode length: 248.52
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 16.81s
                        Total time: 17130.50s
                               ETA: 902390.3s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.653s, learning 0.183s)
               Value function loss: 4.7611
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 272.16
               Mean episode length: 248.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 16.84s
                        Total time: 17147.34s
                               ETA: 902783.3s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.737s, learning 0.243s)
               Value function loss: 4.8301
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 268.68
               Mean episode length: 248.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 16.98s
                        Total time: 17164.32s
                               ETA: 903183.5s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.528s, learning 0.198s)
               Value function loss: 4.9947
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 269.31
               Mean episode length: 246.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 16.73s
                        Total time: 17181.04s
                               ETA: 903570.0s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.588s, learning 0.215s)
               Value function loss: 6.7151
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 260.49
               Mean episode length: 241.85
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 16.80s
                        Total time: 17197.85s
                               ETA: 903960.0s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.868s, learning 0.171s)
               Value function loss: 5.9207
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 265.50
               Mean episode length: 244.05
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 17.04s
                        Total time: 17214.88s
                               ETA: 904362.0s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.040s, learning 0.196s)
               Value function loss: 5.0260
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 273.26
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 17.24s
                        Total time: 17232.12s
                               ETA: 904773.9s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.951s, learning 0.247s)
               Value function loss: 4.8664
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 267.70
               Mean episode length: 246.86
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 17.20s
                        Total time: 17249.32s
                               ETA: 905183.4s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.024s, learning 0.312s)
               Value function loss: 5.4250
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 263.60
               Mean episode length: 240.45
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 17.34s
                        Total time: 17266.65s
                               ETA: 905599.6s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.232s, learning 0.172s)
               Value function loss: 5.5318
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 264.22
               Mean episode length: 241.06
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 17.40s
                        Total time: 17284.06s
                               ETA: 906018.9s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.828s, learning 0.257s)
               Value function loss: 6.2957
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 272.40
               Mean episode length: 249.10
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 17.09s
                        Total time: 17301.14s
                               ETA: 906421.1s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.598s, learning 0.182s)
               Value function loss: 5.4322
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 268.64
               Mean episode length: 246.97
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 16.78s
                        Total time: 17317.92s
                               ETA: 906806.8s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.952s, learning 0.245s)
               Value function loss: 7.2246
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 276.46
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 17.20s
                        Total time: 17335.12s
                               ETA: 907213.9s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.846s, learning 0.188s)
               Value function loss: 4.8512
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 279.08
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 17.03s
                        Total time: 17352.15s
                               ETA: 907612.0s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.132s, learning 0.196s)
               Value function loss: 4.9770
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 274.81
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 17.33s
                        Total time: 17369.48s
                               ETA: 908025.1s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 939 steps/s (collection: 17.236s, learning 0.196s)
               Value function loss: 3.1210
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 273.98
               Mean episode length: 249.70
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 17.43s
                        Total time: 17386.91s
                               ETA: 908443.1s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.031s, learning 0.174s)
               Value function loss: 4.6388
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 275.66
               Mean episode length: 247.70
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 17.21s
                        Total time: 17404.12s
                               ETA: 908848.9s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.720s, learning 0.177s)
               Value function loss: 5.5583
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 274.20
               Mean episode length: 247.90
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 16.90s
                        Total time: 17421.02s
                               ETA: 909238.0s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.803s, learning 0.201s)
               Value function loss: 5.5984
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 275.90
               Mean episode length: 249.25
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 17.00s
                        Total time: 17438.02s
                               ETA: 909632.4s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1072 steps/s (collection: 14.972s, learning 0.298s)
               Value function loss: 5.2338
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 274.28
               Mean episode length: 249.25
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 15.27s
                        Total time: 17453.29s
                               ETA: 909935.9s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.767s, learning 0.178s)
               Value function loss: 4.4128
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 273.10
               Mean episode length: 248.26
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 8.95s
                        Total time: 17462.24s
                               ETA: 909909.5s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.776s, learning 0.186s)
               Value function loss: 5.2148
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 270.91
               Mean episode length: 244.30
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 8.96s
                        Total time: 17471.20s
                               ETA: 909884.0s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.932s, learning 0.193s)
               Value function loss: 5.6138
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 276.04
               Mean episode length: 248.25
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 9.12s
                        Total time: 17480.32s
                               ETA: 909867.0s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.838s, learning 0.190s)
               Value function loss: 4.2384
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 274.86
               Mean episode length: 247.44
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 9.03s
                        Total time: 17489.35s
                               ETA: 909845.0s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.398s, learning 0.244s)
               Value function loss: 4.2676
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 272.72
               Mean episode length: 246.30
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 8.64s
                        Total time: 17497.99s
                               ETA: 909802.9s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.585s, learning 0.169s)
               Value function loss: 3.9117
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 273.24
               Mean episode length: 247.15
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 8.75s
                        Total time: 17506.75s
                               ETA: 909766.7s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.321s, learning 0.204s)
               Value function loss: 4.1293
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 275.38
               Mean episode length: 248.29
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 8.52s
                        Total time: 17515.27s
                               ETA: 909718.5s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.644s, learning 0.170s)
               Value function loss: 4.9897
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 281.87
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 8.81s
                        Total time: 17524.09s
                               ETA: 909685.5s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.815s, learning 0.195s)
               Value function loss: 5.4000
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 276.80
               Mean episode length: 245.93
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 9.01s
                        Total time: 17533.10s
                               ETA: 909662.6s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.759s, learning 0.256s)
               Value function loss: 6.8143
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 279.04
               Mean episode length: 247.55
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 9.01s
                        Total time: 17542.11s
                               ETA: 909640.0s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.538s, learning 0.200s)
               Value function loss: 5.3041
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 284.08
               Mean episode length: 248.75
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 8.74s
                        Total time: 17550.85s
                               ETA: 909603.0s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.446s, learning 0.222s)
               Value function loss: 5.3122
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 280.31
               Mean episode length: 248.09
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 8.67s
                        Total time: 17559.52s
                               ETA: 909562.5s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1780 steps/s (collection: 8.951s, learning 0.252s)
               Value function loss: 5.3151
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 280.74
               Mean episode length: 247.79
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 9.20s
                        Total time: 17568.72s
                               ETA: 909549.8s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.432s, learning 0.201s)
               Value function loss: 5.2291
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 281.55
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 8.63s
                        Total time: 17577.35s
                               ETA: 909507.5s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1768 steps/s (collection: 8.950s, learning 0.317s)
               Value function loss: 5.1886
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 279.93
               Mean episode length: 248.59
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 9.27s
                        Total time: 17586.62s
                               ETA: 909498.0s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.823s, learning 0.180s)
               Value function loss: 7.0937
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 286.40
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 9.00s
                        Total time: 17595.62s
                               ETA: 909474.8s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.721s, learning 0.197s)
               Value function loss: 5.4675
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 279.72
               Mean episode length: 248.12
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 8.92s
                        Total time: 17604.54s
                               ETA: 909447.4s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.370s, learning 0.215s)
               Value function loss: 6.7421
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 279.66
               Mean episode length: 245.26
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 8.59s
                        Total time: 17613.13s
                               ETA: 909402.8s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.381s, learning 0.182s)
               Value function loss: 5.9372
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 282.81
               Mean episode length: 248.07
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 8.56s
                        Total time: 17621.69s
                               ETA: 909357.0s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.712s, learning 0.173s)
               Value function loss: 5.2264
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 284.85
               Mean episode length: 249.85
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 8.88s
                        Total time: 17630.57s
                               ETA: 909327.9s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.465s, learning 0.172s)
               Value function loss: 5.4729
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 285.60
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 8.64s
                        Total time: 17639.21s
                               ETA: 909286.0s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.620s, learning 0.176s)
               Value function loss: 5.3258
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 284.95
               Mean episode length: 249.43
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 8.80s
                        Total time: 17648.01s
                               ETA: 909252.4s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.845s, learning 0.176s)
               Value function loss: 7.0365
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 278.19
               Mean episode length: 245.04
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 9.02s
                        Total time: 17657.03s
                               ETA: 909230.3s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.516s, learning 0.184s)
               Value function loss: 6.1101
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 276.44
               Mean episode length: 243.92
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 8.70s
                        Total time: 17665.73s
                               ETA: 909191.8s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.160s, learning 0.180s)
               Value function loss: 7.9780
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 280.01
               Mean episode length: 246.68
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 9.34s
                        Total time: 17675.07s
                               ETA: 909186.2s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.428s, learning 0.221s)
               Value function loss: 7.6358
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 280.95
               Mean episode length: 245.77
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 8.65s
                        Total time: 17683.72s
                               ETA: 909145.1s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.570s, learning 0.178s)
               Value function loss: 4.6659
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 281.85
               Mean episode length: 245.32
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 8.75s
                        Total time: 17692.46s
                               ETA: 909109.1s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.523s, learning 0.287s)
               Value function loss: 5.9719
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 278.27
               Mean episode length: 243.69
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 8.81s
                        Total time: 17701.27s
                               ETA: 909076.3s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.672s, learning 0.226s)
               Value function loss: 4.7601
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 279.74
               Mean episode length: 246.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 8.90s
                        Total time: 17710.17s
                               ETA: 909048.1s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.595s, learning 0.185s)
               Value function loss: 6.1145
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 280.41
               Mean episode length: 247.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 8.78s
                        Total time: 17718.95s
                               ETA: 909013.8s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.755s, learning 0.202s)
               Value function loss: 4.3653
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 282.26
               Mean episode length: 248.77
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 8.96s
                        Total time: 17727.91s
                               ETA: 908988.6s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.798s, learning 0.175s)
               Value function loss: 5.8043
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 278.42
               Mean episode length: 246.91
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 8.97s
                        Total time: 17736.88s
                               ETA: 908964.3s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.660s, learning 0.175s)
               Value function loss: 6.0366
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 281.63
               Mean episode length: 249.31
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 8.83s
                        Total time: 17745.72s
                               ETA: 908932.9s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.887s, learning 0.188s)
               Value function loss: 5.2958
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 274.96
               Mean episode length: 243.68
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 9.07s
                        Total time: 17754.79s
                               ETA: 908913.8s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.860s, learning 0.208s)
               Value function loss: 3.8573
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 276.79
               Mean episode length: 245.17
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 9.07s
                        Total time: 17763.86s
                               ETA: 908894.4s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.880s, learning 0.197s)
               Value function loss: 4.2126
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 278.60
               Mean episode length: 246.85
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 9.08s
                        Total time: 17772.94s
                               ETA: 908875.4s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.411s, learning 0.180s)
               Value function loss: 3.2968
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 282.02
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 8.59s
                        Total time: 17781.53s
                               ETA: 908831.7s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.863s, learning 0.200s)
               Value function loss: 4.0014
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 277.47
               Mean episode length: 245.50
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 9.06s
                        Total time: 17790.59s
                               ETA: 908812.0s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.611s, learning 0.196s)
               Value function loss: 4.4825
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 272.97
               Mean episode length: 240.36
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 8.81s
                        Total time: 17799.40s
                               ETA: 908779.4s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.422s, learning 0.209s)
               Value function loss: 5.3576
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 285.11
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 8.63s
                        Total time: 17808.03s
                               ETA: 908737.7s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.414s, learning 0.311s)
               Value function loss: 5.1258
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 284.00
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 8.73s
                        Total time: 17816.76s
                               ETA: 908700.9s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.843s, learning 0.229s)
               Value function loss: 4.6294
                    Surrogate loss: 0.0014
             Mean action noise std: 0.76
                       Mean reward: 282.82
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 9.07s
                        Total time: 17825.83s
                               ETA: 908681.9s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.483s, learning 0.205s)
               Value function loss: 5.7673
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 277.17
               Mean episode length: 245.45
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 8.69s
                        Total time: 17834.52s
                               ETA: 908643.2s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.733s, learning 0.196s)
               Value function loss: 4.9705
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 281.89
               Mean episode length: 248.53
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 8.93s
                        Total time: 17843.45s
                               ETA: 908616.8s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.401s, learning 0.188s)
               Value function loss: 4.9874
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 285.82
               Mean episode length: 249.66
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 8.59s
                        Total time: 17852.03s
                               ETA: 908573.1s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.782s, learning 0.222s)
               Value function loss: 3.7874
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 284.31
               Mean episode length: 249.44
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 9.00s
                        Total time: 17861.04s
                               ETA: 908550.7s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.341s, learning 0.190s)
               Value function loss: 5.3024
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 282.13
               Mean episode length: 248.26
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 8.53s
                        Total time: 17869.57s
                               ETA: 908504.1s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.514s, learning 0.240s)
               Value function loss: 5.6399
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 280.28
               Mean episode length: 246.78
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 8.75s
                        Total time: 17878.32s
                               ETA: 908469.0s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.335s, learning 0.181s)
               Value function loss: 5.7678
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 283.94
               Mean episode length: 248.47
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 8.52s
                        Total time: 17886.84s
                               ETA: 908421.8s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.418s, learning 0.194s)
               Value function loss: 5.5809
                    Surrogate loss: 0.0019
             Mean action noise std: 0.76
                       Mean reward: 281.22
               Mean episode length: 246.81
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 8.61s
                        Total time: 17895.45s
                               ETA: 908379.4s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.894s, learning 0.265s)
               Value function loss: 5.2811
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 281.93
               Mean episode length: 247.61
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 9.16s
                        Total time: 17904.61s
                               ETA: 908364.9s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.433s, learning 0.226s)
               Value function loss: 6.0975
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 281.94
               Mean episode length: 247.03
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 8.66s
                        Total time: 17913.27s
                               ETA: 908325.1s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.876s, learning 0.195s)
               Value function loss: 4.2771
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 284.75
               Mean episode length: 248.31
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 9.07s
                        Total time: 17922.34s
                               ETA: 908306.1s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.540s, learning 0.185s)
               Value function loss: 5.8955
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 284.21
               Mean episode length: 249.32
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 8.72s
                        Total time: 17931.07s
                               ETA: 908269.6s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.349s, learning 0.205s)
               Value function loss: 5.3026
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 285.41
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 8.55s
                        Total time: 17939.62s
                               ETA: 908224.5s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.457s, learning 0.226s)
               Value function loss: 5.3403
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 283.84
               Mean episode length: 248.46
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 8.68s
                        Total time: 17948.30s
                               ETA: 908185.9s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.417s, learning 0.184s)
               Value function loss: 4.6810
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 283.73
               Mean episode length: 248.46
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 8.60s
                        Total time: 17956.90s
                               ETA: 908143.2s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.191s)
               Value function loss: 3.7375
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 283.41
               Mean episode length: 248.48
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 8.79s
                        Total time: 17965.69s
                               ETA: 908110.3s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.457s, learning 0.202s)
               Value function loss: 5.1237
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 284.01
               Mean episode length: 248.48
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 8.66s
                        Total time: 17974.35s
                               ETA: 908070.6s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.879s, learning 0.209s)
               Value function loss: 3.9539
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 282.39
               Mean episode length: 248.02
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 9.09s
                        Total time: 17983.44s
                               ETA: 908052.7s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.684s, learning 0.286s)
               Value function loss: 6.4285
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 283.97
               Mean episode length: 248.30
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 8.97s
                        Total time: 17992.41s
                               ETA: 908028.7s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.779s, learning 0.200s)
               Value function loss: 4.1720
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 284.59
               Mean episode length: 247.85
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 8.98s
                        Total time: 18001.39s
                               ETA: 908005.3s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.801s, learning 0.223s)
               Value function loss: 5.5537
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 285.58
               Mean episode length: 248.06
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 9.02s
                        Total time: 18010.41s
                               ETA: 907984.2s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.349s, learning 0.293s)
               Value function loss: 4.6269
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 287.25
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 8.64s
                        Total time: 18019.06s
                               ETA: 907943.8s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.433s, learning 0.179s)
               Value function loss: 5.5737
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 286.51
               Mean episode length: 249.92
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 8.61s
                        Total time: 18027.67s
                               ETA: 907901.9s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.720s, learning 0.206s)
               Value function loss: 5.0360
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 286.39
               Mean episode length: 249.92
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 8.93s
                        Total time: 18036.59s
                               ETA: 907875.9s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.638s, learning 0.191s)
               Value function loss: 5.1662
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 285.78
               Mean episode length: 248.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 8.83s
                        Total time: 18045.42s
                               ETA: 907845.0s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.650s, learning 0.196s)
               Value function loss: 4.2777
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 284.34
               Mean episode length: 246.63
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 8.85s
                        Total time: 18054.27s
                               ETA: 907814.9s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.515s, learning 0.198s)
               Value function loss: 4.8420
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 282.77
               Mean episode length: 245.87
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 8.71s
                        Total time: 18062.98s
                               ETA: 907778.2s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.693s, learning 0.256s)
               Value function loss: 5.9781
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 281.19
               Mean episode length: 243.94
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 8.95s
                        Total time: 18071.93s
                               ETA: 907753.4s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.451s, learning 0.252s)
               Value function loss: 4.6307
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 283.77
               Mean episode length: 246.26
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 8.70s
                        Total time: 18080.63s
                               ETA: 907716.3s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.480s, learning 0.194s)
               Value function loss: 6.4131
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: 288.05
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 8.67s
                        Total time: 18089.31s
                               ETA: 907677.7s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.098s, learning 0.199s)
               Value function loss: 6.0317
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 286.37
               Mean episode length: 248.25
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 8.30s
                        Total time: 18097.60s
                               ETA: 907620.3s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.594s, learning 0.270s)
               Value function loss: 6.2171
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 285.80
               Mean episode length: 248.25
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 8.86s
                        Total time: 18106.47s
                               ETA: 907591.3s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.170s, learning 0.193s)
               Value function loss: 5.6390
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 290.80
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 8.36s
                        Total time: 18114.83s
                               ETA: 907537.3s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.542s, learning 0.246s)
               Value function loss: 4.7011
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 288.25
               Mean episode length: 248.45
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 8.79s
                        Total time: 18123.62s
                               ETA: 907504.6s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.636s, learning 0.213s)
               Value function loss: 5.4179
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 289.28
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 8.85s
                        Total time: 18132.47s
                               ETA: 907474.9s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.624s, learning 0.178s)
               Value function loss: 6.0124
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 286.87
               Mean episode length: 248.08
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 8.80s
                        Total time: 18141.27s
                               ETA: 907443.0s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.633s, learning 0.194s)
               Value function loss: 6.2182
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 285.73
               Mean episode length: 248.08
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 8.83s
                        Total time: 18150.10s
                               ETA: 907412.3s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.445s, learning 0.268s)
               Value function loss: 7.5840
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 285.71
               Mean episode length: 248.17
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 8.71s
                        Total time: 18158.81s
                               ETA: 907375.9s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.319s, learning 0.221s)
               Value function loss: 5.9059
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 283.33
               Mean episode length: 246.18
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 8.54s
                        Total time: 18167.35s
                               ETA: 907330.9s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.579s, learning 0.173s)
               Value function loss: 5.0804
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 286.10
               Mean episode length: 248.01
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 8.75s
                        Total time: 18176.10s
                               ETA: 907296.6s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.498s, learning 0.221s)
               Value function loss: 5.6236
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 288.77
               Mean episode length: 248.34
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 8.72s
                        Total time: 18184.82s
                               ETA: 907260.6s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.406s, learning 0.184s)
               Value function loss: 4.8889
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 289.67
               Mean episode length: 248.39
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 8.59s
                        Total time: 18193.41s
                               ETA: 907218.2s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.818s, learning 0.278s)
               Value function loss: 5.6586
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 292.09
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 9.10s
                        Total time: 18202.51s
                               ETA: 907201.1s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.737s, learning 0.201s)
               Value function loss: 5.5492
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 287.97
               Mean episode length: 248.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 8.94s
                        Total time: 18211.44s
                               ETA: 907176.1s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.638s, learning 0.190s)
               Value function loss: 5.9775
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 286.76
               Mean episode length: 247.66
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 8.83s
                        Total time: 18220.27s
                               ETA: 907145.6s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.715s, learning 0.268s)
               Value function loss: 6.0952
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 286.18
               Mean episode length: 248.19
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 8.98s
                        Total time: 18229.26s
                               ETA: 907122.9s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.544s, learning 0.279s)
               Value function loss: 4.4143
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 285.51
               Mean episode length: 247.99
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 8.82s
                        Total time: 18238.08s
                               ETA: 907092.3s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.359s, learning 0.228s)
               Value function loss: 3.6231
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 283.10
               Mean episode length: 247.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 8.59s
                        Total time: 18246.66s
                               ETA: 907049.9s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.410s, learning 0.195s)
               Value function loss: 4.8712
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 283.07
               Mean episode length: 246.15
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 8.61s
                        Total time: 18255.27s
                               ETA: 907008.4s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.378s, learning 0.177s)
               Value function loss: 5.9929
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 288.42
               Mean episode length: 248.40
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 8.56s
                        Total time: 18263.83s
                               ETA: 906964.5s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.424s, learning 0.175s)
               Value function loss: 4.7520
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 287.11
               Mean episode length: 247.70
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 8.60s
                        Total time: 18272.42s
                               ETA: 906922.9s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.709s, learning 0.178s)
               Value function loss: 5.6820
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 287.93
               Mean episode length: 247.69
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 8.89s
                        Total time: 18281.31s
                               ETA: 906895.5s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.434s, learning 0.182s)
               Value function loss: 5.6160
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 289.03
               Mean episode length: 249.69
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 8.62s
                        Total time: 18289.93s
                               ETA: 906854.7s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.632s, learning 0.239s)
               Value function loss: 5.1408
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 290.25
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 8.87s
                        Total time: 18298.80s
                               ETA: 906826.6s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.535s, learning 0.190s)
               Value function loss: 4.9061
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 289.61
               Mean episode length: 249.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 8.72s
                        Total time: 18307.52s
                               ETA: 906791.3s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.362s, learning 0.173s)
               Value function loss: 4.2044
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 287.58
               Mean episode length: 249.34
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 8.54s
                        Total time: 18316.06s
                               ETA: 906746.6s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.529s, learning 0.201s)
               Value function loss: 4.0403
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 284.91
               Mean episode length: 249.12
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 8.73s
                        Total time: 18324.79s
                               ETA: 906711.6s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.835s, learning 0.195s)
               Value function loss: 3.6639
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 285.21
               Mean episode length: 248.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 9.03s
                        Total time: 18333.82s
                               ETA: 906691.5s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.776s, learning 0.216s)
               Value function loss: 4.8026
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 286.18
               Mean episode length: 247.53
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 8.99s
                        Total time: 18342.81s
                               ETA: 906669.4s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.505s, learning 0.185s)
               Value function loss: 4.5357
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 286.80
               Mean episode length: 248.27
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 8.69s
                        Total time: 18351.50s
                               ETA: 906632.6s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.496s, learning 0.226s)
               Value function loss: 5.5876
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 287.33
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 8.72s
                        Total time: 18360.22s
                               ETA: 906597.3s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.470s, learning 0.194s)
               Value function loss: 6.0288
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 288.89
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 8.66s
                        Total time: 18368.89s
                               ETA: 906559.1s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.579s, learning 0.303s)
               Value function loss: 5.2873
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 288.60
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 8.88s
                        Total time: 18377.77s
                               ETA: 906531.8s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.668s, learning 0.178s)
               Value function loss: 5.5508
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 286.56
               Mean episode length: 249.16
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 8.85s
                        Total time: 18386.62s
                               ETA: 906502.7s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.542s, learning 0.191s)
               Value function loss: 4.8779
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 289.18
               Mean episode length: 248.29
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 8.73s
                        Total time: 18395.35s
                               ETA: 906468.0s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.541s, learning 0.217s)
               Value function loss: 5.1409
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 287.30
               Mean episode length: 248.13
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 8.76s
                        Total time: 18404.11s
                               ETA: 906434.6s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.617s, learning 0.184s)
               Value function loss: 4.9849
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 287.40
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 8.80s
                        Total time: 18412.91s
                               ETA: 906403.3s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1781 steps/s (collection: 8.938s, learning 0.259s)
               Value function loss: 6.1026
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 288.57
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 9.20s
                        Total time: 18422.10s
                               ETA: 906391.6s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.361s, learning 0.205s)
               Value function loss: 6.6366
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 289.18
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 8.57s
                        Total time: 18430.67s
                               ETA: 906348.8s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.303s, learning 0.196s)
               Value function loss: 5.5003
                    Surrogate loss: 0.0025
             Mean action noise std: 0.76
                       Mean reward: 288.50
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 8.50s
                        Total time: 18439.17s
                               ETA: 906302.7s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.639s, learning 0.172s)
               Value function loss: 5.9280
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 288.67
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 8.81s
                        Total time: 18447.98s
                               ETA: 906272.0s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.406s, learning 0.192s)
               Value function loss: 5.7917
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 287.83
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 8.60s
                        Total time: 18456.58s
                               ETA: 906230.9s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.755s, learning 0.192s)
               Value function loss: 4.9297
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 287.88
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 8.95s
                        Total time: 18465.52s
                               ETA: 906206.9s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.761s, learning 0.218s)
               Value function loss: 6.2682
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 285.42
               Mean episode length: 248.15
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 8.98s
                        Total time: 18474.50s
                               ETA: 906184.6s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.536s, learning 0.191s)
               Value function loss: 5.9697
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 287.49
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 8.73s
                        Total time: 18483.23s
                               ETA: 906149.9s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.895s, learning 0.179s)
               Value function loss: 8.2183
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 287.63
               Mean episode length: 249.41
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 9.07s
                        Total time: 18492.31s
                               ETA: 906132.2s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.422s, learning 0.243s)
               Value function loss: 5.5049
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 285.02
               Mean episode length: 248.04
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 8.66s
                        Total time: 18500.97s
                               ETA: 906094.5s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.765s, learning 0.202s)
               Value function loss: 4.9887
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 284.62
               Mean episode length: 248.63
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 8.97s
                        Total time: 18509.94s
                               ETA: 906071.5s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.495s, learning 0.200s)
               Value function loss: 3.9908
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 283.98
               Mean episode length: 247.72
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 8.70s
                        Total time: 18518.63s
                               ETA: 906035.4s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.268s, learning 0.242s)
               Value function loss: 5.3284
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 286.19
               Mean episode length: 248.32
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 8.51s
                        Total time: 18527.14s
                               ETA: 905990.2s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.643s, learning 0.189s)
               Value function loss: 5.8114
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 285.47
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 8.83s
                        Total time: 18535.97s
                               ETA: 905960.7s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.425s, learning 0.177s)
               Value function loss: 5.0525
                    Surrogate loss: -0.0002
             Mean action noise std: 0.76
                       Mean reward: 286.19
               Mean episode length: 248.25
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 8.60s
                        Total time: 18544.57s
                               ETA: 905920.0s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.493s, learning 0.220s)
               Value function loss: 5.2382
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 284.40
               Mean episode length: 246.06
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 8.71s
                        Total time: 18553.29s
                               ETA: 905884.8s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.721s, learning 0.327s)
               Value function loss: 5.1719
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 281.35
               Mean episode length: 245.11
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 9.05s
                        Total time: 18562.34s
                               ETA: 905866.0s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.690s, learning 0.180s)
               Value function loss: 5.6486
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 284.36
               Mean episode length: 248.25
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 8.87s
                        Total time: 18571.21s
                               ETA: 905838.5s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.664s, learning 0.204s)
               Value function loss: 5.6245
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 282.52
               Mean episode length: 246.44
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 8.87s
                        Total time: 18580.07s
                               ETA: 905810.9s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.801s, learning 0.189s)
               Value function loss: 4.4465
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 282.44
               Mean episode length: 246.46
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 8.99s
                        Total time: 18589.06s
                               ETA: 905789.3s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.436s, learning 0.229s)
               Value function loss: 4.7800
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 282.56
               Mean episode length: 248.26
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 8.67s
                        Total time: 18597.73s
                               ETA: 905751.9s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.480s, learning 0.220s)
               Value function loss: 4.2117
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 285.42
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 8.70s
                        Total time: 18606.43s
                               ETA: 905716.2s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.354s, learning 0.191s)
               Value function loss: 6.2105
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 285.63
               Mean episode length: 249.45
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 8.55s
                        Total time: 18614.97s
                               ETA: 905673.0s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.633s, learning 0.166s)
               Value function loss: 5.3589
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 279.82
               Mean episode length: 246.12
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 8.80s
                        Total time: 18623.77s
                               ETA: 905642.2s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.577s, learning 0.174s)
               Value function loss: 5.9656
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 283.39
               Mean episode length: 248.68
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.75s
                        Total time: 18632.52s
                               ETA: 905609.0s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.580s, learning 0.171s)
               Value function loss: 7.0997
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 284.82
               Mean episode length: 248.25
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 8.75s
                        Total time: 18641.27s
                               ETA: 905575.9s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.546s, learning 0.195s)
               Value function loss: 6.0970
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 282.61
               Mean episode length: 248.25
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 8.74s
                        Total time: 18650.02s
                               ETA: 905542.4s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.549s, learning 0.226s)
               Value function loss: 6.7253
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 283.92
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 8.77s
                        Total time: 18658.79s
                               ETA: 905510.5s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.562s, learning 0.229s)
               Value function loss: 5.9608
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 282.71
               Mean episode length: 249.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 8.79s
                        Total time: 18667.58s
                               ETA: 905479.4s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.578s, learning 0.186s)
               Value function loss: 5.6462
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 282.30
               Mean episode length: 249.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 8.76s
                        Total time: 18676.35s
                               ETA: 905447.0s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.422s, learning 0.182s)
               Value function loss: 5.5888
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 280.68
               Mean episode length: 248.06
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 8.60s
                        Total time: 18684.95s
                               ETA: 905406.9s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.531s, learning 0.203s)
               Value function loss: 7.5211
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 279.82
               Mean episode length: 247.60
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 8.73s
                        Total time: 18693.69s
                               ETA: 905373.1s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.657s, learning 0.341s)
               Value function loss: 6.8748
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 281.82
               Mean episode length: 249.82
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 9.00s
                        Total time: 18702.68s
                               ETA: 905352.2s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.385s, learning 0.235s)
               Value function loss: 6.4233
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 281.85
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 8.62s
                        Total time: 18711.30s
                               ETA: 905312.9s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.427s, learning 0.201s)
               Value function loss: 6.8758
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 282.10
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 8.63s
                        Total time: 18719.93s
                               ETA: 905274.1s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.456s, learning 0.173s)
               Value function loss: 6.4163
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 281.01
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 8.63s
                        Total time: 18728.56s
                               ETA: 905235.3s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.477s, learning 0.182s)
               Value function loss: 5.8429
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 275.64
               Mean episode length: 247.11
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 8.66s
                        Total time: 18737.22s
                               ETA: 905198.0s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.382s, learning 0.215s)
               Value function loss: 5.6704
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 278.11
               Mean episode length: 247.11
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 8.60s
                        Total time: 18745.82s
                               ETA: 905157.8s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.573s, learning 0.199s)
               Value function loss: 6.3607
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 281.14
               Mean episode length: 249.36
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 8.77s
                        Total time: 18754.59s
                               ETA: 905126.0s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.469s, learning 0.181s)
               Value function loss: 4.5998
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 285.84
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 8.65s
                        Total time: 18763.24s
                               ETA: 905088.4s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.534s, learning 0.256s)
               Value function loss: 5.9268
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 283.36
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 8.79s
                        Total time: 18772.03s
                               ETA: 905057.6s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.497s, learning 0.182s)
               Value function loss: 5.4166
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 282.23
               Mean episode length: 248.04
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 8.68s
                        Total time: 18780.71s
                               ETA: 905021.4s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.578s, learning 0.205s)
               Value function loss: 4.4122
                    Surrogate loss: 0.0153
             Mean action noise std: 0.76
                       Mean reward: 280.90
               Mean episode length: 246.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 8.78s
                        Total time: 18789.49s
                               ETA: 904990.2s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.501s, learning 0.208s)
               Value function loss: 6.1988
                    Surrogate loss: 0.0028
             Mean action noise std: 0.76
                       Mean reward: 282.49
               Mean episode length: 248.25
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 8.71s
                        Total time: 18798.20s
                               ETA: 904955.5s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.556s, learning 0.172s)
               Value function loss: 4.5899
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 281.24
               Mean episode length: 247.98
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 8.73s
                        Total time: 18806.93s
                               ETA: 904921.7s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.560s, learning 0.193s)
               Value function loss: 6.7342
                    Surrogate loss: -0.0003
             Mean action noise std: 0.76
                       Mean reward: 282.17
               Mean episode length: 247.98
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 8.75s
                        Total time: 18815.68s
                               ETA: 904889.2s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.513s, learning 0.188s)
               Value function loss: 4.5564
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 284.13
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 8.70s
                        Total time: 18824.38s
                               ETA: 904854.2s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.897s, learning 0.166s)
               Value function loss: 6.2772
                    Surrogate loss: 0.0113
             Mean action noise std: 0.76
                       Mean reward: 285.00
               Mean episode length: 249.58
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 9.06s
                        Total time: 18833.44s
                               ETA: 904836.6s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1777 steps/s (collection: 8.895s, learning 0.322s)
               Value function loss: 6.0344
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 281.29
               Mean episode length: 248.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 9.22s
                        Total time: 18842.66s
                               ETA: 904826.4s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.720s, learning 0.187s)
               Value function loss: 6.1316
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 281.39
               Mean episode length: 248.42
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 8.91s
                        Total time: 18851.57s
                               ETA: 904801.4s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.817s, learning 0.204s)
               Value function loss: 5.0595
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 284.37
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 9.02s
                        Total time: 18860.59s
                               ETA: 904781.8s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.587s, learning 0.222s)
               Value function loss: 5.1412
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 283.12
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 8.81s
                        Total time: 18869.40s
                               ETA: 904752.0s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.566s, learning 0.245s)
               Value function loss: 4.3321
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 282.77
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 8.81s
                        Total time: 18878.21s
                               ETA: 904722.5s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.746s, learning 0.180s)
               Value function loss: 6.0588
                    Surrogate loss: 0.0009
             Mean action noise std: 0.76
                       Mean reward: 282.30
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 8.93s
                        Total time: 18887.13s
                               ETA: 904698.4s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.578s, learning 0.179s)
               Value function loss: 5.9860
                    Surrogate loss: 0.0008
             Mean action noise std: 0.76
                       Mean reward: 283.44
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 8.76s
                        Total time: 18895.89s
                               ETA: 904666.2s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.577s, learning 0.173s)
               Value function loss: 6.2069
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 284.17
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 8.75s
                        Total time: 18904.64s
                               ETA: 904633.7s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.603s, learning 0.184s)
               Value function loss: 5.8517
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 284.30
               Mean episode length: 249.85
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 8.79s
                        Total time: 18913.43s
                               ETA: 904603.1s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.258s, learning 0.180s)
               Value function loss: 5.4629
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 281.58
               Mean episode length: 249.63
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 8.44s
                        Total time: 18921.87s
                               ETA: 904555.7s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.717s, learning 0.192s)
               Value function loss: 6.8779
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 276.00
               Mean episode length: 248.20
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 8.91s
                        Total time: 18930.77s
                               ETA: 904530.9s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.635s, learning 0.205s)
               Value function loss: 5.6901
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 274.39
               Mean episode length: 245.94
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 8.84s
                        Total time: 18939.61s
                               ETA: 904502.8s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.639s, learning 0.177s)
               Value function loss: 5.2635
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 278.44
               Mean episode length: 249.47
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 8.82s
                        Total time: 18948.43s
                               ETA: 904473.6s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.398s, learning 0.250s)
               Value function loss: 3.9871
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 278.15
               Mean episode length: 249.21
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 8.65s
                        Total time: 18957.08s
                               ETA: 904436.4s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.482s, learning 0.170s)
               Value function loss: 5.5032
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 279.55
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 8.65s
                        Total time: 18965.73s
                               ETA: 904399.4s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.334s, learning 0.185s)
               Value function loss: 5.6068
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 278.44
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 8.52s
                        Total time: 18974.25s
                               ETA: 904356.1s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.723s, learning 0.284s)
               Value function loss: 6.3063
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 276.14
               Mean episode length: 248.08
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 9.01s
                        Total time: 18983.26s
                               ETA: 904336.1s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.674s, learning 0.250s)
               Value function loss: 4.7383
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 277.49
               Mean episode length: 248.08
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 8.92s
                        Total time: 18992.18s
                               ETA: 904312.1s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.643s, learning 0.176s)
               Value function loss: 4.5499
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 278.09
               Mean episode length: 249.40
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 8.82s
                        Total time: 19001.00s
                               ETA: 904283.2s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.707s, learning 0.215s)
               Value function loss: 4.5986
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 278.37
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 8.92s
                        Total time: 19009.92s
                               ETA: 904259.2s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.885s, learning 0.190s)
               Value function loss: 3.8059
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 278.12
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 9.08s
                        Total time: 19019.00s
                               ETA: 904242.4s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.443s, learning 0.201s)
               Value function loss: 4.7832
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 277.26
               Mean episode length: 249.32
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 8.64s
                        Total time: 19027.64s
                               ETA: 904205.2s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.631s, learning 0.184s)
               Value function loss: 4.1771
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 276.76
               Mean episode length: 249.64
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 8.82s
                        Total time: 19036.45s
                               ETA: 904176.2s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.747s, learning 0.260s)
               Value function loss: 4.4231
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 277.30
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 9.01s
                        Total time: 19045.46s
                               ETA: 904156.3s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.748s, learning 0.196s)
               Value function loss: 4.2266
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 276.52
               Mean episode length: 248.49
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 8.94s
                        Total time: 19054.41s
                               ETA: 904133.4s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.628s, learning 0.182s)
               Value function loss: 3.4709
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 278.11
               Mean episode length: 248.49
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 8.81s
                        Total time: 19063.22s
                               ETA: 904104.1s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.945s, learning 0.196s)
               Value function loss: 3.4490
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 280.78
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 9.14s
                        Total time: 19072.36s
                               ETA: 904090.6s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.534s, learning 0.175s)
               Value function loss: 2.9831
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 281.49
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 8.71s
                        Total time: 19081.06s
                               ETA: 904056.6s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.682s, learning 0.170s)
               Value function loss: 4.4381
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 281.56
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 8.85s
                        Total time: 19089.92s
                               ETA: 904029.4s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.425s, learning 0.168s)
               Value function loss: 3.1774
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 282.44
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 8.59s
                        Total time: 19098.51s
                               ETA: 903990.0s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.297s, learning 0.181s)
               Value function loss: 4.0931
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 278.84
               Mean episode length: 248.95
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 8.48s
                        Total time: 19106.99s
                               ETA: 903945.1s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.196s, learning 0.182s)
               Value function loss: 3.1658
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 278.86
               Mean episode length: 248.85
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 8.38s
                        Total time: 19115.36s
                               ETA: 903895.6s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.361s, learning 0.191s)
               Value function loss: 3.7685
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 279.08
               Mean episode length: 249.86
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 8.55s
                        Total time: 19123.92s
                               ETA: 903854.3s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.243s, learning 0.204s)
               Value function loss: 3.3617
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 277.83
               Mean episode length: 249.86
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 8.45s
                        Total time: 19132.36s
                               ETA: 903808.1s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.416s, learning 0.207s)
               Value function loss: 2.9944
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 277.31
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 8.62s
                        Total time: 19140.99s
                               ETA: 903770.2s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.752s, learning 0.180s)
               Value function loss: 3.0065
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 277.74
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 8.93s
                        Total time: 19149.92s
                               ETA: 903746.9s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.496s, learning 0.208s)
               Value function loss: 3.5166
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 277.65
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 8.70s
                        Total time: 19158.62s
                               ETA: 903712.9s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.229s, learning 0.186s)
               Value function loss: 4.4782
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 277.42
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 8.42s
                        Total time: 19167.04s
                               ETA: 903665.4s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.527s, learning 0.209s)
               Value function loss: 3.5805
                    Surrogate loss: 0.0172
             Mean action noise std: 0.76
                       Mean reward: 275.00
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 8.74s
                        Total time: 19175.77s
                               ETA: 903632.9s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.767s, learning 0.196s)
               Value function loss: 4.4335
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 273.73
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 8.96s
                        Total time: 19184.74s
                               ETA: 903611.2s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.488s, learning 0.195s)
               Value function loss: 4.2713
                    Surrogate loss: -0.0023
             Mean action noise std: 0.76
                       Mean reward: 276.53
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 8.68s
                        Total time: 19193.42s
                               ETA: 903576.3s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.462s, learning 0.176s)
               Value function loss: 4.2114
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 275.34
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 8.64s
                        Total time: 19202.06s
                               ETA: 903539.3s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.707s, learning 0.179s)
               Value function loss: 5.5796
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 270.96
               Mean episode length: 248.52
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 8.89s
                        Total time: 19210.94s
                               ETA: 903514.0s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.520s, learning 0.185s)
               Value function loss: 3.2922
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 272.75
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 8.70s
                        Total time: 19219.65s
                               ETA: 903480.2s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.460s, learning 0.272s)
               Value function loss: 3.9315
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 277.01
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 8.73s
                        Total time: 19228.38s
                               ETA: 903447.7s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.629s, learning 0.181s)
               Value function loss: 4.2519
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 272.88
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 8.81s
                        Total time: 19237.19s
                               ETA: 903418.9s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.739s, learning 0.253s)
               Value function loss: 5.0746
                    Surrogate loss: 0.0051
             Mean action noise std: 0.76
                       Mean reward: 271.35
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 8.99s
                        Total time: 19246.18s
                               ETA: 903398.7s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.977s, learning 0.190s)
               Value function loss: 5.2624
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 272.99
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 9.17s
                        Total time: 19255.35s
                               ETA: 903386.7s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.608s, learning 0.285s)
               Value function loss: 4.3864
                    Surrogate loss: -0.0014
             Mean action noise std: 0.76
                       Mean reward: 270.75
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 8.89s
                        Total time: 19264.24s
                               ETA: 903361.8s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.791s, learning 0.206s)
               Value function loss: 4.5382
                    Surrogate loss: 0.0033
             Mean action noise std: 0.76
                       Mean reward: 272.24
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 9.00s
                        Total time: 19273.24s
                               ETA: 903341.8s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.802s, learning 0.209s)
               Value function loss: 4.8617
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 271.90
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 9.01s
                        Total time: 19282.25s
                               ETA: 903322.5s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.642s, learning 0.212s)
               Value function loss: 3.7683
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 270.05
               Mean episode length: 249.35
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 8.85s
                        Total time: 19291.10s
                               ETA: 903295.9s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.781s, learning 0.268s)
               Value function loss: 4.5988
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 271.79
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 9.05s
                        Total time: 19300.15s
                               ETA: 903278.4s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.708s, learning 0.221s)
               Value function loss: 4.6779
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 270.79
               Mean episode length: 248.15
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 8.93s
                        Total time: 19309.08s
                               ETA: 903255.3s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.696s, learning 0.223s)
               Value function loss: 4.5189
                    Surrogate loss: -0.0001
             Mean action noise std: 0.76
                       Mean reward: 269.44
               Mean episode length: 248.15
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 8.92s
                        Total time: 19318.00s
                               ETA: 903231.7s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.781s, learning 0.181s)
               Value function loss: 5.0702
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 265.55
               Mean episode length: 245.59
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 8.96s
                        Total time: 19326.96s
                               ETA: 903210.1s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.389s, learning 0.171s)
               Value function loss: 3.6334
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 269.31
               Mean episode length: 249.72
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 8.56s
                        Total time: 19335.52s
                               ETA: 903169.9s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.928s, learning 0.174s)
               Value function loss: 2.8091
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 269.00
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 9.10s
                        Total time: 19344.62s
                               ETA: 903154.9s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.810s, learning 0.185s)
               Value function loss: 4.0572
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 267.56
               Mean episode length: 249.87
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 9.00s
                        Total time: 19353.62s
                               ETA: 903134.9s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.112s, learning 0.178s)
               Value function loss: 4.7721
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 268.33
               Mean episode length: 249.87
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 9.29s
                        Total time: 19362.91s
                               ETA: 903128.7s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.478s, learning 0.180s)
               Value function loss: 3.3587
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 268.26
               Mean episode length: 249.61
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 8.66s
                        Total time: 19371.56s
                               ETA: 903093.1s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.691s, learning 0.194s)
               Value function loss: 5.0808
                    Surrogate loss: 0.0101
             Mean action noise std: 0.76
                       Mean reward: 267.06
               Mean episode length: 248.04
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 8.88s
                        Total time: 19380.45s
                               ETA: 903068.1s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.502s, learning 0.178s)
               Value function loss: 4.2663
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 269.59
               Mean episode length: 248.04
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 8.68s
                        Total time: 19389.13s
                               ETA: 903033.5s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.529s, learning 0.257s)
               Value function loss: 4.0389
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 271.34
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 8.79s
                        Total time: 19397.92s
                               ETA: 903003.8s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.842s, learning 0.195s)
               Value function loss: 3.7211
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 265.91
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 9.04s
                        Total time: 19406.95s
                               ETA: 902985.9s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.535s, learning 0.218s)
               Value function loss: 3.3926
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 266.72
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 8.75s
                        Total time: 19415.70s
                               ETA: 902954.7s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.697s, learning 0.201s)
               Value function loss: 3.5953
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 266.16
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 8.90s
                        Total time: 19424.60s
                               ETA: 902930.4s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.590s, learning 0.209s)
               Value function loss: 2.8736
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 260.92
               Mean episode length: 248.56
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 8.80s
                        Total time: 19433.40s
                               ETA: 902901.4s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.547s, learning 0.179s)
               Value function loss: 4.4182
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 265.36
               Mean episode length: 249.74
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 8.73s
                        Total time: 19442.13s
                               ETA: 902869.1s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.575s, learning 0.175s)
               Value function loss: 3.8408
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 267.97
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 8.75s
                        Total time: 19450.88s
                               ETA: 902837.9s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.320s, learning 0.217s)
               Value function loss: 5.4530
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 265.01
               Mean episode length: 248.84
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 8.54s
                        Total time: 19459.41s
                               ETA: 902796.8s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.764s, learning 0.166s)
               Value function loss: 5.1638
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 267.47
               Mean episode length: 249.37
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 8.93s
                        Total time: 19468.34s
                               ETA: 902774.1s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.669s, learning 0.207s)
               Value function loss: 4.3944
                    Surrogate loss: -0.0006
             Mean action noise std: 0.76
                       Mean reward: 268.01
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 8.88s
                        Total time: 19477.22s
                               ETA: 902748.8s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.734s, learning 0.204s)
               Value function loss: 5.4239
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 265.07
               Mean episode length: 247.86
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 8.94s
                        Total time: 19486.16s
                               ETA: 902726.4s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.297s, learning 0.179s)
               Value function loss: 4.4485
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 266.51
               Mean episode length: 248.87
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 8.48s
                        Total time: 19494.63s
                               ETA: 902682.6s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.402s, learning 0.197s)
               Value function loss: 4.5733
                    Surrogate loss: 0.0011
             Mean action noise std: 0.76
                       Mean reward: 263.52
               Mean episode length: 247.14
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 8.60s
                        Total time: 19503.23s
                               ETA: 902644.6s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.443s, learning 0.175s)
               Value function loss: 4.7980
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 266.10
               Mean episode length: 247.84
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 8.62s
                        Total time: 19511.85s
                               ETA: 902607.5s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.449s, learning 0.179s)
               Value function loss: 5.0622
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 268.86
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 8.63s
                        Total time: 19520.48s
                               ETA: 902570.9s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.688s, learning 0.180s)
               Value function loss: 5.5070
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 266.21
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 8.87s
                        Total time: 19529.35s
                               ETA: 902545.3s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.723s, learning 0.168s)
               Value function loss: 4.5038
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 268.28
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 8.89s
                        Total time: 19538.24s
                               ETA: 902520.9s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.408s, learning 0.219s)
               Value function loss: 4.1742
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 268.50
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 8.63s
                        Total time: 19546.86s
                               ETA: 902484.3s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.214s, learning 0.226s)
               Value function loss: 4.5334
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 267.77
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 8.44s
                        Total time: 19555.30s
                               ETA: 902439.0s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.481s, learning 0.321s)
               Value function loss: 4.4834
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 267.48
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 8.80s
                        Total time: 19564.11s
                               ETA: 902410.5s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.667s, learning 0.197s)
               Value function loss: 5.1188
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 265.56
               Mean episode length: 248.66
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 8.86s
                        Total time: 19572.97s
                               ETA: 902384.9s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.551s, learning 0.182s)
               Value function loss: 5.5213
                    Surrogate loss: 0.0024
             Mean action noise std: 0.76
                       Mean reward: 268.80
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 8.73s
                        Total time: 19581.70s
                               ETA: 902353.2s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.404s, learning 0.205s)
               Value function loss: 6.7818
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 265.92
               Mean episode length: 248.77
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 8.61s
                        Total time: 19590.31s
                               ETA: 902315.9s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.857s, learning 0.237s)
               Value function loss: 4.4064
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 265.25
               Mean episode length: 248.57
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 9.09s
                        Total time: 19599.41s
                               ETA: 902301.0s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.724s, learning 0.192s)
               Value function loss: 4.7755
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 263.67
               Mean episode length: 247.84
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 8.92s
                        Total time: 19608.32s
                               ETA: 902277.8s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.502s, learning 0.256s)
               Value function loss: 3.2156
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 261.68
               Mean episode length: 246.07
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 8.76s
                        Total time: 19617.08s
                               ETA: 902247.4s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.543s, learning 0.178s)
               Value function loss: 4.7651
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 263.19
               Mean episode length: 248.03
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 8.72s
                        Total time: 19625.80s
                               ETA: 902215.3s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.952s, learning 0.169s)
               Value function loss: 5.0150
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 264.51
               Mean episode length: 247.71
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 9.12s
                        Total time: 19634.92s
                               ETA: 902201.6s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.440s, learning 0.178s)
               Value function loss: 5.1802
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 269.47
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 8.62s
                        Total time: 19643.54s
                               ETA: 902164.9s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.743s, learning 0.175s)
               Value function loss: 4.3217
                    Surrogate loss: 0.0156
             Mean action noise std: 0.76
                       Mean reward: 263.73
               Mean episode length: 247.71
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 8.92s
                        Total time: 19652.46s
                               ETA: 902141.9s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.611s, learning 0.187s)
               Value function loss: 4.5738
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 259.10
               Mean episode length: 242.68
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 8.80s
                        Total time: 19661.26s
                               ETA: 902113.4s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.553s, learning 0.176s)
               Value function loss: 4.7071
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 261.72
               Mean episode length: 246.52
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 8.73s
                        Total time: 19669.99s
                               ETA: 902081.7s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.437s, learning 0.166s)
               Value function loss: 4.7461
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 264.14
               Mean episode length: 246.69
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 8.60s
                        Total time: 19678.59s
                               ETA: 902044.4s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.328s, learning 0.177s)
               Value function loss: 3.7234
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 262.72
               Mean episode length: 246.91
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 8.51s
                        Total time: 19687.09s
                               ETA: 902002.5s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.540s, learning 0.232s)
               Value function loss: 3.7889
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 262.03
               Mean episode length: 248.67
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 8.77s
                        Total time: 19695.87s
                               ETA: 901972.9s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.712s, learning 0.221s)
               Value function loss: 3.5173
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 266.81
               Mean episode length: 248.64
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 8.93s
                        Total time: 19704.80s
                               ETA: 901950.7s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.500s, learning 0.176s)
               Value function loss: 4.5754
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 266.86
               Mean episode length: 249.38
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 8.68s
                        Total time: 19713.47s
                               ETA: 901916.8s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.342s, learning 0.190s)
               Value function loss: 4.5873
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 261.14
               Mean episode length: 246.66
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 8.53s
                        Total time: 19722.01s
                               ETA: 901876.3s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.532s, learning 0.214s)
               Value function loss: 5.0056
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 262.29
               Mean episode length: 248.30
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 8.75s
                        Total time: 19730.75s
                               ETA: 901845.6s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.671s, learning 0.198s)
               Value function loss: 5.2029
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 261.38
               Mean episode length: 246.54
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 8.87s
                        Total time: 19739.62s
                               ETA: 901820.6s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.065s, learning 0.189s)
               Value function loss: 4.3750
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 259.96
               Mean episode length: 245.80
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 8.25s
                        Total time: 19747.88s
                               ETA: 901767.5s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.573s, learning 0.186s)
               Value function loss: 5.6632
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 265.68
               Mean episode length: 248.36
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 8.76s
                        Total time: 19756.64s
                               ETA: 901737.5s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.509s, learning 0.178s)
               Value function loss: 5.2939
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 269.36
               Mean episode length: 248.77
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 8.69s
                        Total time: 19765.32s
                               ETA: 901704.2s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.709s, learning 0.223s)
               Value function loss: 5.2746
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 264.38
               Mean episode length: 245.83
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 8.93s
                        Total time: 19774.25s
                               ETA: 901682.1s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.554s, learning 0.186s)
               Value function loss: 4.7787
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 260.11
               Mean episode length: 243.15
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 8.74s
                        Total time: 19783.00s
                               ETA: 901651.2s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.528s, learning 0.172s)
               Value function loss: 5.6362
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 256.75
               Mean episode length: 242.99
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 8.70s
                        Total time: 19791.70s
                               ETA: 901618.6s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.176s)
               Value function loss: 4.8663
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 266.84
               Mean episode length: 246.59
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 8.56s
                        Total time: 19800.26s
                               ETA: 901579.8s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.544s, learning 0.187s)
               Value function loss: 5.2420
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 265.56
               Mean episode length: 246.81
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 8.73s
                        Total time: 19808.99s
                               ETA: 901548.6s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.563s, learning 0.280s)
               Value function loss: 4.5162
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 265.66
               Mean episode length: 248.14
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 8.84s
                        Total time: 19817.83s
                               ETA: 901522.5s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.464s, learning 0.210s)
               Value function loss: 4.6808
                    Surrogate loss: 0.0008
             Mean action noise std: 0.76
                       Mean reward: 267.84
               Mean episode length: 249.14
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 8.67s
                        Total time: 19826.51s
                               ETA: 901488.8s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.210s, learning 0.211s)
               Value function loss: 4.7728
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 263.41
               Mean episode length: 248.23
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 8.42s
                        Total time: 19834.93s
                               ETA: 901443.6s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.644s, learning 0.178s)
               Value function loss: 4.4402
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 266.94
               Mean episode length: 249.74
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 8.82s
                        Total time: 19843.75s
                               ETA: 901416.6s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.445s, learning 0.175s)
               Value function loss: 4.9396
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 263.68
               Mean episode length: 247.82
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 8.62s
                        Total time: 19852.37s
                               ETA: 901380.5s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.516s, learning 0.193s)
               Value function loss: 4.0723
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 259.66
               Mean episode length: 243.94
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 8.71s
                        Total time: 19861.08s
                               ETA: 901348.4s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.516s, learning 0.178s)
               Value function loss: 4.4698
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 260.65
               Mean episode length: 246.78
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 8.69s
                        Total time: 19869.77s
                               ETA: 901315.7s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.749s, learning 0.179s)
               Value function loss: 4.5650
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 265.70
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 8.93s
                        Total time: 19878.70s
                               ETA: 901293.6s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.742s, learning 0.182s)
               Value function loss: 3.3574
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 264.41
               Mean episode length: 248.33
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 8.92s
                        Total time: 19887.62s
                               ETA: 901271.4s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.294s, learning 0.185s)
               Value function loss: 4.1043
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 261.50
               Mean episode length: 247.18
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 8.48s
                        Total time: 19896.10s
                               ETA: 901229.0s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.400s, learning 0.173s)
               Value function loss: 3.6778
                    Surrogate loss: 0.0040
             Mean action noise std: 0.76
                       Mean reward: 259.18
               Mean episode length: 242.79
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 8.57s
                        Total time: 19904.68s
                               ETA: 901190.9s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.699s, learning 0.261s)
               Value function loss: 5.1028
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 260.31
               Mean episode length: 244.56
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 8.96s
                        Total time: 19913.64s
                               ETA: 901170.3s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.388s, learning 0.203s)
               Value function loss: 3.9147
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 258.18
               Mean episode length: 248.04
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 8.59s
                        Total time: 19922.23s
                               ETA: 901133.0s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.409s, learning 0.274s)
               Value function loss: 4.5661
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 264.74
               Mean episode length: 248.61
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 8.68s
                        Total time: 19930.91s
                               ETA: 901100.0s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.395s, learning 0.201s)
               Value function loss: 4.3044
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 262.57
               Mean episode length: 248.61
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 8.60s
                        Total time: 19939.51s
                               ETA: 901063.0s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.393s, learning 0.209s)
               Value function loss: 4.2636
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 264.55
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 8.60s
                        Total time: 19948.11s
                               ETA: 901026.3s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.713s, learning 0.223s)
               Value function loss: 3.6189
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 264.13
               Mean episode length: 246.13
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 8.94s
                        Total time: 19957.04s
                               ETA: 901004.8s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.678s, learning 0.171s)
               Value function loss: 3.5537
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 265.10
               Mean episode length: 246.72
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 8.85s
                        Total time: 19965.89s
                               ETA: 900979.3s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.325s, learning 0.197s)
               Value function loss: 3.2508
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 257.90
               Mean episode length: 248.26
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 8.52s
                        Total time: 19974.41s
                               ETA: 900939.1s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.797s, learning 0.174s)
               Value function loss: 4.1543
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 260.93
               Mean episode length: 246.79
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 8.97s
                        Total time: 19983.38s
                               ETA: 900919.1s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.496s, learning 0.180s)
               Value function loss: 4.1182
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 268.56
               Mean episode length: 247.70
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 8.68s
                        Total time: 19992.06s
                               ETA: 900885.9s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.481s, learning 0.168s)
               Value function loss: 4.3955
                    Surrogate loss: -0.0045
             Mean action noise std: 0.76
                       Mean reward: 269.32
               Mean episode length: 248.35
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 8.65s
                        Total time: 20000.71s
                               ETA: 900851.5s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.742s, learning 0.177s)
               Value function loss: 5.1542
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 274.09
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 8.92s
                        Total time: 20009.63s
                               ETA: 900829.3s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.456s, learning 0.169s)
               Value function loss: 4.2845
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 273.32
               Mean episode length: 248.28
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 8.62s
                        Total time: 20018.25s
                               ETA: 900793.8s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.233s, learning 0.177s)
               Value function loss: 5.6331
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 264.00
               Mean episode length: 243.82
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 8.41s
                        Total time: 20026.66s
                               ETA: 900748.7s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.585s, learning 0.176s)
               Value function loss: 5.0741
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 262.73
               Mean episode length: 241.25
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 8.76s
                        Total time: 20035.42s
                               ETA: 900719.4s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.261s, learning 0.198s)
               Value function loss: 5.3057
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 264.54
               Mean episode length: 247.46
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 8.46s
                        Total time: 20043.88s
                               ETA: 900676.6s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.446s, learning 0.168s)
               Value function loss: 4.2303
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 256.74
               Mean episode length: 241.71
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 8.61s
                        Total time: 20052.50s
                               ETA: 900640.8s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.517s, learning 0.172s)
               Value function loss: 5.6833
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 259.11
               Mean episode length: 244.08
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 8.69s
                        Total time: 20061.19s
                               ETA: 900608.3s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.659s, learning 0.174s)
               Value function loss: 5.6012
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 267.72
               Mean episode length: 243.68
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 8.83s
                        Total time: 20070.02s
                               ETA: 900582.4s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.130s, learning 0.184s)
               Value function loss: 5.2225
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 267.30
               Mean episode length: 243.09
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 8.31s
                        Total time: 20078.33s
                               ETA: 900533.1s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.381s, learning 0.270s)
               Value function loss: 4.6813
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 273.85
               Mean episode length: 248.20
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 8.65s
                        Total time: 20086.99s
                               ETA: 900499.0s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.346s, learning 0.188s)
               Value function loss: 4.4667
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 267.72
               Mean episode length: 246.16
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 8.53s
                        Total time: 20095.52s
                               ETA: 900459.7s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.429s, learning 0.208s)
               Value function loss: 4.9485
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 270.11
               Mean episode length: 248.06
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 8.64s
                        Total time: 20104.16s
                               ETA: 900425.1s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.557s, learning 0.202s)
               Value function loss: 4.8892
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 264.44
               Mean episode length: 242.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 8.76s
                        Total time: 20112.92s
                               ETA: 900395.9s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.432s, learning 0.238s)
               Value function loss: 5.4801
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 264.98
               Mean episode length: 243.50
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 8.67s
                        Total time: 20121.59s
                               ETA: 900362.8s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.277s, learning 0.232s)
               Value function loss: 5.6625
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 266.87
               Mean episode length: 244.05
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 8.51s
                        Total time: 20130.10s
                               ETA: 900322.5s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.278s, learning 0.191s)
               Value function loss: 5.8323
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 267.92
               Mean episode length: 244.05
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 8.47s
                        Total time: 20138.57s
                               ETA: 900280.4s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.408s, learning 0.170s)
               Value function loss: 5.2839
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 256.89
               Mean episode length: 238.21
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 8.58s
                        Total time: 20147.14s
                               ETA: 900243.2s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.664s, learning 0.178s)
               Value function loss: 4.4226
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 268.15
               Mean episode length: 245.94
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 8.84s
                        Total time: 20155.99s
                               ETA: 900217.8s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.368s, learning 0.177s)
               Value function loss: 4.9151
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 258.93
               Mean episode length: 234.16
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 8.54s
                        Total time: 20164.53s
                               ETA: 900179.2s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.616s, learning 0.167s)
               Value function loss: 4.4425
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 256.39
               Mean episode length: 232.65
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 8.78s
                        Total time: 20173.31s
                               ETA: 900151.2s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.659s, learning 0.251s)
               Value function loss: 6.4085
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 261.81
               Mean episode length: 240.45
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 8.91s
                        Total time: 20182.22s
                               ETA: 900128.9s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.434s, learning 0.177s)
               Value function loss: 4.0205
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 263.04
               Mean episode length: 241.92
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 8.61s
                        Total time: 20190.83s
                               ETA: 900093.3s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.281s, learning 0.208s)
               Value function loss: 6.2605
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 269.28
               Mean episode length: 247.95
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 8.49s
                        Total time: 20199.32s
                               ETA: 900052.3s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.593s, learning 0.299s)
               Value function loss: 4.4033
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 258.58
               Mean episode length: 240.18
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 8.89s
                        Total time: 20208.21s
                               ETA: 900029.3s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.628s, learning 0.171s)
               Value function loss: 5.6506
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 259.61
               Mean episode length: 244.16
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 8.80s
                        Total time: 20217.01s
                               ETA: 900002.2s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.409s, learning 0.170s)
               Value function loss: 5.1958
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 261.67
               Mean episode length: 241.87
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 8.58s
                        Total time: 20225.59s
                               ETA: 899965.2s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.287s, learning 0.181s)
               Value function loss: 4.8215
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 264.00
               Mean episode length: 244.70
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 8.47s
                        Total time: 20234.06s
                               ETA: 899923.3s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.619s, learning 0.197s)
               Value function loss: 3.6986
                    Surrogate loss: 0.0043
             Mean action noise std: 0.76
                       Mean reward: 256.43
               Mean episode length: 240.68
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 8.82s
                        Total time: 20242.88s
                               ETA: 899897.0s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.571s, learning 0.180s)
               Value function loss: 4.6963
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 257.29
               Mean episode length: 242.22
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 8.75s
                        Total time: 20251.63s
                               ETA: 899867.8s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.640s, learning 0.186s)
               Value function loss: 5.6745
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 252.24
               Mean episode length: 237.82
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 8.83s
                        Total time: 20260.45s
                               ETA: 899842.0s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.805s, learning 0.172s)
               Value function loss: 4.0110
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 247.85
               Mean episode length: 234.18
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 7.98s
                        Total time: 20268.43s
                               ETA: 899778.4s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.516s, learning 0.178s)
               Value function loss: 6.0007
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 254.04
               Mean episode length: 234.30
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 8.69s
                        Total time: 20277.12s
                               ETA: 899746.7s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.306s, learning 0.168s)
               Value function loss: 5.5964
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 255.37
               Mean episode length: 242.64
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 8.47s
                        Total time: 20285.60s
                               ETA: 899705.3s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.409s, learning 0.166s)
               Value function loss: 5.8677
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 249.53
               Mean episode length: 234.05
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 8.57s
                        Total time: 20294.17s
                               ETA: 899668.4s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.673s, learning 0.167s)
               Value function loss: 6.7799
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 252.36
               Mean episode length: 239.26
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 8.84s
                        Total time: 20303.01s
                               ETA: 899643.3s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.480s, learning 0.165s)
               Value function loss: 4.6078
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 250.91
               Mean episode length: 238.15
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 8.65s
                        Total time: 20311.66s
                               ETA: 899609.6s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.577s, learning 0.192s)
               Value function loss: 5.4354
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 261.52
               Mean episode length: 244.88
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 8.77s
                        Total time: 20320.43s
                               ETA: 899581.3s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.595s, learning 0.201s)
               Value function loss: 5.8742
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 259.78
               Mean episode length: 241.05
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 8.80s
                        Total time: 20329.22s
                               ETA: 899554.3s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.540s, learning 0.193s)
               Value function loss: 5.2961
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 259.96
               Mean episode length: 241.46
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 8.73s
                        Total time: 20337.96s
                               ETA: 899524.5s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.408s, learning 0.184s)
               Value function loss: 6.9135
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 261.87
               Mean episode length: 242.18
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 8.59s
                        Total time: 20346.55s
                               ETA: 899488.5s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.822s, learning 0.165s)
               Value function loss: 5.5285
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 265.66
               Mean episode length: 246.10
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 8.99s
                        Total time: 20355.54s
                               ETA: 899470.0s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.737s, learning 0.205s)
               Value function loss: 5.1036
                    Surrogate loss: 0.0049
             Mean action noise std: 0.76
                       Mean reward: 262.80
               Mean episode length: 240.28
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 8.94s
                        Total time: 20364.48s
                               ETA: 899449.5s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.650s, learning 0.174s)
               Value function loss: 5.9150
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 261.78
               Mean episode length: 238.45
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 8.82s
                        Total time: 20373.30s
                               ETA: 899423.7s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.381s, learning 0.169s)
               Value function loss: 5.0314
                    Surrogate loss: -0.0028
             Mean action noise std: 0.76
                       Mean reward: 263.29
               Mean episode length: 242.71
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 8.55s
                        Total time: 20381.85s
                               ETA: 899385.9s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.475s, learning 0.255s)
               Value function loss: 6.2083
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 267.37
               Mean episode length: 244.41
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 8.73s
                        Total time: 20390.58s
                               ETA: 899356.1s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.630s, learning 0.258s)
               Value function loss: 6.3392
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 264.44
               Mean episode length: 244.13
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 8.89s
                        Total time: 20399.47s
                               ETA: 899333.3s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.419s, learning 0.327s)
               Value function loss: 6.0788
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 265.61
               Mean episode length: 245.89
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 8.75s
                        Total time: 20408.21s
                               ETA: 899304.2s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.809s, learning 0.204s)
               Value function loss: 7.3829
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 256.03
               Mean episode length: 237.29
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 9.01s
                        Total time: 20417.23s
                               ETA: 899286.9s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.720s, learning 0.207s)
               Value function loss: 4.8836
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 260.26
               Mean episode length: 239.40
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 8.93s
                        Total time: 20426.15s
                               ETA: 899265.8s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.890s, learning 0.203s)
               Value function loss: 4.6539
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 263.06
               Mean episode length: 241.19
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 9.09s
                        Total time: 20435.25s
                               ETA: 899252.1s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.624s, learning 0.179s)
               Value function loss: 4.5888
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 259.23
               Mean episode length: 238.54
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 8.80s
                        Total time: 20444.05s
                               ETA: 899225.6s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.496s, learning 0.166s)
               Value function loss: 5.8561
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 258.59
               Mean episode length: 237.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 8.66s
                        Total time: 20452.71s
                               ETA: 899192.9s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.669s, learning 0.263s)
               Value function loss: 4.2953
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 265.37
               Mean episode length: 244.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 8.93s
                        Total time: 20461.65s
                               ETA: 899172.1s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.272s, learning 0.180s)
               Value function loss: 5.5040
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 264.94
               Mean episode length: 243.95
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 8.45s
                        Total time: 20470.10s
                               ETA: 899130.2s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.679s, learning 0.189s)
               Value function loss: 4.6095
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 260.98
               Mean episode length: 241.43
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 8.87s
                        Total time: 20478.97s
                               ETA: 899106.6s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.818s, learning 0.193s)
               Value function loss: 4.3149
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 258.12
               Mean episode length: 238.44
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 9.01s
                        Total time: 20487.98s
                               ETA: 899089.3s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.598s, learning 0.348s)
               Value function loss: 4.2836
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 269.72
               Mean episode length: 248.02
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 8.95s
                        Total time: 20496.92s
                               ETA: 899069.2s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.827s, learning 0.186s)
               Value function loss: 4.4499
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 265.69
               Mean episode length: 244.10
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 9.01s
                        Total time: 20505.94s
                               ETA: 899052.0s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.404s, learning 0.171s)
               Value function loss: 4.3470
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 264.06
               Mean episode length: 242.13
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 8.57s
                        Total time: 20514.51s
                               ETA: 899015.6s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.218s, learning 0.183s)
               Value function loss: 3.6095
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 262.65
               Mean episode length: 242.20
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 8.40s
                        Total time: 20522.91s
                               ETA: 898971.6s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.734s, learning 0.168s)
               Value function loss: 5.1161
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 266.80
               Mean episode length: 241.89
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 8.90s
                        Total time: 20531.82s
                               ETA: 898949.6s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1781 steps/s (collection: 9.019s, learning 0.177s)
               Value function loss: 4.4600
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 261.17
               Mean episode length: 238.36
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 9.20s
                        Total time: 20541.01s
                               ETA: 898940.5s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.541s, learning 0.185s)
               Value function loss: 4.9187
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 261.43
               Mean episode length: 238.92
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 8.73s
                        Total time: 20549.74s
                               ETA: 898910.8s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.567s, learning 0.214s)
               Value function loss: 5.3936
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 270.68
               Mean episode length: 247.40
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 8.78s
                        Total time: 20558.52s
                               ETA: 898883.6s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.440s, learning 0.176s)
               Value function loss: 4.8200
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 266.67
               Mean episode length: 244.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 8.62s
                        Total time: 20567.14s
                               ETA: 898849.1s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.748s, learning 0.183s)
               Value function loss: 6.0336
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 275.55
               Mean episode length: 247.61
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 8.93s
                        Total time: 20576.07s
                               ETA: 898828.4s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.487s, learning 0.175s)
               Value function loss: 4.3784
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 275.52
               Mean episode length: 247.91
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 8.66s
                        Total time: 20584.73s
                               ETA: 898796.0s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.398s, learning 0.231s)
               Value function loss: 4.5415
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 267.43
               Mean episode length: 246.20
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 8.63s
                        Total time: 20593.36s
                               ETA: 898762.2s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.393s, learning 0.169s)
               Value function loss: 5.1212
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 272.76
               Mean episode length: 245.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 8.56s
                        Total time: 20601.92s
                               ETA: 898725.5s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.521s, learning 0.171s)
               Value function loss: 5.4279
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 266.69
               Mean episode length: 242.73
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 8.69s
                        Total time: 20610.61s
                               ETA: 898694.4s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.625s, learning 0.178s)
               Value function loss: 5.5857
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 266.99
               Mean episode length: 243.44
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 8.80s
                        Total time: 20619.42s
                               ETA: 898668.2s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.660s, learning 0.268s)
               Value function loss: 4.5577
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 256.82
               Mean episode length: 235.37
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 8.93s
                        Total time: 20628.34s
                               ETA: 898647.5s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.333s, learning 0.169s)
               Value function loss: 4.9163
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 264.12
               Mean episode length: 241.96
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 8.50s
                        Total time: 20636.85s
                               ETA: 898608.2s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.165s)
               Value function loss: 5.0321
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 271.63
               Mean episode length: 244.91
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 8.66s
                        Total time: 20645.50s
                               ETA: 898575.8s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.532s, learning 0.172s)
               Value function loss: 5.9168
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 266.24
               Mean episode length: 240.48
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 8.70s
                        Total time: 20654.21s
                               ETA: 898545.4s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.796s, learning 0.229s)
               Value function loss: 6.4500
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 265.74
               Mean episode length: 240.96
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 9.03s
                        Total time: 20663.23s
                               ETA: 898529.0s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.610s, learning 0.169s)
               Value function loss: 6.5983
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 274.71
               Mean episode length: 246.55
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 8.78s
                        Total time: 20672.01s
                               ETA: 898501.9s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.034s, learning 0.224s)
               Value function loss: 7.2750
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 272.54
               Mean episode length: 245.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 8.26s
                        Total time: 20680.27s
                               ETA: 898452.1s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.709s, learning 0.183s)
               Value function loss: 5.5467
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 265.88
               Mean episode length: 242.51
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 8.89s
                        Total time: 20689.16s
                               ETA: 898430.0s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.844s, learning 0.227s)
               Value function loss: 5.3757
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 276.48
               Mean episode length: 246.28
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 9.07s
                        Total time: 20698.24s
                               ETA: 898415.5s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.760s, learning 0.175s)
               Value function loss: 4.3635
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 266.58
               Mean episode length: 240.73
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 8.94s
                        Total time: 20707.17s
                               ETA: 898395.3s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.524s, learning 0.169s)
               Value function loss: 5.1411
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 258.62
               Mean episode length: 239.15
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 8.69s
                        Total time: 20715.86s
                               ETA: 898364.5s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.555s, learning 0.184s)
               Value function loss: 6.7488
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 260.93
               Mean episode length: 238.90
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 8.74s
                        Total time: 20724.60s
                               ETA: 898335.7s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.514s, learning 0.167s)
               Value function loss: 6.2574
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 259.99
               Mean episode length: 235.27
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 8.68s
                        Total time: 20733.28s
                               ETA: 898304.4s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.638s, learning 0.175s)
               Value function loss: 6.3775
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 251.64
               Mean episode length: 228.57
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 8.81s
                        Total time: 20742.10s
                               ETA: 898278.9s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.606s, learning 0.196s)
               Value function loss: 5.3715
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 266.85
               Mean episode length: 241.32
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 8.80s
                        Total time: 20750.90s
                               ETA: 898252.9s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.525s, learning 0.186s)
               Value function loss: 7.2224
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 261.08
               Mean episode length: 236.66
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 8.71s
                        Total time: 20759.61s
                               ETA: 898223.0s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.478s, learning 0.172s)
               Value function loss: 6.2859
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 264.08
               Mean episode length: 236.52
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 8.65s
                        Total time: 20768.26s
                               ETA: 898190.4s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.604s, learning 0.189s)
               Value function loss: 5.3706
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 258.80
               Mean episode length: 232.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 8.79s
                        Total time: 20777.05s
                               ETA: 898164.1s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.428s, learning 0.180s)
               Value function loss: 5.3955
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 255.03
               Mean episode length: 229.39
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 8.61s
                        Total time: 20785.66s
                               ETA: 898129.8s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.681s, learning 0.166s)
               Value function loss: 4.1372
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 259.63
               Mean episode length: 236.08
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 8.85s
                        Total time: 20794.51s
                               ETA: 898105.9s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.251s, learning 0.192s)
               Value function loss: 6.3019
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 257.05
               Mean episode length: 237.74
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 8.44s
                        Total time: 20802.95s
                               ETA: 898064.5s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.643s, learning 0.224s)
               Value function loss: 5.8155
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 264.93
               Mean episode length: 238.59
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 8.87s
                        Total time: 20811.82s
                               ETA: 898041.4s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.239s, learning 0.214s)
               Value function loss: 5.5266
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 273.24
               Mean episode length: 247.87
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 8.45s
                        Total time: 20820.27s
                               ETA: 898000.5s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.389s, learning 0.180s)
               Value function loss: 6.1031
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 271.50
               Mean episode length: 244.73
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 8.57s
                        Total time: 20828.84s
                               ETA: 897964.6s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.546s, learning 0.237s)
               Value function loss: 5.6804
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 270.51
               Mean episode length: 243.73
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 8.78s
                        Total time: 20837.62s
                               ETA: 897938.0s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.542s, learning 0.179s)
               Value function loss: 7.3915
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 266.57
               Mean episode length: 238.96
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 8.72s
                        Total time: 20846.34s
                               ETA: 897908.7s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.350s, learning 0.180s)
               Value function loss: 6.0122
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 266.15
               Mean episode length: 238.33
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 8.53s
                        Total time: 20854.87s
                               ETA: 897871.2s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.441s, learning 0.179s)
               Value function loss: 5.5882
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 260.12
               Mean episode length: 234.12
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 8.62s
                        Total time: 20863.49s
                               ETA: 897837.6s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.233s, learning 0.187s)
               Value function loss: 5.2886
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 268.18
               Mean episode length: 239.56
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 8.42s
                        Total time: 20871.91s
                               ETA: 897795.4s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.553s, learning 0.178s)
               Value function loss: 7.7351
                    Surrogate loss: -0.0011
             Mean action noise std: 0.76
                       Mean reward: 269.57
               Mean episode length: 242.61
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 8.73s
                        Total time: 20880.65s
                               ETA: 897766.7s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.536s, learning 0.192s)
               Value function loss: 6.7187
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 271.01
               Mean episode length: 242.23
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 8.73s
                        Total time: 20889.37s
                               ETA: 897737.8s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.474s, learning 0.185s)
               Value function loss: 5.9631
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 268.99
               Mean episode length: 242.38
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 8.66s
                        Total time: 20898.03s
                               ETA: 897706.0s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.384s, learning 0.217s)
               Value function loss: 5.6884
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 262.45
               Mean episode length: 237.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 8.60s
                        Total time: 20906.63s
                               ETA: 897671.7s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.680s, learning 0.182s)
               Value function loss: 5.4984
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 259.39
               Mean episode length: 237.43
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 8.86s
                        Total time: 20915.50s
                               ETA: 897648.6s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.541s, learning 0.185s)
               Value function loss: 6.5973
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 261.40
               Mean episode length: 237.95
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 8.73s
                        Total time: 20924.22s
                               ETA: 897619.7s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.516s, learning 0.216s)
               Value function loss: 6.2334
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 259.46
               Mean episode length: 237.25
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 8.73s
                        Total time: 20932.95s
                               ETA: 897591.1s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.384s, learning 0.243s)
               Value function loss: 6.5904
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 258.10
               Mean episode length: 234.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 8.63s
                        Total time: 20941.58s
                               ETA: 897558.0s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.390s, learning 0.176s)
               Value function loss: 5.1559
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 265.18
               Mean episode length: 236.57
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 8.57s
                        Total time: 20950.15s
                               ETA: 897522.3s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.841s, learning 0.222s)
               Value function loss: 6.3484
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 264.82
               Mean episode length: 237.47
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 8.06s
                        Total time: 20958.21s
                               ETA: 897465.0s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.612s, learning 0.304s)
               Value function loss: 5.5644
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 268.49
               Mean episode length: 237.78
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 8.92s
                        Total time: 20967.13s
                               ETA: 897444.4s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.256s, learning 0.227s)
               Value function loss: 4.6904
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 259.48
               Mean episode length: 230.04
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 8.48s
                        Total time: 20975.61s
                               ETA: 897405.2s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.528s, learning 0.223s)
               Value function loss: 5.5745
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 264.33
               Mean episode length: 238.12
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 8.75s
                        Total time: 20984.36s
                               ETA: 897377.6s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.438s, learning 0.178s)
               Value function loss: 5.2917
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 258.44
               Mean episode length: 231.86
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 8.62s
                        Total time: 20992.98s
                               ETA: 897344.1s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.651s, learning 0.188s)
               Value function loss: 6.6906
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 253.24
               Mean episode length: 231.69
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 8.84s
                        Total time: 21001.82s
                               ETA: 897320.2s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.647s, learning 0.197s)
               Value function loss: 6.2151
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 242.88
               Mean episode length: 222.82
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 8.84s
                        Total time: 21010.66s
                               ETA: 897296.6s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.226s, learning 0.169s)
               Value function loss: 5.8875
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 254.40
               Mean episode length: 231.30
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 8.39s
                        Total time: 21019.05s
                               ETA: 897253.8s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.290s, learning 0.296s)
               Value function loss: 5.9733
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 266.11
               Mean episode length: 238.76
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 8.59s
                        Total time: 21027.64s
                               ETA: 897219.1s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.839s, learning 0.199s)
               Value function loss: 5.7649
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 273.51
               Mean episode length: 242.01
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 9.04s
                        Total time: 21036.68s
                               ETA: 897203.8s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.414s, learning 0.188s)
               Value function loss: 4.9657
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 272.41
               Mean episode length: 242.02
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 8.60s
                        Total time: 21045.28s
                               ETA: 897169.8s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.450s, learning 0.179s)
               Value function loss: 6.5377
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 266.54
               Mean episode length: 239.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 8.63s
                        Total time: 21053.91s
                               ETA: 897137.1s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.529s, learning 0.200s)
               Value function loss: 4.5264
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 256.13
               Mean episode length: 231.30
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 8.73s
                        Total time: 21062.64s
                               ETA: 897108.6s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.628s, learning 0.177s)
               Value function loss: 6.5094
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 263.68
               Mean episode length: 238.16
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 8.81s
                        Total time: 21071.44s
                               ETA: 897083.4s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.554s, learning 0.193s)
               Value function loss: 6.0621
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 264.99
               Mean episode length: 241.03
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 8.75s
                        Total time: 21080.19s
                               ETA: 897055.8s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.516s, learning 0.181s)
               Value function loss: 6.6011
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 260.36
               Mean episode length: 234.06
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 8.70s
                        Total time: 21088.89s
                               ETA: 897026.0s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.425s, learning 0.173s)
               Value function loss: 6.2704
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 263.32
               Mean episode length: 238.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 8.60s
                        Total time: 21097.49s
                               ETA: 896992.0s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.498s, learning 0.268s)
               Value function loss: 6.0576
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 264.30
               Mean episode length: 238.92
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 8.77s
                        Total time: 21106.25s
                               ETA: 896965.2s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1774 steps/s (collection: 9.053s, learning 0.182s)
               Value function loss: 7.6565
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 257.49
               Mean episode length: 234.53
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 9.24s
                        Total time: 21115.49s
                               ETA: 896958.3s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.356s, learning 0.223s)
               Value function loss: 7.1254
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 258.46
               Mean episode length: 234.22
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 8.58s
                        Total time: 21124.07s
                               ETA: 896923.6s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.508s, learning 0.181s)
               Value function loss: 4.7939
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 265.88
               Mean episode length: 237.73
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 8.69s
                        Total time: 21132.76s
                               ETA: 896893.6s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.747s, learning 0.175s)
               Value function loss: 5.1188
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 262.86
               Mean episode length: 236.93
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 8.92s
                        Total time: 21141.68s
                               ETA: 896873.5s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.554s, learning 0.184s)
               Value function loss: 6.7830
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 273.15
               Mean episode length: 243.49
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 8.74s
                        Total time: 21150.42s
                               ETA: 896845.5s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.548s, learning 0.184s)
               Value function loss: 6.2065
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 270.76
               Mean episode length: 244.49
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 8.73s
                        Total time: 21159.15s
                               ETA: 896817.3s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.479s, learning 0.182s)
               Value function loss: 6.0630
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 265.48
               Mean episode length: 237.37
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 8.66s
                        Total time: 21167.81s
                               ETA: 896786.2s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.546s, learning 0.186s)
               Value function loss: 4.5415
                    Surrogate loss: 0.0080
             Mean action noise std: 0.76
                       Mean reward: 265.43
               Mean episode length: 239.85
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 8.73s
                        Total time: 21176.54s
                               ETA: 896758.0s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.396s, learning 0.180s)
               Value function loss: 5.8064
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 257.41
               Mean episode length: 236.38
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 8.58s
                        Total time: 21185.11s
                               ETA: 896723.3s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.761s, learning 0.185s)
               Value function loss: 6.1501
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 256.43
               Mean episode length: 237.84
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 8.95s
                        Total time: 21194.06s
                               ETA: 896704.3s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.523s, learning 0.169s)
               Value function loss: 5.7476
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 260.14
               Mean episode length: 238.64
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 8.69s
                        Total time: 21202.75s
                               ETA: 896674.5s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.876s, learning 0.178s)
               Value function loss: 5.2659
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 256.16
               Mean episode length: 231.72
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 9.05s
                        Total time: 21211.81s
                               ETA: 896660.1s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.373s, learning 0.175s)
               Value function loss: 5.2368
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 260.44
               Mean episode length: 238.25
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 8.55s
                        Total time: 21220.36s
                               ETA: 896624.3s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.523s, learning 0.180s)
               Value function loss: 5.7002
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 265.20
               Mean episode length: 242.37
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 8.70s
                        Total time: 21229.06s
                               ETA: 896595.0s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.489s, learning 0.180s)
               Value function loss: 6.3144
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 268.23
               Mean episode length: 246.14
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 8.67s
                        Total time: 21237.73s
                               ETA: 896564.3s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.743s, learning 0.260s)
               Value function loss: 5.0644
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 269.86
               Mean episode length: 243.69
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 9.00s
                        Total time: 21246.73s
                               ETA: 896547.8s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.501s, learning 0.375s)
               Value function loss: 5.1145
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 266.99
               Mean episode length: 240.65
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 8.88s
                        Total time: 21255.61s
                               ETA: 896525.9s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.727s, learning 0.172s)
               Value function loss: 4.9757
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 253.26
               Mean episode length: 233.47
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 8.90s
                        Total time: 21264.51s
                               ETA: 896504.9s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.638s, learning 0.206s)
               Value function loss: 5.9929
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 259.49
               Mean episode length: 239.33
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 8.84s
                        Total time: 21273.35s
                               ETA: 896481.7s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.484s, learning 0.188s)
               Value function loss: 5.2194
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 267.64
               Mean episode length: 244.09
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 8.67s
                        Total time: 21282.02s
                               ETA: 896451.2s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1985 steps/s (collection: 7.960s, learning 0.290s)
               Value function loss: 5.3080
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 264.22
               Mean episode length: 241.19
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 8.25s
                        Total time: 21290.27s
                               ETA: 896403.0s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1997 steps/s (collection: 7.998s, learning 0.206s)
               Value function loss: 4.0209
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 255.04
               Mean episode length: 238.63
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 8.20s
                        Total time: 21298.47s
                               ETA: 896352.9s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1786 steps/s (collection: 8.950s, learning 0.221s)
               Value function loss: 5.3272
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 256.08
               Mean episode length: 238.49
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 9.17s
                        Total time: 21307.65s
                               ETA: 896343.5s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.721s, learning 0.188s)
               Value function loss: 4.3440
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 254.27
               Mean episode length: 236.76
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 8.91s
                        Total time: 21316.55s
                               ETA: 896323.0s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.484s, learning 0.196s)
               Value function loss: 4.5494
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 256.68
               Mean episode length: 239.17
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 8.68s
                        Total time: 21325.23s
                               ETA: 896293.0s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.695s, learning 0.176s)
               Value function loss: 3.7491
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 255.18
               Mean episode length: 236.06
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 8.87s
                        Total time: 21334.11s
                               ETA: 896271.0s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.329s, learning 0.181s)
               Value function loss: 5.0057
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 262.35
               Mean episode length: 243.58
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 8.51s
                        Total time: 21342.62s
                               ETA: 896233.9s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.278s, learning 0.173s)
               Value function loss: 5.3514
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 258.48
               Mean episode length: 241.33
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 8.45s
                        Total time: 21351.07s
                               ETA: 896194.3s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.655s, learning 0.209s)
               Value function loss: 4.4340
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 264.81
               Mean episode length: 245.05
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 8.86s
                        Total time: 21359.93s
                               ETA: 896172.1s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.877s, learning 0.195s)
               Value function loss: 4.8034
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 256.03
               Mean episode length: 236.70
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 9.07s
                        Total time: 21369.00s
                               ETA: 896158.5s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.590s, learning 0.197s)
               Value function loss: 5.0305
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 245.10
               Mean episode length: 229.55
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 8.79s
                        Total time: 21377.79s
                               ETA: 896133.1s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.533s, learning 0.208s)
               Value function loss: 7.2108
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 250.97
               Mean episode length: 236.49
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 8.74s
                        Total time: 21386.53s
                               ETA: 896105.7s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.648s, learning 0.188s)
               Value function loss: 6.3953
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 263.84
               Mean episode length: 244.02
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 8.84s
                        Total time: 21395.37s
                               ETA: 896082.3s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.527s, learning 0.182s)
               Value function loss: 4.4979
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 258.56
               Mean episode length: 244.08
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 8.71s
                        Total time: 21404.08s
                               ETA: 896053.7s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.103s, learning 0.200s)
               Value function loss: 5.5254
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 260.65
               Mean episode length: 244.13
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 8.30s
                        Total time: 21412.38s
                               ETA: 896008.0s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.659s, learning 0.194s)
               Value function loss: 5.0933
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 251.91
               Mean episode length: 239.07
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 8.85s
                        Total time: 21421.23s
                               ETA: 895985.5s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.547s, learning 0.184s)
               Value function loss: 6.7837
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 250.07
               Mean episode length: 235.58
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 8.73s
                        Total time: 21429.96s
                               ETA: 895957.8s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.309s, learning 0.178s)
               Value function loss: 7.7970
                    Surrogate loss: 0.0039
             Mean action noise std: 0.76
                       Mean reward: 261.92
               Mean episode length: 243.14
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 8.49s
                        Total time: 21438.45s
                               ETA: 895919.9s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.270s, learning 0.273s)
               Value function loss: 5.2291
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 260.15
               Mean episode length: 241.18
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 8.54s
                        Total time: 21446.99s
                               ETA: 895884.4s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.504s, learning 0.281s)
               Value function loss: 5.2610
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 260.32
               Mean episode length: 241.43
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 8.78s
                        Total time: 21455.78s
                               ETA: 895859.0s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.326s, learning 0.173s)
               Value function loss: 5.3951
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 262.06
               Mean episode length: 239.15
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 8.50s
                        Total time: 21464.28s
                               ETA: 895821.7s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.384s, learning 0.176s)
               Value function loss: 5.1618
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 266.10
               Mean episode length: 245.84
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 8.56s
                        Total time: 21472.84s
                               ETA: 895787.0s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.401s, learning 0.196s)
               Value function loss: 5.6867
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 268.10
               Mean episode length: 247.41
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 8.60s
                        Total time: 21481.43s
                               ETA: 895753.8s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.177s)
               Value function loss: 4.5494
                    Surrogate loss: -0.0004
             Mean action noise std: 0.76
                       Mean reward: 265.03
               Mean episode length: 245.63
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 8.56s
                        Total time: 21490.00s
                               ETA: 895719.3s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.904s, learning 0.178s)
               Value function loss: 5.2744
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 262.67
               Mean episode length: 244.94
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 9.08s
                        Total time: 21499.08s
                               ETA: 895706.3s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.698s, learning 0.183s)
               Value function loss: 6.5015
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 267.58
               Mean episode length: 247.87
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 8.88s
                        Total time: 21507.96s
                               ETA: 895685.1s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.630s, learning 0.196s)
               Value function loss: 5.2240
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 265.93
               Mean episode length: 244.58
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 8.83s
                        Total time: 21516.79s
                               ETA: 895661.5s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.415s, learning 0.286s)
               Value function loss: 4.7274
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 257.57
               Mean episode length: 234.94
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 8.70s
                        Total time: 21525.49s
                               ETA: 895632.7s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.636s, learning 0.170s)
               Value function loss: 4.7935
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 265.15
               Mean episode length: 240.76
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 8.81s
                        Total time: 21534.29s
                               ETA: 895608.4s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.396s, learning 0.190s)
               Value function loss: 6.7739
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 267.16
               Mean episode length: 244.15
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 8.59s
                        Total time: 21542.88s
                               ETA: 895574.9s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.338s, learning 0.173s)
               Value function loss: 6.2910
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 270.50
               Mean episode length: 244.03
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 8.51s
                        Total time: 21551.39s
                               ETA: 895538.3s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.660s, learning 0.260s)
               Value function loss: 5.7172
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 271.26
               Mean episode length: 246.65
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 8.92s
                        Total time: 21560.31s
                               ETA: 895518.7s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.633s, learning 0.264s)
               Value function loss: 5.4288
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 276.75
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 8.90s
                        Total time: 21569.21s
                               ETA: 895498.2s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.492s, learning 0.313s)
               Value function loss: 6.5576
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 269.18
               Mean episode length: 243.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 8.81s
                        Total time: 21578.01s
                               ETA: 895473.8s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.627s, learning 0.214s)
               Value function loss: 5.7753
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 269.11
               Mean episode length: 242.51
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 8.84s
                        Total time: 21586.86s
                               ETA: 895451.0s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.392s, learning 0.198s)
               Value function loss: 4.8939
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 270.89
               Mean episode length: 243.24
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 8.59s
                        Total time: 21595.44s
                               ETA: 895417.8s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.705s, learning 0.270s)
               Value function loss: 5.4888
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 272.61
               Mean episode length: 244.36
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 8.98s
                        Total time: 21604.42s
                               ETA: 895400.5s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.286s, learning 0.263s)
               Value function loss: 4.6733
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 269.70
               Mean episode length: 244.51
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 8.55s
                        Total time: 21612.97s
                               ETA: 895365.6s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.732s, learning 0.201s)
               Value function loss: 6.6054
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 271.02
               Mean episode length: 240.95
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 8.93s
                        Total time: 21621.90s
                               ETA: 895346.6s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.474s, learning 0.166s)
               Value function loss: 5.0132
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 275.39
               Mean episode length: 244.66
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 8.64s
                        Total time: 21630.54s
                               ETA: 895315.6s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.512s, learning 0.257s)
               Value function loss: 5.9238
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 279.63
               Mean episode length: 246.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 8.77s
                        Total time: 21639.31s
                               ETA: 895289.8s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.636s, learning 0.166s)
               Value function loss: 6.7949
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 276.11
               Mean episode length: 246.20
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 8.80s
                        Total time: 21648.11s
                               ETA: 895265.5s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.485s, learning 0.218s)
               Value function loss: 7.5621
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 268.08
               Mean episode length: 239.23
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 8.70s
                        Total time: 21656.82s
                               ETA: 895237.0s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.506s, learning 0.176s)
               Value function loss: 7.4834
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 266.54
               Mean episode length: 240.54
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 8.68s
                        Total time: 21665.50s
                               ETA: 895207.7s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.502s, learning 0.167s)
               Value function loss: 5.5020
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 273.66
               Mean episode length: 243.74
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 8.67s
                        Total time: 21674.17s
                               ETA: 895177.9s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.771s, learning 0.253s)
               Value function loss: 5.9719
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 277.51
               Mean episode length: 248.37
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 9.02s
                        Total time: 21683.19s
                               ETA: 895162.8s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.634s, learning 0.177s)
               Value function loss: 5.3237
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 282.80
               Mean episode length: 248.37
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 8.81s
                        Total time: 21692.00s
                               ETA: 895138.9s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.577s, learning 0.179s)
               Value function loss: 6.5207
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 274.44
               Mean episode length: 245.05
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 8.76s
                        Total time: 21700.76s
                               ETA: 895112.7s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.519s, learning 0.167s)
               Value function loss: 7.0883
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 271.57
               Mean episode length: 244.45
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 8.69s
                        Total time: 21709.44s
                               ETA: 895083.7s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.494s, learning 0.167s)
               Value function loss: 4.9929
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 269.87
               Mean episode length: 242.87
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 8.66s
                        Total time: 21718.11s
                               ETA: 895053.6s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.898s, learning 0.165s)
               Value function loss: 5.9447
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 270.38
               Mean episode length: 240.57
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 9.06s
                        Total time: 21727.17s
                               ETA: 895040.2s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.521s, learning 0.167s)
               Value function loss: 5.8937
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 274.43
               Mean episode length: 244.14
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 8.69s
                        Total time: 21735.86s
                               ETA: 895011.3s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.109s, learning 0.173s)
               Value function loss: 6.1917
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 275.57
               Mean episode length: 244.85
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 8.28s
                        Total time: 21744.14s
                               ETA: 894965.6s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.359s, learning 0.169s)
               Value function loss: 5.9341
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 275.01
               Mean episode length: 243.38
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 8.53s
                        Total time: 21752.67s
                               ETA: 894930.2s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.383s, learning 0.171s)
               Value function loss: 6.2309
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 272.71
               Mean episode length: 243.73
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 8.55s
                        Total time: 21761.22s
                               ETA: 894895.9s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.603s, learning 0.168s)
               Value function loss: 6.5556
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 272.13
               Mean episode length: 241.51
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 8.77s
                        Total time: 21769.99s
                               ETA: 894870.4s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.263s, learning 0.195s)
               Value function loss: 5.3438
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 267.76
               Mean episode length: 239.66
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 8.46s
                        Total time: 21778.45s
                               ETA: 894832.1s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.739s, learning 0.179s)
               Value function loss: 6.0450
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 261.46
               Mean episode length: 232.68
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 8.92s
                        Total time: 21787.37s
                               ETA: 894812.8s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.395s, learning 0.242s)
               Value function loss: 4.5651
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 262.44
               Mean episode length: 233.54
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 8.64s
                        Total time: 21796.00s
                               ETA: 894781.9s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.432s, learning 0.188s)
               Value function loss: 4.8941
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 257.96
               Mean episode length: 233.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 8.62s
                        Total time: 21804.62s
                               ETA: 894750.3s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.394s, learning 0.244s)
               Value function loss: 7.1055
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 274.13
               Mean episode length: 244.40
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 8.64s
                        Total time: 21813.26s
                               ETA: 894719.5s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.673s, learning 0.168s)
               Value function loss: 5.6597
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 278.40
               Mean episode length: 246.47
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 8.84s
                        Total time: 21822.10s
                               ETA: 894697.1s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.376s, learning 0.167s)
               Value function loss: 5.2129
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 277.06
               Mean episode length: 247.07
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 8.54s
                        Total time: 21830.65s
                               ETA: 894662.4s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.692s, learning 0.181s)
               Value function loss: 5.3794
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 278.68
               Mean episode length: 246.56
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 8.87s
                        Total time: 21839.52s
                               ETA: 894641.3s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.671s, learning 0.218s)
               Value function loss: 6.1055
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 276.18
               Mean episode length: 243.01
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 8.89s
                        Total time: 21848.41s
                               ETA: 894620.8s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.589s, learning 0.179s)
               Value function loss: 5.8235
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 273.01
               Mean episode length: 240.12
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 8.77s
                        Total time: 21857.18s
                               ETA: 894595.4s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.515s, learning 0.178s)
               Value function loss: 4.9059
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 277.49
               Mean episode length: 244.91
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 8.69s
                        Total time: 21865.87s
                               ETA: 894567.0s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.345s, learning 0.180s)
               Value function loss: 5.2790
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 276.90
               Mean episode length: 247.09
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 8.52s
                        Total time: 21874.39s
                               ETA: 894531.7s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.412s, learning 0.201s)
               Value function loss: 4.1788
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 273.56
               Mean episode length: 244.99
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 8.61s
                        Total time: 21883.01s
                               ETA: 894500.0s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.308s, learning 0.223s)
               Value function loss: 6.2456
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 273.61
               Mean episode length: 243.22
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 8.53s
                        Total time: 21891.54s
                               ETA: 894465.0s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.592s, learning 0.178s)
               Value function loss: 6.3675
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 278.53
               Mean episode length: 246.80
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 8.77s
                        Total time: 21900.31s
                               ETA: 894439.8s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.264s, learning 0.194s)
               Value function loss: 5.6188
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 279.26
               Mean episode length: 248.24
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 8.46s
                        Total time: 21908.77s
                               ETA: 894401.8s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.169s)
               Value function loss: 5.4456
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 280.78
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 8.55s
                        Total time: 21917.32s
                               ETA: 894367.7s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.421s, learning 0.315s)
               Value function loss: 6.8861
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 278.32
               Mean episode length: 247.25
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 8.74s
                        Total time: 21926.06s
                               ETA: 894341.2s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.641s, learning 0.207s)
               Value function loss: 6.6496
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 282.72
               Mean episode length: 248.57
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 8.85s
                        Total time: 21934.90s
                               ETA: 894319.2s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.572s, learning 0.167s)
               Value function loss: 6.2114
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 277.68
               Mean episode length: 243.87
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 8.74s
                        Total time: 21943.64s
                               ETA: 894292.8s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.800s, learning 0.182s)
               Value function loss: 5.4860
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 277.69
               Mean episode length: 245.93
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 8.98s
                        Total time: 21952.62s
                               ETA: 894276.3s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.709s, learning 0.168s)
               Value function loss: 4.9131
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 282.56
               Mean episode length: 246.95
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 8.88s
                        Total time: 21961.50s
                               ETA: 894255.5s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.344s, learning 0.170s)
               Value function loss: 7.0495
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 283.10
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 8.51s
                        Total time: 21970.01s
                               ETA: 894219.9s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.534s, learning 0.172s)
               Value function loss: 7.0557
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 277.98
               Mean episode length: 247.16
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 8.71s
                        Total time: 21978.72s
                               ETA: 894192.2s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.575s, learning 0.190s)
               Value function loss: 5.7394
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 276.81
               Mean episode length: 245.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 8.77s
                        Total time: 21987.49s
                               ETA: 894166.9s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.455s, learning 0.169s)
               Value function loss: 5.0123
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 273.28
               Mean episode length: 241.93
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 8.62s
                        Total time: 21996.11s
                               ETA: 894135.9s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.450s, learning 0.238s)
               Value function loss: 4.6766
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 268.31
               Mean episode length: 236.24
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 8.69s
                        Total time: 22004.80s
                               ETA: 894107.5s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.391s, learning 0.219s)
               Value function loss: 6.3036
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 274.93
               Mean episode length: 240.80
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 8.61s
                        Total time: 22013.41s
                               ETA: 894076.0s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.461s, learning 0.178s)
               Value function loss: 4.8469
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 272.53
               Mean episode length: 240.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 8.64s
                        Total time: 22022.05s
                               ETA: 894045.6s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.609s, learning 0.166s)
               Value function loss: 5.7601
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 270.98
               Mean episode length: 240.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 8.77s
                        Total time: 22030.82s
                               ETA: 894020.8s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.329s, learning 0.196s)
               Value function loss: 4.6298
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 274.84
               Mean episode length: 242.69
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 8.52s
                        Total time: 22039.35s
                               ETA: 893985.9s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.667s, learning 0.186s)
               Value function loss: 4.7772
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 276.11
               Mean episode length: 242.83
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 8.85s
                        Total time: 22048.20s
                               ETA: 893964.3s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.694s, learning 0.345s)
               Value function loss: 4.7140
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 276.30
               Mean episode length: 244.93
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 9.04s
                        Total time: 22057.24s
                               ETA: 893950.2s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.472s, learning 0.167s)
               Value function loss: 4.4338
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 271.97
               Mean episode length: 239.57
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 8.64s
                        Total time: 22065.88s
                               ETA: 893919.9s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.604s, learning 0.269s)
               Value function loss: 5.0030
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 269.50
               Mean episode length: 236.16
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 8.87s
                        Total time: 22074.75s
                               ETA: 893899.2s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.703s, learning 0.177s)
               Value function loss: 4.8636
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 268.35
               Mean episode length: 237.09
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 8.88s
                        Total time: 22083.63s
                               ETA: 893878.7s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.781s, learning 0.171s)
               Value function loss: 5.2438
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 274.45
               Mean episode length: 242.07
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 8.95s
                        Total time: 22092.58s
                               ETA: 893861.1s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.341s, learning 0.166s)
               Value function loss: 4.8539
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 278.89
               Mean episode length: 245.99
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 8.51s
                        Total time: 22101.09s
                               ETA: 893825.6s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.721s, learning 0.178s)
               Value function loss: 5.2978
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 280.64
               Mean episode length: 246.72
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 8.90s
                        Total time: 22109.99s
                               ETA: 893805.9s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.552s, learning 0.165s)
               Value function loss: 5.0974
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 269.84
               Mean episode length: 239.61
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 8.72s
                        Total time: 22118.71s
                               ETA: 893778.9s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.611s, learning 0.169s)
               Value function loss: 5.6401
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 275.73
               Mean episode length: 243.48
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 8.78s
                        Total time: 22127.49s
                               ETA: 893754.5s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.458s, learning 0.167s)
               Value function loss: 4.1670
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 277.03
               Mean episode length: 245.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 8.62s
                        Total time: 22136.11s
                               ETA: 893723.8s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.569s, learning 0.200s)
               Value function loss: 5.2806
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 274.19
               Mean episode length: 240.52
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 8.77s
                        Total time: 22144.88s
                               ETA: 893698.9s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.293s, learning 0.172s)
               Value function loss: 4.1283
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 266.16
               Mean episode length: 232.79
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 8.47s
                        Total time: 22153.35s
                               ETA: 893661.8s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.704s, learning 0.165s)
               Value function loss: 5.2278
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 270.05
               Mean episode length: 237.82
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 8.87s
                        Total time: 22162.22s
                               ETA: 893641.0s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.463s, learning 0.171s)
               Value function loss: 5.2927
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 277.26
               Mean episode length: 243.69
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 8.63s
                        Total time: 22170.85s
                               ETA: 893610.7s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.590s, learning 0.166s)
               Value function loss: 5.0501
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 281.03
               Mean episode length: 245.53
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 8.76s
                        Total time: 22179.61s
                               ETA: 893585.4s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.520s, learning 0.213s)
               Value function loss: 4.4428
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 278.24
               Mean episode length: 246.16
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 8.73s
                        Total time: 22188.34s
                               ETA: 893559.1s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.439s, learning 0.167s)
               Value function loss: 6.2354
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 272.00
               Mean episode length: 240.58
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 8.61s
                        Total time: 22196.94s
                               ETA: 893527.7s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.472s, learning 0.208s)
               Value function loss: 6.4511
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 272.15
               Mean episode length: 240.45
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 8.68s
                        Total time: 22205.62s
                               ETA: 893499.4s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.667s, learning 0.195s)
               Value function loss: 6.2846
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 267.05
               Mean episode length: 238.02
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 8.86s
                        Total time: 22214.49s
                               ETA: 893478.3s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.648s, learning 0.167s)
               Value function loss: 4.4874
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 266.92
               Mean episode length: 239.46
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 8.82s
                        Total time: 22223.30s
                               ETA: 893455.5s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.757s, learning 0.196s)
               Value function loss: 5.0360
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 267.02
               Mean episode length: 238.42
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 8.95s
                        Total time: 22232.25s
                               ETA: 893438.1s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.528s, learning 0.170s)
               Value function loss: 5.8610
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 267.76
               Mean episode length: 238.73
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 8.70s
                        Total time: 22240.95s
                               ETA: 893410.6s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.642s, learning 0.220s)
               Value function loss: 6.1566
                    Surrogate loss: 0.0005
             Mean action noise std: 0.76
                       Mean reward: 277.55
               Mean episode length: 248.11
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 8.86s
                        Total time: 22249.81s
                               ETA: 893389.6s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.754s, learning 0.276s)
               Value function loss: 6.1366
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 274.81
               Mean episode length: 245.20
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 9.03s
                        Total time: 22258.84s
                               ETA: 893375.4s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.526s, learning 0.197s)
               Value function loss: 4.8171
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 270.76
               Mean episode length: 242.60
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 8.72s
                        Total time: 22267.57s
                               ETA: 893348.8s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.639s, learning 0.177s)
               Value function loss: 4.8390
                    Surrogate loss: 0.0030
             Mean action noise std: 0.76
                       Mean reward: 270.74
               Mean episode length: 241.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 8.82s
                        Total time: 22276.38s
                               ETA: 893326.0s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.801s, learning 0.170s)
               Value function loss: 5.1277
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 269.54
               Mean episode length: 239.55
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 8.97s
                        Total time: 22285.35s
                               ETA: 893309.4s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.612s, learning 0.192s)
               Value function loss: 4.6839
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 269.73
               Mean episode length: 240.92
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 8.80s
                        Total time: 22294.16s
                               ETA: 893286.2s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.263s, learning 0.176s)
               Value function loss: 6.0264
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 254.52
               Mean episode length: 231.47
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 8.44s
                        Total time: 22302.60s
                               ETA: 893248.3s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.433s, learning 0.168s)
               Value function loss: 4.6833
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 266.07
               Mean episode length: 241.50
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 8.60s
                        Total time: 22311.20s
                               ETA: 893217.0s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.480s, learning 0.166s)
               Value function loss: 5.5198
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 269.46
               Mean episode length: 244.85
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 8.65s
                        Total time: 22319.84s
                               ETA: 893187.5s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.502s, learning 0.186s)
               Value function loss: 6.3925
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 266.64
               Mean episode length: 242.54
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 8.69s
                        Total time: 22328.53s
                               ETA: 893159.6s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.591s, learning 0.183s)
               Value function loss: 5.0636
                    Surrogate loss: -0.0045
             Mean action noise std: 0.76
                       Mean reward: 266.08
               Mean episode length: 238.22
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 8.77s
                        Total time: 22337.31s
                               ETA: 893135.2s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.495s, learning 0.174s)
               Value function loss: 5.6028
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 264.57
               Mean episode length: 238.26
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 8.67s
                        Total time: 22345.98s
                               ETA: 893106.7s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.100s, learning 0.167s)
               Value function loss: 5.2574
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 259.07
               Mean episode length: 236.21
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 8.27s
                        Total time: 22354.24s
                               ETA: 893062.1s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.710s, learning 0.266s)
               Value function loss: 6.2573
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 268.00
               Mean episode length: 242.85
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 8.98s
                        Total time: 22363.22s
                               ETA: 893045.8s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.627s, learning 0.170s)
               Value function loss: 5.8243
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 263.14
               Mean episode length: 238.82
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 8.80s
                        Total time: 22372.02s
                               ETA: 893022.4s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.574s, learning 0.170s)
               Value function loss: 5.6718
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 261.74
               Mean episode length: 240.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 8.74s
                        Total time: 22380.76s
                               ETA: 892996.9s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.681s, learning 0.174s)
               Value function loss: 5.2178
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 262.97
               Mean episode length: 240.48
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 8.85s
                        Total time: 22389.61s
                               ETA: 892975.8s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.523s, learning 0.173s)
               Value function loss: 6.4686
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 259.48
               Mean episode length: 241.36
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 8.70s
                        Total time: 22398.31s
                               ETA: 892948.4s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.401s, learning 0.173s)
               Value function loss: 5.2992
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 266.64
               Mean episode length: 245.72
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 8.57s
                        Total time: 22406.88s
                               ETA: 892916.2s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.587s, learning 0.276s)
               Value function loss: 4.9435
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 267.31
               Mean episode length: 239.95
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 8.86s
                        Total time: 22415.75s
                               ETA: 892895.5s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.627s, learning 0.207s)
               Value function loss: 4.6570
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 264.08
               Mean episode length: 239.17
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 8.83s
                        Total time: 22424.58s
                               ETA: 892873.6s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.447s, learning 0.194s)
               Value function loss: 6.1305
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 257.80
               Mean episode length: 235.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 8.64s
                        Total time: 22433.22s
                               ETA: 892844.1s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.551s, learning 0.203s)
               Value function loss: 6.9040
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 265.01
               Mean episode length: 239.98
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 8.75s
                        Total time: 22441.98s
                               ETA: 892819.1s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1772 steps/s (collection: 9.067s, learning 0.177s)
               Value function loss: 5.2294
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 260.73
               Mean episode length: 236.19
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 9.24s
                        Total time: 22451.22s
                               ETA: 892813.6s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.750s, learning 0.178s)
               Value function loss: 5.2869
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 264.20
               Mean episode length: 241.41
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 8.93s
                        Total time: 22460.15s
                               ETA: 892795.5s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1786 steps/s (collection: 9.001s, learning 0.168s)
               Value function loss: 5.4015
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 265.19
               Mean episode length: 243.44
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 9.17s
                        Total time: 22469.32s
                               ETA: 892787.1s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.479s, learning 0.205s)
               Value function loss: 7.9688
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 271.92
               Mean episode length: 243.92
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 8.68s
                        Total time: 22478.00s
                               ETA: 892759.3s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.694s, learning 0.200s)
               Value function loss: 8.2818
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 270.63
               Mean episode length: 245.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 8.89s
                        Total time: 22486.90s
                               ETA: 892739.9s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.632s, learning 0.172s)
               Value function loss: 5.0031
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 270.05
               Mean episode length: 245.30
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 8.80s
                        Total time: 22495.70s
                               ETA: 892717.0s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.219s, learning 0.171s)
               Value function loss: 6.4951
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 268.15
               Mean episode length: 245.05
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 8.39s
                        Total time: 22504.09s
                               ETA: 892677.6s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.331s, learning 0.274s)
               Value function loss: 5.7763
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 270.29
               Mean episode length: 247.37
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 8.61s
                        Total time: 22512.70s
                               ETA: 892646.8s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.662s, learning 0.190s)
               Value function loss: 6.4090
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 266.25
               Mean episode length: 241.99
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 8.85s
                        Total time: 22521.55s
                               ETA: 892625.8s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.548s, learning 0.218s)
               Value function loss: 7.8765
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 256.52
               Mean episode length: 235.72
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 8.77s
                        Total time: 22530.32s
                               ETA: 892601.3s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.367s, learning 0.219s)
               Value function loss: 7.1101
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 259.91
               Mean episode length: 237.52
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 8.59s
                        Total time: 22538.90s
                               ETA: 892569.8s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.818s, learning 0.169s)
               Value function loss: 5.2870
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 264.62
               Mean episode length: 242.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 8.99s
                        Total time: 22547.89s
                               ETA: 892554.2s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.314s, learning 0.180s)
               Value function loss: 5.6260
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 267.61
               Mean episode length: 241.91
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 8.49s
                        Total time: 22556.38s
                               ETA: 892519.0s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.619s, learning 0.172s)
               Value function loss: 6.6120
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 268.48
               Mean episode length: 242.67
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 8.79s
                        Total time: 22565.17s
                               ETA: 892495.7s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.763s, learning 0.217s)
               Value function loss: 7.1365
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 270.75
               Mean episode length: 245.80
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 8.98s
                        Total time: 22574.15s
                               ETA: 892479.8s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.572s, learning 0.260s)
               Value function loss: 5.3307
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 259.20
               Mean episode length: 238.24
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 8.83s
                        Total time: 22582.99s
                               ETA: 892458.0s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.491s, learning 0.224s)
               Value function loss: 5.6528
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 259.81
               Mean episode length: 238.14
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 8.72s
                        Total time: 22591.70s
                               ETA: 892431.7s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.593s, learning 0.182s)
               Value function loss: 7.1560
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 262.60
               Mean episode length: 242.45
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 8.77s
                        Total time: 22600.48s
                               ETA: 892407.7s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.878s, learning 0.217s)
               Value function loss: 5.6604
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 268.62
               Mean episode length: 246.08
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 9.09s
                        Total time: 22609.57s
                               ETA: 892396.4s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.269s, learning 0.194s)
               Value function loss: 5.5518
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 271.71
               Mean episode length: 245.91
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 8.46s
                        Total time: 22618.03s
                               ETA: 892360.1s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.582s, learning 0.193s)
               Value function loss: 4.8741
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 276.79
               Mean episode length: 246.98
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 8.77s
                        Total time: 22626.81s
                               ETA: 892336.2s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.574s, learning 0.215s)
               Value function loss: 6.4156
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 269.63
               Mean episode length: 243.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 8.79s
                        Total time: 22635.60s
                               ETA: 892312.8s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.643s, learning 0.174s)
               Value function loss: 6.0283
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 265.21
               Mean episode length: 238.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 8.82s
                        Total time: 22644.42s
                               ETA: 892290.6s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.459s, learning 0.178s)
               Value function loss: 5.6219
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 263.57
               Mean episode length: 236.91
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 8.64s
                        Total time: 22653.05s
                               ETA: 892261.3s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.797s, learning 0.173s)
               Value function loss: 5.6473
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 268.84
               Mean episode length: 243.98
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 8.97s
                        Total time: 22662.02s
                               ETA: 892245.1s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.789s, learning 0.248s)
               Value function loss: 7.2173
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 271.40
               Mean episode length: 243.70
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 9.04s
                        Total time: 22671.06s
                               ETA: 892231.5s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.641s, learning 0.170s)
               Value function loss: 6.1426
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 274.59
               Mean episode length: 244.54
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 8.81s
                        Total time: 22679.87s
                               ETA: 892209.1s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.562s, learning 0.169s)
               Value function loss: 5.3501
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 267.59
               Mean episode length: 240.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 8.73s
                        Total time: 22688.60s
                               ETA: 892183.5s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.726s, learning 0.171s)
               Value function loss: 4.6457
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 268.93
               Mean episode length: 240.12
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 8.90s
                        Total time: 22697.50s
                               ETA: 892164.5s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.395s, learning 0.187s)
               Value function loss: 6.0186
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 270.96
               Mean episode length: 240.42
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 8.58s
                        Total time: 22706.08s
                               ETA: 892133.1s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.341s, learning 0.177s)
               Value function loss: 5.8257
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 275.46
               Mean episode length: 242.23
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 8.52s
                        Total time: 22714.60s
                               ETA: 892099.2s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.353s, learning 0.199s)
               Value function loss: 6.0111
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 268.18
               Mean episode length: 239.93
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 8.55s
                        Total time: 22723.15s
                               ETA: 892066.6s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.739s, learning 0.169s)
               Value function loss: 4.7108
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 268.77
               Mean episode length: 241.98
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 8.91s
                        Total time: 22732.06s
                               ETA: 892048.1s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.767s, learning 0.209s)
               Value function loss: 6.1750
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 274.18
               Mean episode length: 244.39
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 8.98s
                        Total time: 22741.03s
                               ETA: 892032.2s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.557s, learning 0.281s)
               Value function loss: 7.2111
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 281.13
               Mean episode length: 247.99
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 8.84s
                        Total time: 22749.87s
                               ETA: 892010.9s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.801s, learning 0.192s)
               Value function loss: 6.8953
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 280.40
               Mean episode length: 247.95
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 8.99s
                        Total time: 22758.87s
                               ETA: 891995.7s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.540s, learning 0.177s)
               Value function loss: 6.6431
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 275.99
               Mean episode length: 245.94
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 8.72s
                        Total time: 22767.58s
                               ETA: 891969.7s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.406s, learning 0.218s)
               Value function loss: 5.9931
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 270.47
               Mean episode length: 242.36
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 8.62s
                        Total time: 22776.21s
                               ETA: 891940.0s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.679s, learning 0.176s)
               Value function loss: 4.8390
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 266.34
               Mean episode length: 238.62
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 8.86s
                        Total time: 22785.06s
                               ETA: 891919.5s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.419s, learning 0.176s)
               Value function loss: 6.1761
                    Surrogate loss: 0.0005
             Mean action noise std: 0.76
                       Mean reward: 275.93
               Mean episode length: 243.90
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 8.60s
                        Total time: 22793.66s
                               ETA: 891888.8s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.149s, learning 0.175s)
               Value function loss: 7.1528
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 271.82
               Mean episode length: 243.13
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 8.32s
                        Total time: 22801.98s
                               ETA: 891847.4s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.371s, learning 0.272s)
               Value function loss: 6.0563
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 268.88
               Mean episode length: 241.96
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 8.64s
                        Total time: 22810.62s
                               ETA: 891818.6s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.000s, learning 0.178s)
               Value function loss: 5.5058
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 277.49
               Mean episode length: 243.94
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 9.18s
                        Total time: 22819.80s
                               ETA: 891810.7s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.406s, learning 0.179s)
               Value function loss: 5.4258
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 269.28
               Mean episode length: 239.88
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 8.58s
                        Total time: 22828.39s
                               ETA: 891779.6s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.646s, learning 0.197s)
               Value function loss: 6.6360
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 272.13
               Mean episode length: 239.73
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 8.84s
                        Total time: 22837.23s
                               ETA: 891758.6s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.438s, learning 0.168s)
               Value function loss: 7.5290
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 270.14
               Mean episode length: 238.89
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 8.61s
                        Total time: 22845.83s
                               ETA: 891728.3s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.440s, learning 0.189s)
               Value function loss: 5.9384
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 268.78
               Mean episode length: 240.63
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 8.63s
                        Total time: 22854.46s
                               ETA: 891699.0s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.774s, learning 0.175s)
               Value function loss: 6.1211
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 269.50
               Mean episode length: 239.78
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 8.95s
                        Total time: 22863.41s
                               ETA: 891682.3s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.435s, learning 0.211s)
               Value function loss: 5.9164
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 273.13
               Mean episode length: 243.74
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 8.65s
                        Total time: 22872.06s
                               ETA: 891653.6s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.351s, learning 0.175s)
               Value function loss: 5.8935
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 279.38
               Mean episode length: 244.68
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 8.53s
                        Total time: 22880.58s
                               ETA: 891620.4s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1778 steps/s (collection: 8.994s, learning 0.216s)
               Value function loss: 4.5840
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 280.69
               Mean episode length: 246.71
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 9.21s
                        Total time: 22889.79s
                               ETA: 891613.8s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.386s, learning 0.198s)
               Value function loss: 4.9758
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 270.38
               Mean episode length: 240.13
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 8.58s
                        Total time: 22898.38s
                               ETA: 891582.8s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.729s, learning 0.215s)
               Value function loss: 6.2584
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 276.71
               Mean episode length: 244.83
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 8.94s
                        Total time: 22907.32s
                               ETA: 891565.8s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.579s, learning 0.164s)
               Value function loss: 5.5258
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 278.32
               Mean episode length: 244.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 8.74s
                        Total time: 22916.07s
                               ETA: 891541.0s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.523s, learning 0.204s)
               Value function loss: 5.4346
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 276.64
               Mean episode length: 241.97
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 8.73s
                        Total time: 22924.79s
                               ETA: 891515.6s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.119s, learning 0.185s)
               Value function loss: 5.0411
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 271.06
               Mean episode length: 239.76
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 8.30s
                        Total time: 22933.10s
                               ETA: 891473.8s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.701s, learning 0.196s)
               Value function loss: 6.7907
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 273.01
               Mean episode length: 241.92
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 8.90s
                        Total time: 22941.99s
                               ETA: 891455.1s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.403s, learning 0.213s)
               Value function loss: 5.1781
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 271.90
               Mean episode length: 240.18
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 8.62s
                        Total time: 22950.61s
                               ETA: 891425.4s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.667s, learning 0.185s)
               Value function loss: 4.5910
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 276.45
               Mean episode length: 244.94
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 8.85s
                        Total time: 22959.46s
                               ETA: 891405.0s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.929s, learning 0.200s)
               Value function loss: 5.4114
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 273.08
               Mean episode length: 241.31
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 9.13s
                        Total time: 22968.59s
                               ETA: 891395.3s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.464s, learning 0.170s)
               Value function loss: 4.1683
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 278.95
               Mean episode length: 243.41
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 8.63s
                        Total time: 22977.22s
                               ETA: 891366.4s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.685s, learning 0.170s)
               Value function loss: 5.8636
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 280.32
               Mean episode length: 244.13
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 8.86s
                        Total time: 22986.08s
                               ETA: 891346.1s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.469s, learning 0.164s)
               Value function loss: 6.0056
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 268.13
               Mean episode length: 236.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 8.63s
                        Total time: 22994.71s
                               ETA: 891317.1s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.531s, learning 0.173s)
               Value function loss: 5.0677
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 267.71
               Mean episode length: 234.32
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 8.70s
                        Total time: 23003.42s
                               ETA: 891291.0s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.719s, learning 0.205s)
               Value function loss: 5.5117
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 267.99
               Mean episode length: 234.98
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 8.92s
                        Total time: 23012.34s
                               ETA: 891273.4s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.670s, learning 0.204s)
               Value function loss: 5.5104
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 269.50
               Mean episode length: 238.08
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 8.87s
                        Total time: 23021.21s
                               ETA: 891253.8s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.534s, learning 0.181s)
               Value function loss: 5.6388
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 275.64
               Mean episode length: 239.98
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 8.71s
                        Total time: 23029.93s
                               ETA: 891228.1s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.379s, learning 0.173s)
               Value function loss: 5.7540
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 278.75
               Mean episode length: 241.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 8.55s
                        Total time: 23038.48s
                               ETA: 891196.1s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.538s, learning 0.204s)
               Value function loss: 5.2472
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 275.88
               Mean episode length: 242.02
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 8.74s
                        Total time: 23047.22s
                               ETA: 891171.5s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.430s, learning 0.248s)
               Value function loss: 4.4603
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 266.55
               Mean episode length: 234.23
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 8.68s
                        Total time: 23055.90s
                               ETA: 891144.4s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.794s, learning 0.171s)
               Value function loss: 6.0652
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 265.74
               Mean episode length: 235.02
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 8.97s
                        Total time: 23064.87s
                               ETA: 891128.5s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.253s, learning 0.186s)
               Value function loss: 6.4494
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 272.91
               Mean episode length: 239.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 8.44s
                        Total time: 23073.31s
                               ETA: 891092.2s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.443s, learning 0.180s)
               Value function loss: 6.3645
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 267.71
               Mean episode length: 236.23
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 8.62s
                        Total time: 23081.93s
                               ETA: 891063.1s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.392s, learning 0.197s)
               Value function loss: 5.6543
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 276.23
               Mean episode length: 242.56
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 8.59s
                        Total time: 23090.52s
                               ETA: 891032.6s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.548s, learning 0.208s)
               Value function loss: 5.8934
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 270.36
               Mean episode length: 235.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 8.76s
                        Total time: 23099.28s
                               ETA: 891008.6s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.334s, learning 0.178s)
               Value function loss: 7.3755
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 270.81
               Mean episode length: 237.94
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 8.51s
                        Total time: 23107.79s
                               ETA: 890975.3s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.605s, learning 0.175s)
               Value function loss: 6.6892
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 271.70
               Mean episode length: 238.69
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 8.78s
                        Total time: 23116.57s
                               ETA: 890952.2s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.785s, learning 0.211s)
               Value function loss: 6.9505
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 271.52
               Mean episode length: 240.44
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 9.00s
                        Total time: 23125.56s
                               ETA: 890937.5s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.389s, learning 0.181s)
               Value function loss: 5.4647
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 273.56
               Mean episode length: 240.28
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 8.57s
                        Total time: 23134.13s
                               ETA: 890906.4s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.826s, learning 0.263s)
               Value function loss: 6.6345
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 275.47
               Mean episode length: 240.58
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 9.09s
                        Total time: 23143.22s
                               ETA: 890895.3s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.097s, learning 0.174s)
               Value function loss: 5.4144
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 277.24
               Mean episode length: 242.51
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 9.27s
                        Total time: 23152.49s
                               ETA: 890891.1s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.562s, learning 0.189s)
               Value function loss: 5.6042
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 274.91
               Mean episode length: 241.79
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 8.75s
                        Total time: 23161.24s
                               ETA: 890867.0s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.514s, learning 0.189s)
               Value function loss: 5.9203
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 260.08
               Mean episode length: 229.34
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 8.70s
                        Total time: 23169.95s
                               ETA: 890841.1s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.477s, learning 0.205s)
               Value function loss: 6.5111
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 271.00
               Mean episode length: 236.08
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 8.68s
                        Total time: 23178.63s
                               ETA: 890814.3s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.807s, learning 0.230s)
               Value function loss: 6.5379
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 270.18
               Mean episode length: 236.07
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 9.04s
                        Total time: 23187.67s
                               ETA: 890801.2s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.410s, learning 0.195s)
               Value function loss: 6.3013
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 271.24
               Mean episode length: 235.55
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 8.61s
                        Total time: 23196.27s
                               ETA: 890771.5s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.692s, learning 0.178s)
               Value function loss: 6.5322
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 273.73
               Mean episode length: 238.36
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 8.87s
                        Total time: 23205.14s
                               ETA: 890752.1s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.479s, learning 0.211s)
               Value function loss: 7.3505
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 268.92
               Mean episode length: 236.04
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 8.69s
                        Total time: 23213.83s
                               ETA: 890725.7s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.473s, learning 0.230s)
               Value function loss: 6.3472
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 270.27
               Mean episode length: 238.26
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 8.70s
                        Total time: 23222.53s
                               ETA: 890699.8s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.666s, learning 0.171s)
               Value function loss: 5.5343
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 280.12
               Mean episode length: 245.36
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 8.84s
                        Total time: 23231.37s
                               ETA: 890679.0s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.514s, learning 0.181s)
               Value function loss: 6.7821
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 279.62
               Mean episode length: 245.04
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 8.70s
                        Total time: 23240.07s
                               ETA: 890652.9s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.518s, learning 0.276s)
               Value function loss: 5.3799
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 274.08
               Mean episode length: 241.91
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 8.79s
                        Total time: 23248.86s
                               ETA: 890630.6s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.439s, learning 0.267s)
               Value function loss: 5.9415
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 272.99
               Mean episode length: 239.28
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 8.71s
                        Total time: 23257.57s
                               ETA: 890604.8s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.368s, learning 0.224s)
               Value function loss: 6.3112
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 274.11
               Mean episode length: 241.07
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 8.59s
                        Total time: 23266.16s
                               ETA: 890574.8s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.356s, learning 0.204s)
               Value function loss: 6.1121
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 277.75
               Mean episode length: 244.49
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 8.56s
                        Total time: 23274.72s
                               ETA: 890543.5s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.666s, learning 0.211s)
               Value function loss: 5.5279
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 275.56
               Mean episode length: 241.79
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 8.88s
                        Total time: 23283.59s
                               ETA: 890524.4s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.640s, learning 0.183s)
               Value function loss: 5.3485
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 273.14
               Mean episode length: 239.30
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 8.82s
                        Total time: 23292.42s
                               ETA: 890503.2s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.437s, learning 0.185s)
               Value function loss: 7.2387
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 278.51
               Mean episode length: 243.22
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 8.62s
                        Total time: 23301.04s
                               ETA: 890474.3s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.338s, learning 0.204s)
               Value function loss: 6.9346
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 272.95
               Mean episode length: 239.57
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 8.54s
                        Total time: 23309.58s
                               ETA: 890442.4s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.912s, learning 0.204s)
               Value function loss: 3.9788
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 269.21
               Mean episode length: 237.45
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 9.12s
                        Total time: 23318.70s
                               ETA: 890432.5s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.913s, learning 0.192s)
               Value function loss: 4.3615
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 273.47
               Mean episode length: 242.28
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 9.10s
                        Total time: 23327.80s
                               ETA: 890422.1s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.501s, learning 0.191s)
               Value function loss: 5.7367
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 274.74
               Mean episode length: 242.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 8.69s
                        Total time: 23336.50s
                               ETA: 890396.0s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.486s, learning 0.231s)
               Value function loss: 4.6359
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 268.49
               Mean episode length: 238.44
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 8.72s
                        Total time: 23345.21s
                               ETA: 890370.9s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.547s, learning 0.192s)
               Value function loss: 6.1572
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 277.50
               Mean episode length: 243.23
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 8.74s
                        Total time: 23353.95s
                               ETA: 890346.6s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.779s, learning 0.171s)
               Value function loss: 4.5434
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 273.16
               Mean episode length: 241.01
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 8.95s
                        Total time: 23362.90s
                               ETA: 890330.3s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.289s, learning 0.180s)
               Value function loss: 5.2060
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 280.24
               Mean episode length: 244.86
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 8.47s
                        Total time: 23371.37s
                               ETA: 890295.7s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.276s, learning 0.176s)
               Value function loss: 6.2507
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 268.52
               Mean episode length: 238.96
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 8.45s
                        Total time: 23379.82s
                               ETA: 890260.5s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.732s, learning 0.171s)
               Value function loss: 5.1348
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 270.83
               Mean episode length: 240.69
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 8.90s
                        Total time: 23388.72s
                               ETA: 890242.5s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.434s, learning 0.181s)
               Value function loss: 5.9583
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 265.19
               Mean episode length: 238.86
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 8.61s
                        Total time: 23397.34s
                               ETA: 890213.5s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.795s, learning 0.179s)
               Value function loss: 4.5938
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 269.56
               Mean episode length: 239.83
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 8.97s
                        Total time: 23406.31s
                               ETA: 890198.2s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.315s, learning 0.179s)
               Value function loss: 4.8620
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 272.29
               Mean episode length: 242.53
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 8.49s
                        Total time: 23414.81s
                               ETA: 890164.7s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.488s, learning 0.206s)
               Value function loss: 4.8809
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 273.86
               Mean episode length: 246.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 8.69s
                        Total time: 23423.50s
                               ETA: 890138.8s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.310s, learning 0.173s)
               Value function loss: 4.7459
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 267.63
               Mean episode length: 238.97
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 8.48s
                        Total time: 23431.99s
                               ETA: 890104.8s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.378s, learning 0.198s)
               Value function loss: 4.2915
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 264.99
               Mean episode length: 235.59
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 8.58s
                        Total time: 23440.56s
                               ETA: 890074.5s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.593s, learning 0.178s)
               Value function loss: 4.8525
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 269.68
               Mean episode length: 242.90
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 8.77s
                        Total time: 23449.33s
                               ETA: 890051.5s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.007s, learning 0.206s)
               Value function loss: 5.1350
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 270.19
               Mean episode length: 242.12
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 9.21s
                        Total time: 23458.55s
                               ETA: 890045.4s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.712s, learning 0.246s)
               Value function loss: 4.4856
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 272.57
               Mean episode length: 244.36
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 8.96s
                        Total time: 23467.50s
                               ETA: 890029.5s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.148s, learning 0.211s)
               Value function loss: 4.6424
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 274.33
               Mean episode length: 246.15
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 9.36s
                        Total time: 23476.86s
                               ETA: 890028.8s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.263s, learning 0.191s)
               Value function loss: 5.1765
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 271.05
               Mean episode length: 241.32
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 8.45s
                        Total time: 23485.31s
                               ETA: 889993.9s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.687s, learning 0.188s)
               Value function loss: 6.4682
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 274.05
               Mean episode length: 244.84
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 8.87s
                        Total time: 23494.19s
                               ETA: 889974.9s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.687s, learning 0.221s)
               Value function loss: 4.2770
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 274.22
               Mean episode length: 245.99
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 8.91s
                        Total time: 23503.10s
                               ETA: 889957.2s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.451s, learning 0.207s)
               Value function loss: 5.0138
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 271.79
               Mean episode length: 242.37
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 8.66s
                        Total time: 23511.76s
                               ETA: 889930.0s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.696s, learning 0.182s)
               Value function loss: 4.3418
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 265.69
               Mean episode length: 236.23
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 8.88s
                        Total time: 23520.63s
                               ETA: 889911.2s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.481s, learning 0.279s)
               Value function loss: 5.8521
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 274.44
               Mean episode length: 243.68
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 8.76s
                        Total time: 23529.39s
                               ETA: 889887.9s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.080s, learning 0.199s)
               Value function loss: 6.4664
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 268.96
               Mean episode length: 239.57
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 9.28s
                        Total time: 23538.67s
                               ETA: 889884.3s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.705s, learning 0.176s)
               Value function loss: 5.6816
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 265.10
               Mean episode length: 235.27
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 8.88s
                        Total time: 23547.55s
                               ETA: 889865.6s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.507s, learning 0.181s)
               Value function loss: 5.5649
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 261.22
               Mean episode length: 232.05
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 8.69s
                        Total time: 23556.24s
                               ETA: 889839.6s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.286s, learning 0.197s)
               Value function loss: 4.7629
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 274.21
               Mean episode length: 242.75
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 8.48s
                        Total time: 23564.73s
                               ETA: 889805.9s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1777 steps/s (collection: 8.996s, learning 0.221s)
               Value function loss: 7.6962
                    Surrogate loss: 0.0036
             Mean action noise std: 0.76
                       Mean reward: 277.80
               Mean episode length: 245.06
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 9.22s
                        Total time: 23573.94s
                               ETA: 889799.9s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1782 steps/s (collection: 8.934s, learning 0.257s)
               Value function loss: 7.5779
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 278.23
               Mean episode length: 243.14
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 9.19s
                        Total time: 23583.13s
                               ETA: 889792.9s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.672s, learning 0.240s)
               Value function loss: 5.0047
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 271.95
               Mean episode length: 242.15
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 8.91s
                        Total time: 23592.05s
                               ETA: 889775.4s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.542s, learning 0.205s)
               Value function loss: 6.3727
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 274.62
               Mean episode length: 242.23
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 8.75s
                        Total time: 23600.79s
                               ETA: 889751.7s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.689s, learning 0.184s)
               Value function loss: 5.1459
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 278.62
               Mean episode length: 244.97
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 8.87s
                        Total time: 23609.67s
                               ETA: 889732.7s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.435s, learning 0.254s)
               Value function loss: 5.9514
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 274.32
               Mean episode length: 242.97
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 8.69s
                        Total time: 23618.35s
                               ETA: 889706.9s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.644s, learning 0.191s)
               Value function loss: 7.4552
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 278.45
               Mean episode length: 246.78
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 8.83s
                        Total time: 23627.19s
                               ETA: 889686.5s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.350s, learning 0.189s)
               Value function loss: 6.0462
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 276.92
               Mean episode length: 245.16
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 8.54s
                        Total time: 23635.73s
                               ETA: 889655.0s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.576s, learning 0.237s)
               Value function loss: 5.6033
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 280.41
               Mean episode length: 246.99
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 8.81s
                        Total time: 23644.54s
                               ETA: 889633.8s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.616s, learning 0.204s)
               Value function loss: 5.5554
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 279.11
               Mean episode length: 246.23
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 8.82s
                        Total time: 23653.36s
                               ETA: 889612.9s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.026s, learning 0.193s)
               Value function loss: 5.7723
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 280.79
               Mean episode length: 247.27
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 9.22s
                        Total time: 23662.58s
                               ETA: 889607.0s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.630s, learning 0.203s)
               Value function loss: 7.2840
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 279.48
               Mean episode length: 246.96
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 8.83s
                        Total time: 23671.41s
                               ETA: 889586.6s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.668s, learning 0.189s)
               Value function loss: 5.1470
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 280.76
               Mean episode length: 247.11
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 8.86s
                        Total time: 23680.27s
                               ETA: 889567.1s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.524s, learning 0.210s)
               Value function loss: 6.1114
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 279.57
               Mean episode length: 244.45
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 8.73s
                        Total time: 23689.00s
                               ETA: 889543.0s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.531s, learning 0.196s)
               Value function loss: 5.9831
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 277.67
               Mean episode length: 244.41
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 8.73s
                        Total time: 23697.73s
                               ETA: 889518.7s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.563s, learning 0.214s)
               Value function loss: 4.4342
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 275.74
               Mean episode length: 244.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 8.78s
                        Total time: 23706.51s
                               ETA: 889496.3s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.737s, learning 0.333s)
               Value function loss: 4.3717
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 281.31
               Mean episode length: 247.97
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 9.07s
                        Total time: 23715.58s
                               ETA: 889484.8s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.519s, learning 0.210s)
               Value function loss: 4.7924
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 265.50
               Mean episode length: 239.54
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 8.73s
                        Total time: 23724.31s
                               ETA: 889460.6s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.568s, learning 0.193s)
               Value function loss: 6.0818
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 279.79
               Mean episode length: 247.97
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 8.76s
                        Total time: 23733.07s
                               ETA: 889437.5s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.629s, learning 0.172s)
               Value function loss: 5.2623
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 284.84
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 8.80s
                        Total time: 23741.87s
                               ETA: 889416.0s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.632s, learning 0.198s)
               Value function loss: 5.0565
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 282.79
               Mean episode length: 249.15
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 8.83s
                        Total time: 23750.70s
                               ETA: 889395.6s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.437s, learning 0.221s)
               Value function loss: 4.7504
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 279.90
               Mean episode length: 247.26
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 8.66s
                        Total time: 23759.36s
                               ETA: 889368.7s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.582s, learning 0.176s)
               Value function loss: 5.9036
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 282.71
               Mean episode length: 248.70
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 8.76s
                        Total time: 23768.11s
                               ETA: 889345.6s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.444s, learning 0.254s)
               Value function loss: 4.2930
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 276.85
               Mean episode length: 244.42
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 8.70s
                        Total time: 23776.81s
                               ETA: 889320.3s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.511s, learning 0.322s)
               Value function loss: 4.7646
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 282.62
               Mean episode length: 246.27
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 8.83s
                        Total time: 23785.64s
                               ETA: 889300.0s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.174s)
               Value function loss: 4.5225
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 274.77
               Mean episode length: 242.02
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 8.59s
                        Total time: 23794.23s
                               ETA: 889270.7s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.411s, learning 0.199s)
               Value function loss: 4.9604
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 270.47
               Mean episode length: 239.50
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 8.61s
                        Total time: 23802.85s
                               ETA: 889242.2s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.748s, learning 0.197s)
               Value function loss: 5.3438
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 276.19
               Mean episode length: 243.97
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 8.95s
                        Total time: 23811.79s
                               ETA: 889226.1s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.406s, learning 0.208s)
               Value function loss: 4.8551
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 274.56
               Mean episode length: 244.65
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 8.61s
                        Total time: 23820.40s
                               ETA: 889197.7s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.421s, learning 0.207s)
               Value function loss: 4.7532
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 276.78
               Mean episode length: 246.24
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 8.63s
                        Total time: 23829.03s
                               ETA: 889169.9s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.453s, learning 0.186s)
               Value function loss: 5.5420
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 276.75
               Mean episode length: 242.57
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 8.64s
                        Total time: 23837.67s
                               ETA: 889142.4s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.689s, learning 0.209s)
               Value function loss: 6.8044
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 278.54
               Mean episode length: 245.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 8.90s
                        Total time: 23846.57s
                               ETA: 889124.7s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.075s, learning 0.196s)
               Value function loss: 6.4152
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 280.34
               Mean episode length: 248.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 8.27s
                        Total time: 23854.84s
                               ETA: 889083.6s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.797s, learning 0.203s)
               Value function loss: 5.4586
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 274.65
               Mean episode length: 244.66
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 9.00s
                        Total time: 23863.84s
                               ETA: 889069.6s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.913s, learning 0.179s)
               Value function loss: 6.2005
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 274.21
               Mean episode length: 243.85
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 9.09s
                        Total time: 23872.93s
                               ETA: 889059.1s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.449s, learning 0.176s)
               Value function loss: 4.4489
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 277.39
               Mean episode length: 243.68
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 8.62s
                        Total time: 23881.56s
                               ETA: 889031.1s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.921s, learning 0.180s)
               Value function loss: 5.7492
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 276.77
               Mean episode length: 243.74
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 9.10s
                        Total time: 23890.66s
                               ETA: 889021.0s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.410s, learning 0.169s)
               Value function loss: 6.3978
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 275.69
               Mean episode length: 246.36
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 8.58s
                        Total time: 23899.24s
                               ETA: 888991.4s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.876s, learning 0.195s)
               Value function loss: 5.6170
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 285.51
               Mean episode length: 249.45
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 9.07s
                        Total time: 23908.31s
                               ETA: 888980.1s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.529s, learning 0.268s)
               Value function loss: 5.1420
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 276.61
               Mean episode length: 243.45
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 8.80s
                        Total time: 23917.10s
                               ETA: 888958.6s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.171s)
               Value function loss: 5.4590
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 272.19
               Mean episode length: 241.89
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 8.86s
                        Total time: 23925.97s
                               ETA: 888939.6s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.360s, learning 0.180s)
               Value function loss: 6.6895
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 275.80
               Mean episode length: 242.08
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 8.54s
                        Total time: 23934.51s
                               ETA: 888908.6s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.676s, learning 0.234s)
               Value function loss: 7.2132
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 275.59
               Mean episode length: 241.72
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 8.91s
                        Total time: 23943.42s
                               ETA: 888891.3s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.503s, learning 0.188s)
               Value function loss: 5.5727
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 273.19
               Mean episode length: 241.11
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 8.69s
                        Total time: 23952.11s
                               ETA: 888866.0s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.755s, learning 0.186s)
               Value function loss: 6.1543
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 272.25
               Mean episode length: 239.17
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 8.94s
                        Total time: 23961.05s
                               ETA: 888850.0s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.759s, learning 0.269s)
               Value function loss: 5.4581
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 275.49
               Mean episode length: 242.99
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 9.03s
                        Total time: 23970.08s
                               ETA: 888837.1s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.853s, learning 0.199s)
               Value function loss: 5.0621
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 279.09
               Mean episode length: 247.93
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 9.05s
                        Total time: 23979.13s
                               ETA: 888825.2s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.698s, learning 0.170s)
               Value function loss: 4.8690
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 279.34
               Mean episode length: 247.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 8.87s
                        Total time: 23988.00s
                               ETA: 888806.4s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.634s, learning 0.172s)
               Value function loss: 5.2087
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 271.01
               Mean episode length: 241.86
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 8.81s
                        Total time: 23996.80s
                               ETA: 888785.4s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.656s, learning 0.182s)
               Value function loss: 6.3235
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 268.35
               Mean episode length: 242.42
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 8.84s
                        Total time: 24005.64s
                               ETA: 888765.5s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.566s, learning 0.179s)
               Value function loss: 6.1851
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 263.00
               Mean episode length: 236.77
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 8.75s
                        Total time: 24014.39s
                               ETA: 888742.2s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.499s, learning 0.243s)
               Value function loss: 5.6759
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 269.80
               Mean episode length: 242.48
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 8.74s
                        Total time: 24023.13s
                               ETA: 888718.8s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.781s, learning 0.254s)
               Value function loss: 4.7135
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 276.59
               Mean episode length: 246.22
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 9.03s
                        Total time: 24032.16s
                               ETA: 888706.3s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.439s, learning 0.204s)
               Value function loss: 5.8158
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 274.32
               Mean episode length: 239.50
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 8.64s
                        Total time: 24040.81s
                               ETA: 888679.2s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.678s, learning 0.292s)
               Value function loss: 4.3933
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 279.50
               Mean episode length: 247.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 8.97s
                        Total time: 24049.78s
                               ETA: 888664.3s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.820s, learning 0.204s)
               Value function loss: 4.3969
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 272.46
               Mean episode length: 243.97
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 9.02s
                        Total time: 24058.80s
                               ETA: 888651.4s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.174s)
               Value function loss: 5.3081
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 275.97
               Mean episode length: 243.12
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 8.59s
                        Total time: 24067.39s
                               ETA: 888622.3s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.397s, learning 0.195s)
               Value function loss: 4.5883
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 275.77
               Mean episode length: 241.30
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 8.59s
                        Total time: 24075.98s
                               ETA: 888593.4s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.528s, learning 0.400s)
               Value function loss: 5.1951
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 269.34
               Mean episode length: 241.61
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 8.93s
                        Total time: 24084.91s
                               ETA: 888577.0s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.701s, learning 0.200s)
               Value function loss: 5.1632
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 270.14
               Mean episode length: 244.01
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 8.90s
                        Total time: 24093.81s
                               ETA: 888559.5s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.863s, learning 0.170s)
               Value function loss: 5.0720
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 276.26
               Mean episode length: 245.01
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 9.03s
                        Total time: 24102.84s
                               ETA: 888546.9s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.541s, learning 0.184s)
               Value function loss: 5.3187
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 274.25
               Mean episode length: 240.69
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 8.72s
                        Total time: 24111.56s
                               ETA: 888523.0s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.506s, learning 0.195s)
               Value function loss: 5.3000
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 276.50
               Mean episode length: 242.91
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 8.70s
                        Total time: 24120.26s
                               ETA: 888498.2s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.481s, learning 0.197s)
               Value function loss: 5.6880
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 275.24
               Mean episode length: 242.82
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 8.68s
                        Total time: 24128.94s
                               ETA: 888472.5s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.237s, learning 0.214s)
               Value function loss: 5.3722
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 273.95
               Mean episode length: 243.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 8.45s
                        Total time: 24137.39s
                               ETA: 888438.6s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.755s, learning 0.178s)
               Value function loss: 4.5288
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 277.35
               Mean episode length: 245.30
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 8.93s
                        Total time: 24146.33s
                               ETA: 888422.4s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.755s, learning 0.191s)
               Value function loss: 4.1497
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 274.15
               Mean episode length: 243.73
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 8.95s
                        Total time: 24155.27s
                               ETA: 888406.6s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.614s, learning 0.173s)
               Value function loss: 5.8071
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 278.45
               Mean episode length: 248.14
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 8.79s
                        Total time: 24164.06s
                               ETA: 888385.1s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.422s, learning 0.176s)
               Value function loss: 5.5549
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 270.68
               Mean episode length: 242.34
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 8.60s
                        Total time: 24172.66s
                               ETA: 888356.5s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.797s, learning 0.183s)
               Value function loss: 5.2408
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 274.48
               Mean episode length: 243.03
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 8.98s
                        Total time: 24181.64s
                               ETA: 888342.1s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.455s, learning 0.219s)
               Value function loss: 4.8066
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 268.40
               Mean episode length: 240.82
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 8.67s
                        Total time: 24190.31s
                               ETA: 888316.4s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.435s, learning 0.241s)
               Value function loss: 4.9109
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 266.11
               Mean episode length: 239.35
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 8.68s
                        Total time: 24198.99s
                               ETA: 888290.7s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.784s, learning 0.179s)
               Value function loss: 7.0161
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 268.61
               Mean episode length: 240.79
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 8.96s
                        Total time: 24207.95s
                               ETA: 888275.7s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.334s, learning 0.198s)
               Value function loss: 5.7121
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 272.11
               Mean episode length: 242.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 8.53s
                        Total time: 24216.48s
                               ETA: 888244.8s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.707s, learning 0.179s)
               Value function loss: 5.9998
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 278.19
               Mean episode length: 246.21
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 8.89s
                        Total time: 24225.36s
                               ETA: 888226.9s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.461s, learning 0.176s)
               Value function loss: 5.0887
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 282.50
               Mean episode length: 247.90
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 8.64s
                        Total time: 24234.00s
                               ETA: 888199.9s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.706s, learning 0.169s)
               Value function loss: 4.5521
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 282.30
               Mean episode length: 247.99
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 8.87s
                        Total time: 24242.88s
                               ETA: 888181.6s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.555s, learning 0.179s)
               Value function loss: 4.5565
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 275.26
               Mean episode length: 244.38
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 8.73s
                        Total time: 24251.61s
                               ETA: 888158.2s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.837s, learning 0.297s)
               Value function loss: 3.6862
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 274.79
               Mean episode length: 243.46
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 9.13s
                        Total time: 24260.74s
                               ETA: 888149.5s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.611s, learning 0.263s)
               Value function loss: 5.7004
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 273.12
               Mean episode length: 244.65
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 8.87s
                        Total time: 24269.62s
                               ETA: 888131.2s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.624s, learning 0.180s)
               Value function loss: 4.9536
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 272.99
               Mean episode length: 247.60
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 8.80s
                        Total time: 24278.42s
                               ETA: 888110.3s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.544s, learning 0.176s)
               Value function loss: 5.1820
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 277.88
               Mean episode length: 247.43
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 8.72s
                        Total time: 24287.14s
                               ETA: 888086.4s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.750s, learning 0.185s)
               Value function loss: 4.9429
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 271.77
               Mean episode length: 243.14
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 8.93s
                        Total time: 24296.08s
                               ETA: 888070.4s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.175s)
               Value function loss: 4.9468
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 266.55
               Mean episode length: 240.71
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 8.87s
                        Total time: 24304.94s
                               ETA: 888051.8s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.868s, learning 0.185s)
               Value function loss: 5.5393
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 275.73
               Mean episode length: 246.54
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 9.05s
                        Total time: 24313.99s
                               ETA: 888040.1s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.691s, learning 0.205s)
               Value function loss: 5.5357
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 281.22
               Mean episode length: 249.91
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 8.90s
                        Total time: 24322.89s
                               ETA: 888022.7s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.635s, learning 0.208s)
               Value function loss: 4.0080
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 280.93
               Mean episode length: 247.86
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 8.84s
                        Total time: 24331.73s
                               ETA: 888003.3s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.784s, learning 0.178s)
               Value function loss: 6.1904
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 279.17
               Mean episode length: 247.98
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 8.96s
                        Total time: 24340.69s
                               ETA: 887988.3s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.341s, learning 0.179s)
               Value function loss: 4.9604
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 280.02
               Mean episode length: 246.81
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 8.52s
                        Total time: 24349.21s
                               ETA: 887957.2s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.555s, learning 0.174s)
               Value function loss: 5.8087
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 281.25
               Mean episode length: 247.91
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 8.73s
                        Total time: 24357.94s
                               ETA: 887933.7s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.730s, learning 0.192s)
               Value function loss: 5.8710
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 278.61
               Mean episode length: 245.18
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 8.92s
                        Total time: 24366.87s
                               ETA: 887917.3s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.595s, learning 0.170s)
               Value function loss: 6.0786
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 266.61
               Mean episode length: 236.37
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 8.77s
                        Total time: 24375.63s
                               ETA: 887895.1s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.599s, learning 0.170s)
               Value function loss: 6.0853
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 275.60
               Mean episode length: 244.02
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 8.77s
                        Total time: 24384.40s
                               ETA: 887873.2s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.628s, learning 0.178s)
               Value function loss: 5.1820
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 277.41
               Mean episode length: 244.68
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 8.81s
                        Total time: 24393.21s
                               ETA: 887852.5s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.647s, learning 0.181s)
               Value function loss: 6.8802
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 275.55
               Mean episode length: 242.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 8.83s
                        Total time: 24402.03s
                               ETA: 887832.7s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.591s, learning 0.173s)
               Value function loss: 6.7401
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 281.43
               Mean episode length: 246.47
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 8.76s
                        Total time: 24410.80s
                               ETA: 887810.5s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.427s, learning 0.333s)
               Value function loss: 4.6297
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 279.18
               Mean episode length: 246.59
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 8.76s
                        Total time: 24419.56s
                               ETA: 887788.3s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.622s, learning 0.195s)
               Value function loss: 5.0285
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 279.12
               Mean episode length: 246.03
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 8.82s
                        Total time: 24428.38s
                               ETA: 887768.0s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.412s, learning 0.190s)
               Value function loss: 6.1334
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 276.88
               Mean episode length: 243.62
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 8.60s
                        Total time: 24436.98s
                               ETA: 887740.0s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.840s, learning 0.183s)
               Value function loss: 6.0454
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 277.86
               Mean episode length: 245.21
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 9.02s
                        Total time: 24446.00s
                               ETA: 887727.3s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.592s, learning 0.194s)
               Value function loss: 7.1396
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 278.45
               Mean episode length: 244.77
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 8.79s
                        Total time: 24454.79s
                               ETA: 887706.0s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.729s, learning 0.183s)
               Value function loss: 6.0880
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 282.93
               Mean episode length: 247.73
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 8.91s
                        Total time: 24463.70s
                               ETA: 887689.3s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.704s, learning 0.184s)
               Value function loss: 6.5528
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 280.67
               Mean episode length: 244.37
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 8.89s
                        Total time: 24472.59s
                               ETA: 887671.7s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.876s, learning 0.171s)
               Value function loss: 7.4030
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 278.57
               Mean episode length: 246.20
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 9.05s
                        Total time: 24481.63s
                               ETA: 887659.9s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.581s, learning 0.177s)
               Value function loss: 6.2424
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 272.45
               Mean episode length: 241.23
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 8.76s
                        Total time: 24490.39s
                               ETA: 887637.6s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.484s, learning 0.167s)
               Value function loss: 6.2855
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 275.93
               Mean episode length: 243.10
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 8.65s
                        Total time: 24499.04s
                               ETA: 887611.5s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.833s, learning 0.169s)
               Value function loss: 5.2771
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 275.27
               Mean episode length: 243.53
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 9.00s
                        Total time: 24508.04s
                               ETA: 887598.0s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.401s, learning 0.177s)
               Value function loss: 4.6523
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 276.42
               Mean episode length: 242.60
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 8.58s
                        Total time: 24516.62s
                               ETA: 887569.2s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.868s, learning 0.220s)
               Value function loss: 5.9596
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 278.70
               Mean episode length: 245.80
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 9.09s
                        Total time: 24525.71s
                               ETA: 887558.9s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.514s, learning 0.195s)
               Value function loss: 5.0048
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 278.74
               Mean episode length: 245.88
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 8.71s
                        Total time: 24534.42s
                               ETA: 887534.9s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.493s, learning 0.181s)
               Value function loss: 5.5617
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 277.15
               Mean episode length: 242.83
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 8.67s
                        Total time: 24543.09s
                               ETA: 887509.6s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.888s, learning 0.201s)
               Value function loss: 6.0333
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 279.14
               Mean episode length: 244.18
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 9.09s
                        Total time: 24552.18s
                               ETA: 887499.4s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.720s, learning 0.180s)
               Value function loss: 5.9138
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 276.73
               Mean episode length: 244.48
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 8.90s
                        Total time: 24561.08s
                               ETA: 887482.3s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.898s, learning 0.175s)
               Value function loss: 5.6120
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 286.22
               Mean episode length: 249.24
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 9.07s
                        Total time: 24570.16s
                               ETA: 887471.5s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.779s, learning 0.251s)
               Value function loss: 5.3473
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 273.29
               Mean episode length: 239.83
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 9.03s
                        Total time: 24579.19s
                               ETA: 887459.1s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.569s, learning 0.246s)
               Value function loss: 5.4374
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 277.45
               Mean episode length: 242.66
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 8.81s
                        Total time: 24588.00s
                               ETA: 887438.9s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.349s, learning 0.165s)
               Value function loss: 6.8393
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 276.83
               Mean episode length: 241.24
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 8.51s
                        Total time: 24596.51s
                               ETA: 887407.9s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.670s, learning 0.168s)
               Value function loss: 4.3585
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 274.79
               Mean episode length: 240.51
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 8.84s
                        Total time: 24605.35s
                               ETA: 887388.6s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.724s, learning 0.211s)
               Value function loss: 5.1624
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 277.74
               Mean episode length: 244.06
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 8.94s
                        Total time: 24614.29s
                               ETA: 887372.9s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.540s, learning 0.235s)
               Value function loss: 4.8654
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 274.95
               Mean episode length: 241.05
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 8.77s
                        Total time: 24623.06s
                               ETA: 887351.3s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.474s, learning 0.174s)
               Value function loss: 5.6725
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 277.72
               Mean episode length: 242.55
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 8.65s
                        Total time: 24631.71s
                               ETA: 887325.2s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.685s, learning 0.281s)
               Value function loss: 7.2294
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 284.39
               Mean episode length: 248.06
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 8.97s
                        Total time: 24640.68s
                               ETA: 887310.6s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.503s, learning 0.167s)
               Value function loss: 5.6965
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 284.86
               Mean episode length: 248.06
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 8.67s
                        Total time: 24649.35s
                               ETA: 887285.3s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.770s, learning 0.176s)
               Value function loss: 5.0097
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 284.82
               Mean episode length: 247.89
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 8.95s
                        Total time: 24658.29s
                               ETA: 887269.9s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.676s, learning 0.179s)
               Value function loss: 5.4420
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 276.58
               Mean episode length: 242.83
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 8.86s
                        Total time: 24667.15s
                               ETA: 887251.3s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.399s, learning 0.189s)
               Value function loss: 6.4175
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 274.42
               Mean episode length: 241.71
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 8.59s
                        Total time: 24675.74s
                               ETA: 887223.1s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.463s, learning 0.201s)
               Value function loss: 7.7008
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 281.03
               Mean episode length: 244.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 8.66s
                        Total time: 24684.40s
                               ETA: 887197.6s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.460s, learning 0.218s)
               Value function loss: 5.1874
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 280.62
               Mean episode length: 241.82
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 8.68s
                        Total time: 24693.08s
                               ETA: 887172.7s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.657s, learning 0.183s)
               Value function loss: 6.7272
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 279.60
               Mean episode length: 241.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 8.84s
                        Total time: 24701.92s
                               ETA: 887153.6s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.634s, learning 0.177s)
               Value function loss: 6.3970
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 280.34
               Mean episode length: 243.11
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 8.81s
                        Total time: 24710.73s
                               ETA: 887133.4s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.575s, learning 0.173s)
               Value function loss: 6.3030
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 278.91
               Mean episode length: 244.52
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 8.75s
                        Total time: 24719.48s
                               ETA: 887111.0s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.484s, learning 0.211s)
               Value function loss: 8.0926
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 283.17
               Mean episode length: 246.28
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 8.70s
                        Total time: 24728.17s
                               ETA: 887086.7s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.397s, learning 0.168s)
               Value function loss: 6.1974
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 289.11
               Mean episode length: 249.75
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 8.57s
                        Total time: 24736.74s
                               ETA: 887057.8s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.280s, learning 0.181s)
               Value function loss: 5.6543
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 287.79
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 8.46s
                        Total time: 24745.20s
                               ETA: 887025.1s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.614s, learning 0.170s)
               Value function loss: 6.4639
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 288.46
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 8.78s
                        Total time: 24753.98s
                               ETA: 887004.1s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.719s, learning 0.188s)
               Value function loss: 6.5212
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 285.53
               Mean episode length: 248.19
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 12.91s
                        Total time: 24766.89s
                               ETA: 887130.7s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.924s, learning 0.188s)
               Value function loss: 8.0133
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 281.49
               Mean episode length: 248.19
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 17.11s
                        Total time: 24784.00s
                               ETA: 887407.8s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 951 steps/s (collection: 16.964s, learning 0.254s)
               Value function loss: 5.9670
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 284.62
               Mean episode length: 247.93
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 17.22s
                        Total time: 24801.22s
                               ETA: 887688.4s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.715s, learning 0.183s)
               Value function loss: 6.5584
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 283.34
               Mean episode length: 246.73
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 16.90s
                        Total time: 24818.12s
                               ETA: 887957.4s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.741s, learning 0.212s)
               Value function loss: 6.3857
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 282.70
               Mean episode length: 247.93
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 16.95s
                        Total time: 24835.07s
                               ETA: 888228.1s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.846s, learning 0.195s)
               Value function loss: 4.6803
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 282.93
               Mean episode length: 247.93
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 17.04s
                        Total time: 24852.11s
                               ETA: 888501.8s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.621s, learning 0.170s)
               Value function loss: 4.9503
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 285.70
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 16.79s
                        Total time: 24868.90s
                               ETA: 888766.4s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.072s, learning 0.218s)
               Value function loss: 6.2139
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 290.09
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 17.29s
                        Total time: 24886.19s
                               ETA: 889048.5s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.695s, learning 0.191s)
               Value function loss: 7.1933
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 284.66
               Mean episode length: 248.56
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 16.89s
                        Total time: 24903.08s
                               ETA: 889316.0s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.851s, learning 0.181s)
               Value function loss: 6.9199
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 282.94
               Mean episode length: 244.58
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 17.03s
                        Total time: 24920.11s
                               ETA: 889588.5s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.793s, learning 0.236s)
               Value function loss: 6.3247
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 281.48
               Mean episode length: 243.03
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 17.03s
                        Total time: 24937.14s
                               ETA: 889860.7s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.885s, learning 0.280s)
               Value function loss: 6.1662
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 284.69
               Mean episode length: 246.60
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 17.16s
                        Total time: 24954.30s
                               ETA: 890137.5s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 940 steps/s (collection: 17.159s, learning 0.262s)
               Value function loss: 7.0879
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 288.13
               Mean episode length: 249.59
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 17.42s
                        Total time: 24971.73s
                               ETA: 890423.3s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.797s, learning 0.182s)
               Value function loss: 5.1277
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 282.13
               Mean episode length: 246.82
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 16.98s
                        Total time: 24988.71s
                               ETA: 890693.1s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 937 steps/s (collection: 17.246s, learning 0.231s)
               Value function loss: 5.4441
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 272.95
               Mean episode length: 240.26
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 17.48s
                        Total time: 25006.18s
                               ETA: 890980.3s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.776s, learning 0.189s)
               Value function loss: 6.1934
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 282.20
               Mean episode length: 247.10
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 16.96s
                        Total time: 25023.15s
                               ETA: 891249.2s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.817s, learning 0.222s)
               Value function loss: 6.4677
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 279.39
               Mean episode length: 242.70
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 17.04s
                        Total time: 25040.19s
                               ETA: 891520.4s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.519s, learning 0.190s)
               Value function loss: 6.5168
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 278.47
               Mean episode length: 242.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 16.71s
                        Total time: 25056.89s
                               ETA: 891779.7s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.468s, learning 0.230s)
               Value function loss: 6.6407
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 283.69
               Mean episode length: 248.73
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 16.70s
                        Total time: 25073.59s
                               ETA: 892038.5s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.113s, learning 0.195s)
               Value function loss: 5.1985
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 280.24
               Mean episode length: 242.01
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 17.31s
                        Total time: 25090.90s
                               ETA: 892318.7s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.889s, learning 0.178s)
               Value function loss: 6.6943
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 277.87
               Mean episode length: 242.07
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 17.07s
                        Total time: 25107.97s
                               ETA: 892590.1s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.978s, learning 0.207s)
               Value function loss: 6.8655
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 276.47
               Mean episode length: 243.26
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 17.18s
                        Total time: 25125.15s
                               ETA: 892865.5s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.914s, learning 0.210s)
               Value function loss: 6.5207
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 286.09
               Mean episode length: 248.29
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 17.12s
                        Total time: 25142.28s
                               ETA: 893138.5s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 943 steps/s (collection: 17.195s, learning 0.175s)
               Value function loss: 6.4008
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 281.28
               Mean episode length: 242.90
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 17.37s
                        Total time: 25159.65s
                               ETA: 893420.0s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.876s, learning 0.273s)
               Value function loss: 6.2865
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 278.86
               Mean episode length: 241.06
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 17.15s
                        Total time: 25176.79s
                               ETA: 893693.5s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.981s, learning 0.193s)
               Value function loss: 5.6435
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 284.72
               Mean episode length: 248.32
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 17.17s
                        Total time: 25193.97s
                               ETA: 893967.7s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.752s, learning 0.252s)
               Value function loss: 5.5222
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 282.54
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 17.00s
                        Total time: 25210.97s
                               ETA: 894235.6s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.689s, learning 0.188s)
               Value function loss: 6.8450
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 284.86
               Mean episode length: 248.56
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 16.88s
                        Total time: 25227.85s
                               ETA: 894498.8s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 944 steps/s (collection: 17.110s, learning 0.245s)
               Value function loss: 5.3940
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 283.26
               Mean episode length: 246.04
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 17.36s
                        Total time: 25245.20s
                               ETA: 894778.7s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.451s, learning 0.191s)
               Value function loss: 5.2656
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 279.56
               Mean episode length: 243.64
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 16.64s
                        Total time: 25261.85s
                               ETA: 895033.2s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.680s, learning 0.224s)
               Value function loss: 5.3314
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 283.62
               Mean episode length: 246.04
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 16.90s
                        Total time: 25278.75s
                               ETA: 895296.8s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.926s, learning 0.196s)
               Value function loss: 6.6803
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 281.84
               Mean episode length: 245.13
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 17.12s
                        Total time: 25295.87s
                               ETA: 895567.9s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.744s, learning 0.216s)
               Value function loss: 7.2041
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 276.72
               Mean episode length: 243.15
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 16.96s
                        Total time: 25312.83s
                               ETA: 895833.0s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.858s, learning 0.264s)
               Value function loss: 5.0817
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 272.49
               Mean episode length: 240.28
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 17.12s
                        Total time: 25329.96s
                               ETA: 896103.6s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.333s, learning 0.170s)
               Value function loss: 6.5364
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 276.89
               Mean episode length: 243.28
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 16.50s
                        Total time: 25346.46s
                               ETA: 896352.2s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.621s, learning 0.265s)
               Value function loss: 5.3162
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 279.60
               Mean episode length: 246.39
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 16.89s
                        Total time: 25363.34s
                               ETA: 896614.0s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.147s, learning 0.184s)
               Value function loss: 5.1320
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 283.29
               Mean episode length: 246.43
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 17.33s
                        Total time: 25380.68s
                               ETA: 896891.4s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.828s, learning 0.238s)
               Value function loss: 5.0204
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 281.25
               Mean episode length: 245.49
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 17.07s
                        Total time: 25397.74s
                               ETA: 897159.3s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1595 steps/s (collection: 10.054s, learning 0.214s)
               Value function loss: 5.8365
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 277.70
               Mean episode length: 243.37
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 10.27s
                        Total time: 25408.01s
                               ETA: 897186.9s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.666s, learning 0.188s)
               Value function loss: 6.6040
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 283.18
               Mean episode length: 248.19
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 8.85s
                        Total time: 25416.86s
                               ETA: 897164.5s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.747s, learning 0.246s)
               Value function loss: 7.0992
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 278.80
               Mean episode length: 245.27
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 8.99s
                        Total time: 25425.86s
                               ETA: 897147.1s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.831s, learning 0.194s)
               Value function loss: 5.8936
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 281.31
               Mean episode length: 244.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 9.03s
                        Total time: 25434.88s
                               ETA: 897130.8s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.098s, learning 0.215s)
               Value function loss: 5.6043
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 276.32
               Mean episode length: 241.82
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 8.31s
                        Total time: 25443.19s
                               ETA: 897089.4s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.475s, learning 0.204s)
               Value function loss: 6.2002
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 276.45
               Mean episode length: 240.85
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 8.68s
                        Total time: 25451.87s
                               ETA: 897060.9s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.549s, learning 0.213s)
               Value function loss: 5.8996
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 280.97
               Mean episode length: 245.86
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 8.76s
                        Total time: 25460.64s
                               ETA: 897035.4s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.698s, learning 0.185s)
               Value function loss: 5.4989
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 279.50
               Mean episode length: 243.43
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 8.88s
                        Total time: 25469.52s
                               ETA: 897014.1s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.283s, learning 0.179s)
               Value function loss: 5.8460
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 283.09
               Mean episode length: 246.11
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 8.46s
                        Total time: 25477.98s
                               ETA: 896978.0s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.891s, learning 0.211s)
               Value function loss: 6.0830
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 284.35
               Mean episode length: 246.58
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 9.10s
                        Total time: 25487.08s
                               ETA: 896964.5s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.603s, learning 0.175s)
               Value function loss: 6.4738
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 277.22
               Mean episode length: 243.97
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 8.78s
                        Total time: 25495.86s
                               ETA: 896939.5s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.804s, learning 0.179s)
               Value function loss: 6.1331
                    Surrogate loss: -0.0012
             Mean action noise std: 0.76
                       Mean reward: 279.46
               Mean episode length: 248.28
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 8.98s
                        Total time: 25504.84s
                               ETA: 896921.8s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.506s, learning 0.173s)
               Value function loss: 6.2945
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 281.72
               Mean episode length: 246.53
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 8.68s
                        Total time: 25513.52s
                               ETA: 896893.5s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.658s, learning 0.241s)
               Value function loss: 6.3710
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 282.19
               Mean episode length: 248.16
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 8.90s
                        Total time: 25522.42s
                               ETA: 896872.8s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.702s, learning 0.198s)
               Value function loss: 7.0685
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 278.85
               Mean episode length: 246.58
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 8.90s
                        Total time: 25531.32s
                               ETA: 896852.3s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.691s, learning 0.262s)
               Value function loss: 6.5598
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 280.02
               Mean episode length: 246.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 8.95s
                        Total time: 25540.28s
                               ETA: 896833.5s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.581s, learning 0.183s)
               Value function loss: 7.6175
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 286.13
               Mean episode length: 248.65
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 8.76s
                        Total time: 25549.04s
                               ETA: 896808.2s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.746s, learning 0.233s)
               Value function loss: 6.7244
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 279.74
               Mean episode length: 248.35
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 8.98s
                        Total time: 25558.02s
                               ETA: 896790.4s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.803s, learning 0.237s)
               Value function loss: 4.9567
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 278.35
               Mean episode length: 247.24
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 9.04s
                        Total time: 25567.06s
                               ETA: 896774.7s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.564s, learning 0.206s)
               Value function loss: 6.7292
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 286.11
               Mean episode length: 248.32
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 8.77s
                        Total time: 25575.83s
                               ETA: 896749.6s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.416s, learning 0.165s)
               Value function loss: 6.9298
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 287.86
               Mean episode length: 249.94
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 8.58s
                        Total time: 25584.41s
                               ETA: 896717.9s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.271s, learning 0.219s)
               Value function loss: 7.1008
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 284.62
               Mean episode length: 249.12
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 8.49s
                        Total time: 25592.90s
                               ETA: 896683.0s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.656s, learning 0.170s)
               Value function loss: 6.9630
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 282.85
               Mean episode length: 245.42
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 8.83s
                        Total time: 25601.73s
                               ETA: 896659.9s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.789s, learning 0.179s)
               Value function loss: 6.2216
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 281.41
               Mean episode length: 245.42
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 8.97s
                        Total time: 25610.69s
                               ETA: 896641.8s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.616s, learning 0.213s)
               Value function loss: 7.7941
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 278.12
               Mean episode length: 243.22
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 8.83s
                        Total time: 25619.52s
                               ETA: 896618.8s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.709s, learning 0.187s)
               Value function loss: 6.9968
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 272.84
               Mean episode length: 238.91
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 8.90s
                        Total time: 25628.42s
                               ETA: 896598.1s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.504s, learning 0.187s)
               Value function loss: 7.3613
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 278.51
               Mean episode length: 243.49
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 8.69s
                        Total time: 25637.11s
                               ETA: 896570.3s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.619s, learning 0.237s)
               Value function loss: 6.0787
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 284.97
               Mean episode length: 249.15
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 8.86s
                        Total time: 25645.97s
                               ETA: 896548.3s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.617s, learning 0.198s)
               Value function loss: 6.0894
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 284.19
               Mean episode length: 246.09
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 8.82s
                        Total time: 25654.78s
                               ETA: 896524.9s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.551s, learning 0.239s)
               Value function loss: 5.7198
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 286.46
               Mean episode length: 248.71
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 8.79s
                        Total time: 25663.57s
                               ETA: 896500.6s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.415s, learning 0.194s)
               Value function loss: 5.3609
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 282.11
               Mean episode length: 248.59
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 8.61s
                        Total time: 25672.18s
                               ETA: 896470.0s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.604s, learning 0.196s)
               Value function loss: 7.3619
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 281.74
               Mean episode length: 246.81
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 8.80s
                        Total time: 25680.98s
                               ETA: 896446.1s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.587s, learning 0.264s)
               Value function loss: 6.0940
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 287.10
               Mean episode length: 248.38
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 8.85s
                        Total time: 25689.83s
                               ETA: 896423.9s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.640s, learning 0.215s)
               Value function loss: 6.3010
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 285.73
               Mean episode length: 246.56
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 8.86s
                        Total time: 25698.69s
                               ETA: 896401.9s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.788s, learning 0.189s)
               Value function loss: 5.7911
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 285.04
               Mean episode length: 246.85
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 8.98s
                        Total time: 25707.66s
                               ETA: 896384.2s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.659s, learning 0.200s)
               Value function loss: 5.9847
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 279.20
               Mean episode length: 243.99
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 8.86s
                        Total time: 25716.52s
                               ETA: 896362.3s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.562s, learning 0.199s)
               Value function loss: 6.6897
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 285.47
               Mean episode length: 245.35
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 8.76s
                        Total time: 25725.28s
                               ETA: 896337.1s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.775s, learning 0.233s)
               Value function loss: 7.2222
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 288.82
               Mean episode length: 246.67
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 9.01s
                        Total time: 25734.29s
                               ETA: 896320.5s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.545s, learning 0.218s)
               Value function loss: 5.2801
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 283.08
               Mean episode length: 245.63
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 8.76s
                        Total time: 25743.05s
                               ETA: 896295.3s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.391s, learning 0.248s)
               Value function loss: 6.7236
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 280.35
               Mean episode length: 244.39
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 8.64s
                        Total time: 25751.69s
                               ETA: 896265.8s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.791s, learning 0.263s)
               Value function loss: 6.1296
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 283.23
               Mean episode length: 247.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 9.05s
                        Total time: 25760.75s
                               ETA: 896250.8s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.530s, learning 0.197s)
               Value function loss: 7.0434
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 281.47
               Mean episode length: 243.93
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 8.73s
                        Total time: 25769.47s
                               ETA: 896224.4s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.621s, learning 0.210s)
               Value function loss: 6.9283
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 283.90
               Mean episode length: 244.67
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 8.83s
                        Total time: 25778.30s
                               ETA: 896201.7s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.712s, learning 0.199s)
               Value function loss: 7.7942
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 283.78
               Mean episode length: 248.33
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 8.91s
                        Total time: 25787.21s
                               ETA: 896181.8s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.880s, learning 0.205s)
               Value function loss: 6.2770
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 283.88
               Mean episode length: 248.43
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 9.08s
                        Total time: 25796.30s
                               ETA: 896167.9s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.594s, learning 0.170s)
               Value function loss: 6.0911
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 284.79
               Mean episode length: 245.95
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 8.76s
                        Total time: 25805.06s
                               ETA: 896142.8s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.651s, learning 0.211s)
               Value function loss: 7.2234
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 282.32
               Mean episode length: 244.99
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 8.86s
                        Total time: 25813.92s
                               ETA: 896121.2s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.820s, learning 0.199s)
               Value function loss: 7.0551
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 287.09
               Mean episode length: 246.81
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 9.02s
                        Total time: 25822.94s
                               ETA: 896105.0s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.587s, learning 0.202s)
               Value function loss: 5.7571
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 283.69
               Mean episode length: 244.79
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 8.79s
                        Total time: 25831.73s
                               ETA: 896080.8s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.718s, learning 0.180s)
               Value function loss: 5.7479
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 288.63
               Mean episode length: 248.18
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 8.90s
                        Total time: 25840.63s
                               ETA: 896060.5s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.523s, learning 0.173s)
               Value function loss: 6.8804
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 278.33
               Mean episode length: 241.78
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 8.70s
                        Total time: 25849.33s
                               ETA: 896033.2s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.780s, learning 0.189s)
               Value function loss: 5.5690
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 280.85
               Mean episode length: 244.26
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 8.97s
                        Total time: 25858.30s
                               ETA: 896015.3s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.810s, learning 0.168s)
               Value function loss: 7.9827
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 288.27
               Mean episode length: 248.05
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 8.98s
                        Total time: 25867.27s
                               ETA: 895997.7s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.629s, learning 0.175s)
               Value function loss: 6.0235
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 282.70
               Mean episode length: 244.28
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 8.80s
                        Total time: 25876.08s
                               ETA: 895974.2s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.836s, learning 0.212s)
               Value function loss: 6.6253
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 282.80
               Mean episode length: 244.02
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 9.05s
                        Total time: 25885.13s
                               ETA: 895959.0s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.253s, learning 0.158s)
               Value function loss: 7.8548
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 277.13
               Mean episode length: 243.28
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 9.41s
                        Total time: 25894.54s
                               ETA: 895956.5s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.654s, learning 0.181s)
               Value function loss: 6.3154
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 273.94
               Mean episode length: 241.15
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 8.84s
                        Total time: 25903.37s
                               ETA: 895934.0s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.537s, learning 0.201s)
               Value function loss: 6.8096
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 284.55
               Mean episode length: 247.36
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 8.74s
                        Total time: 25912.11s
                               ETA: 895908.2s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1768 steps/s (collection: 9.080s, learning 0.184s)
               Value function loss: 5.1389
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 284.42
               Mean episode length: 247.34
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 9.26s
                        Total time: 25921.37s
                               ETA: 895900.5s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.112s, learning 0.226s)
               Value function loss: 5.3579
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 279.85
               Mean episode length: 244.52
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 9.34s
                        Total time: 25930.71s
                               ETA: 895895.4s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.674s, learning 0.182s)
               Value function loss: 5.8538
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 273.19
               Mean episode length: 242.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 8.86s
                        Total time: 25939.57s
                               ETA: 895873.7s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.495s, learning 0.167s)
               Value function loss: 4.4912
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 273.47
               Mean episode length: 241.79
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 8.66s
                        Total time: 25948.23s
                               ETA: 895845.3s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.530s, learning 0.171s)
               Value function loss: 5.5387
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 281.30
               Mean episode length: 244.91
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 8.70s
                        Total time: 25956.93s
                               ETA: 895818.3s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.851s, learning 0.219s)
               Value function loss: 5.6532
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 282.12
               Mean episode length: 246.05
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 9.07s
                        Total time: 25966.00s
                               ETA: 895803.9s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.727s, learning 0.218s)
               Value function loss: 5.5893
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 280.84
               Mean episode length: 246.40
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 8.95s
                        Total time: 25974.95s
                               ETA: 895785.3s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.829s, learning 0.175s)
               Value function loss: 5.9152
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 270.15
               Mean episode length: 239.18
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 9.00s
                        Total time: 25983.95s
                               ETA: 895768.8s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.416s, learning 0.252s)
               Value function loss: 5.3899
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 270.05
               Mean episode length: 240.04
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 8.67s
                        Total time: 25992.62s
                               ETA: 895740.6s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.837s, learning 0.174s)
               Value function loss: 5.5122
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 269.60
               Mean episode length: 241.52
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 9.01s
                        Total time: 26001.63s
                               ETA: 895724.3s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.406s, learning 0.182s)
               Value function loss: 6.7379
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 283.83
               Mean episode length: 249.47
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 8.59s
                        Total time: 26010.22s
                               ETA: 895693.4s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1785 steps/s (collection: 8.993s, learning 0.182s)
               Value function loss: 4.3976
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 282.93
               Mean episode length: 248.32
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 9.18s
                        Total time: 26019.39s
                               ETA: 895682.7s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.387s, learning 0.163s)
               Value function loss: 4.7864
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 276.21
               Mean episode length: 246.49
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 8.55s
                        Total time: 26027.94s
                               ETA: 895650.6s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.777s, learning 0.210s)
               Value function loss: 4.8784
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 274.56
               Mean episode length: 247.46
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 8.99s
                        Total time: 26036.93s
                               ETA: 895633.4s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.379s, learning 0.189s)
               Value function loss: 6.3059
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 276.04
               Mean episode length: 245.39
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 8.57s
                        Total time: 26045.49s
                               ETA: 895601.9s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.964s, learning 0.184s)
               Value function loss: 6.9975
                    Surrogate loss: 0.0017
             Mean action noise std: 0.76
                       Mean reward: 274.64
               Mean episode length: 245.21
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 9.15s
                        Total time: 26054.64s
                               ETA: 895590.3s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.673s, learning 0.177s)
               Value function loss: 5.8487
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 275.85
               Mean episode length: 244.88
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 8.85s
                        Total time: 26063.49s
                               ETA: 895568.5s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.386s, learning 0.175s)
               Value function loss: 5.5801
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 272.22
               Mean episode length: 244.44
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 8.56s
                        Total time: 26072.05s
                               ETA: 895536.8s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.566s, learning 0.177s)
               Value function loss: 5.7924
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 273.73
               Mean episode length: 244.54
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 8.74s
                        Total time: 26080.80s
                               ETA: 895511.4s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.603s, learning 0.182s)
               Value function loss: 6.2743
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 272.14
               Mean episode length: 244.93
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 8.78s
                        Total time: 26089.58s
                               ETA: 895487.4s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.461s, learning 0.242s)
               Value function loss: 6.3844
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 271.60
               Mean episode length: 243.86
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 8.70s
                        Total time: 26098.29s
                               ETA: 895460.5s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.805s, learning 0.189s)
               Value function loss: 5.0319
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 270.43
               Mean episode length: 242.89
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 8.99s
                        Total time: 26107.28s
                               ETA: 895443.7s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.745s, learning 0.205s)
               Value function loss: 7.2131
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 274.96
               Mean episode length: 245.55
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 8.95s
                        Total time: 26116.23s
                               ETA: 895425.4s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.353s, learning 0.202s)
               Value function loss: 6.7243
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 268.35
               Mean episode length: 240.49
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 8.55s
                        Total time: 26124.78s
                               ETA: 895393.6s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.460s, learning 0.173s)
               Value function loss: 5.1346
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 267.51
               Mean episode length: 242.75
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 8.63s
                        Total time: 26133.42s
                               ETA: 895364.4s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.615s, learning 0.179s)
               Value function loss: 6.5992
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 268.25
               Mean episode length: 242.77
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 8.79s
                        Total time: 26142.21s
                               ETA: 895340.8s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.727s, learning 0.226s)
               Value function loss: 5.2447
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 275.64
               Mean episode length: 246.47
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 8.95s
                        Total time: 26151.16s
                               ETA: 895322.6s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.870s, learning 0.204s)
               Value function loss: 5.5904
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 280.05
               Mean episode length: 249.26
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 9.07s
                        Total time: 26160.24s
                               ETA: 895308.6s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.557s, learning 0.218s)
               Value function loss: 6.1467
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 280.46
               Mean episode length: 249.70
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 8.77s
                        Total time: 26169.01s
                               ETA: 895284.3s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.947s, learning 0.214s)
               Value function loss: 7.1339
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 277.65
               Mean episode length: 246.47
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 9.16s
                        Total time: 26178.17s
                               ETA: 895273.3s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.935s, learning 0.191s)
               Value function loss: 8.2091
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 274.43
               Mean episode length: 244.71
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 9.13s
                        Total time: 26187.30s
                               ETA: 895261.0s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.549s, learning 0.226s)
               Value function loss: 6.6003
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 273.23
               Mean episode length: 242.27
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 8.78s
                        Total time: 26196.08s
                               ETA: 895236.8s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.910s, learning 0.217s)
               Value function loss: 6.4903
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 270.75
               Mean episode length: 241.21
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 9.13s
                        Total time: 26205.20s
                               ETA: 895224.6s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.058s, learning 0.282s)
               Value function loss: 6.6392
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 279.63
               Mean episode length: 247.60
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 9.34s
                        Total time: 26214.54s
                               ETA: 895219.7s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.670s, learning 0.210s)
               Value function loss: 5.0401
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 258.41
               Mean episode length: 233.35
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 8.88s
                        Total time: 26223.42s
                               ETA: 895199.1s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.658s, learning 0.223s)
               Value function loss: 5.6200
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 262.63
               Mean episode length: 238.10
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 8.88s
                        Total time: 26232.30s
                               ETA: 895178.5s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.798s, learning 0.208s)
               Value function loss: 5.4072
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 274.36
               Mean episode length: 246.65
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 9.01s
                        Total time: 26241.31s
                               ETA: 895162.2s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.825s, learning 0.212s)
               Value function loss: 6.5328
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 278.37
               Mean episode length: 246.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 9.04s
                        Total time: 26250.35s
                               ETA: 895146.9s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.429s, learning 0.224s)
               Value function loss: 6.9256
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 278.53
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 8.65s
                        Total time: 26259.00s
                               ETA: 895118.6s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.872s, learning 0.193s)
               Value function loss: 5.8766
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 272.57
               Mean episode length: 245.07
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 9.06s
                        Total time: 26268.06s
                               ETA: 895104.3s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.961s, learning 0.204s)
               Value function loss: 6.4985
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 269.98
               Mean episode length: 240.06
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 9.17s
                        Total time: 26277.23s
                               ETA: 895093.4s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1776 steps/s (collection: 9.038s, learning 0.186s)
               Value function loss: 6.5512
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 277.59
               Mean episode length: 245.05
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 9.22s
                        Total time: 26286.45s
                               ETA: 895084.6s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.660s, learning 0.181s)
               Value function loss: 5.9117
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 278.44
               Mean episode length: 244.83
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 8.84s
                        Total time: 26295.29s
                               ETA: 895062.7s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.535s, learning 0.210s)
               Value function loss: 4.8178
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 270.32
               Mean episode length: 239.65
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 8.74s
                        Total time: 26304.04s
                               ETA: 895037.5s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.695s, learning 0.207s)
               Value function loss: 4.9683
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 269.24
               Mean episode length: 238.68
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 8.90s
                        Total time: 26312.94s
                               ETA: 895017.7s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.675s, learning 0.207s)
               Value function loss: 6.1155
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 274.04
               Mean episode length: 244.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 8.88s
                        Total time: 26321.82s
                               ETA: 894997.3s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.647s, learning 0.206s)
               Value function loss: 6.6809
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 276.11
               Mean episode length: 243.18
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 8.85s
                        Total time: 26330.68s
                               ETA: 894975.8s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.668s, learning 0.171s)
               Value function loss: 5.7415
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 274.75
               Mean episode length: 243.33
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 8.84s
                        Total time: 26339.51s
                               ETA: 894953.9s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.441s, learning 0.184s)
               Value function loss: 5.2233
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 281.75
               Mean episode length: 245.33
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 8.62s
                        Total time: 26348.14s
                               ETA: 894924.7s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.897s, learning 0.208s)
               Value function loss: 6.8327
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 281.29
               Mean episode length: 246.45
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 9.10s
                        Total time: 26357.24s
                               ETA: 894911.8s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.594s, learning 0.263s)
               Value function loss: 6.4442
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 278.07
               Mean episode length: 243.88
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 8.86s
                        Total time: 26366.10s
                               ETA: 894890.5s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1776 steps/s (collection: 9.038s, learning 0.186s)
               Value function loss: 6.3768
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 278.26
               Mean episode length: 244.08
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 9.22s
                        Total time: 26375.33s
                               ETA: 894881.7s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.749s, learning 0.195s)
               Value function loss: 5.2324
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 278.06
               Mean episode length: 243.26
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 8.94s
                        Total time: 26384.27s
                               ETA: 894863.4s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.586s, learning 0.255s)
               Value function loss: 5.7787
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 282.52
               Mean episode length: 245.04
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 8.84s
                        Total time: 26393.11s
                               ETA: 894841.6s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.000s, learning 0.269s)
               Value function loss: 5.5873
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 289.11
               Mean episode length: 249.31
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 9.27s
                        Total time: 26402.38s
                               ETA: 894834.3s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.644s, learning 0.173s)
               Value function loss: 5.3538
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 282.31
               Mean episode length: 244.32
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 8.82s
                        Total time: 26411.20s
                               ETA: 894811.7s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.420s, learning 0.179s)
               Value function loss: 6.1140
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 282.58
               Mean episode length: 244.76
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 8.60s
                        Total time: 26419.80s
                               ETA: 894781.7s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.302s, learning 0.172s)
               Value function loss: 4.9718
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 284.62
               Mean episode length: 247.15
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 8.47s
                        Total time: 26428.27s
                               ETA: 894747.5s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.566s, learning 0.184s)
               Value function loss: 5.1839
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 284.45
               Mean episode length: 245.93
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 8.75s
                        Total time: 26437.02s
                               ETA: 894722.7s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.743s, learning 0.185s)
               Value function loss: 5.5345
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 280.39
               Mean episode length: 242.41
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 8.93s
                        Total time: 26445.95s
                               ETA: 894703.9s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.471s, learning 0.208s)
               Value function loss: 6.3694
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 276.12
               Mean episode length: 240.38
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 8.68s
                        Total time: 26454.63s
                               ETA: 894676.7s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.603s, learning 0.214s)
               Value function loss: 6.8127
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 284.00
               Mean episode length: 243.22
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 8.82s
                        Total time: 26463.44s
                               ETA: 894654.2s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.839s, learning 0.186s)
               Value function loss: 5.3868
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 289.71
               Mean episode length: 247.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 9.02s
                        Total time: 26472.47s
                               ETA: 894638.7s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.512s, learning 0.186s)
               Value function loss: 5.7429
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 288.11
               Mean episode length: 247.28
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 8.70s
                        Total time: 26481.17s
                               ETA: 894612.1s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.490s, learning 0.174s)
               Value function loss: 4.7667
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 283.47
               Mean episode length: 244.54
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 8.66s
                        Total time: 26489.83s
                               ETA: 894584.4s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.523s, learning 0.193s)
               Value function loss: 5.1928
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 286.28
               Mean episode length: 246.16
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 8.72s
                        Total time: 26498.55s
                               ETA: 894558.5s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1780 steps/s (collection: 8.974s, learning 0.227s)
               Value function loss: 5.3468
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 285.69
               Mean episode length: 245.41
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 9.20s
                        Total time: 26507.75s
                               ETA: 894549.0s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.333s, learning 0.199s)
               Value function loss: 5.0643
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 283.77
               Mean episode length: 243.44
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 8.53s
                        Total time: 26516.28s
                               ETA: 894516.9s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.703s, learning 0.201s)
               Value function loss: 6.0228
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 287.84
               Mean episode length: 244.66
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 8.90s
                        Total time: 26525.18s
                               ETA: 894497.3s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.433s, learning 0.186s)
               Value function loss: 7.2196
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 292.56
               Mean episode length: 247.91
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 8.62s
                        Total time: 26533.80s
                               ETA: 894468.2s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.748s, learning 0.198s)
               Value function loss: 5.6791
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 281.78
               Mean episode length: 243.86
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 8.95s
                        Total time: 26542.75s
                               ETA: 894450.1s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.975s, learning 0.187s)
               Value function loss: 5.7362
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 278.32
               Mean episode length: 239.11
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 9.16s
                        Total time: 26551.91s
                               ETA: 894439.3s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.803s, learning 0.178s)
               Value function loss: 6.3351
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 287.85
               Mean episode length: 243.19
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 8.98s
                        Total time: 26560.89s
                               ETA: 894422.4s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.727s, learning 0.222s)
               Value function loss: 5.4605
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 290.54
               Mean episode length: 247.09
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 8.95s
                        Total time: 26569.84s
                               ETA: 894404.4s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.280s, learning 0.174s)
               Value function loss: 4.7753
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 284.84
               Mean episode length: 245.96
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 8.45s
                        Total time: 26578.30s
                               ETA: 894369.8s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.171s, learning 0.189s)
               Value function loss: 6.4470
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 286.02
               Mean episode length: 246.24
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 8.36s
                        Total time: 26586.66s
                               ETA: 894332.0s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.473s, learning 0.246s)
               Value function loss: 4.9005
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 284.61
               Mean episode length: 244.90
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 8.72s
                        Total time: 26595.37s
                               ETA: 894306.3s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.731s, learning 0.216s)
               Value function loss: 6.8974
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 291.63
               Mean episode length: 245.81
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 8.95s
                        Total time: 26604.32s
                               ETA: 894288.3s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.628s, learning 0.226s)
               Value function loss: 6.5978
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 283.72
               Mean episode length: 243.54
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 8.85s
                        Total time: 26613.18s
                               ETA: 894267.2s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.721s, learning 0.193s)
               Value function loss: 5.6472
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 283.05
               Mean episode length: 245.35
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 8.91s
                        Total time: 26622.09s
                               ETA: 894248.1s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.450s, learning 0.225s)
               Value function loss: 5.6377
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 282.08
               Mean episode length: 244.51
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 8.67s
                        Total time: 26630.77s
                               ETA: 894221.0s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.952s, learning 0.199s)
               Value function loss: 6.3215
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 285.17
               Mean episode length: 246.68
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 9.15s
                        Total time: 26639.92s
                               ETA: 894209.8s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.300s, learning 0.195s)
               Value function loss: 7.2739
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 277.21
               Mean episode length: 239.93
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 8.49s
                        Total time: 26648.41s
                               ETA: 894176.7s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.695s, learning 0.185s)
               Value function loss: 6.8573
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 284.01
               Mean episode length: 242.70
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 8.88s
                        Total time: 26657.29s
                               ETA: 894156.5s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.590s, learning 0.180s)
               Value function loss: 7.7817
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 286.96
               Mean episode length: 243.11
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 8.77s
                        Total time: 26666.06s
                               ETA: 894132.6s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.452s, learning 0.206s)
               Value function loss: 5.3361
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 288.28
               Mean episode length: 245.14
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 8.66s
                        Total time: 26674.72s
                               ETA: 894104.9s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.914s, learning 0.175s)
               Value function loss: 6.8377
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 292.06
               Mean episode length: 248.77
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 9.09s
                        Total time: 26683.81s
                               ETA: 894091.7s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.642s, learning 0.197s)
               Value function loss: 6.4053
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 289.55
               Mean episode length: 245.82
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 8.84s
                        Total time: 26692.65s
                               ETA: 894070.2s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.172s)
               Value function loss: 6.6340
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 289.14
               Mean episode length: 246.55
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 8.82s
                        Total time: 26701.47s
                               ETA: 894048.1s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.674s, learning 0.198s)
               Value function loss: 6.5945
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 281.73
               Mean episode length: 239.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 8.87s
                        Total time: 26710.34s
                               ETA: 894027.7s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.563s, learning 0.244s)
               Value function loss: 5.5946
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 278.35
               Mean episode length: 239.27
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 8.81s
                        Total time: 26719.15s
                               ETA: 894005.1s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.639s, learning 0.289s)
               Value function loss: 6.9954
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 284.44
               Mean episode length: 242.57
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 8.93s
                        Total time: 26728.08s
                               ETA: 893986.5s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.662s, learning 0.220s)
               Value function loss: 7.4311
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 288.73
               Mean episode length: 243.12
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 8.88s
                        Total time: 26736.96s
                               ETA: 893966.4s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.352s, learning 0.222s)
               Value function loss: 6.1594
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 281.74
               Mean episode length: 240.07
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 8.57s
                        Total time: 26745.53s
                               ETA: 893936.1s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.356s, learning 0.170s)
               Value function loss: 5.5297
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 279.68
               Mean episode length: 240.10
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 8.53s
                        Total time: 26754.06s
                               ETA: 893904.1s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.873s, learning 0.173s)
               Value function loss: 5.3735
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 281.16
               Mean episode length: 241.65
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 9.05s
                        Total time: 26763.11s
                               ETA: 893889.6s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.882s, learning 0.172s)
               Value function loss: 5.6871
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 288.18
               Mean episode length: 245.63
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 9.05s
                        Total time: 26772.16s
                               ETA: 893875.3s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.269s, learning 0.182s)
               Value function loss: 4.0730
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 282.81
               Mean episode length: 242.88
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 8.45s
                        Total time: 26780.61s
                               ETA: 893840.8s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.881s, learning 0.241s)
               Value function loss: 6.2225
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 282.72
               Mean episode length: 243.90
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 9.12s
                        Total time: 26789.73s
                               ETA: 893828.8s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.717s, learning 0.214s)
               Value function loss: 5.2049
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 280.72
               Mean episode length: 239.91
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 8.93s
                        Total time: 26798.66s
                               ETA: 893810.5s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.327s, learning 0.254s)
               Value function loss: 5.5738
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 283.38
               Mean episode length: 243.04
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 8.58s
                        Total time: 26807.24s
                               ETA: 893780.4s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.624s, learning 0.215s)
               Value function loss: 4.7805
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 288.58
               Mean episode length: 244.11
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 8.84s
                        Total time: 26816.08s
                               ETA: 893759.0s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.606s, learning 0.179s)
               Value function loss: 5.3726
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 284.86
               Mean episode length: 245.72
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 8.78s
                        Total time: 26824.87s
                               ETA: 893735.7s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.219s, learning 0.201s)
               Value function loss: 4.9109
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 282.75
               Mean episode length: 247.57
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 8.42s
                        Total time: 26833.29s
                               ETA: 893700.4s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.353s, learning 0.192s)
               Value function loss: 4.5079
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 277.31
               Mean episode length: 240.85
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 8.54s
                        Total time: 26841.83s
                               ETA: 893669.2s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.818s, learning 0.201s)
               Value function loss: 4.0653
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 275.77
               Mean episode length: 236.75
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 9.02s
                        Total time: 26850.85s
                               ETA: 893653.8s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.802s, learning 0.313s)
               Value function loss: 5.3645
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 287.52
               Mean episode length: 246.07
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 9.11s
                        Total time: 26859.97s
                               ETA: 893641.6s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.328s, learning 0.171s)
               Value function loss: 4.8434
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 292.41
               Mean episode length: 247.71
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 8.50s
                        Total time: 26868.46s
                               ETA: 893608.9s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.169s)
               Value function loss: 5.0882
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 290.21
               Mean episode length: 246.02
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 8.43s
                        Total time: 26876.89s
                               ETA: 893573.9s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.357s, learning 0.243s)
               Value function loss: 5.3288
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 285.93
               Mean episode length: 246.19
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 8.60s
                        Total time: 26885.49s
                               ETA: 893544.6s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.192s, learning 0.274s)
               Value function loss: 5.9330
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 285.38
               Mean episode length: 243.57
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 8.47s
                        Total time: 26893.96s
                               ETA: 893510.8s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.366s, learning 0.199s)
               Value function loss: 5.0533
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 283.38
               Mean episode length: 242.03
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 8.56s
                        Total time: 26902.52s
                               ETA: 893480.4s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.354s, learning 0.173s)
               Value function loss: 4.7319
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 285.58
               Mean episode length: 242.85
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 8.53s
                        Total time: 26911.05s
                               ETA: 893448.7s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.414s, learning 0.194s)
               Value function loss: 5.8285
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 273.32
               Mean episode length: 237.76
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 8.61s
                        Total time: 26919.66s
                               ETA: 893419.7s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.569s, learning 0.165s)
               Value function loss: 6.1497
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 280.93
               Mean episode length: 243.28
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 8.73s
                        Total time: 26928.39s
                               ETA: 893395.0s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.019s, learning 0.199s)
               Value function loss: 5.2418
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 284.09
               Mean episode length: 245.83
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 8.22s
                        Total time: 26936.61s
                               ETA: 893353.1s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.184s, learning 0.166s)
               Value function loss: 5.3191
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 275.28
               Mean episode length: 237.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 8.35s
                        Total time: 26944.96s
                               ETA: 893315.6s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.394s, learning 0.179s)
               Value function loss: 5.9865
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 281.74
               Mean episode length: 241.55
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 8.57s
                        Total time: 26953.53s
                               ETA: 893285.6s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.512s, learning 0.189s)
               Value function loss: 5.0971
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 283.98
               Mean episode length: 242.84
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 8.70s
                        Total time: 26962.23s
                               ETA: 893259.7s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.569s, learning 0.216s)
               Value function loss: 7.3381
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 282.60
               Mean episode length: 244.30
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 8.78s
                        Total time: 26971.02s
                               ETA: 893236.7s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.523s, learning 0.286s)
               Value function loss: 5.7563
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 282.43
               Mean episode length: 244.67
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 8.81s
                        Total time: 26979.83s
                               ETA: 893214.5s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.449s, learning 0.174s)
               Value function loss: 6.7231
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 286.62
               Mean episode length: 245.58
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 8.62s
                        Total time: 26988.45s
                               ETA: 893186.1s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.256s, learning 0.296s)
               Value function loss: 7.0925
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 286.97
               Mean episode length: 243.09
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 8.55s
                        Total time: 26997.00s
                               ETA: 893155.4s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.615s, learning 0.189s)
               Value function loss: 7.9896
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 289.91
               Mean episode length: 246.16
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 8.80s
                        Total time: 27005.81s
                               ETA: 893133.1s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.564s, learning 0.165s)
               Value function loss: 6.9868
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 286.17
               Mean episode length: 248.23
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 8.73s
                        Total time: 27014.53s
                               ETA: 893108.2s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.001s, learning 0.210s)
               Value function loss: 5.6513
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 285.02
               Mean episode length: 244.46
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 9.21s
                        Total time: 27023.75s
                               ETA: 893099.4s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.739s, learning 0.184s)
               Value function loss: 5.8808
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 289.70
               Mean episode length: 245.88
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 8.92s
                        Total time: 27032.67s
                               ETA: 893081.0s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.643s, learning 0.176s)
               Value function loss: 6.4670
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 288.29
               Mean episode length: 244.26
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 8.82s
                        Total time: 27041.49s
                               ETA: 893059.2s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.426s, learning 0.217s)
               Value function loss: 4.6546
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 288.68
               Mean episode length: 247.91
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 8.64s
                        Total time: 27050.13s
                               ETA: 893031.5s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.701s, learning 0.179s)
               Value function loss: 7.2475
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 289.63
               Mean episode length: 248.13
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 8.88s
                        Total time: 27059.01s
                               ETA: 893011.7s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.727s, learning 0.177s)
               Value function loss: 5.4145
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 291.42
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 8.90s
                        Total time: 27067.91s
                               ETA: 892992.8s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.318s, learning 0.171s)
               Value function loss: 6.3961
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 290.36
               Mean episode length: 248.43
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 8.49s
                        Total time: 27076.40s
                               ETA: 892960.1s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.401s, learning 0.168s)
               Value function loss: 6.1768
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 288.93
               Mean episode length: 247.78
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 8.57s
                        Total time: 27084.97s
                               ETA: 892930.0s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1791 steps/s (collection: 8.947s, learning 0.199s)
               Value function loss: 7.7271
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 276.89
               Mean episode length: 238.15
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 9.15s
                        Total time: 27094.12s
                               ETA: 892919.1s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.288s, learning 0.181s)
               Value function loss: 5.5553
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 274.34
               Mean episode length: 235.86
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 8.47s
                        Total time: 27102.59s
                               ETA: 892885.8s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.692s, learning 0.173s)
               Value function loss: 6.4374
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 279.45
               Mean episode length: 242.33
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 8.87s
                        Total time: 27111.45s
                               ETA: 892865.6s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.339s, learning 0.188s)
               Value function loss: 4.8876
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 284.60
               Mean episode length: 243.77
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 8.53s
                        Total time: 27119.98s
                               ETA: 892834.2s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.542s, learning 0.184s)
               Value function loss: 4.5858
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 288.87
               Mean episode length: 247.88
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 8.73s
                        Total time: 27128.70s
                               ETA: 892809.4s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.554s, learning 0.176s)
               Value function loss: 5.8478
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 282.24
               Mean episode length: 242.56
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 8.73s
                        Total time: 27137.43s
                               ETA: 892784.8s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.483s, learning 0.185s)
               Value function loss: 6.0723
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 288.49
               Mean episode length: 246.18
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 8.67s
                        Total time: 27146.10s
                               ETA: 892758.1s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.548s, learning 0.181s)
               Value function loss: 8.0526
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 274.82
               Mean episode length: 237.72
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 8.73s
                        Total time: 27154.83s
                               ETA: 892733.4s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.640s, learning 0.172s)
               Value function loss: 6.8763
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 275.23
               Mean episode length: 235.38
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 8.81s
                        Total time: 27163.64s
                               ETA: 892711.5s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.528s, learning 0.265s)
               Value function loss: 5.9989
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 277.85
               Mean episode length: 243.88
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 8.79s
                        Total time: 27172.43s
                               ETA: 892689.0s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.585s, learning 0.276s)
               Value function loss: 6.8198
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 274.77
               Mean episode length: 240.51
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 8.86s
                        Total time: 27181.30s
                               ETA: 892668.7s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.586s, learning 0.173s)
               Value function loss: 7.5204
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 273.12
               Mean episode length: 235.98
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 8.76s
                        Total time: 27190.06s
                               ETA: 892645.1s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.734s, learning 0.168s)
               Value function loss: 7.5926
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 280.72
               Mean episode length: 241.26
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 8.90s
                        Total time: 27198.96s
                               ETA: 892626.2s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.504s, learning 0.164s)
               Value function loss: 5.5033
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 279.69
               Mean episode length: 239.56
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 8.67s
                        Total time: 27207.63s
                               ETA: 892599.6s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.675s, learning 0.212s)
               Value function loss: 7.5473
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 285.72
               Mean episode length: 243.11
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 8.89s
                        Total time: 27216.51s
                               ETA: 892580.2s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.275s, learning 0.232s)
               Value function loss: 6.3864
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 288.04
               Mean episode length: 246.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 8.51s
                        Total time: 27225.02s
                               ETA: 892548.4s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.431s, learning 0.180s)
               Value function loss: 6.6282
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 286.28
               Mean episode length: 246.20
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 8.61s
                        Total time: 27233.63s
                               ETA: 892520.0s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.471s, learning 0.196s)
               Value function loss: 7.2912
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 282.14
               Mean episode length: 244.26
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 8.67s
                        Total time: 27242.30s
                               ETA: 892493.4s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.606s, learning 0.167s)
               Value function loss: 6.1231
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 283.77
               Mean episode length: 247.86
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 8.77s
                        Total time: 27251.07s
                               ETA: 892470.3s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.342s, learning 0.173s)
               Value function loss: 6.6071
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 283.35
               Mean episode length: 246.32
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 8.52s
                        Total time: 27259.59s
                               ETA: 892438.8s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.621s, learning 0.232s)
               Value function loss: 6.6335
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 284.76
               Mean episode length: 243.97
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 8.85s
                        Total time: 27268.44s
                               ETA: 892418.3s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.458s, learning 0.278s)
               Value function loss: 6.7165
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 285.66
               Mean episode length: 246.07
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 8.74s
                        Total time: 27277.18s
                               ETA: 892394.0s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.658s, learning 0.183s)
               Value function loss: 7.4349
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 274.68
               Mean episode length: 240.19
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 8.84s
                        Total time: 27286.02s
                               ETA: 892373.2s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.478s, learning 0.178s)
               Value function loss: 5.8490
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 277.29
               Mean episode length: 240.10
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 8.66s
                        Total time: 27294.67s
                               ETA: 892346.3s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.416s, learning 0.175s)
               Value function loss: 5.2020
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 275.46
               Mean episode length: 241.94
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 8.59s
                        Total time: 27303.26s
                               ETA: 892317.3s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.518s, learning 0.177s)
               Value function loss: 5.1570
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 280.77
               Mean episode length: 244.07
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 8.70s
                        Total time: 27311.96s
                               ETA: 892291.8s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.401s, learning 0.192s)
               Value function loss: 4.4929
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 279.57
               Mean episode length: 242.25
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 8.59s
                        Total time: 27320.55s
                               ETA: 892262.9s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.315s, learning 0.280s)
               Value function loss: 5.6664
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 279.35
               Mean episode length: 246.22
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 8.59s
                        Total time: 27329.15s
                               ETA: 892234.1s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.624s, learning 0.184s)
               Value function loss: 4.3978
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 281.52
               Mean episode length: 248.11
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 8.81s
                        Total time: 27337.95s
                               ETA: 892212.2s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.895s, learning 0.178s)
               Value function loss: 6.2641
                    Surrogate loss: -0.0028
             Mean action noise std: 0.76
                       Mean reward: 272.55
               Mean episode length: 240.59
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 9.07s
                        Total time: 27347.03s
                               ETA: 892199.1s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.713s, learning 0.177s)
               Value function loss: 5.3910
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 275.91
               Mean episode length: 242.72
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 8.89s
                        Total time: 27355.92s
                               ETA: 892179.9s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.322s, learning 0.190s)
               Value function loss: 6.1004
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 273.92
               Mean episode length: 240.27
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 8.51s
                        Total time: 27364.43s
                               ETA: 892148.4s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.495s, learning 0.247s)
               Value function loss: 5.0140
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 268.39
               Mean episode length: 237.97
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 8.74s
                        Total time: 27373.17s
                               ETA: 892124.4s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.491s, learning 0.217s)
               Value function loss: 5.4690
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 275.89
               Mean episode length: 241.61
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 8.71s
                        Total time: 27381.88s
                               ETA: 892099.4s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.494s, learning 0.170s)
               Value function loss: 5.8871
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 282.37
               Mean episode length: 245.25
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 8.66s
                        Total time: 27390.54s
                               ETA: 892072.9s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.370s, learning 0.173s)
               Value function loss: 4.9233
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 277.64
               Mean episode length: 240.37
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 8.54s
                        Total time: 27399.08s
                               ETA: 892042.5s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.562s, learning 0.227s)
               Value function loss: 5.5637
                    Surrogate loss: -0.0023
             Mean action noise std: 0.76
                       Mean reward: 272.94
               Mean episode length: 238.23
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 8.79s
                        Total time: 27407.87s
                               ETA: 892020.1s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.769s, learning 0.272s)
               Value function loss: 4.9998
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 278.90
               Mean episode length: 243.85
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 9.04s
                        Total time: 27416.91s
                               ETA: 892005.9s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.297s, learning 0.166s)
               Value function loss: 7.0971
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 278.79
               Mean episode length: 244.71
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 8.46s
                        Total time: 27425.38s
                               ETA: 891972.9s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.587s, learning 0.170s)
               Value function loss: 5.7483
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 276.47
               Mean episode length: 243.77
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 8.76s
                        Total time: 27434.13s
                               ETA: 891949.5s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.727s, learning 0.165s)
               Value function loss: 5.3759
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 270.56
               Mean episode length: 237.27
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 8.89s
                        Total time: 27443.02s
                               ETA: 891930.5s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.699s, learning 0.242s)
               Value function loss: 6.8199
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 276.33
               Mean episode length: 238.42
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 8.94s
                        Total time: 27451.97s
                               ETA: 891913.1s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.484s, learning 0.205s)
               Value function loss: 6.6920
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 283.06
               Mean episode length: 242.87
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 8.69s
                        Total time: 27460.65s
                               ETA: 891887.5s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.421s, learning 0.169s)
               Value function loss: 6.0285
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 278.95
               Mean episode length: 242.04
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 8.59s
                        Total time: 27469.24s
                               ETA: 891858.7s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.406s, learning 0.222s)
               Value function loss: 3.9845
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 280.66
               Mean episode length: 245.84
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 8.63s
                        Total time: 27477.87s
                               ETA: 891831.1s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.553s, learning 0.171s)
               Value function loss: 6.3583
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 286.63
               Mean episode length: 248.08
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 8.72s
                        Total time: 27486.60s
                               ETA: 891806.8s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.200s, learning 0.220s)
               Value function loss: 5.7418
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 276.52
               Mean episode length: 240.91
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 8.42s
                        Total time: 27495.02s
                               ETA: 891772.5s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.497s, learning 0.171s)
               Value function loss: 5.5509
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 280.07
               Mean episode length: 244.22
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 8.67s
                        Total time: 27503.68s
                               ETA: 891746.3s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.669s, learning 0.171s)
               Value function loss: 6.0671
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 278.17
               Mean episode length: 242.26
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 8.84s
                        Total time: 27512.52s
                               ETA: 891725.7s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.707s, learning 0.239s)
               Value function loss: 4.7245
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 286.70
               Mean episode length: 248.31
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 8.95s
                        Total time: 27521.47s
                               ETA: 891708.5s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.565s, learning 0.199s)
               Value function loss: 5.3252
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 280.97
               Mean episode length: 243.40
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 8.76s
                        Total time: 27530.23s
                               ETA: 891685.4s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.600s, learning 0.181s)
               Value function loss: 5.2690
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 274.02
               Mean episode length: 242.69
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 8.78s
                        Total time: 27539.01s
                               ETA: 891662.9s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.557s, learning 0.183s)
               Value function loss: 5.0849
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 280.22
               Mean episode length: 248.15
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 8.74s
                        Total time: 27547.76s
                               ETA: 891639.1s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.165s)
               Value function loss: 6.7252
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 280.64
               Mean episode length: 244.21
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 8.66s
                        Total time: 27556.41s
                               ETA: 891612.7s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.540s, learning 0.164s)
               Value function loss: 5.4236
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 282.05
               Mean episode length: 244.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 8.70s
                        Total time: 27565.12s
                               ETA: 891587.7s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.547s, learning 0.171s)
               Value function loss: 5.9170
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 275.66
               Mean episode length: 240.51
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 8.72s
                        Total time: 27573.84s
                               ETA: 891563.3s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.639s, learning 0.198s)
               Value function loss: 4.5525
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 275.89
               Mean episode length: 240.99
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 8.84s
                        Total time: 27582.67s
                               ETA: 891542.6s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.689s, learning 0.276s)
               Value function loss: 5.1039
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 278.64
               Mean episode length: 242.16
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 8.96s
                        Total time: 27591.64s
                               ETA: 891526.1s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.716s, learning 0.170s)
               Value function loss: 4.9208
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 285.69
               Mean episode length: 248.09
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 8.89s
                        Total time: 27600.52s
                               ETA: 891507.0s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.379s, learning 0.171s)
               Value function loss: 4.5606
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 290.36
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 8.55s
                        Total time: 27609.07s
                               ETA: 891477.1s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.325s, learning 0.171s)
               Value function loss: 5.4087
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 278.80
               Mean episode length: 243.81
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 8.50s
                        Total time: 27617.57s
                               ETA: 891445.5s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.834s, learning 0.224s)
               Value function loss: 5.1390
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 278.94
               Mean episode length: 244.30
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 9.06s
                        Total time: 27626.63s
                               ETA: 891432.0s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.488s, learning 0.181s)
               Value function loss: 5.0766
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 280.16
               Mean episode length: 245.96
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 8.67s
                        Total time: 27635.30s
                               ETA: 891406.0s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.654s, learning 0.180s)
               Value function loss: 4.9823
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 282.68
               Mean episode length: 248.23
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 8.83s
                        Total time: 27644.13s
                               ETA: 891385.3s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.620s, learning 0.179s)
               Value function loss: 5.4739
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 282.76
               Mean episode length: 245.72
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 8.80s
                        Total time: 27652.93s
                               ETA: 891363.6s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.657s, learning 0.193s)
               Value function loss: 4.3027
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 274.71
               Mean episode length: 238.53
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 8.85s
                        Total time: 27661.78s
                               ETA: 891343.4s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.592s, learning 0.182s)
               Value function loss: 4.2167
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 278.31
               Mean episode length: 241.52
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 8.77s
                        Total time: 27670.55s
                               ETA: 891320.9s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.900s, learning 0.194s)
               Value function loss: 5.3277
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 283.42
               Mean episode length: 245.78
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 9.09s
                        Total time: 27679.65s
                               ETA: 891308.6s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.717s, learning 0.167s)
               Value function loss: 4.4610
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 278.37
               Mean episode length: 244.91
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 8.88s
                        Total time: 27688.53s
                               ETA: 891289.5s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.597s, learning 0.178s)
               Value function loss: 5.6988
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 277.96
               Mean episode length: 243.46
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 8.77s
                        Total time: 27697.31s
                               ETA: 891267.0s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.518s, learning 0.291s)
               Value function loss: 6.5289
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 276.95
               Mean episode length: 243.98
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 8.81s
                        Total time: 27706.12s
                               ETA: 891245.6s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.897s, learning 0.188s)
               Value function loss: 5.8518
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 280.79
               Mean episode length: 245.43
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 9.09s
                        Total time: 27715.20s
                               ETA: 891233.0s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.231s, learning 0.177s)
               Value function loss: 5.0621
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 283.14
               Mean episode length: 247.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 8.41s
                        Total time: 27723.61s
                               ETA: 891198.7s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.778s, learning 0.185s)
               Value function loss: 5.7509
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 282.44
               Mean episode length: 247.09
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 8.96s
                        Total time: 27732.57s
                               ETA: 891182.2s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.617s, learning 0.185s)
               Value function loss: 6.2521
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 278.56
               Mean episode length: 246.40
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 8.80s
                        Total time: 27741.37s
                               ETA: 891160.6s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.760s, learning 0.198s)
               Value function loss: 5.3649
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 280.41
               Mean episode length: 246.29
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 8.96s
                        Total time: 27750.33s
                               ETA: 891144.0s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.624s, learning 0.275s)
               Value function loss: 4.8014
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 279.28
               Mean episode length: 246.54
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 8.90s
                        Total time: 27759.23s
                               ETA: 891125.5s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.163s, learning 0.315s)
               Value function loss: 5.2422
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 282.96
               Mean episode length: 248.07
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 8.48s
                        Total time: 27767.71s
                               ETA: 891093.5s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.531s, learning 0.168s)
               Value function loss: 6.0446
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 276.04
               Mean episode length: 241.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 8.70s
                        Total time: 27776.41s
                               ETA: 891068.6s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.933s, learning 0.185s)
               Value function loss: 6.7269
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 279.32
               Mean episode length: 246.59
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 9.12s
                        Total time: 27785.52s
                               ETA: 891057.1s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.562s, learning 0.175s)
               Value function loss: 5.1478
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 281.49
               Mean episode length: 247.55
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 8.74s
                        Total time: 27794.26s
                               ETA: 891033.5s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.773s, learning 0.163s)
               Value function loss: 5.4399
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 274.56
               Mean episode length: 240.78
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 8.94s
                        Total time: 27803.20s
                               ETA: 891016.2s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.330s, learning 0.167s)
               Value function loss: 5.5320
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 273.82
               Mean episode length: 242.11
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 8.50s
                        Total time: 27811.69s
                               ETA: 890984.9s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.643s, learning 0.171s)
               Value function loss: 6.7465
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 280.82
               Mean episode length: 248.09
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 8.81s
                        Total time: 27820.51s
                               ETA: 890963.7s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.622s, learning 0.276s)
               Value function loss: 5.8582
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 272.67
               Mean episode length: 242.86
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 8.90s
                        Total time: 27829.41s
                               ETA: 890945.3s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.918s, learning 0.191s)
               Value function loss: 5.9132
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 276.35
               Mean episode length: 244.02
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 9.11s
                        Total time: 27838.52s
                               ETA: 890933.6s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.905s, learning 0.196s)
               Value function loss: 5.6947
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 278.03
               Mean episode length: 241.68
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 9.10s
                        Total time: 27847.62s
                               ETA: 890921.6s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.011s, learning 0.170s)
               Value function loss: 4.6119
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 278.02
               Mean episode length: 241.26
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 8.18s
                        Total time: 27855.80s
                               ETA: 890880.3s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.468s, learning 0.210s)
               Value function loss: 5.2234
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 287.42
               Mean episode length: 249.03
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 8.68s
                        Total time: 27864.48s
                               ETA: 890854.8s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.293s, learning 0.168s)
               Value function loss: 4.5318
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 277.32
               Mean episode length: 242.16
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 8.46s
                        Total time: 27872.94s
                               ETA: 890822.4s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.405s, learning 0.193s)
               Value function loss: 5.7326
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 283.06
               Mean episode length: 246.55
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 8.60s
                        Total time: 27881.54s
                               ETA: 890794.4s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.734s, learning 0.166s)
               Value function loss: 5.6682
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 282.00
               Mean episode length: 246.53
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 8.90s
                        Total time: 27890.44s
                               ETA: 890776.1s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.693s, learning 0.178s)
               Value function loss: 4.9029
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 283.04
               Mean episode length: 248.16
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 8.87s
                        Total time: 27899.31s
                               ETA: 890756.8s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.505s, learning 0.179s)
               Value function loss: 5.6302
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 279.57
               Mean episode length: 245.35
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 8.68s
                        Total time: 27907.99s
                               ETA: 890731.6s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.695s, learning 0.285s)
               Value function loss: 5.9944
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 277.04
               Mean episode length: 243.15
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 8.98s
                        Total time: 27916.97s
                               ETA: 890715.8s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.607s, learning 0.209s)
               Value function loss: 5.8064
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 280.57
               Mean episode length: 245.66
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 8.82s
                        Total time: 27925.79s
                               ETA: 890694.8s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.868s, learning 0.171s)
               Value function loss: 5.3919
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 275.22
               Mean episode length: 239.24
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 9.04s
                        Total time: 27934.83s
                               ETA: 890680.9s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.756s, learning 0.192s)
               Value function loss: 4.3556
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 276.93
               Mean episode length: 242.74
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 8.95s
                        Total time: 27943.77s
                               ETA: 890664.1s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.294s, learning 0.265s)
               Value function loss: 5.2209
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 281.08
               Mean episode length: 247.94
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 8.56s
                        Total time: 27952.33s
                               ETA: 890635.0s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.661s, learning 0.208s)
               Value function loss: 5.4932
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 280.39
               Mean episode length: 246.29
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 8.87s
                        Total time: 27961.20s
                               ETA: 890615.7s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.734s, learning 0.171s)
               Value function loss: 4.6901
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 280.75
               Mean episode length: 245.84
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 8.90s
                        Total time: 27970.11s
                               ETA: 890597.6s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.630s, learning 0.168s)
               Value function loss: 6.4706
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 277.57
               Mean episode length: 243.24
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 8.80s
                        Total time: 27978.90s
                               ETA: 890576.0s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.610s, learning 0.172s)
               Value function loss: 6.4931
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 275.47
               Mean episode length: 241.03
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 8.78s
                        Total time: 27987.69s
                               ETA: 890554.0s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.907s, learning 0.250s)
               Value function loss: 4.9490
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 280.35
               Mean episode length: 244.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 9.16s
                        Total time: 27996.84s
                               ETA: 890543.9s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.680s, learning 0.173s)
               Value function loss: 4.6621
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 275.85
               Mean episode length: 244.84
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 8.85s
                        Total time: 28005.70s
                               ETA: 890524.2s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.776s, learning 0.174s)
               Value function loss: 6.5445
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 282.98
               Mean episode length: 248.28
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 8.95s
                        Total time: 28014.65s
                               ETA: 890507.5s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.597s, learning 0.178s)
               Value function loss: 5.3743
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 276.12
               Mean episode length: 242.80
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 8.77s
                        Total time: 28023.42s
                               ETA: 890485.3s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.534s, learning 0.195s)
               Value function loss: 4.7069
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 272.97
               Mean episode length: 239.18
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 8.73s
                        Total time: 28032.15s
                               ETA: 890461.6s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.212s, learning 0.188s)
               Value function loss: 4.4640
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 280.19
               Mean episode length: 244.87
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 8.40s
                        Total time: 28040.55s
                               ETA: 890427.5s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.493s, learning 0.198s)
               Value function loss: 6.6161
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 275.00
               Mean episode length: 237.78
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 8.69s
                        Total time: 28049.24s
                               ETA: 890402.7s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.716s, learning 0.194s)
               Value function loss: 5.3846
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 266.62
               Mean episode length: 233.36
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 8.91s
                        Total time: 28058.15s
                               ETA: 890384.8s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.533s, learning 0.284s)
               Value function loss: 6.8081
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 277.87
               Mean episode length: 243.60
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 8.82s
                        Total time: 28066.97s
                               ETA: 890364.0s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.773s, learning 0.198s)
               Value function loss: 5.2527
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 283.12
               Mean episode length: 248.46
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 8.97s
                        Total time: 28075.94s
                               ETA: 890348.0s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.162s, learning 0.249s)
               Value function loss: 6.2477
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 275.98
               Mean episode length: 242.19
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 8.41s
                        Total time: 28084.35s
                               ETA: 890314.3s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.545s, learning 0.239s)
               Value function loss: 5.8836
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 290.51
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 8.78s
                        Total time: 28093.13s
                               ETA: 890292.5s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.622s, learning 0.205s)
               Value function loss: 6.2109
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 289.56
               Mean episode length: 248.50
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 8.83s
                        Total time: 28101.96s
                               ETA: 890272.0s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.563s, learning 0.172s)
               Value function loss: 6.5905
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 283.86
               Mean episode length: 246.39
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 8.74s
                        Total time: 28110.70s
                               ETA: 890248.6s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.557s, learning 0.220s)
               Value function loss: 5.7791
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 278.07
               Mean episode length: 241.49
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 8.78s
                        Total time: 28119.47s
                               ETA: 890226.5s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.638s, learning 0.179s)
               Value function loss: 4.9324
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 277.53
               Mean episode length: 242.38
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 8.82s
                        Total time: 28128.29s
                               ETA: 890205.8s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.698s, learning 0.171s)
               Value function loss: 6.3332
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 279.94
               Mean episode length: 241.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 8.87s
                        Total time: 28137.16s
                               ETA: 890186.7s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.521s, learning 0.205s)
               Value function loss: 4.7407
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 280.57
               Mean episode length: 243.29
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 8.73s
                        Total time: 28145.89s
                               ETA: 890163.0s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.772s, learning 0.175s)
               Value function loss: 5.7027
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 279.17
               Mean episode length: 246.84
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 8.95s
                        Total time: 28154.83s
                               ETA: 890146.4s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.210s, learning 0.185s)
               Value function loss: 5.2562
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 280.17
               Mean episode length: 245.48
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 8.40s
                        Total time: 28163.23s
                               ETA: 890112.3s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.525s, learning 0.177s)
               Value function loss: 6.0278
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 283.51
               Mean episode length: 246.99
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 8.70s
                        Total time: 28171.93s
                               ETA: 890087.9s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.740s, learning 0.176s)
               Value function loss: 5.7969
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 277.83
               Mean episode length: 244.27
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 8.92s
                        Total time: 28180.85s
                               ETA: 890070.3s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.589s, learning 0.173s)
               Value function loss: 6.8021
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 268.72
               Mean episode length: 239.01
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 8.76s
                        Total time: 28189.61s
                               ETA: 890047.9s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.460s, learning 0.174s)
               Value function loss: 5.0630
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 273.16
               Mean episode length: 240.46
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 8.63s
                        Total time: 28198.24s
                               ETA: 890021.4s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.797s, learning 0.182s)
               Value function loss: 7.1020
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 278.73
               Mean episode length: 244.59
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 8.98s
                        Total time: 28207.22s
                               ETA: 890005.8s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.381s, learning 0.194s)
               Value function loss: 4.4529
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 279.48
               Mean episode length: 244.44
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 8.58s
                        Total time: 28215.80s
                               ETA: 889977.5s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.873s, learning 0.175s)
               Value function loss: 4.1278
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 279.76
               Mean episode length: 244.44
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 9.05s
                        Total time: 28224.85s
                               ETA: 889964.1s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.570s, learning 0.228s)
               Value function loss: 5.3361
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 284.25
               Mean episode length: 248.47
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 8.80s
                        Total time: 28233.64s
                               ETA: 889942.8s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.303s, learning 0.169s)
               Value function loss: 5.3096
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 282.30
               Mean episode length: 246.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 8.47s
                        Total time: 28242.12s
                               ETA: 889911.3s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.646s, learning 0.205s)
               Value function loss: 7.6376
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 282.33
               Mean episode length: 247.14
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 8.85s
                        Total time: 28250.97s
                               ETA: 889891.7s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.250s, learning 0.215s)
               Value function loss: 6.0967
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 287.67
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 8.47s
                        Total time: 28259.43s
                               ETA: 889859.9s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.503s, learning 0.268s)
               Value function loss: 5.6111
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 285.86
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 8.77s
                        Total time: 28268.20s
                               ETA: 889837.8s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.629s, learning 0.174s)
               Value function loss: 5.4586
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 282.03
               Mean episode length: 246.82
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 8.80s
                        Total time: 28277.00s
                               ETA: 889816.7s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.419s, learning 0.182s)
               Value function loss: 6.7197
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 283.86
               Mean episode length: 246.82
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 8.60s
                        Total time: 28285.61s
                               ETA: 889789.3s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.472s, learning 0.185s)
               Value function loss: 5.9624
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 285.16
               Mean episode length: 248.08
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 8.66s
                        Total time: 28294.26s
                               ETA: 889763.7s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.702s, learning 0.223s)
               Value function loss: 4.1009
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 283.80
               Mean episode length: 248.08
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 8.92s
                        Total time: 28303.19s
                               ETA: 889746.4s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.810s, learning 0.217s)
               Value function loss: 5.0842
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 284.74
               Mean episode length: 248.62
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 9.03s
                        Total time: 28312.21s
                               ETA: 889732.4s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.533s, learning 0.166s)
               Value function loss: 5.4404
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 280.14
               Mean episode length: 246.69
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 8.70s
                        Total time: 28320.91s
                               ETA: 889708.1s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.360s, learning 0.167s)
               Value function loss: 5.7921
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 277.23
               Mean episode length: 244.06
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 8.53s
                        Total time: 28329.44s
                               ETA: 889678.4s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.530s, learning 0.220s)
               Value function loss: 6.0869
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 274.65
               Mean episode length: 242.78
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 8.75s
                        Total time: 28338.19s
                               ETA: 889655.7s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.469s, learning 0.188s)
               Value function loss: 4.8245
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 285.40
               Mean episode length: 248.88
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 8.66s
                        Total time: 28346.85s
                               ETA: 889630.2s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.775s, learning 0.189s)
               Value function loss: 5.9072
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 286.12
               Mean episode length: 248.57
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 8.96s
                        Total time: 28355.81s
                               ETA: 889614.2s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.542s, learning 0.182s)
               Value function loss: 5.6132
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 283.92
               Mean episode length: 248.07
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 8.72s
                        Total time: 28364.53s
                               ETA: 889590.8s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.375s, learning 0.168s)
               Value function loss: 5.1797
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 279.67
               Mean episode length: 244.62
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 8.54s
                        Total time: 28373.08s
                               ETA: 889561.6s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.456s, learning 0.170s)
               Value function loss: 6.6279
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 284.22
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 8.63s
                        Total time: 28381.70s
                               ETA: 889535.1s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.576s, learning 0.187s)
               Value function loss: 5.4936
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 283.49
               Mean episode length: 248.21
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 8.76s
                        Total time: 28390.47s
                               ETA: 889512.9s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.711s, learning 0.187s)
               Value function loss: 5.3684
                    Surrogate loss: -0.0045
             Mean action noise std: 0.76
                       Mean reward: 283.84
               Mean episode length: 248.76
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 8.90s
                        Total time: 28399.36s
                               ETA: 889494.9s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.488s, learning 0.167s)
               Value function loss: 5.3452
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 285.46
               Mean episode length: 248.36
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 8.65s
                        Total time: 28408.02s
                               ETA: 889469.3s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.622s, learning 0.316s)
               Value function loss: 4.6412
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 285.04
               Mean episode length: 248.88
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 8.94s
                        Total time: 28416.96s
                               ETA: 889452.6s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.347s, learning 0.187s)
               Value function loss: 4.6652
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 285.29
               Mean episode length: 248.64
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 8.53s
                        Total time: 28425.49s
                               ETA: 889423.3s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.497s, learning 0.306s)
               Value function loss: 4.3654
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 285.41
               Mean episode length: 248.64
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 8.80s
                        Total time: 28434.29s
                               ETA: 889402.3s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.438s, learning 0.178s)
               Value function loss: 6.0701
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 286.34
               Mean episode length: 248.20
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 8.62s
                        Total time: 28442.91s
                               ETA: 889375.6s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.494s, learning 0.181s)
               Value function loss: 5.7795
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 287.46
               Mean episode length: 248.20
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 8.68s
                        Total time: 28451.59s
                               ETA: 889350.7s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.642s, learning 0.177s)
               Value function loss: 5.8871
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 280.46
               Mean episode length: 248.28
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 8.82s
                        Total time: 28460.41s
                               ETA: 889330.3s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.628s, learning 0.190s)
               Value function loss: 4.9657
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 285.02
               Mean episode length: 248.29
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 8.82s
                        Total time: 28469.22s
                               ETA: 889309.9s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.636s, learning 0.189s)
               Value function loss: 6.2433
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 284.33
               Mean episode length: 248.07
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 8.82s
                        Total time: 28478.05s
                               ETA: 889289.7s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.476s, learning 0.200s)
               Value function loss: 4.6431
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 283.10
               Mean episode length: 246.49
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 8.68s
                        Total time: 28486.73s
                               ETA: 889264.9s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.288s, learning 0.194s)
               Value function loss: 4.0800
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 281.79
               Mean episode length: 245.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 8.48s
                        Total time: 28495.21s
                               ETA: 889234.0s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.578s, learning 0.174s)
               Value function loss: 5.2875
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 285.00
               Mean episode length: 246.58
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 8.75s
                        Total time: 28503.96s
                               ETA: 889211.6s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.201s, learning 0.276s)
               Value function loss: 5.9534
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 281.29
               Mean episode length: 246.24
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 8.48s
                        Total time: 28512.44s
                               ETA: 889180.6s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.632s, learning 0.295s)
               Value function loss: 6.5971
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 279.53
               Mean episode length: 246.24
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 8.93s
                        Total time: 28521.36s
                               ETA: 889163.6s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.522s, learning 0.204s)
               Value function loss: 6.4772
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 279.12
               Mean episode length: 244.63
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 8.73s
                        Total time: 28530.09s
                               ETA: 889140.4s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.887s, learning 0.187s)
               Value function loss: 6.0128
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 277.91
               Mean episode length: 242.99
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 9.07s
                        Total time: 28539.16s
                               ETA: 889128.0s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.792s, learning 0.180s)
               Value function loss: 6.7450
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 281.12
               Mean episode length: 245.85
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 8.97s
                        Total time: 28548.14s
                               ETA: 889112.5s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.461s, learning 0.172s)
               Value function loss: 6.7265
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 277.68
               Mean episode length: 241.01
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 8.63s
                        Total time: 28556.77s
                               ETA: 889086.3s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.511s, learning 0.169s)
               Value function loss: 6.7710
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 284.02
               Mean episode length: 246.81
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 8.68s
                        Total time: 28565.45s
                               ETA: 889061.7s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.904s, learning 0.171s)
               Value function loss: 5.3001
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 288.59
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 9.08s
                        Total time: 28574.52s
                               ETA: 889049.4s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.659s, learning 0.168s)
               Value function loss: 6.6736
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 287.54
               Mean episode length: 248.44
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 8.83s
                        Total time: 28583.35s
                               ETA: 889029.4s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.931s, learning 0.224s)
               Value function loss: 7.2111
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 287.95
               Mean episode length: 247.84
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 9.16s
                        Total time: 28592.51s
                               ETA: 889019.5s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.537s, learning 0.210s)
               Value function loss: 8.0261
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 284.60
               Mean episode length: 247.82
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 8.75s
                        Total time: 28601.25s
                               ETA: 888997.0s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.489s, learning 0.213s)
               Value function loss: 8.0405
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 285.64
               Mean episode length: 247.58
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 8.70s
                        Total time: 28609.95s
                               ETA: 888973.1s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.245s, learning 0.210s)
               Value function loss: 5.4789
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 286.02
               Mean episode length: 248.41
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 8.45s
                        Total time: 28618.41s
                               ETA: 888941.6s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.038s, learning 0.179s)
               Value function loss: 7.4261
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 279.60
               Mean episode length: 244.98
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 9.22s
                        Total time: 28627.63s
                               ETA: 888933.7s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.490s, learning 0.228s)
               Value function loss: 6.7344
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 282.54
               Mean episode length: 243.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 8.72s
                        Total time: 28636.34s
                               ETA: 888910.3s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.503s, learning 0.270s)
               Value function loss: 6.6704
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 285.87
               Mean episode length: 246.55
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 8.77s
                        Total time: 28645.12s
                               ETA: 888888.6s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.529s, learning 0.235s)
               Value function loss: 8.4611
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 289.43
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 8.76s
                        Total time: 28653.88s
                               ETA: 888866.7s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.080s, learning 0.178s)
               Value function loss: 6.9165
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 285.12
               Mean episode length: 246.92
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 8.26s
                        Total time: 28662.14s
                               ETA: 888829.1s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.410s, learning 0.311s)
               Value function loss: 6.4678
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 286.72
               Mean episode length: 247.98
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 8.72s
                        Total time: 28670.86s
                               ETA: 888805.8s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.538s, learning 0.177s)
               Value function loss: 5.8767
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 283.57
               Mean episode length: 245.04
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 8.71s
                        Total time: 28679.57s
                               ETA: 888782.4s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.201s, learning 0.168s)
               Value function loss: 6.0480
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 282.03
               Mean episode length: 244.45
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 8.37s
                        Total time: 28687.94s
                               ETA: 888748.2s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.331s, learning 0.173s)
               Value function loss: 4.9968
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 282.46
               Mean episode length: 246.12
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 8.50s
                        Total time: 28696.45s
                               ETA: 888718.3s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.419s, learning 0.180s)
               Value function loss: 4.3565
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 277.27
               Mean episode length: 241.21
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 8.60s
                        Total time: 28705.05s
                               ETA: 888691.3s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.639s, learning 0.172s)
               Value function loss: 6.3660
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 284.90
               Mean episode length: 248.21
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 8.81s
                        Total time: 28713.86s
                               ETA: 888670.9s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.570s, learning 0.198s)
               Value function loss: 5.4047
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 285.64
               Mean episode length: 248.59
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 8.77s
                        Total time: 28722.62s
                               ETA: 888649.2s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.607s, learning 0.201s)
               Value function loss: 6.1602
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 280.57
               Mean episode length: 244.43
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 8.81s
                        Total time: 28731.43s
                               ETA: 888628.7s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.664s, learning 0.215s)
               Value function loss: 6.0073
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 285.60
               Mean episode length: 247.31
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 8.88s
                        Total time: 28740.31s
                               ETA: 888610.4s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.414s, learning 0.184s)
               Value function loss: 6.1069
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 277.54
               Mean episode length: 241.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 8.60s
                        Total time: 28748.91s
                               ETA: 888583.4s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.667s, learning 0.211s)
               Value function loss: 5.1134
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 281.46
               Mean episode length: 244.63
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 8.88s
                        Total time: 28757.79s
                               ETA: 888565.1s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.639s, learning 0.200s)
               Value function loss: 4.7731
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 280.04
               Mean episode length: 245.28
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 8.84s
                        Total time: 28766.62s
                               ETA: 888545.6s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.527s, learning 0.274s)
               Value function loss: 4.7423
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 283.79
               Mean episode length: 248.76
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 8.80s
                        Total time: 28775.43s
                               ETA: 888525.0s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.759s, learning 0.169s)
               Value function loss: 5.0413
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 283.88
               Mean episode length: 248.12
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 8.93s
                        Total time: 28784.35s
                               ETA: 888508.2s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.846s, learning 0.174s)
               Value function loss: 5.5370
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 280.81
               Mean episode length: 245.04
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 9.02s
                        Total time: 28793.37s
                               ETA: 888494.4s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.126s, learning 0.247s)
               Value function loss: 5.8121
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 280.82
               Mean episode length: 247.45
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 9.37s
                        Total time: 28802.75s
                               ETA: 888491.4s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.460s, learning 0.179s)
               Value function loss: 5.9649
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 282.94
               Mean episode length: 246.65
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 8.64s
                        Total time: 28811.39s
                               ETA: 888465.8s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.673s, learning 0.191s)
               Value function loss: 5.3026
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 285.70
               Mean episode length: 248.10
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 8.86s
                        Total time: 28820.25s
                               ETA: 888447.1s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.767s, learning 0.179s)
               Value function loss: 5.9725
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 283.73
               Mean episode length: 248.40
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 8.95s
                        Total time: 28829.20s
                               ETA: 888430.9s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.392s, learning 0.167s)
               Value function loss: 6.0605
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 284.17
               Mean episode length: 248.13
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 8.56s
                        Total time: 28837.76s
                               ETA: 888402.8s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.531s, learning 0.194s)
               Value function loss: 5.0605
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 279.40
               Mean episode length: 244.96
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 8.73s
                        Total time: 28846.48s
                               ETA: 888379.9s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.223s, learning 0.195s)
               Value function loss: 5.3254
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 279.64
               Mean episode length: 243.01
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 8.42s
                        Total time: 28854.90s
                               ETA: 888347.5s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.748s, learning 0.241s)
               Value function loss: 5.1056
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 285.07
               Mean episode length: 248.02
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 8.99s
                        Total time: 28863.89s
                               ETA: 888332.7s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.821s, learning 0.228s)
               Value function loss: 6.9489
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 284.62
               Mean episode length: 247.07
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 9.05s
                        Total time: 28872.94s
                               ETA: 888319.7s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.215s, learning 0.182s)
               Value function loss: 7.6055
                    Surrogate loss: -0.0002
             Mean action noise std: 0.76
                       Mean reward: 283.79
               Mean episode length: 244.66
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 8.40s
                        Total time: 28881.33s
                               ETA: 888286.7s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.535s, learning 0.228s)
               Value function loss: 5.6675
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 281.88
               Mean episode length: 244.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 8.76s
                        Total time: 28890.10s
                               ETA: 888265.0s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.749s, learning 0.173s)
               Value function loss: 6.4273
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 273.32
               Mean episode length: 241.41
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 8.92s
                        Total time: 28899.02s
                               ETA: 888248.1s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.541s, learning 0.185s)
               Value function loss: 7.4515
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 276.50
               Mean episode length: 243.22
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 8.73s
                        Total time: 28907.74s
                               ETA: 888225.2s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.655s, learning 0.168s)
               Value function loss: 7.4912
                    Surrogate loss: 0.0060
             Mean action noise std: 0.76
                       Mean reward: 279.48
               Mean episode length: 243.64
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 8.82s
                        Total time: 28916.57s
                               ETA: 888205.4s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.891s, learning 0.183s)
               Value function loss: 7.0339
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 280.84
               Mean episode length: 243.92
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 9.07s
                        Total time: 28925.64s
                               ETA: 888193.2s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.768s, learning 0.201s)
               Value function loss: 8.3560
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 283.60
               Mean episode length: 246.94
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 8.97s
                        Total time: 28934.61s
                               ETA: 888177.9s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.672s, learning 0.233s)
               Value function loss: 7.0107
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 288.33
               Mean episode length: 247.39
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 8.91s
                        Total time: 28943.51s
                               ETA: 888160.5s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.719s, learning 0.200s)
               Value function loss: 6.2278
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 285.52
               Mean episode length: 245.73
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 8.92s
                        Total time: 28952.43s
                               ETA: 888143.6s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.525s, learning 0.317s)
               Value function loss: 7.3227
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 285.89
               Mean episode length: 247.65
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 8.84s
                        Total time: 28961.27s
                               ETA: 888124.3s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.604s, learning 0.177s)
               Value function loss: 5.1273
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 285.04
               Mean episode length: 246.53
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 8.78s
                        Total time: 28970.06s
                               ETA: 888103.2s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.674s, learning 0.169s)
               Value function loss: 7.4145
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 282.73
               Mean episode length: 246.53
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 8.84s
                        Total time: 28978.90s
                               ETA: 888084.0s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.585s, learning 0.171s)
               Value function loss: 7.0276
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 285.35
               Mean episode length: 245.92
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 8.76s
                        Total time: 28987.65s
                               ETA: 888062.2s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.422s, learning 0.233s)
               Value function loss: 5.7322
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 287.20
               Mean episode length: 245.63
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 8.65s
                        Total time: 28996.31s
                               ETA: 888037.2s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.608s, learning 0.173s)
               Value function loss: 7.5836
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 288.59
               Mean episode length: 247.47
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 8.78s
                        Total time: 29005.09s
                               ETA: 888016.1s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.231s, learning 0.173s)
               Value function loss: 7.5354
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 288.62
               Mean episode length: 248.44
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 8.40s
                        Total time: 29013.49s
                               ETA: 887983.4s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.666s, learning 0.244s)
               Value function loss: 7.0252
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 286.48
               Mean episode length: 246.48
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 8.91s
                        Total time: 29022.40s
                               ETA: 887966.4s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.830s, learning 0.207s)
               Value function loss: 6.6674
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 285.46
               Mean episode length: 246.21
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 9.04s
                        Total time: 29031.44s
                               ETA: 887953.1s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.681s, learning 0.211s)
               Value function loss: 5.7291
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 284.85
               Mean episode length: 246.60
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 8.89s
                        Total time: 29040.33s
                               ETA: 887935.4s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.643s, learning 0.202s)
               Value function loss: 6.3379
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 289.47
               Mean episode length: 248.56
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 8.84s
                        Total time: 29049.18s
                               ETA: 887916.3s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.524s, learning 0.174s)
               Value function loss: 5.4919
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 286.63
               Mean episode length: 243.43
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 8.70s
                        Total time: 29057.88s
                               ETA: 887892.8s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.547s, learning 0.186s)
               Value function loss: 5.6970
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 289.00
               Mean episode length: 246.24
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 8.73s
                        Total time: 29066.61s
                               ETA: 887870.3s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.525s, learning 0.195s)
               Value function loss: 6.9281
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 290.44
               Mean episode length: 248.10
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 8.72s
                        Total time: 29075.33s
                               ETA: 887847.4s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.500s, learning 0.188s)
               Value function loss: 6.7420
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 292.85
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 8.69s
                        Total time: 29084.02s
                               ETA: 887823.6s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.217s, learning 0.199s)
               Value function loss: 5.1683
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 293.87
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 8.42s
                        Total time: 29092.43s
                               ETA: 887791.4s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.599s, learning 0.185s)
               Value function loss: 5.2763
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 283.74
               Mean episode length: 245.06
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 8.78s
                        Total time: 29101.22s
                               ETA: 887770.5s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.382s, learning 0.178s)
               Value function loss: 6.5500
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 284.44
               Mean episode length: 246.63
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 8.56s
                        Total time: 29109.78s
                               ETA: 887742.7s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.543s, learning 0.191s)
               Value function loss: 5.2190
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 283.16
               Mean episode length: 245.10
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 8.73s
                        Total time: 29118.51s
                               ETA: 887720.3s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.577s, learning 0.202s)
               Value function loss: 4.4435
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 293.52
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 8.78s
                        Total time: 29127.29s
                               ETA: 887699.3s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1774 steps/s (collection: 9.047s, learning 0.188s)
               Value function loss: 4.7601
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 292.53
               Mean episode length: 247.65
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 9.23s
                        Total time: 29136.52s
                               ETA: 887692.1s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.651s, learning 0.188s)
               Value function loss: 5.6111
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 289.09
               Mean episode length: 246.42
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 8.84s
                        Total time: 29145.36s
                               ETA: 887672.9s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.513s, learning 0.209s)
               Value function loss: 6.6831
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 291.95
               Mean episode length: 248.56
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 8.72s
                        Total time: 29154.08s
                               ETA: 887650.2s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.393s, learning 0.225s)
               Value function loss: 6.4369
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 282.01
               Mean episode length: 242.80
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 8.62s
                        Total time: 29162.70s
                               ETA: 887624.3s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.274s, learning 0.187s)
               Value function loss: 5.3338
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 285.15
               Mean episode length: 244.24
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 8.46s
                        Total time: 29171.16s
                               ETA: 887593.6s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.523s, learning 0.239s)
               Value function loss: 6.6617
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 289.66
               Mean episode length: 246.55
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 8.76s
                        Total time: 29179.92s
                               ETA: 887572.1s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.615s, learning 0.178s)
               Value function loss: 5.4097
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 289.80
               Mean episode length: 247.37
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 8.79s
                        Total time: 29188.72s
                               ETA: 887551.5s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.504s, learning 0.213s)
               Value function loss: 6.5799
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 293.95
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 8.72s
                        Total time: 29197.43s
                               ETA: 887528.7s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.544s, learning 0.183s)
               Value function loss: 6.1648
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 294.74
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 8.73s
                        Total time: 29206.16s
                               ETA: 887506.1s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.522s, learning 0.189s)
               Value function loss: 5.4386
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 292.10
               Mean episode length: 248.17
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 8.71s
                        Total time: 29214.87s
                               ETA: 887483.1s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.283s, learning 0.208s)
               Value function loss: 4.6135
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 291.27
               Mean episode length: 248.17
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 8.49s
                        Total time: 29223.36s
                               ETA: 887453.4s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.466s, learning 0.200s)
               Value function loss: 6.1040
                    Surrogate loss: -0.0000
             Mean action noise std: 0.76
                       Mean reward: 293.02
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 8.67s
                        Total time: 29232.03s
                               ETA: 887429.0s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.345s, learning 0.182s)
               Value function loss: 4.2097
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 292.31
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 8.53s
                        Total time: 29240.56s
                               ETA: 887400.5s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.829s, learning 0.173s)
               Value function loss: 4.9277
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 290.83
               Mean episode length: 248.37
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 9.00s
                        Total time: 29249.56s
                               ETA: 887386.3s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.620s, learning 0.170s)
               Value function loss: 4.8969
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 290.53
               Mean episode length: 248.37
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 8.79s
                        Total time: 29258.35s
                               ETA: 887365.8s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.233s, learning 0.169s)
               Value function loss: 5.1472
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 286.59
               Mean episode length: 244.69
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 8.40s
                        Total time: 29266.75s
                               ETA: 887333.4s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.414s, learning 0.181s)
               Value function loss: 5.0228
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 286.20
               Mean episode length: 244.69
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 8.59s
                        Total time: 29275.35s
                               ETA: 887306.9s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.799s, learning 0.173s)
               Value function loss: 7.2025
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 289.43
               Mean episode length: 248.25
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 8.97s
                        Total time: 29284.32s
                               ETA: 887291.9s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.634s, learning 0.169s)
               Value function loss: 4.9268
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 289.33
               Mean episode length: 246.96
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 8.80s
                        Total time: 29293.12s
                               ETA: 887271.7s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.685s, learning 0.185s)
               Value function loss: 6.7390
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 290.06
               Mean episode length: 248.30
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 8.87s
                        Total time: 29301.99s
                               ETA: 887253.6s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.702s, learning 0.221s)
               Value function loss: 4.9819
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 286.52
               Mean episode length: 245.18
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 8.92s
                        Total time: 29310.91s
                               ETA: 887237.1s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.680s, learning 0.186s)
               Value function loss: 4.2474
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 285.82
               Mean episode length: 243.74
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 8.87s
                        Total time: 29319.78s
                               ETA: 887218.8s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.461s, learning 0.174s)
               Value function loss: 5.5311
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 285.44
               Mean episode length: 243.27
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 8.63s
                        Total time: 29328.41s
                               ETA: 887193.6s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.777s, learning 0.168s)
               Value function loss: 5.2784
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 286.80
               Mean episode length: 244.86
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 8.94s
                        Total time: 29337.36s
                               ETA: 887177.8s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.671s, learning 0.196s)
               Value function loss: 8.3820
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 289.66
               Mean episode length: 247.72
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 8.87s
                        Total time: 29346.22s
                               ETA: 887159.6s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.304s, learning 0.206s)
               Value function loss: 6.4803
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 289.49
               Mean episode length: 248.90
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 8.51s
                        Total time: 29354.73s
                               ETA: 887130.6s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.370s, learning 0.174s)
               Value function loss: 6.4091
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 289.48
               Mean episode length: 247.38
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 8.54s
                        Total time: 29363.28s
                               ETA: 887102.7s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.425s, learning 0.200s)
               Value function loss: 6.5072
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 290.26
               Mean episode length: 249.47
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 8.63s
                        Total time: 29371.90s
                               ETA: 887077.3s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.602s, learning 0.247s)
               Value function loss: 7.2940
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 292.41
               Mean episode length: 247.77
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 8.85s
                        Total time: 29380.75s
                               ETA: 887058.6s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.807s, learning 0.329s)
               Value function loss: 6.7756
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 289.87
               Mean episode length: 247.25
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 9.14s
                        Total time: 29389.89s
                               ETA: 887048.6s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.382s, learning 0.217s)
               Value function loss: 5.0262
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 290.41
               Mean episode length: 247.25
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 8.60s
                        Total time: 29398.49s
                               ETA: 887022.3s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.527s, learning 0.306s)
               Value function loss: 6.4049
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 294.53
               Mean episode length: 249.97
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 8.83s
                        Total time: 29407.32s
                               ETA: 887003.2s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.771s, learning 0.196s)
               Value function loss: 6.8816
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 294.54
               Mean episode length: 248.48
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 8.97s
                        Total time: 29416.29s
                               ETA: 886988.1s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.484s, learning 0.226s)
               Value function loss: 7.3764
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 289.69
               Mean episode length: 248.03
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 8.71s
                        Total time: 29425.00s
                               ETA: 886965.2s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.343s, learning 0.221s)
               Value function loss: 7.0304
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 288.45
               Mean episode length: 248.22
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 8.56s
                        Total time: 29433.56s
                               ETA: 886938.0s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.419s, learning 0.329s)
               Value function loss: 4.7127
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 291.66
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 8.75s
                        Total time: 29442.31s
                               ETA: 886916.3s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.509s, learning 0.174s)
               Value function loss: 6.1303
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 288.09
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 8.68s
                        Total time: 29450.99s
                               ETA: 886892.6s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.457s, learning 0.170s)
               Value function loss: 5.8124
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 294.98
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 8.63s
                        Total time: 29459.62s
                               ETA: 886867.3s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.517s, learning 0.187s)
               Value function loss: 6.4594
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 289.52
               Mean episode length: 246.84
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 8.70s
                        Total time: 29468.32s
                               ETA: 886844.3s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.579s, learning 0.189s)
               Value function loss: 6.1570
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 287.95
               Mean episode length: 246.12
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 8.77s
                        Total time: 29477.09s
                               ETA: 886823.3s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.556s, learning 0.190s)
               Value function loss: 5.9992
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 289.47
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 8.75s
                        Total time: 29485.83s
                               ETA: 886801.6s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.370s, learning 0.216s)
               Value function loss: 5.2427
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 286.92
               Mean episode length: 247.84
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 8.59s
                        Total time: 29494.42s
                               ETA: 886775.1s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.674s, learning 0.175s)
               Value function loss: 5.7714
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 285.68
               Mean episode length: 246.75
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 8.85s
                        Total time: 29503.27s
                               ETA: 886756.5s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.378s, learning 0.166s)
               Value function loss: 4.6292
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 288.79
               Mean episode length: 248.11
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 8.54s
                        Total time: 29511.81s
                               ETA: 886728.8s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.567s, learning 0.168s)
               Value function loss: 4.8459
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 289.19
               Mean episode length: 247.84
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 8.74s
                        Total time: 29520.55s
                               ETA: 886706.8s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.550s, learning 0.204s)
               Value function loss: 4.5689
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 290.17
               Mean episode length: 249.73
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 8.75s
                        Total time: 29529.30s
                               ETA: 886685.3s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.270s, learning 0.290s)
               Value function loss: 6.0016
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 292.75
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 8.56s
                        Total time: 29537.86s
                               ETA: 886658.1s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.453s, learning 0.173s)
               Value function loss: 4.8239
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 287.83
               Mean episode length: 247.19
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 8.63s
                        Total time: 29546.49s
                               ETA: 886632.9s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.513s, learning 0.179s)
               Value function loss: 7.2803
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 292.29
               Mean episode length: 249.07
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 8.69s
                        Total time: 29555.18s
                               ETA: 886609.7s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.463s, learning 0.212s)
               Value function loss: 5.1403
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 292.74
               Mean episode length: 248.06
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 8.68s
                        Total time: 29563.86s
                               ETA: 886585.9s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.537s, learning 0.174s)
               Value function loss: 6.6926
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 291.79
               Mean episode length: 248.43
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 8.71s
                        Total time: 29572.57s
                               ETA: 886563.2s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.417s, learning 0.197s)
               Value function loss: 5.3486
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 282.24
               Mean episode length: 244.47
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 8.61s
                        Total time: 29581.18s
                               ETA: 886537.7s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.583s, learning 0.201s)
               Value function loss: 3.7584
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 282.70
               Mean episode length: 246.04
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 8.78s
                        Total time: 29589.97s
                               ETA: 886517.2s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.460s, learning 0.166s)
               Value function loss: 4.7037
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 287.61
               Mean episode length: 246.35
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 8.63s
                        Total time: 29598.59s
                               ETA: 886492.1s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.643s, learning 0.188s)
               Value function loss: 5.1807
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 288.68
               Mean episode length: 246.35
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 8.83s
                        Total time: 29607.42s
                               ETA: 886473.0s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.531s, learning 0.169s)
               Value function loss: 7.0241
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 290.39
               Mean episode length: 248.21
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 8.70s
                        Total time: 29616.12s
                               ETA: 886450.1s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.498s, learning 0.192s)
               Value function loss: 6.0936
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 292.02
               Mean episode length: 248.21
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 8.69s
                        Total time: 29624.81s
                               ETA: 886426.8s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.379s, learning 0.178s)
               Value function loss: 5.7763
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 286.00
               Mean episode length: 246.81
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 8.56s
                        Total time: 29633.37s
                               ETA: 886399.6s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.735s, learning 0.189s)
               Value function loss: 6.6844
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 287.25
               Mean episode length: 246.37
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 8.92s
                        Total time: 29642.30s
                               ETA: 886383.4s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.571s, learning 0.169s)
               Value function loss: 6.4425
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 290.53
               Mean episode length: 248.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 8.74s
                        Total time: 29651.04s
                               ETA: 886361.7s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.608s, learning 0.206s)
               Value function loss: 6.2657
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 289.41
               Mean episode length: 247.12
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 8.81s
                        Total time: 29659.85s
                               ETA: 886342.2s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.521s, learning 0.176s)
               Value function loss: 4.2619
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 290.37
               Mean episode length: 249.12
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 8.70s
                        Total time: 29668.55s
                               ETA: 886319.2s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.391s, learning 0.231s)
               Value function loss: 5.2434
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 293.14
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 8.62s
                        Total time: 29677.17s
                               ETA: 886294.0s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.193s, learning 0.198s)
               Value function loss: 6.0118
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 290.19
               Mean episode length: 248.27
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 8.39s
                        Total time: 29685.56s
                               ETA: 886261.9s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.827s, learning 0.185s)
               Value function loss: 7.0733
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 283.31
               Mean episode length: 245.82
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 9.01s
                        Total time: 29694.57s
                               ETA: 886248.3s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.846s, learning 0.173s)
               Value function loss: 6.4068
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 290.33
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 9.02s
                        Total time: 29703.59s
                               ETA: 886235.0s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.556s, learning 0.204s)
               Value function loss: 4.4679
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 291.47
               Mean episode length: 247.38
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 8.76s
                        Total time: 29712.35s
                               ETA: 886213.9s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1779 steps/s (collection: 8.960s, learning 0.245s)
               Value function loss: 5.8631
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 288.54
               Mean episode length: 248.24
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 9.20s
                        Total time: 29721.56s
                               ETA: 886206.1s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.509s, learning 0.177s)
               Value function loss: 5.2442
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 290.54
               Mean episode length: 247.90
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 8.69s
                        Total time: 29730.24s
                               ETA: 886182.9s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.740s, learning 0.193s)
               Value function loss: 5.5505
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 291.65
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 8.93s
                        Total time: 29739.18s
                               ETA: 886167.0s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.596s, learning 0.169s)
               Value function loss: 6.4688
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 293.57
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 8.77s
                        Total time: 29747.94s
                               ETA: 886146.1s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.300s, learning 0.207s)
               Value function loss: 5.6870
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 290.38
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 8.51s
                        Total time: 29756.45s
                               ETA: 886117.6s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.699s, learning 0.183s)
               Value function loss: 5.7965
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 292.84
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 8.88s
                        Total time: 29765.33s
                               ETA: 886100.2s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.313s, learning 0.186s)
               Value function loss: 5.3311
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 289.78
               Mean episode length: 246.83
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 8.50s
                        Total time: 29773.83s
                               ETA: 886071.4s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.704s, learning 0.204s)
               Value function loss: 5.2894
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 292.13
               Mean episode length: 248.70
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 8.91s
                        Total time: 29782.74s
                               ETA: 886054.7s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.378s, learning 0.192s)
               Value function loss: 5.0113
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 293.36
               Mean episode length: 248.41
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 8.57s
                        Total time: 29791.31s
                               ETA: 886028.1s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.201s, learning 0.224s)
               Value function loss: 4.8209
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 294.96
               Mean episode length: 247.95
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 8.42s
                        Total time: 29799.73s
                               ETA: 885997.1s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.427s, learning 0.167s)
               Value function loss: 5.8469
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 291.27
               Mean episode length: 247.95
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 8.59s
                        Total time: 29808.33s
                               ETA: 885971.2s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.628s, learning 0.241s)
               Value function loss: 5.0448
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 290.22
               Mean episode length: 248.18
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 8.87s
                        Total time: 29817.20s
                               ETA: 885953.5s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.861s, learning 0.169s)
               Value function loss: 5.6443
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 288.67
               Mean episode length: 246.33
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 9.03s
                        Total time: 29826.23s
                               ETA: 885940.5s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.174s)
               Value function loss: 5.9661
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 295.12
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 8.59s
                        Total time: 29834.81s
                               ETA: 885914.4s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.739s, learning 0.178s)
               Value function loss: 5.8851
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 292.08
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 8.92s
                        Total time: 29843.73s
                               ETA: 885898.1s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.780s, learning 0.210s)
               Value function loss: 5.6500
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 291.58
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 8.99s
                        Total time: 29852.72s
                               ETA: 885884.0s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.557s, learning 0.168s)
               Value function loss: 4.6532
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 291.05
               Mean episode length: 248.32
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 8.72s
                        Total time: 29861.44s
                               ETA: 885862.0s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.755s, learning 0.171s)
               Value function loss: 5.0529
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 293.52
               Mean episode length: 248.32
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 8.93s
                        Total time: 29870.37s
                               ETA: 885846.0s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.460s, learning 0.180s)
               Value function loss: 5.8800
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 289.89
               Mean episode length: 246.40
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 8.64s
                        Total time: 29879.01s
                               ETA: 885821.5s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.314s, learning 0.175s)
               Value function loss: 6.4643
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 293.13
               Mean episode length: 248.09
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 8.49s
                        Total time: 29887.50s
                               ETA: 885792.6s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.466s, learning 0.202s)
               Value function loss: 6.7746
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 294.50
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 8.67s
                        Total time: 29896.17s
                               ETA: 885769.0s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.764s, learning 0.202s)
               Value function loss: 6.7700
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 294.55
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 8.97s
                        Total time: 29905.13s
                               ETA: 885754.2s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.368s, learning 0.202s)
               Value function loss: 6.1452
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 293.29
               Mean episode length: 249.70
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 8.57s
                        Total time: 29913.70s
                               ETA: 885727.6s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.396s, learning 0.193s)
               Value function loss: 6.4483
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 291.17
               Mean episode length: 247.63
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 8.59s
                        Total time: 29922.29s
                               ETA: 885701.7s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.359s, learning 0.170s)
               Value function loss: 6.2379
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 293.21
               Mean episode length: 248.25
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 8.53s
                        Total time: 29930.82s
                               ETA: 885674.0s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.683s, learning 0.173s)
               Value function loss: 5.8532
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 293.03
               Mean episode length: 248.49
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 8.86s
                        Total time: 29939.68s
                               ETA: 885655.9s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.607s, learning 0.197s)
               Value function loss: 5.1343
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 292.01
               Mean episode length: 247.16
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 8.80s
                        Total time: 29948.48s
                               ETA: 885636.4s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.622s, learning 0.178s)
               Value function loss: 5.4264
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 294.40
               Mean episode length: 248.67
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 8.80s
                        Total time: 29957.28s
                               ETA: 885616.7s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.781s, learning 0.200s)
               Value function loss: 7.5271
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 291.91
               Mean episode length: 246.29
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 8.98s
                        Total time: 29966.26s
                               ETA: 885602.4s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.643s, learning 0.249s)
               Value function loss: 7.3331
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 290.63
               Mean episode length: 248.07
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 8.89s
                        Total time: 29975.16s
                               ETA: 885585.5s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.872s, learning 0.205s)
               Value function loss: 6.1325
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 289.51
               Mean episode length: 246.89
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 9.08s
                        Total time: 29984.23s
                               ETA: 885574.0s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.406s, learning 0.214s)
               Value function loss: 6.1802
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 290.95
               Mean episode length: 245.96
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 8.62s
                        Total time: 29992.85s
                               ETA: 885549.1s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.535s, learning 0.223s)
               Value function loss: 6.7797
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 293.33
               Mean episode length: 247.97
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 8.76s
                        Total time: 30001.61s
                               ETA: 885528.2s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.466s, learning 0.185s)
               Value function loss: 6.6962
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 295.15
               Mean episode length: 246.75
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 8.65s
                        Total time: 30010.26s
                               ETA: 885504.1s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.498s, learning 0.182s)
               Value function loss: 5.3963
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 293.00
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 8.68s
                        Total time: 30018.94s
                               ETA: 885481.0s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.842s, learning 0.177s)
               Value function loss: 6.9703
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 290.74
               Mean episode length: 247.64
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 9.02s
                        Total time: 30027.96s
                               ETA: 885467.8s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.525s, learning 0.202s)
               Value function loss: 5.3945
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 291.18
               Mean episode length: 249.38
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 8.73s
                        Total time: 30036.69s
                               ETA: 885446.0s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.600s, learning 0.354s)
               Value function loss: 5.5039
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 292.55
               Mean episode length: 248.37
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 8.95s
                        Total time: 30045.64s
                               ETA: 885431.0s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.852s, learning 0.218s)
               Value function loss: 5.1924
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 296.92
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 9.07s
                        Total time: 30054.71s
                               ETA: 885419.3s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.158s, learning 0.215s)
               Value function loss: 4.1520
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 292.75
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 8.37s
                        Total time: 30063.08s
                               ETA: 885387.1s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.505s, learning 0.209s)
               Value function loss: 5.3808
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 293.95
               Mean episode length: 248.18
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 8.71s
                        Total time: 30071.80s
                               ETA: 885365.0s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.441s, learning 0.267s)
               Value function loss: 5.6029
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 295.08
               Mean episode length: 248.18
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 8.71s
                        Total time: 30080.51s
                               ETA: 885342.7s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.429s, learning 0.204s)
               Value function loss: 4.7409
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 289.62
               Mean episode length: 248.23
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 8.63s
                        Total time: 30089.14s
                               ETA: 885318.2s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.662s, learning 0.210s)
               Value function loss: 5.4099
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 290.79
               Mean episode length: 248.23
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 8.87s
                        Total time: 30098.01s
                               ETA: 885300.8s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.568s, learning 0.173s)
               Value function loss: 6.7286
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 295.02
               Mean episode length: 249.22
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 8.74s
                        Total time: 30106.75s
                               ETA: 885279.5s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.495s, learning 0.240s)
               Value function loss: 5.7788
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 298.52
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 8.74s
                        Total time: 30115.49s
                               ETA: 885258.0s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.669s, learning 0.277s)
               Value function loss: 5.3829
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 297.62
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 8.95s
                        Total time: 30124.43s
                               ETA: 885242.8s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1785 steps/s (collection: 8.927s, learning 0.251s)
               Value function loss: 5.0429
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 294.23
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 9.18s
                        Total time: 30133.61s
                               ETA: 885234.3s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.590s, learning 0.253s)
               Value function loss: 4.7475
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 295.53
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 8.84s
                        Total time: 30142.45s
                               ETA: 885216.0s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.410s, learning 0.202s)
               Value function loss: 5.5114
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 295.55
               Mean episode length: 247.86
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 8.61s
                        Total time: 30151.07s
                               ETA: 885191.0s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.413s, learning 0.173s)
               Value function loss: 5.1951
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 292.07
               Mean episode length: 245.82
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 8.59s
                        Total time: 30159.65s
                               ETA: 885165.2s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.616s, learning 0.185s)
               Value function loss: 7.0170
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 292.58
               Mean episode length: 247.28
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 8.80s
                        Total time: 30168.45s
                               ETA: 885145.7s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.665s, learning 0.174s)
               Value function loss: 6.9160
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 287.78
               Mean episode length: 244.63
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 8.84s
                        Total time: 30177.29s
                               ETA: 885127.3s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.742s, learning 0.176s)
               Value function loss: 4.9745
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 292.26
               Mean episode length: 246.63
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 8.92s
                        Total time: 30186.21s
                               ETA: 885111.3s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.404s, learning 0.181s)
               Value function loss: 5.5577
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 296.86
               Mean episode length: 248.77
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 8.59s
                        Total time: 30194.80s
                               ETA: 885085.5s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.748s, learning 0.202s)
               Value function loss: 6.8384
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 298.03
               Mean episode length: 248.25
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 8.95s
                        Total time: 30203.75s
                               ETA: 885070.4s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.506s, learning 0.174s)
               Value function loss: 5.8421
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 294.78
               Mean episode length: 248.17
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 8.68s
                        Total time: 30212.43s
                               ETA: 885047.4s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.369s, learning 0.231s)
               Value function loss: 4.9007
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 292.90
               Mean episode length: 246.08
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 8.60s
                        Total time: 30221.03s
                               ETA: 885022.1s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.367s, learning 0.274s)
               Value function loss: 5.0874
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 294.92
               Mean episode length: 247.91
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 8.64s
                        Total time: 30229.67s
                               ETA: 884998.0s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.909s, learning 0.210s)
               Value function loss: 6.5368
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 296.16
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 9.12s
                        Total time: 30238.79s
                               ETA: 884987.9s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.764s, learning 0.223s)
               Value function loss: 7.1011
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 291.13
               Mean episode length: 243.87
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 8.99s
                        Total time: 30247.77s
                               ETA: 884973.9s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.629s, learning 0.170s)
               Value function loss: 7.1570
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 292.37
               Mean episode length: 244.67
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 8.80s
                        Total time: 30256.57s
                               ETA: 884954.4s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.611s, learning 0.201s)
               Value function loss: 5.2206
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 293.37
               Mean episode length: 245.52
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 8.81s
                        Total time: 30265.39s
                               ETA: 884935.4s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.605s, learning 0.171s)
               Value function loss: 6.0306
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 296.88
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 8.78s
                        Total time: 30274.16s
                               ETA: 884915.2s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.503s, learning 0.207s)
               Value function loss: 5.8527
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 295.98
               Mean episode length: 248.56
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 8.71s
                        Total time: 30282.87s
                               ETA: 884893.2s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.439s, learning 0.175s)
               Value function loss: 6.3351
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 291.51
               Mean episode length: 246.26
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 8.61s
                        Total time: 30291.49s
                               ETA: 884868.3s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.636s, learning 0.205s)
               Value function loss: 6.1785
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 294.09
               Mean episode length: 247.97
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 8.84s
                        Total time: 30300.33s
                               ETA: 884850.1s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.475s, learning 0.313s)
               Value function loss: 5.5554
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 296.25
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 8.79s
                        Total time: 30309.12s
                               ETA: 884830.3s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.598s, learning 0.206s)
               Value function loss: 5.7868
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 296.35
               Mean episode length: 248.60
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 8.80s
                        Total time: 30317.92s
                               ETA: 884811.0s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.642s, learning 0.197s)
               Value function loss: 6.8291
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 298.09
               Mean episode length: 248.74
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 8.84s
                        Total time: 30326.76s
                               ETA: 884792.8s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.405s, learning 0.222s)
               Value function loss: 4.9034
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 291.06
               Mean episode length: 245.12
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 8.63s
                        Total time: 30335.39s
                               ETA: 884768.4s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.515s, learning 0.325s)
               Value function loss: 5.5469
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 289.80
               Mean episode length: 246.07
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 8.84s
                        Total time: 30344.23s
                               ETA: 884750.2s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.400s, learning 0.186s)
               Value function loss: 4.8047
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 292.52
               Mean episode length: 248.21
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 8.59s
                        Total time: 30352.81s
                               ETA: 884724.6s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.314s, learning 0.255s)
               Value function loss: 5.8169
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 294.38
               Mean episode length: 247.96
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 8.57s
                        Total time: 30361.38s
                               ETA: 884698.5s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.678s, learning 0.176s)
               Value function loss: 4.8221
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 295.14
               Mean episode length: 247.71
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 8.85s
                        Total time: 30370.24s
                               ETA: 884680.7s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.518s, learning 0.303s)
               Value function loss: 7.5757
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 296.70
               Mean episode length: 248.19
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 8.82s
                        Total time: 30379.06s
                               ETA: 884661.9s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.392s, learning 0.276s)
               Value function loss: 5.0631
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 296.14
               Mean episode length: 248.19
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 8.67s
                        Total time: 30387.72s
                               ETA: 884638.7s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.273s, learning 0.202s)
               Value function loss: 6.6123
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 295.01
               Mean episode length: 247.87
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 8.47s
                        Total time: 30396.20s
                               ETA: 884609.9s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.553s, learning 0.176s)
               Value function loss: 4.7340
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 291.82
               Mean episode length: 247.87
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 8.73s
                        Total time: 30404.93s
                               ETA: 884588.5s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.602s, learning 0.168s)
               Value function loss: 4.1650
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 292.91
               Mean episode length: 247.81
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 8.77s
                        Total time: 30413.70s
                               ETA: 884568.3s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.215s, learning 0.186s)
               Value function loss: 5.2833
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 295.67
               Mean episode length: 247.81
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 8.40s
                        Total time: 30422.10s
                               ETA: 884537.4s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.365s, learning 0.172s)
               Value function loss: 4.9856
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 296.34
               Mean episode length: 248.16
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 8.54s
                        Total time: 30430.63s
                               ETA: 884510.4s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.473s, learning 0.220s)
               Value function loss: 7.0900
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 292.73
               Mean episode length: 246.05
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 8.69s
                        Total time: 30439.33s
                               ETA: 884488.0s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.495s, learning 0.195s)
               Value function loss: 6.6637
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 296.93
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 8.69s
                        Total time: 30448.02s
                               ETA: 884465.5s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.251s, learning 0.173s)
               Value function loss: 6.1380
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 298.31
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 8.42s
                        Total time: 30456.44s
                               ETA: 884435.3s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.579s, learning 0.177s)
               Value function loss: 6.0215
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 297.76
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 8.76s
                        Total time: 30465.20s
                               ETA: 884414.8s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.196s, learning 0.227s)
               Value function loss: 6.8915
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 298.16
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 8.42s
                        Total time: 30473.62s
                               ETA: 884384.5s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1761 steps/s (collection: 9.120s, learning 0.182s)
               Value function loss: 6.2925
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 296.33
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 9.30s
                        Total time: 30482.92s
                               ETA: 884379.8s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.582s, learning 0.199s)
               Value function loss: 4.0540
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 289.59
               Mean episode length: 244.24
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 8.78s
                        Total time: 30491.70s
                               ETA: 884360.0s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.576s, learning 0.212s)
               Value function loss: 5.5581
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 287.98
               Mean episode length: 244.24
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 8.79s
                        Total time: 30500.49s
                               ETA: 884340.4s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.637s, learning 0.192s)
               Value function loss: 6.0268
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 294.94
               Mean episode length: 247.22
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 8.83s
                        Total time: 30509.32s
                               ETA: 884322.0s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.222s, learning 0.304s)
               Value function loss: 7.2371
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 297.27
               Mean episode length: 249.96
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 8.53s
                        Total time: 30517.85s
                               ETA: 884294.9s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.624s, learning 0.189s)
               Value function loss: 7.3963
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 296.23
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 8.81s
                        Total time: 30526.66s
                               ETA: 884276.0s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.657s, learning 0.205s)
               Value function loss: 5.2109
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 296.68
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 8.86s
                        Total time: 30535.52s
                               ETA: 884258.6s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.568s, learning 0.195s)
               Value function loss: 6.2431
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 294.85
               Mean episode length: 248.13
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 8.76s
                        Total time: 30544.28s
                               ETA: 884238.3s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.648s, learning 0.218s)
               Value function loss: 5.7049
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 294.41
               Mean episode length: 248.13
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 8.87s
                        Total time: 30553.15s
                               ETA: 884221.0s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.420s, learning 0.181s)
               Value function loss: 6.4546
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 294.90
               Mean episode length: 248.13
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.60s
                        Total time: 30561.75s
                               ETA: 884196.0s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.689s, learning 0.176s)
               Value function loss: 6.6104
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 297.46
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 8.86s
                        Total time: 30570.62s
                               ETA: 884178.7s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.662s, learning 0.179s)
               Value function loss: 6.2302
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 295.94
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 8.84s
                        Total time: 30579.46s
                               ETA: 884160.7s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.368s, learning 0.286s)
               Value function loss: 6.0865
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 296.95
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 8.65s
                        Total time: 30588.11s
                               ETA: 884137.2s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.673s, learning 0.179s)
               Value function loss: 6.0851
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 289.38
               Mean episode length: 244.26
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 8.85s
                        Total time: 30596.96s
                               ETA: 884119.6s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.843s, learning 0.197s)
               Value function loss: 5.4240
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 292.43
               Mean episode length: 246.07
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 9.04s
                        Total time: 30606.00s
                               ETA: 884107.3s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.425s, learning 0.181s)
               Value function loss: 5.2196
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 295.93
               Mean episode length: 247.85
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 8.61s
                        Total time: 30614.61s
                               ETA: 884082.5s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.536s, learning 0.338s)
               Value function loss: 4.5647
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 296.50
               Mean episode length: 249.19
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 8.87s
                        Total time: 30623.48s
                               ETA: 884065.5s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.484s, learning 0.178s)
               Value function loss: 6.6517
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 299.53
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 8.66s
                        Total time: 30632.14s
                               ETA: 884042.4s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.442s, learning 0.254s)
               Value function loss: 4.7446
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 294.87
               Mean episode length: 248.15
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 8.70s
                        Total time: 30640.84s
                               ETA: 884020.2s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.762s, learning 0.179s)
               Value function loss: 7.8598
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 293.67
               Mean episode length: 248.16
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 8.94s
                        Total time: 30649.78s
                               ETA: 884005.1s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.329s, learning 0.324s)
               Value function loss: 5.0591
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 293.73
               Mean episode length: 246.23
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 8.65s
                        Total time: 30658.43s
                               ETA: 883981.8s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1773 steps/s (collection: 9.041s, learning 0.195s)
               Value function loss: 6.5609
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 292.33
               Mean episode length: 246.96
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 9.24s
                        Total time: 30667.67s
                               ETA: 883975.2s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.660s, learning 0.247s)
               Value function loss: 6.3478
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 290.98
               Mean episode length: 246.59
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 8.91s
                        Total time: 30676.58s
                               ETA: 883959.2s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.816s, learning 0.189s)
               Value function loss: 4.3414
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 290.86
               Mean episode length: 244.66
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 9.00s
                        Total time: 30685.58s
                               ETA: 883945.9s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.943s, learning 0.199s)
               Value function loss: 5.0918
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 295.41
               Mean episode length: 248.07
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 9.14s
                        Total time: 30694.72s
                               ETA: 883936.7s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.416s, learning 0.196s)
               Value function loss: 5.2087
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 296.63
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 8.61s
                        Total time: 30703.33s
                               ETA: 883912.1s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.309s, learning 0.172s)
               Value function loss: 7.4587
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 294.48
               Mean episode length: 248.60
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 8.48s
                        Total time: 30711.82s
                               ETA: 883883.9s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.340s, learning 0.177s)
               Value function loss: 7.0441
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 295.28
               Mean episode length: 248.60
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 8.52s
                        Total time: 30720.33s
                               ETA: 883856.6s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.097s, learning 0.224s)
               Value function loss: 6.8207
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 297.79
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 9.32s
                        Total time: 30729.65s
                               ETA: 883852.5s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.491s, learning 0.281s)
               Value function loss: 6.9712
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 296.40
               Mean episode length: 249.31
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 8.77s
                        Total time: 30738.43s
                               ETA: 883832.6s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.716s, learning 0.212s)
               Value function loss: 6.8891
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 295.87
               Mean episode length: 248.20
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 8.93s
                        Total time: 30747.35s
                               ETA: 883817.2s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.734s, learning 0.181s)
               Value function loss: 6.7207
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 295.71
               Mean episode length: 248.20
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 8.92s
                        Total time: 30756.27s
                               ETA: 883801.5s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.741s, learning 0.181s)
               Value function loss: 4.7608
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 297.56
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 8.92s
                        Total time: 30765.19s
                               ETA: 883785.9s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.563s, learning 0.175s)
               Value function loss: 5.8001
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 297.58
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 8.74s
                        Total time: 30773.93s
                               ETA: 883765.0s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.610s, learning 0.198s)
               Value function loss: 6.9564
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 297.56
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 8.81s
                        Total time: 30782.74s
                               ETA: 883746.2s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.963s, learning 0.202s)
               Value function loss: 7.5662
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 291.76
               Mean episode length: 246.03
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 9.16s
                        Total time: 30791.90s
                               ETA: 883737.6s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.291s, learning 0.258s)
               Value function loss: 7.3215
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 295.57
               Mean episode length: 248.16
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 8.55s
                        Total time: 30800.45s
                               ETA: 883711.4s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.428s, learning 0.250s)
               Value function loss: 5.9140
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 301.03
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 8.68s
                        Total time: 30809.13s
                               ETA: 883688.8s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.421s, learning 0.176s)
               Value function loss: 6.5863
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 299.45
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 8.60s
                        Total time: 30817.72s
                               ETA: 883664.0s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.544s, learning 0.175s)
               Value function loss: 6.3079
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 292.77
               Mean episode length: 248.19
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 8.72s
                        Total time: 30826.44s
                               ETA: 883642.6s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1773 steps/s (collection: 9.065s, learning 0.172s)
               Value function loss: 5.7996
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 292.72
               Mean episode length: 248.19
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 9.24s
                        Total time: 30835.68s
                               ETA: 883636.1s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.676s, learning 0.243s)
               Value function loss: 7.1239
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 292.77
               Mean episode length: 248.38
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 8.92s
                        Total time: 30844.60s
                               ETA: 883620.5s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.784s, learning 0.296s)
               Value function loss: 5.9612
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 296.75
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 9.08s
                        Total time: 30853.68s
                               ETA: 883609.5s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.700s, learning 0.241s)
               Value function loss: 5.7368
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 294.70
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 8.94s
                        Total time: 30862.62s
                               ETA: 883594.5s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.450s, learning 0.178s)
               Value function loss: 5.8751
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 293.99
               Mean episode length: 249.80
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 8.63s
                        Total time: 30871.25s
                               ETA: 883570.5s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.313s, learning 0.190s)
               Value function loss: 5.7282
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 297.21
               Mean episode length: 249.80
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 8.50s
                        Total time: 30879.75s
                               ETA: 883543.1s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.194s, learning 0.234s)
               Value function loss: 5.2240
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 292.59
               Mean episode length: 248.16
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 9.43s
                        Total time: 30889.18s
                               ETA: 883542.1s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.427s, learning 0.224s)
               Value function loss: 4.6108
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 293.64
               Mean episode length: 248.16
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 8.65s
                        Total time: 30897.83s
                               ETA: 883518.8s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 1784 steps/s (collection: 9.003s, learning 0.179s)
               Value function loss: 5.9337
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 295.59
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 9.18s
                        Total time: 30907.01s
                               ETA: 883510.7s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.605s, learning 0.196s)
               Value function loss: 5.4980
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 295.76
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 8.80s
                        Total time: 30915.81s
                               ETA: 883491.8s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.432s, learning 0.169s)
               Value function loss: 6.0475
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 296.57
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 8.60s
                        Total time: 30924.41s
                               ETA: 883467.1s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.795s, learning 0.184s)
               Value function loss: 5.6033
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 294.16
               Mean episode length: 248.19
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 8.98s
                        Total time: 30933.39s
                               ETA: 883453.3s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.688s, learning 0.231s)
               Value function loss: 7.2095
                    Surrogate loss: -0.0007
             Mean action noise std: 0.76
                       Mean reward: 295.99
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 8.92s
                        Total time: 30942.31s
                               ETA: 883437.7s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.420s, learning 0.314s)
               Value function loss: 6.0379
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 297.08
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 8.73s
                        Total time: 30951.04s
                               ETA: 883416.9s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.793s, learning 0.180s)
               Value function loss: 4.7071
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 295.48
               Mean episode length: 248.15
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 8.97s
                        Total time: 30960.02s
                               ETA: 883402.9s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.318s, learning 0.199s)
               Value function loss: 5.3963
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 292.48
               Mean episode length: 248.15
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 8.52s
                        Total time: 30968.54s
                               ETA: 883375.9s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.370s, learning 0.180s)
               Value function loss: 6.1199
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 294.23
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 8.55s
                        Total time: 30977.09s
                               ETA: 883349.8s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.470s, learning 0.184s)
               Value function loss: 7.0384
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 295.20
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 8.65s
                        Total time: 30985.74s
                               ETA: 883326.7s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.465s, learning 0.174s)
               Value function loss: 6.9385
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 291.44
               Mean episode length: 247.97
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 8.64s
                        Total time: 30994.38s
                               ETA: 883303.2s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.857s, learning 0.220s)
               Value function loss: 7.2178
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 294.70
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 9.08s
                        Total time: 31003.45s
                               ETA: 883292.2s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.475s, learning 0.171s)
               Value function loss: 5.9186
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 295.84
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 8.65s
                        Total time: 31012.10s
                               ETA: 883268.9s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.718s, learning 0.200s)
               Value function loss: 5.8482
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 293.24
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 8.92s
                        Total time: 31021.02s
                               ETA: 883253.4s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.849s, learning 0.180s)
               Value function loss: 6.2977
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 293.46
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 9.03s
                        Total time: 31030.05s
                               ETA: 883241.0s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.490s, learning 0.257s)
               Value function loss: 5.3517
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 293.42
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 8.75s
                        Total time: 31038.80s
                               ETA: 883220.6s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.445s, learning 0.196s)
               Value function loss: 5.6727
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 293.87
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 8.64s
                        Total time: 31047.44s
                               ETA: 883197.2s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.429s, learning 0.307s)
               Value function loss: 5.1700
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 294.87
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 8.74s
                        Total time: 31056.17s
                               ETA: 883176.5s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.686s, learning 0.280s)
               Value function loss: 7.3112
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 289.87
               Mean episode length: 247.90
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 8.97s
                        Total time: 31065.14s
                               ETA: 883162.4s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.439s, learning 0.206s)
               Value function loss: 6.9489
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 287.24
               Mean episode length: 244.13
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 8.65s
                        Total time: 31073.79s
                               ETA: 883139.1s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.475s, learning 0.186s)
               Value function loss: 6.1108
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 292.16
               Mean episode length: 247.98
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 8.66s
                        Total time: 31082.45s
                               ETA: 883116.3s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.785s, learning 0.286s)
               Value function loss: 5.8030
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 290.20
               Mean episode length: 246.96
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 9.07s
                        Total time: 31091.52s
                               ETA: 883105.1s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.788s, learning 0.299s)
               Value function loss: 6.0595
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 289.90
               Mean episode length: 246.91
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 9.09s
                        Total time: 31100.60s
                               ETA: 883094.4s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.045s, learning 0.174s)
               Value function loss: 6.0607
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 292.18
               Mean episode length: 247.93
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 9.22s
                        Total time: 31109.82s
                               ETA: 883087.5s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.717s, learning 0.201s)
               Value function loss: 5.0630
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 293.87
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 8.92s
                        Total time: 31118.74s
                               ETA: 883072.0s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.584s, learning 0.193s)
               Value function loss: 6.5789
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 291.62
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 8.78s
                        Total time: 31127.52s
                               ETA: 883052.5s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.493s, learning 0.239s)
               Value function loss: 5.6864
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 290.08
               Mean episode length: 247.89
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 8.73s
                        Total time: 31136.25s
                               ETA: 883031.7s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.306s, learning 0.243s)
               Value function loss: 4.7430
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 286.67
               Mean episode length: 246.27
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 8.55s
                        Total time: 31144.80s
                               ETA: 883005.7s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.563s, learning 0.215s)
               Value function loss: 5.0908
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 286.81
               Mean episode length: 246.98
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 8.78s
                        Total time: 31153.57s
                               ETA: 882986.3s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.511s, learning 0.198s)
               Value function loss: 4.4376
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 290.17
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 8.71s
                        Total time: 31162.28s
                               ETA: 882964.9s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.971s, learning 0.171s)
               Value function loss: 4.7580
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 291.66
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 9.14s
                        Total time: 31171.43s
                               ETA: 882955.8s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.774s, learning 0.174s)
               Value function loss: 4.6554
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 289.59
               Mean episode length: 248.31
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 8.95s
                        Total time: 31180.37s
                               ETA: 882941.2s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.737s, learning 0.192s)
               Value function loss: 4.5063
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 287.63
               Mean episode length: 248.03
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 8.93s
                        Total time: 31189.30s
                               ETA: 882926.0s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.791s, learning 0.228s)
               Value function loss: 4.8702
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 285.78
               Mean episode length: 246.07
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 9.02s
                        Total time: 31198.32s
                               ETA: 882913.4s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.716s, learning 0.175s)
               Value function loss: 5.8123
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 287.47
               Mean episode length: 247.86
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 8.89s
                        Total time: 31207.21s
                               ETA: 882897.2s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.508s, learning 0.283s)
               Value function loss: 5.0229
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 287.87
               Mean episode length: 247.20
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 8.79s
                        Total time: 31216.00s
                               ETA: 882878.1s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.618s, learning 0.213s)
               Value function loss: 5.1826
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 289.84
               Mean episode length: 248.51
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 8.83s
                        Total time: 31224.83s
                               ETA: 882860.2s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.543s, learning 0.190s)
               Value function loss: 4.6864
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 289.72
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 8.73s
                        Total time: 31233.56s
                               ETA: 882839.5s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.908s, learning 0.196s)
               Value function loss: 4.2753
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 286.80
               Mean episode length: 248.14
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 9.10s
                        Total time: 31242.67s
                               ETA: 882829.3s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.434s, learning 0.175s)
               Value function loss: 4.1784
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 286.82
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 8.61s
                        Total time: 31251.28s
                               ETA: 882805.2s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.455s, learning 0.215s)
               Value function loss: 4.2161
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 285.86
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 8.67s
                        Total time: 31259.95s
                               ETA: 882782.8s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.584s, learning 0.211s)
               Value function loss: 5.6360
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 286.07
               Mean episode length: 248.34
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 8.79s
                        Total time: 31268.74s
                               ETA: 882763.9s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.570s, learning 0.184s)
               Value function loss: 5.2924
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 283.59
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 8.75s
                        Total time: 31277.50s
                               ETA: 882743.8s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.979s, learning 0.183s)
               Value function loss: 4.4378
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 281.90
               Mean episode length: 248.22
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 9.16s
                        Total time: 31286.66s
                               ETA: 882735.3s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.416s, learning 0.215s)
               Value function loss: 4.9398
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 285.20
               Mean episode length: 248.61
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 8.63s
                        Total time: 31295.29s
                               ETA: 882711.8s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.463s, learning 0.199s)
               Value function loss: 5.2946
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 277.99
               Mean episode length: 244.05
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 8.66s
                        Total time: 31303.95s
                               ETA: 882689.2s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.610s, learning 0.257s)
               Value function loss: 5.3278
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 282.01
               Mean episode length: 246.73
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 8.87s
                        Total time: 31312.82s
                               ETA: 882672.4s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.525s, learning 0.192s)
               Value function loss: 4.8722
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 283.31
               Mean episode length: 247.94
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 8.72s
                        Total time: 31321.54s
                               ETA: 882651.3s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.580s, learning 0.178s)
               Value function loss: 4.1009
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 281.55
               Mean episode length: 246.16
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 8.76s
                        Total time: 31330.29s
                               ETA: 882631.4s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.332s, learning 0.187s)
               Value function loss: 5.4217
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 287.89
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 8.52s
                        Total time: 31338.81s
                               ETA: 882604.8s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.770s, learning 0.183s)
               Value function loss: 6.5026
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 283.07
               Mean episode length: 244.27
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 8.95s
                        Total time: 31347.77s
                               ETA: 882590.4s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.638s, learning 0.301s)
               Value function loss: 6.6677
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 285.14
               Mean episode length: 248.14
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 8.94s
                        Total time: 31356.71s
                               ETA: 882575.7s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.816s, learning 0.200s)
               Value function loss: 4.5536
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 280.54
               Mean episode length: 246.92
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 9.02s
                        Total time: 31365.72s
                               ETA: 882563.1s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.482s, learning 0.203s)
               Value function loss: 5.4884
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 275.94
               Mean episode length: 242.19
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 8.68s
                        Total time: 31374.41s
                               ETA: 882541.1s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.291s, learning 0.186s)
               Value function loss: 4.8948
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 280.48
               Mean episode length: 245.53
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 8.48s
                        Total time: 31382.88s
                               ETA: 882513.4s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.286s, learning 0.308s)
               Value function loss: 5.9539
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 287.01
               Mean episode length: 248.58
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 8.59s
                        Total time: 31391.48s
                               ETA: 882488.9s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.822s, learning 0.214s)
               Value function loss: 5.5078
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 279.80
               Mean episode length: 243.34
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 9.04s
                        Total time: 31400.51s
                               ETA: 882476.9s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.437s, learning 0.183s)
               Value function loss: 4.1217
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 278.61
               Mean episode length: 244.96
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 8.62s
                        Total time: 31409.13s
                               ETA: 882453.2s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.437s, learning 0.256s)
               Value function loss: 4.7720
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 282.91
               Mean episode length: 248.04
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 8.69s
                        Total time: 31417.83s
                               ETA: 882431.5s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.734s, learning 0.216s)
               Value function loss: 6.3212
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 277.53
               Mean episode length: 243.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 8.95s
                        Total time: 31426.78s
                               ETA: 882417.1s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.308s, learning 0.226s)
               Value function loss: 4.1905
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 278.82
               Mean episode length: 243.34
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 8.53s
                        Total time: 31435.31s
                               ETA: 882391.0s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.705s, learning 0.276s)
               Value function loss: 4.5627
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 283.11
               Mean episode length: 247.05
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 8.98s
                        Total time: 31444.29s
                               ETA: 882377.5s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.539s, learning 0.221s)
               Value function loss: 4.7445
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 280.18
               Mean episode length: 243.64
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 8.76s
                        Total time: 31453.05s
                               ETA: 882357.7s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.534s, learning 0.260s)
               Value function loss: 4.7901
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 280.76
               Mean episode length: 244.89
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 8.79s
                        Total time: 31461.85s
                               ETA: 882338.9s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.548s, learning 0.205s)
               Value function loss: 4.7106
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 283.65
               Mean episode length: 244.94
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 8.75s
                        Total time: 31470.60s
                               ETA: 882319.0s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.901s, learning 0.173s)
               Value function loss: 6.6578
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 283.35
               Mean episode length: 245.16
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 9.07s
                        Total time: 31479.67s
                               ETA: 882308.1s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.474s, learning 0.254s)
               Value function loss: 4.6620
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 282.95
               Mean episode length: 244.82
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 8.73s
                        Total time: 31488.40s
                               ETA: 882287.4s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.712s, learning 0.201s)
               Value function loss: 5.4435
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 283.80
               Mean episode length: 245.43
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 8.91s
                        Total time: 31497.31s
                               ETA: 882272.0s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.467s, learning 0.199s)
               Value function loss: 4.9241
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 282.16
               Mean episode length: 245.33
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 8.67s
                        Total time: 31505.98s
                               ETA: 882249.7s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.812s, learning 0.210s)
               Value function loss: 3.5896
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 276.17
               Mean episode length: 242.40
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 9.02s
                        Total time: 31515.00s
                               ETA: 882237.3s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.399s, learning 0.185s)
               Value function loss: 4.1317
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 276.98
               Mean episode length: 243.99
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 8.58s
                        Total time: 31523.59s
                               ETA: 882212.7s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.337s, learning 0.181s)
               Value function loss: 3.5128
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 281.60
               Mean episode length: 246.59
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 8.52s
                        Total time: 31532.10s
                               ETA: 882186.2s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.584s, learning 0.173s)
               Value function loss: 5.9703
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 283.79
               Mean episode length: 249.81
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 8.76s
                        Total time: 31540.86s
                               ETA: 882166.4s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.389s, learning 0.174s)
               Value function loss: 5.4877
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 279.32
               Mean episode length: 245.41
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 8.56s
                        Total time: 31549.42s
                               ETA: 882141.2s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.597s, learning 0.223s)
               Value function loss: 4.8814
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 287.03
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 8.82s
                        Total time: 31558.24s
                               ETA: 882123.2s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.659s, learning 0.189s)
               Value function loss: 4.6899
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 283.88
               Mean episode length: 248.09
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 8.85s
                        Total time: 31567.09s
                               ETA: 882106.0s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.527s, learning 0.235s)
               Value function loss: 5.4655
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 275.34
               Mean episode length: 239.85
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 8.76s
                        Total time: 31575.85s
                               ETA: 882086.4s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 1749 steps/s (collection: 8.989s, learning 0.376s)
               Value function loss: 4.5852
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 278.26
               Mean episode length: 244.58
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 9.37s
                        Total time: 31585.22s
                               ETA: 882083.7s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.580s, learning 0.241s)
               Value function loss: 3.7921
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 282.18
               Mean episode length: 248.42
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 8.82s
                        Total time: 31594.04s
                               ETA: 882065.7s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1771 steps/s (collection: 9.031s, learning 0.217s)
               Value function loss: 4.7482
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 278.41
               Mean episode length: 242.85
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 9.25s
                        Total time: 31603.29s
                               ETA: 882059.7s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.388s, learning 0.184s)
               Value function loss: 4.6963
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 283.53
               Mean episode length: 246.67
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 8.57s
                        Total time: 31611.86s
                               ETA: 882034.8s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.772s, learning 0.255s)
               Value function loss: 5.9986
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 281.32
               Mean episode length: 245.66
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 9.03s
                        Total time: 31620.89s
                               ETA: 882022.7s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.762s, learning 0.214s)
               Value function loss: 5.0979
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 282.33
               Mean episode length: 247.01
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 8.98s
                        Total time: 31629.86s
                               ETA: 882009.0s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.317s, learning 0.193s)
               Value function loss: 3.7256
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 281.78
               Mean episode length: 246.07
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 8.51s
                        Total time: 31638.37s
                               ETA: 881982.4s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.413s, learning 0.188s)
               Value function loss: 5.1904
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 287.44
               Mean episode length: 249.50
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 8.60s
                        Total time: 31646.97s
                               ETA: 881958.4s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.805s, learning 0.180s)
               Value function loss: 5.0130
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 281.33
               Mean episode length: 245.56
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 8.98s
                        Total time: 31655.96s
                               ETA: 881945.0s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.655s, learning 0.173s)
               Value function loss: 5.8442
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 282.51
               Mean episode length: 244.14
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 8.83s
                        Total time: 31664.79s
                               ETA: 881927.3s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.431s, learning 0.181s)
               Value function loss: 5.4753
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 279.81
               Mean episode length: 243.69
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 8.61s
                        Total time: 31673.40s
                               ETA: 881903.6s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.515s, learning 0.226s)
               Value function loss: 4.2064
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 271.01
               Mean episode length: 240.04
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 8.74s
                        Total time: 31682.14s
                               ETA: 881883.5s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.202s, learning 0.202s)
               Value function loss: 4.2836
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 283.26
               Mean episode length: 246.38
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 8.40s
                        Total time: 31690.54s
                               ETA: 881854.0s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.530s, learning 0.192s)
               Value function loss: 5.0053
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 287.33
               Mean episode length: 247.94
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 8.72s
                        Total time: 31699.26s
                               ETA: 881833.4s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.612s, learning 0.183s)
               Value function loss: 4.2334
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 286.39
               Mean episode length: 248.58
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 8.79s
                        Total time: 31708.06s
                               ETA: 881814.7s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.381s, learning 0.179s)
               Value function loss: 4.1850
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 281.73
               Mean episode length: 246.19
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 8.56s
                        Total time: 31716.62s
                               ETA: 881789.6s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.470s, learning 0.218s)
               Value function loss: 3.8957
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 285.02
               Mean episode length: 246.38
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 8.69s
                        Total time: 31725.31s
                               ETA: 881768.1s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.421s, learning 0.173s)
               Value function loss: 5.3184
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 284.42
               Mean episode length: 246.58
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 8.59s
                        Total time: 31733.90s
                               ETA: 881743.9s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.486s, learning 0.189s)
               Value function loss: 4.4338
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 282.98
               Mean episode length: 245.33
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 8.68s
                        Total time: 31742.58s
                               ETA: 881722.0s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.942s, learning 0.180s)
               Value function loss: 6.6936
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 288.10
               Mean episode length: 248.39
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 9.12s
                        Total time: 31751.70s
                               ETA: 881712.5s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.508s, learning 0.180s)
               Value function loss: 5.0285
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 282.65
               Mean episode length: 244.76
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 8.69s
                        Total time: 31760.39s
                               ETA: 881691.0s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1778 steps/s (collection: 8.955s, learning 0.257s)
               Value function loss: 5.2280
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 276.57
               Mean episode length: 243.41
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 9.21s
                        Total time: 31769.60s
                               ETA: 881684.0s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.171s)
               Value function loss: 5.9040
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 272.89
               Mean episode length: 241.42
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 8.82s
                        Total time: 31778.42s
                               ETA: 881666.3s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.418s, learning 0.166s)
               Value function loss: 3.7075
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 281.60
               Mean episode length: 244.53
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 8.58s
                        Total time: 31787.01s
                               ETA: 881641.9s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.622s, learning 0.183s)
               Value function loss: 4.5797
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 285.52
               Mean episode length: 247.83
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 8.81s
                        Total time: 31795.81s
                               ETA: 881623.6s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.717s, learning 0.173s)
               Value function loss: 4.7787
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 284.57
               Mean episode length: 246.01
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 8.89s
                        Total time: 31804.70s
                               ETA: 881607.7s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.652s, learning 0.203s)
               Value function loss: 6.0938
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 283.02
               Mean episode length: 244.78
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 8.85s
                        Total time: 31813.56s
                               ETA: 881590.8s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.729s, learning 0.166s)
               Value function loss: 5.4758
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 284.61
               Mean episode length: 247.40
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 8.90s
                        Total time: 31822.45s
                               ETA: 881575.1s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.419s, learning 0.190s)
               Value function loss: 4.8296
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 284.99
               Mean episode length: 246.16
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 8.61s
                        Total time: 31831.06s
                               ETA: 881551.4s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.666s, learning 0.178s)
               Value function loss: 4.8757
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 280.30
               Mean episode length: 245.77
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 8.84s
                        Total time: 31839.90s
                               ETA: 881534.3s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.985s, learning 0.180s)
               Value function loss: 5.5011
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 282.97
               Mean episode length: 249.22
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 9.16s
                        Total time: 31849.07s
                               ETA: 881526.0s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.511s, learning 0.203s)
               Value function loss: 5.0136
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 282.27
               Mean episode length: 248.25
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 8.71s
                        Total time: 31857.78s
                               ETA: 881505.2s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.082s, learning 0.198s)
               Value function loss: 3.9475
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 284.70
               Mean episode length: 248.25
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 9.28s
                        Total time: 31867.06s
                               ETA: 881500.2s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.586s, learning 0.181s)
               Value function loss: 5.4420
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 288.27
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 8.77s
                        Total time: 31875.83s
                               ETA: 881480.9s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.549s, learning 0.179s)
               Value function loss: 5.6127
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 279.33
               Mean episode length: 244.33
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 8.73s
                        Total time: 31884.56s
                               ETA: 881460.5s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.432s, learning 0.196s)
               Value function loss: 7.0068
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 285.85
               Mean episode length: 247.12
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 8.63s
                        Total time: 31893.19s
                               ETA: 881437.4s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.886s, learning 0.178s)
               Value function loss: 5.6614
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 287.30
               Mean episode length: 247.22
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 9.06s
                        Total time: 31902.25s
                               ETA: 881426.4s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.648s, learning 0.212s)
               Value function loss: 4.8822
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 289.51
               Mean episode length: 249.63
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 8.86s
                        Total time: 31911.11s
                               ETA: 881409.7s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.651s, learning 0.237s)
               Value function loss: 5.5800
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 286.82
               Mean episode length: 247.63
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 8.89s
                        Total time: 31920.00s
                               ETA: 881393.8s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.546s, learning 0.179s)
               Value function loss: 5.3324
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 289.55
               Mean episode length: 248.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 8.73s
                        Total time: 31928.72s
                               ETA: 881373.4s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.544s, learning 0.175s)
               Value function loss: 5.0920
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 284.21
               Mean episode length: 246.66
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 8.72s
                        Total time: 31937.44s
                               ETA: 881352.8s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.424s, learning 0.180s)
               Value function loss: 5.6478
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 285.65
               Mean episode length: 248.26
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 8.60s
                        Total time: 31946.05s
                               ETA: 881329.1s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.451s, learning 0.203s)
               Value function loss: 5.3540
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 288.88
               Mean episode length: 248.26
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 8.65s
                        Total time: 31954.70s
                               ETA: 881306.8s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1768 steps/s (collection: 8.993s, learning 0.270s)
               Value function loss: 4.9618
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 292.69
               Mean episode length: 249.53
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 9.26s
                        Total time: 31963.96s
                               ETA: 881301.2s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.354s, learning 0.169s)
               Value function loss: 4.9197
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 288.90
               Mean episode length: 247.75
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 8.52s
                        Total time: 31972.49s
                               ETA: 881275.3s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.520s, learning 0.184s)
               Value function loss: 5.5504
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 287.83
               Mean episode length: 247.90
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 8.70s
                        Total time: 31981.19s
                               ETA: 881254.4s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.525s, learning 0.257s)
               Value function loss: 4.1868
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 279.76
               Mean episode length: 243.25
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 8.78s
                        Total time: 31989.97s
                               ETA: 881235.6s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.605s, learning 0.230s)
               Value function loss: 3.9150
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 282.74
               Mean episode length: 243.25
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 8.84s
                        Total time: 31998.81s
                               ETA: 881218.3s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.628s, learning 0.207s)
               Value function loss: 4.5070
                    Surrogate loss: -0.0182
             Mean action noise std: 0.76
                       Mean reward: 286.38
               Mean episode length: 247.14
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 8.84s
                        Total time: 32007.64s
                               ETA: 881201.0s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.826s, learning 0.205s)
               Value function loss: 4.7313
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 285.42
               Mean episode length: 247.30
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 9.03s
                        Total time: 32016.67s
                               ETA: 881189.1s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.337s, learning 0.238s)
               Value function loss: 5.9529
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 287.44
               Mean episode length: 247.77
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 8.58s
                        Total time: 32025.25s
                               ETA: 881164.6s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.736s, learning 0.195s)
               Value function loss: 4.7731
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 289.65
               Mean episode length: 248.35
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 8.93s
                        Total time: 32034.18s
                               ETA: 881150.0s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.653s, learning 0.219s)
               Value function loss: 5.6005
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 286.01
               Mean episode length: 246.51
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 8.87s
                        Total time: 32043.05s
                               ETA: 881133.7s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.325s, learning 0.213s)
               Value function loss: 4.9604
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 284.17
               Mean episode length: 242.83
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 8.54s
                        Total time: 32051.59s
                               ETA: 881108.2s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.747s, learning 0.208s)
               Value function loss: 4.2206
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 281.59
               Mean episode length: 240.74
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 8.96s
                        Total time: 32060.54s
                               ETA: 881094.2s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.700s, learning 0.199s)
               Value function loss: 4.0679
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 285.08
               Mean episode length: 244.79
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 8.90s
                        Total time: 32069.44s
                               ETA: 881078.7s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.646s, learning 0.286s)
               Value function loss: 4.2462
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 289.55
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 8.93s
                        Total time: 32078.37s
                               ETA: 881064.1s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.566s, learning 0.196s)
               Value function loss: 4.9248
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 283.59
               Mean episode length: 245.38
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 8.76s
                        Total time: 32087.14s
                               ETA: 881044.9s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.561s, learning 0.193s)
               Value function loss: 5.7973
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 291.20
               Mean episode length: 248.97
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 8.75s
                        Total time: 32095.89s
                               ETA: 881025.4s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.635s, learning 0.207s)
               Value function loss: 4.9134
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 288.01
               Mean episode length: 248.97
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 8.84s
                        Total time: 32104.73s
                               ETA: 881008.3s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.171s)
               Value function loss: 4.6025
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 287.46
               Mean episode length: 247.81
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 8.62s
                        Total time: 32113.36s
                               ETA: 880985.2s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.637s, learning 0.195s)
               Value function loss: 5.5299
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 288.64
               Mean episode length: 249.25
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 8.83s
                        Total time: 32122.19s
                               ETA: 880967.9s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.434s, learning 0.196s)
               Value function loss: 5.3281
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 291.19
               Mean episode length: 247.94
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 8.63s
                        Total time: 32130.82s
                               ETA: 880945.0s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.329s, learning 0.173s)
               Value function loss: 4.5532
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 287.71
               Mean episode length: 245.54
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 8.50s
                        Total time: 32139.32s
                               ETA: 880918.6s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.421s, learning 0.201s)
               Value function loss: 4.3701
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 283.12
               Mean episode length: 242.65
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 8.62s
                        Total time: 32147.94s
                               ETA: 880895.6s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.528s, learning 0.218s)
               Value function loss: 4.6624
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 284.12
               Mean episode length: 242.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 8.75s
                        Total time: 32156.69s
                               ETA: 880875.9s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.288s, learning 0.181s)
               Value function loss: 5.7859
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 285.18
               Mean episode length: 247.55
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 8.47s
                        Total time: 32165.16s
                               ETA: 880848.7s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.625s, learning 0.199s)
               Value function loss: 6.2108
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 286.45
               Mean episode length: 248.85
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 8.82s
                        Total time: 32173.98s
                               ETA: 880831.2s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.671s, learning 0.176s)
               Value function loss: 5.6674
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 286.39
               Mean episode length: 247.13
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 8.85s
                        Total time: 32182.83s
                               ETA: 880814.3s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.668s, learning 0.187s)
               Value function loss: 5.4231
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 287.92
               Mean episode length: 247.94
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 8.85s
                        Total time: 32191.68s
                               ETA: 880797.6s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.163s, learning 0.201s)
               Value function loss: 5.9968
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 287.86
               Mean episode length: 247.53
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 8.36s
                        Total time: 32200.04s
                               ETA: 880767.5s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.417s, learning 0.203s)
               Value function loss: 5.4875
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 289.63
               Mean episode length: 249.54
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 8.62s
                        Total time: 32208.67s
                               ETA: 880744.5s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.831s, learning 0.252s)
               Value function loss: 4.8609
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 287.67
               Mean episode length: 247.96
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 9.08s
                        Total time: 32217.75s
                               ETA: 880734.1s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.301s, learning 0.185s)
               Value function loss: 5.8747
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 288.55
               Mean episode length: 248.33
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 8.49s
                        Total time: 32226.23s
                               ETA: 880707.4s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.367s, learning 0.192s)
               Value function loss: 4.5158
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 278.91
               Mean episode length: 243.41
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 8.56s
                        Total time: 32234.79s
                               ETA: 880682.7s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.357s, learning 0.200s)
               Value function loss: 4.5841
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 276.55
               Mean episode length: 243.59
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 8.56s
                        Total time: 32243.35s
                               ETA: 880657.9s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.728s, learning 0.187s)
               Value function loss: 5.7116
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 285.13
               Mean episode length: 247.88
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 8.92s
                        Total time: 32252.27s
                               ETA: 880643.0s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.718s, learning 0.231s)
               Value function loss: 4.1033
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 283.55
               Mean episode length: 246.73
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 8.95s
                        Total time: 32261.22s
                               ETA: 880628.9s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.857s, learning 0.251s)
               Value function loss: 4.8871
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 285.60
               Mean episode length: 246.11
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 9.11s
                        Total time: 32270.32s
                               ETA: 880619.2s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.455s, learning 0.226s)
               Value function loss: 4.8513
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 287.23
               Mean episode length: 246.40
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 8.68s
                        Total time: 32279.00s
                               ETA: 880597.9s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.745s, learning 0.187s)
               Value function loss: 3.9397
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 287.28
               Mean episode length: 248.47
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 8.93s
                        Total time: 32287.94s
                               ETA: 880583.4s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.445s, learning 0.174s)
               Value function loss: 5.5199
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 286.49
               Mean episode length: 247.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 8.62s
                        Total time: 32296.56s
                               ETA: 880560.4s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.645s, learning 0.174s)
               Value function loss: 5.6018
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 287.64
               Mean episode length: 246.41
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 8.82s
                        Total time: 32305.38s
                               ETA: 880542.8s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.552s, learning 0.179s)
               Value function loss: 5.2019
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 281.20
               Mean episode length: 240.78
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 8.73s
                        Total time: 32314.11s
                               ETA: 880522.9s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.388s, learning 0.215s)
               Value function loss: 5.5087
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 279.80
               Mean episode length: 242.76
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 8.60s
                        Total time: 32322.71s
                               ETA: 880499.4s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.435s, learning 0.193s)
               Value function loss: 4.7329
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 279.55
               Mean episode length: 241.64
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 8.63s
                        Total time: 32331.34s
                               ETA: 880476.7s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.610s, learning 0.180s)
               Value function loss: 3.8286
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 282.03
               Mean episode length: 243.48
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 8.79s
                        Total time: 32340.13s
                               ETA: 880458.4s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.636s, learning 0.179s)
               Value function loss: 4.0288
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 283.36
               Mean episode length: 245.14
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 8.82s
                        Total time: 32348.94s
                               ETA: 880440.7s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.355s, learning 0.205s)
               Value function loss: 4.5707
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 284.54
               Mean episode length: 246.14
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 8.56s
                        Total time: 32357.50s
                               ETA: 880416.1s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.616s, learning 0.256s)
               Value function loss: 5.2392
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 286.64
               Mean episode length: 248.05
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 8.87s
                        Total time: 32366.37s
                               ETA: 880400.1s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.271s, learning 0.198s)
               Value function loss: 4.5678
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 285.42
               Mean episode length: 249.07
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 8.47s
                        Total time: 32374.84s
                               ETA: 880373.0s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.566s, learning 0.175s)
               Value function loss: 4.9707
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 280.38
               Mean episode length: 243.28
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 8.74s
                        Total time: 32383.58s
                               ETA: 880353.4s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.645s, learning 0.282s)
               Value function loss: 4.8094
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 277.03
               Mean episode length: 242.09
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 8.93s
                        Total time: 32392.51s
                               ETA: 880338.8s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1783 steps/s (collection: 8.951s, learning 0.237s)
               Value function loss: 5.5340
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 273.35
               Mean episode length: 242.58
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 9.19s
                        Total time: 32401.70s
                               ETA: 880331.4s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.484s, learning 0.187s)
               Value function loss: 4.5531
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 277.53
               Mean episode length: 242.35
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 8.67s
                        Total time: 32410.37s
                               ETA: 880309.8s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.792s, learning 0.178s)
               Value function loss: 4.8419
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 276.50
               Mean episode length: 237.98
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 8.97s
                        Total time: 32419.34s
                               ETA: 880296.5s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.641s, learning 0.204s)
               Value function loss: 4.7121
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 278.89
               Mean episode length: 240.46
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 8.84s
                        Total time: 32428.19s
                               ETA: 880279.7s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.731s, learning 0.177s)
               Value function loss: 5.2032
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 281.20
               Mean episode length: 245.15
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 8.91s
                        Total time: 32437.09s
                               ETA: 880264.6s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.544s, learning 0.190s)
               Value function loss: 6.4744
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 282.90
               Mean episode length: 243.71
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 8.73s
                        Total time: 32445.83s
                               ETA: 880244.8s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.412s, learning 0.197s)
               Value function loss: 5.4087
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 287.65
               Mean episode length: 247.30
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 8.61s
                        Total time: 32454.44s
                               ETA: 880221.7s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.856s, learning 0.240s)
               Value function loss: 4.5291
                    Surrogate loss: -0.0168
             Mean action noise std: 0.76
                       Mean reward: 282.18
               Mean episode length: 243.19
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 9.10s
                        Total time: 32463.53s
                               ETA: 880211.7s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.380s, learning 0.177s)
               Value function loss: 5.5738
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 286.91
               Mean episode length: 247.43
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 8.56s
                        Total time: 32472.09s
                               ETA: 880187.1s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.346s, learning 0.253s)
               Value function loss: 4.9151
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 292.05
               Mean episode length: 248.48
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 8.60s
                        Total time: 32480.69s
                               ETA: 880163.7s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.701s, learning 0.252s)
               Value function loss: 5.2739
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 283.35
               Mean episode length: 244.95
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 8.95s
                        Total time: 32489.64s
                               ETA: 880149.9s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.776s, learning 0.320s)
               Value function loss: 4.9125
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 286.82
               Mean episode length: 248.86
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 9.10s
                        Total time: 32498.74s
                               ETA: 880140.0s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.784s, learning 0.183s)
               Value function loss: 4.0183
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 284.92
               Mean episode length: 246.73
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 8.97s
                        Total time: 32507.71s
                               ETA: 880126.5s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.844s, learning 0.255s)
               Value function loss: 4.9121
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 291.14
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 9.10s
                        Total time: 32516.81s
                               ETA: 880116.7s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.301s, learning 0.229s)
               Value function loss: 5.9634
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 288.42
               Mean episode length: 247.66
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 8.53s
                        Total time: 32525.34s
                               ETA: 880091.4s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.648s, learning 0.184s)
               Value function loss: 4.3775
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 290.41
               Mean episode length: 248.08
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 8.83s
                        Total time: 32534.17s
                               ETA: 880074.3s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.610s, learning 0.200s)
               Value function loss: 4.0885
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 288.94
               Mean episode length: 247.69
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 8.81s
                        Total time: 32542.98s
                               ETA: 880056.6s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.448s, learning 0.208s)
               Value function loss: 3.9970
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 286.76
               Mean episode length: 248.09
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 8.66s
                        Total time: 32551.63s
                               ETA: 880034.8s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.225s, learning 0.177s)
               Value function loss: 4.6921
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 285.61
               Mean episode length: 248.55
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 8.40s
                        Total time: 32560.03s
                               ETA: 880006.1s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.236s, learning 0.273s)
               Value function loss: 5.0081
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 282.92
               Mean episode length: 246.40
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 8.51s
                        Total time: 32568.54s
                               ETA: 879980.3s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.903s, learning 0.196s)
               Value function loss: 6.6719
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 279.20
               Mean episode length: 243.79
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 9.10s
                        Total time: 32577.64s
                               ETA: 879970.5s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.783s, learning 0.181s)
               Value function loss: 4.4815
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 284.97
               Mean episode length: 247.09
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 8.96s
                        Total time: 32586.61s
                               ETA: 879957.0s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.180s, learning 0.182s)
               Value function loss: 6.2086
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 288.02
               Mean episode length: 246.13
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 8.36s
                        Total time: 32594.97s
                               ETA: 879927.3s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.652s, learning 0.253s)
               Value function loss: 5.0391
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 283.46
               Mean episode length: 245.69
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 8.90s
                        Total time: 32603.87s
                               ETA: 879912.2s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.592s, learning 0.206s)
               Value function loss: 4.2445
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 277.63
               Mean episode length: 242.20
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 8.80s
                        Total time: 32612.67s
                               ETA: 879894.2s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.662s, learning 0.210s)
               Value function loss: 4.2218
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 283.99
               Mean episode length: 244.59
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 8.87s
                        Total time: 32621.54s
                               ETA: 879878.3s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.573s, learning 0.213s)
               Value function loss: 5.0264
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 282.04
               Mean episode length: 244.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 8.79s
                        Total time: 32630.33s
                               ETA: 879860.0s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.871s, learning 0.182s)
               Value function loss: 5.7283
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 282.22
               Mean episode length: 243.78
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 9.05s
                        Total time: 32639.38s
                               ETA: 879849.0s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.391s, learning 0.183s)
               Value function loss: 4.7784
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 290.03
               Mean episode length: 247.89
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 8.57s
                        Total time: 32647.96s
                               ETA: 879825.0s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.642s, learning 0.195s)
               Value function loss: 4.8423
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 288.15
               Mean episode length: 248.15
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 8.84s
                        Total time: 32656.79s
                               ETA: 879808.1s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.807s, learning 0.193s)
               Value function loss: 4.7428
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 286.93
               Mean episode length: 247.79
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 9.00s
                        Total time: 32665.79s
                               ETA: 879795.6s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.503s, learning 0.181s)
               Value function loss: 6.1429
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 287.93
               Mean episode length: 245.59
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 8.68s
                        Total time: 32674.48s
                               ETA: 879774.7s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.168s)
               Value function loss: 4.9245
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 293.10
               Mean episode length: 249.63
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 8.39s
                        Total time: 32682.87s
                               ETA: 879745.9s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 1782 steps/s (collection: 8.936s, learning 0.255s)
               Value function loss: 3.9132
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 289.83
               Mean episode length: 247.65
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 9.19s
                        Total time: 32692.06s
                               ETA: 879738.5s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.447s, learning 0.231s)
               Value function loss: 5.1690
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 292.96
               Mean episode length: 248.05
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 8.68s
                        Total time: 32700.74s
                               ETA: 879717.4s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.462s, learning 0.181s)
               Value function loss: 4.6883
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 288.32
               Mean episode length: 248.05
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 8.64s
                        Total time: 32709.38s
                               ETA: 879695.3s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.443s, learning 0.192s)
               Value function loss: 6.2801
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 290.34
               Mean episode length: 247.64
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 8.63s
                        Total time: 32718.02s
                               ETA: 879673.0s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.329s, learning 0.222s)
               Value function loss: 5.6850
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 294.15
               Mean episode length: 249.38
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 8.55s
                        Total time: 32726.57s
                               ETA: 879648.5s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.478s, learning 0.175s)
               Value function loss: 4.6093
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 279.09
               Mean episode length: 239.17
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 8.65s
                        Total time: 32735.22s
                               ETA: 879626.7s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.614s, learning 0.176s)
               Value function loss: 5.0488
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 283.30
               Mean episode length: 243.31
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 8.79s
                        Total time: 32744.01s
                               ETA: 879608.6s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.549s, learning 0.176s)
               Value function loss: 4.7698
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 290.62
               Mean episode length: 247.94
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 8.72s
                        Total time: 32752.73s
                               ETA: 879588.8s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.436s, learning 0.174s)
               Value function loss: 5.2822
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 294.59
               Mean episode length: 249.13
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 8.61s
                        Total time: 32761.34s
                               ETA: 879565.9s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.588s, learning 0.181s)
               Value function loss: 4.5123
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 293.31
               Mean episode length: 248.49
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 8.77s
                        Total time: 32770.11s
                               ETA: 879547.2s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.411s, learning 0.166s)
               Value function loss: 4.3146
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 288.27
               Mean episode length: 246.50
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 8.58s
                        Total time: 32778.69s
                               ETA: 879523.5s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.855s, learning 0.169s)
               Value function loss: 4.8556
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 288.78
               Mean episode length: 247.40
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 9.02s
                        Total time: 32787.71s
                               ETA: 879511.7s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.430s, learning 0.229s)
               Value function loss: 5.4650
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 282.60
               Mean episode length: 241.19
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 8.66s
                        Total time: 32796.37s
                               ETA: 879490.1s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.289s, learning 0.170s)
               Value function loss: 3.8403
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 288.81
               Mean episode length: 245.02
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 8.46s
                        Total time: 32804.83s
                               ETA: 879463.2s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.346s, learning 0.180s)
               Value function loss: 3.5500
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 287.63
               Mean episode length: 244.73
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 8.53s
                        Total time: 32813.36s
                               ETA: 879438.1s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.381s, learning 0.176s)
               Value function loss: 3.7622
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 284.72
               Mean episode length: 243.96
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 8.56s
                        Total time: 32821.91s
                               ETA: 879413.8s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.603s, learning 0.175s)
               Value function loss: 4.8695
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 286.63
               Mean episode length: 245.92
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 8.78s
                        Total time: 32830.69s
                               ETA: 879395.5s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.574s, learning 0.181s)
               Value function loss: 3.8151
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 287.95
               Mean episode length: 246.61
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 8.75s
                        Total time: 32839.45s
                               ETA: 879376.5s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.471s, learning 0.178s)
               Value function loss: 5.7938
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 290.99
               Mean episode length: 247.95
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 8.65s
                        Total time: 32848.10s
                               ETA: 879354.7s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.336s, learning 0.191s)
               Value function loss: 4.6434
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 293.78
               Mean episode length: 248.57
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 8.53s
                        Total time: 32856.62s
                               ETA: 879329.7s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.725s, learning 0.249s)
               Value function loss: 5.2827
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 295.43
               Mean episode length: 249.40
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 8.97s
                        Total time: 32865.60s
                               ETA: 879316.6s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.411s, learning 0.179s)
               Value function loss: 4.8024
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 293.45
               Mean episode length: 248.27
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 8.59s
                        Total time: 32874.19s
                               ETA: 879293.3s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.132s, learning 0.187s)
               Value function loss: 3.7388
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 287.40
               Mean episode length: 244.59
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 8.32s
                        Total time: 32882.51s
                               ETA: 879262.7s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.457s, learning 0.181s)
               Value function loss: 3.7978
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 286.76
               Mean episode length: 244.22
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 8.64s
                        Total time: 32891.14s
                               ETA: 879240.7s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.241s, learning 0.177s)
               Value function loss: 4.2168
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 290.12
               Mean episode length: 247.06
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 8.42s
                        Total time: 32899.56s
                               ETA: 879212.7s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.602s, learning 0.186s)
               Value function loss: 5.0509
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 291.84
               Mean episode length: 248.04
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 8.79s
                        Total time: 32908.35s
                               ETA: 879194.7s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.565s, learning 0.170s)
               Value function loss: 5.2180
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 292.44
               Mean episode length: 247.92
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 8.74s
                        Total time: 32917.08s
                               ETA: 879175.3s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.325s, learning 0.180s)
               Value function loss: 3.9378
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 286.95
               Mean episode length: 243.64
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 8.51s
                        Total time: 32925.59s
                               ETA: 879149.7s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.370s, learning 0.169s)
               Value function loss: 4.5331
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 289.56
               Mean episode length: 245.97
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 8.54s
                        Total time: 32934.13s
                               ETA: 879125.1s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.385s, learning 0.181s)
               Value function loss: 5.5430
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 295.75
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 8.57s
                        Total time: 32942.70s
                               ETA: 879101.2s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.468s, learning 0.175s)
               Value function loss: 4.6397
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 294.17
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 8.64s
                        Total time: 32951.34s
                               ETA: 879079.3s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.456s, learning 0.230s)
               Value function loss: 3.6035
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 295.04
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 8.69s
                        Total time: 32960.02s
                               ETA: 879058.6s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.429s, learning 0.220s)
               Value function loss: 4.7765
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 296.68
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 8.65s
                        Total time: 32968.67s
                               ETA: 879036.9s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.578s, learning 0.189s)
               Value function loss: 4.8346
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 292.33
               Mean episode length: 245.92
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 8.77s
                        Total time: 32977.44s
                               ETA: 879018.4s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.682s, learning 0.273s)
               Value function loss: 5.7315
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 292.25
               Mean episode length: 249.21
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 8.95s
                        Total time: 32986.39s
                               ETA: 879004.9s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.482s, learning 0.229s)
               Value function loss: 4.8155
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 293.45
               Mean episode length: 249.21
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 8.71s
                        Total time: 32995.11s
                               ETA: 878984.9s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.433s, learning 0.255s)
               Value function loss: 4.0220
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 294.80
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 8.69s
                        Total time: 33003.79s
                               ETA: 878964.2s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.700s, learning 0.175s)
               Value function loss: 5.3385
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 297.39
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 8.87s
                        Total time: 33012.67s
                               ETA: 878948.6s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.438s, learning 0.243s)
               Value function loss: 4.3521
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 297.22
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 8.68s
                        Total time: 33021.35s
                               ETA: 878927.8s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.562s, learning 0.187s)
               Value function loss: 4.7995
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 299.09
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 8.75s
                        Total time: 33030.10s
                               ETA: 878908.8s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.590s, learning 0.175s)
               Value function loss: 4.6948
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 295.00
               Mean episode length: 246.33
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 8.76s
                        Total time: 33038.86s
                               ETA: 878890.3s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.681s, learning 0.181s)
               Value function loss: 4.3747
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 296.22
               Mean episode length: 248.47
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 8.86s
                        Total time: 33047.72s
                               ETA: 878874.3s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.490s, learning 0.195s)
               Value function loss: 4.3867
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 296.55
               Mean episode length: 249.72
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 8.69s
                        Total time: 33056.41s
                               ETA: 878853.7s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.369s, learning 0.216s)
               Value function loss: 4.5728
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 295.85
               Mean episode length: 247.20
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 8.58s
                        Total time: 33064.99s
                               ETA: 878830.4s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.760s, learning 0.182s)
               Value function loss: 4.9241
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 295.34
               Mean episode length: 247.32
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 8.94s
                        Total time: 33073.94s
                               ETA: 878816.5s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.791s, learning 0.192s)
               Value function loss: 3.7217
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 293.32
               Mean episode length: 245.38
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 8.98s
                        Total time: 33082.92s
                               ETA: 878803.8s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.739s, learning 0.198s)
               Value function loss: 3.7019
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 294.78
               Mean episode length: 245.97
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 8.94s
                        Total time: 33091.86s
                               ETA: 878789.9s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.078s, learning 0.225s)
               Value function loss: 4.6569
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 297.27
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 8.30s
                        Total time: 33100.16s
                               ETA: 878759.1s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.455s, learning 0.231s)
               Value function loss: 4.3804
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 294.67
               Mean episode length: 247.42
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 8.69s
                        Total time: 33108.85s
                               ETA: 878738.5s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.590s, learning 0.175s)
               Value function loss: 5.7964
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 299.22
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 8.76s
                        Total time: 33117.61s
                               ETA: 878720.0s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.384s, learning 0.194s)
               Value function loss: 4.9859
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 299.18
               Mean episode length: 249.40
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 8.58s
                        Total time: 33126.19s
                               ETA: 878696.5s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.306s, learning 0.215s)
               Value function loss: 5.2354
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 298.61
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 8.52s
                        Total time: 33134.71s
                               ETA: 878671.6s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.210s, learning 0.187s)
               Value function loss: 4.6950
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 294.07
               Mean episode length: 248.01
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 8.40s
                        Total time: 33143.11s
                               ETA: 878643.3s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.334s, learning 0.188s)
               Value function loss: 4.8612
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 297.60
               Mean episode length: 248.06
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 8.52s
                        Total time: 33151.63s
                               ETA: 878618.4s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.323s, learning 0.180s)
               Value function loss: 4.3965
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 293.95
               Mean episode length: 246.06
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 8.50s
                        Total time: 33160.13s
                               ETA: 878593.0s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.658s, learning 0.202s)
               Value function loss: 4.5218
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 296.94
               Mean episode length: 248.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 8.86s
                        Total time: 33168.99s
                               ETA: 878577.1s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.178s, learning 0.177s)
               Value function loss: 4.5951
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 296.71
               Mean episode length: 247.37
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 16.35s
                        Total time: 33185.35s
                               ETA: 878759.6s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.358s, learning 0.198s)
               Value function loss: 5.0207
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 298.14
               Mean episode length: 247.45
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 16.56s
                        Total time: 33201.90s
                               ETA: 878947.4s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.863s, learning 0.171s)
               Value function loss: 4.1900
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 295.01
               Mean episode length: 244.94
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 17.03s
                        Total time: 33218.94s
                               ETA: 879147.6s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.462s, learning 0.199s)
               Value function loss: 4.6630
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 288.16
               Mean episode length: 243.09
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 16.66s
                        Total time: 33235.60s
                               ETA: 879337.9s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.383s, learning 0.207s)
               Value function loss: 5.0355
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 293.57
               Mean episode length: 245.75
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 16.59s
                        Total time: 33252.19s
                               ETA: 879526.2s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.912s, learning 0.201s)
               Value function loss: 4.6867
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 292.35
               Mean episode length: 243.22
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 17.11s
                        Total time: 33269.30s
                               ETA: 879728.3s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.423s, learning 0.176s)
               Value function loss: 4.3369
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 295.58
               Mean episode length: 245.39
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 16.60s
                        Total time: 33285.90s
                               ETA: 879916.6s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.351s, learning 0.247s)
               Value function loss: 3.8689
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 296.78
               Mean episode length: 245.91
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 16.60s
                        Total time: 33302.49s
                               ETA: 880104.7s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.909s, learning 0.184s)
               Value function loss: 4.5518
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 295.74
               Mean episode length: 245.58
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 17.09s
                        Total time: 33319.59s
                               ETA: 880305.9s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.467s, learning 0.179s)
               Value function loss: 5.8178
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 297.67
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 16.65s
                        Total time: 33336.23s
                               ETA: 880495.1s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.552s, learning 0.241s)
               Value function loss: 5.6067
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 299.37
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 16.79s
                        Total time: 33353.03s
                               ETA: 880688.1s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.027s, learning 0.264s)
               Value function loss: 4.7862
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 300.28
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 17.29s
                        Total time: 33370.32s
                               ETA: 880894.1s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.494s, learning 0.261s)
               Value function loss: 4.3468
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 294.91
               Mean episode length: 247.65
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 16.76s
                        Total time: 33387.07s
                               ETA: 881085.8s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.025s, learning 0.178s)
               Value function loss: 4.9109
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 295.17
               Mean episode length: 246.36
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 16.20s
                        Total time: 33403.27s
                               ETA: 881262.9s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.433s, learning 0.227s)
               Value function loss: 4.9757
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 291.25
               Mean episode length: 243.37
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 16.66s
                        Total time: 33419.94s
                               ETA: 881451.9s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.022s, learning 0.193s)
               Value function loss: 4.7176
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 288.21
               Mean episode length: 240.23
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 17.21s
                        Total time: 33437.15s
                               ETA: 881655.5s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.710s, learning 0.197s)
               Value function loss: 5.8892
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 290.20
               Mean episode length: 241.01
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 16.91s
                        Total time: 33454.06s
                               ETA: 881850.8s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.914s, learning 0.176s)
               Value function loss: 5.3051
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 283.37
               Mean episode length: 235.52
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 17.09s
                        Total time: 33471.15s
                               ETA: 882050.8s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.364s, learning 0.177s)
               Value function loss: 4.9082
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 288.39
               Mean episode length: 240.12
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 16.54s
                        Total time: 33487.69s
                               ETA: 882236.2s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.492s, learning 0.356s)
               Value function loss: 5.5147
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 294.22
               Mean episode length: 245.01
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 16.85s
                        Total time: 33504.54s
                               ETA: 882429.6s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.024s, learning 0.299s)
               Value function loss: 3.7144
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 295.00
               Mean episode length: 246.07
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 17.32s
                        Total time: 33521.86s
                               ETA: 882635.4s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.385s, learning 0.168s)
               Value function loss: 4.2665
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 294.39
               Mean episode length: 245.96
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 16.55s
                        Total time: 33538.41s
                               ETA: 882820.8s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.484s, learning 0.176s)
               Value function loss: 4.6519
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 287.83
               Mean episode length: 239.66
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 16.66s
                        Total time: 33555.07s
                               ETA: 883008.9s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.541s, learning 0.177s)
               Value function loss: 4.7850
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 286.73
               Mean episode length: 238.53
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 16.72s
                        Total time: 33571.79s
                               ETA: 883198.5s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.933s, learning 0.273s)
               Value function loss: 6.0642
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 282.75
               Mean episode length: 237.15
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 17.21s
                        Total time: 33589.00s
                               ETA: 883400.7s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.460s, learning 0.212s)
               Value function loss: 6.3885
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 271.23
               Mean episode length: 229.59
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 16.67s
                        Total time: 33605.67s
                               ETA: 883588.8s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.294s, learning 0.179s)
               Value function loss: 5.1799
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 287.62
               Mean episode length: 240.99
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 16.47s
                        Total time: 33622.14s
                               ETA: 883771.6s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.437s, learning 0.178s)
               Value function loss: 5.1933
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 291.08
               Mean episode length: 244.16
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 16.62s
                        Total time: 33638.76s
                               ETA: 883957.9s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.395s, learning 0.300s)
               Value function loss: 4.8762
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 289.69
               Mean episode length: 242.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 16.69s
                        Total time: 33655.45s
                               ETA: 884146.3s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.541s, learning 0.293s)
               Value function loss: 4.4345
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 282.42
               Mean episode length: 236.41
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 16.83s
                        Total time: 33672.29s
                               ETA: 884338.2s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.698s, learning 0.225s)
               Value function loss: 5.0866
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 284.36
               Mean episode length: 238.38
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 16.92s
                        Total time: 33689.21s
                               ETA: 884532.3s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.842s, learning 0.186s)
               Value function loss: 5.4046
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 286.56
               Mean episode length: 240.36
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 17.03s
                        Total time: 33706.24s
                               ETA: 884729.1s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.776s, learning 0.172s)
               Value function loss: 6.5894
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 286.09
               Mean episode length: 238.13
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 16.95s
                        Total time: 33723.19s
                               ETA: 884923.6s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.270s, learning 0.182s)
               Value function loss: 5.6456
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 293.05
               Mean episode length: 245.93
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 16.45s
                        Total time: 33739.64s
                               ETA: 885105.0s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.535s, learning 0.306s)
               Value function loss: 6.3036
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 283.66
               Mean episode length: 237.94
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 16.84s
                        Total time: 33756.48s
                               ETA: 885296.5s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.918s, learning 0.172s)
               Value function loss: 5.5221
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 270.90
               Mean episode length: 228.42
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 17.09s
                        Total time: 33773.57s
                               ETA: 885494.5s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.806s, learning 0.207s)
               Value function loss: 6.7743
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 284.50
               Mean episode length: 239.89
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 17.01s
                        Total time: 33790.58s
                               ETA: 885690.3s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 1118 steps/s (collection: 14.475s, learning 0.177s)
               Value function loss: 4.5867
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 287.70
               Mean episode length: 242.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 14.65s
                        Total time: 33805.23s
                               ETA: 885824.1s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.333s, learning 0.183s)
               Value function loss: 5.3373
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 275.70
               Mean episode length: 232.20
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 8.52s
                        Total time: 33813.75s
                               ETA: 885797.1s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.479s, learning 0.233s)
               Value function loss: 5.4880
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 274.79
               Mean episode length: 231.95
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 8.71s
                        Total time: 33822.46s
                               ETA: 885775.2s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.760s, learning 0.241s)
               Value function loss: 5.2644
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 288.88
               Mean episode length: 244.24
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 9.00s
                        Total time: 33831.46s
                               ETA: 885760.9s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.570s, learning 0.200s)
               Value function loss: 6.7534
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 281.63
               Mean episode length: 238.23
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 8.77s
                        Total time: 33840.23s
                               ETA: 885740.5s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.721s, learning 0.179s)
               Value function loss: 6.0428
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 289.83
               Mean episode length: 243.15
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 8.90s
                        Total time: 33849.13s
                               ETA: 885723.6s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.303s, learning 0.168s)
               Value function loss: 5.4343
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 285.15
               Mean episode length: 240.52
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 8.47s
                        Total time: 33857.60s
                               ETA: 885695.4s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.656s, learning 0.191s)
               Value function loss: 6.2615
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 291.72
               Mean episode length: 247.86
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 8.85s
                        Total time: 33866.45s
                               ETA: 885677.1s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.286s, learning 0.200s)
               Value function loss: 5.6441
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 290.95
               Mean episode length: 246.46
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 8.49s
                        Total time: 33874.94s
                               ETA: 885649.4s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.634s, learning 0.184s)
               Value function loss: 6.3881
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 290.73
               Mean episode length: 244.61
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 8.82s
                        Total time: 33883.75s
                               ETA: 885630.3s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.534s, learning 0.206s)
               Value function loss: 6.0489
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 287.05
               Mean episode length: 241.94
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 8.74s
                        Total time: 33892.49s
                               ETA: 885609.2s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.750s, learning 0.181s)
               Value function loss: 5.5670
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 283.49
               Mean episode length: 240.22
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 8.93s
                        Total time: 33901.43s
                               ETA: 885593.2s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.459s, learning 0.233s)
               Value function loss: 6.0823
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 271.20
               Mean episode length: 231.65
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 8.69s
                        Total time: 33910.12s
                               ETA: 885570.8s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.706s, learning 0.174s)
               Value function loss: 6.4356
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 275.15
               Mean episode length: 235.12
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 8.88s
                        Total time: 33919.00s
                               ETA: 885553.4s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.193s, learning 0.183s)
               Value function loss: 4.0400
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 289.75
               Mean episode length: 243.90
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 8.38s
                        Total time: 33927.37s
                               ETA: 885522.9s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.518s, learning 0.215s)
               Value function loss: 4.3141
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 288.32
               Mean episode length: 245.25
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 8.73s
                        Total time: 33936.11s
                               ETA: 885501.6s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.843s, learning 0.276s)
               Value function loss: 4.7878
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 276.15
               Mean episode length: 236.74
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 9.12s
                        Total time: 33945.23s
                               ETA: 885490.5s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.594s, learning 0.272s)
               Value function loss: 6.1132
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 273.26
               Mean episode length: 232.49
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 8.87s
                        Total time: 33954.09s
                               ETA: 885472.7s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.666s, learning 0.168s)
               Value function loss: 5.8076
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 269.34
               Mean episode length: 229.62
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 8.83s
                        Total time: 33962.93s
                               ETA: 885454.1s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.269s, learning 0.171s)
               Value function loss: 7.5658
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 276.63
               Mean episode length: 234.43
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 8.44s
                        Total time: 33971.37s
                               ETA: 885425.3s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.398s, learning 0.185s)
               Value function loss: 5.2478
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 284.74
               Mean episode length: 240.01
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 8.58s
                        Total time: 33979.95s
                               ETA: 885400.2s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.525s, learning 0.212s)
               Value function loss: 5.8329
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 289.47
               Mean episode length: 243.55
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 8.74s
                        Total time: 33988.69s
                               ETA: 885379.1s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.891s, learning 0.184s)
               Value function loss: 6.0379
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 281.57
               Mean episode length: 239.62
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 9.08s
                        Total time: 33997.76s
                               ETA: 885366.8s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.803s, learning 0.186s)
               Value function loss: 4.9871
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 276.36
               Mean episode length: 233.92
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 8.99s
                        Total time: 34006.75s
                               ETA: 885352.3s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.384s, learning 0.173s)
               Value function loss: 4.7476
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 282.32
               Mean episode length: 238.90
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 8.56s
                        Total time: 34015.31s
                               ETA: 885326.6s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.442s, learning 0.190s)
               Value function loss: 5.5602
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 272.93
               Mean episode length: 234.10
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 8.63s
                        Total time: 34023.94s
                               ETA: 885302.8s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.413s, learning 0.181s)
               Value function loss: 7.4926
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 278.73
               Mean episode length: 239.97
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 8.59s
                        Total time: 34032.53s
                               ETA: 885278.0s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.743s, learning 0.257s)
               Value function loss: 5.7045
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 276.53
               Mean episode length: 236.66
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 9.00s
                        Total time: 34041.54s
                               ETA: 885263.8s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.738s, learning 0.218s)
               Value function loss: 4.9896
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 283.15
               Mean episode length: 241.45
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 8.96s
                        Total time: 34050.49s
                               ETA: 885248.4s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.358s, learning 0.230s)
               Value function loss: 6.1609
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 279.93
               Mean episode length: 238.94
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 8.59s
                        Total time: 34059.08s
                               ETA: 885223.5s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.345s, learning 0.209s)
               Value function loss: 6.5991
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 276.09
               Mean episode length: 239.34
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 8.55s
                        Total time: 34067.63s
                               ETA: 885197.7s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.441s, learning 0.202s)
               Value function loss: 5.4479
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 278.89
               Mean episode length: 242.09
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 8.64s
                        Total time: 34076.28s
                               ETA: 885174.3s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.247s, learning 0.210s)
               Value function loss: 4.8494
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 281.07
               Mean episode length: 242.65
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 8.46s
                        Total time: 34084.73s
                               ETA: 885146.0s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.473s, learning 0.166s)
               Value function loss: 6.7012
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 284.19
               Mean episode length: 242.46
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 8.64s
                        Total time: 34093.37s
                               ETA: 885122.4s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.405s, learning 0.203s)
               Value function loss: 5.2990
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 277.54
               Mean episode length: 237.28
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 8.61s
                        Total time: 34101.98s
                               ETA: 885098.0s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.360s, learning 0.177s)
               Value function loss: 7.9999
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 273.46
               Mean episode length: 235.60
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 8.54s
                        Total time: 34110.52s
                               ETA: 885071.9s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.810s, learning 0.202s)
               Value function loss: 6.2389
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 275.73
               Mean episode length: 232.88
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 9.01s
                        Total time: 34119.53s
                               ETA: 885058.0s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.427s, learning 0.295s)
               Value function loss: 4.5713
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 282.62
               Mean episode length: 240.10
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 8.72s
                        Total time: 34128.25s
                               ETA: 885036.6s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.464s, learning 0.209s)
               Value function loss: 5.9441
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 286.29
               Mean episode length: 242.43
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 8.67s
                        Total time: 34136.92s
                               ETA: 885014.0s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.414s, learning 0.189s)
               Value function loss: 5.2848
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 285.38
               Mean episode length: 243.85
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 8.60s
                        Total time: 34145.53s
                               ETA: 884989.6s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.540s, learning 0.216s)
               Value function loss: 6.4747
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 279.74
               Mean episode length: 239.52
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 8.76s
                        Total time: 34154.28s
                               ETA: 884969.1s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.496s, learning 0.184s)
               Value function loss: 5.5500
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 280.48
               Mean episode length: 241.24
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 8.68s
                        Total time: 34162.96s
                               ETA: 884946.7s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.840s, learning 0.211s)
               Value function loss: 6.4915
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: 280.22
               Mean episode length: 241.17
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 9.05s
                        Total time: 34172.01s
                               ETA: 884933.9s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.745s, learning 0.187s)
               Value function loss: 5.1703
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 280.86
               Mean episode length: 240.95
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 8.93s
                        Total time: 34180.95s
                               ETA: 884918.0s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.536s, learning 0.224s)
               Value function loss: 5.6853
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 285.07
               Mean episode length: 242.23
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 8.76s
                        Total time: 34189.71s
                               ETA: 884897.6s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.147s, learning 0.188s)
               Value function loss: 4.9828
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 281.16
               Mean episode length: 242.26
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 8.33s
                        Total time: 34198.04s
                               ETA: 884866.3s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.447s, learning 0.173s)
               Value function loss: 4.6098
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 271.88
               Mean episode length: 234.22
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 8.62s
                        Total time: 34206.66s
                               ETA: 884842.3s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.145s, learning 0.207s)
               Value function loss: 4.1662
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 275.20
               Mean episode length: 234.70
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 8.35s
                        Total time: 34215.01s
                               ETA: 884811.5s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.756s, learning 0.186s)
               Value function loss: 5.9470
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 282.31
               Mean episode length: 242.54
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 8.94s
                        Total time: 34223.96s
                               ETA: 884795.9s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.254s, learning 0.213s)
               Value function loss: 5.3618
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 272.50
               Mean episode length: 235.23
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 8.47s
                        Total time: 34232.42s
                               ETA: 884768.0s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.284s, learning 0.210s)
               Value function loss: 6.6335
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 280.77
               Mean episode length: 242.72
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 8.49s
                        Total time: 34240.92s
                               ETA: 884740.8s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.525s, learning 0.209s)
               Value function loss: 5.1350
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 271.07
               Mean episode length: 234.19
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 8.73s
                        Total time: 34249.65s
                               ETA: 884719.8s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.440s, learning 0.229s)
               Value function loss: 5.8732
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 262.94
               Mean episode length: 228.84
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 8.67s
                        Total time: 34258.32s
                               ETA: 884697.2s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.879s, learning 0.174s)
               Value function loss: 5.1677
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 275.69
               Mean episode length: 239.09
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 9.05s
                        Total time: 34267.37s
                               ETA: 884684.5s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.298s, learning 0.251s)
               Value function loss: 4.8729
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 275.73
               Mean episode length: 238.15
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 8.55s
                        Total time: 34275.92s
                               ETA: 884658.8s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.876s, learning 0.283s)
               Value function loss: 4.0950
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 271.54
               Mean episode length: 234.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 9.16s
                        Total time: 34285.08s
                               ETA: 884648.8s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.341s, learning 0.185s)
               Value function loss: 4.6707
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 280.48
               Mean episode length: 240.77
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 8.53s
                        Total time: 34293.61s
                               ETA: 884622.5s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.290s, learning 0.268s)
               Value function loss: 6.5975
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 261.19
               Mean episode length: 227.69
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 8.56s
                        Total time: 34302.16s
                               ETA: 884597.0s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.484s, learning 0.210s)
               Value function loss: 4.7972
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 267.51
               Mean episode length: 232.78
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 8.69s
                        Total time: 34310.86s
                               ETA: 884575.1s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.548s, learning 0.205s)
               Value function loss: 4.6756
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 274.16
               Mean episode length: 240.64
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 8.75s
                        Total time: 34319.61s
                               ETA: 884554.7s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.941s, learning 0.172s)
               Value function loss: 5.4038
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 269.31
               Mean episode length: 236.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 9.11s
                        Total time: 34328.72s
                               ETA: 884543.5s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.293s, learning 0.167s)
               Value function loss: 5.2339
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 270.85
               Mean episode length: 236.20
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 8.46s
                        Total time: 34337.18s
                               ETA: 884515.6s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.705s, learning 0.176s)
               Value function loss: 4.7590
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 273.62
               Mean episode length: 241.69
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 8.88s
                        Total time: 34346.07s
                               ETA: 884498.5s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.832s, learning 0.190s)
               Value function loss: 3.2197
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 280.84
               Mean episode length: 246.32
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 9.02s
                        Total time: 34355.09s
                               ETA: 884485.0s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.694s, learning 0.180s)
               Value function loss: 4.3177
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 279.29
               Mean episode length: 245.40
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 8.87s
                        Total time: 34363.96s
                               ETA: 884467.7s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.767s, learning 0.172s)
               Value function loss: 4.6091
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 270.39
               Mean episode length: 240.55
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 8.94s
                        Total time: 34372.90s
                               ETA: 884452.2s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.338s, learning 0.199s)
               Value function loss: 4.9875
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 263.48
               Mean episode length: 235.44
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 8.54s
                        Total time: 34381.44s
                               ETA: 884426.2s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.280s, learning 0.216s)
               Value function loss: 3.8053
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 275.99
               Mean episode length: 242.65
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 8.50s
                        Total time: 34389.94s
                               ETA: 884399.2s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.569s, learning 0.206s)
               Value function loss: 4.0118
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 280.30
               Mean episode length: 245.05
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 8.77s
                        Total time: 34398.71s
                               ETA: 884379.4s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.707s, learning 0.200s)
               Value function loss: 4.7277
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 264.35
               Mean episode length: 234.67
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 8.91s
                        Total time: 34407.62s
                               ETA: 884363.0s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.529s, learning 0.184s)
               Value function loss: 4.2901
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 270.10
               Mean episode length: 240.95
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 8.71s
                        Total time: 34416.33s
                               ETA: 884341.6s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.289s, learning 0.194s)
               Value function loss: 4.4094
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 270.91
               Mean episode length: 240.53
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 8.48s
                        Total time: 34424.81s
                               ETA: 884314.4s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.586s, learning 0.197s)
               Value function loss: 4.5447
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 274.55
               Mean episode length: 243.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 8.78s
                        Total time: 34433.60s
                               ETA: 884294.8s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.777s, learning 0.177s)
               Value function loss: 5.1508
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 276.43
               Mean episode length: 242.37
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 8.95s
                        Total time: 34442.55s
                               ETA: 884279.6s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.905s, learning 0.199s)
               Value function loss: 4.4638
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 273.05
               Mean episode length: 242.47
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 9.10s
                        Total time: 34451.65s
                               ETA: 884268.3s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.433s, learning 0.181s)
               Value function loss: 4.8374
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 282.31
               Mean episode length: 247.73
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 8.61s
                        Total time: 34460.27s
                               ETA: 884244.4s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.915s, learning 0.252s)
               Value function loss: 4.4073
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 286.40
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 9.17s
                        Total time: 34469.43s
                               ETA: 884234.7s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.366s, learning 0.194s)
               Value function loss: 2.9930
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 281.48
               Mean episode length: 247.38
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 8.56s
                        Total time: 34478.00s
                               ETA: 884209.5s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.460s, learning 0.170s)
               Value function loss: 4.1287
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 275.67
               Mean episode length: 243.93
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 8.63s
                        Total time: 34486.63s
                               ETA: 884186.0s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.259s, learning 0.187s)
               Value function loss: 4.7752
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 274.27
               Mean episode length: 242.70
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 8.45s
                        Total time: 34495.07s
                               ETA: 884157.8s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.158s, learning 0.247s)
               Value function loss: 4.8320
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 279.71
               Mean episode length: 243.70
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 8.41s
                        Total time: 34503.48s
                               ETA: 884128.6s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.343s, learning 0.188s)
               Value function loss: 5.7015
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 284.57
               Mean episode length: 248.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 8.53s
                        Total time: 34512.01s
                               ETA: 884102.6s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.335s, learning 0.171s)
               Value function loss: 5.4886
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 275.02
               Mean episode length: 242.16
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 8.51s
                        Total time: 34520.51s
                               ETA: 884076.0s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.188s, learning 0.195s)
               Value function loss: 5.4660
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 275.81
               Mean episode length: 242.97
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 8.38s
                        Total time: 34528.90s
                               ETA: 884046.3s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.808s, learning 0.194s)
               Value function loss: 4.6251
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 282.61
               Mean episode length: 246.84
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 9.00s
                        Total time: 34537.90s
                               ETA: 884032.4s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.561s, learning 0.186s)
               Value function loss: 5.3243
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 280.78
               Mean episode length: 244.62
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 8.75s
                        Total time: 34546.65s
                               ETA: 884012.0s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.609s, learning 0.168s)
               Value function loss: 5.2525
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 271.99
               Mean episode length: 238.76
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 8.78s
                        Total time: 34555.42s
                               ETA: 883992.4s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.267s, learning 0.224s)
               Value function loss: 5.3392
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 270.69
               Mean episode length: 236.89
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 8.49s
                        Total time: 34563.91s
                               ETA: 883965.4s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.108s, learning 0.168s)
               Value function loss: 5.7911
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 287.15
               Mean episode length: 247.39
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 8.28s
                        Total time: 34572.19s
                               ETA: 883933.0s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.399s, learning 0.183s)
               Value function loss: 6.5808
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 286.61
               Mean episode length: 248.15
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 8.58s
                        Total time: 34580.77s
                               ETA: 883908.4s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.662s, learning 0.171s)
               Value function loss: 5.5881
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 272.33
               Mean episode length: 237.25
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 8.83s
                        Total time: 34589.60s
                               ETA: 883890.2s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.687s, learning 0.194s)
               Value function loss: 5.4548
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 272.70
               Mean episode length: 236.18
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 8.88s
                        Total time: 34598.48s
                               ETA: 883873.2s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.465s, learning 0.181s)
               Value function loss: 6.2192
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 281.48
               Mean episode length: 240.10
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 8.65s
                        Total time: 34607.13s
                               ETA: 883850.3s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.466s, learning 0.188s)
               Value function loss: 5.2071
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 274.33
               Mean episode length: 237.20
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 8.65s
                        Total time: 34615.78s
                               ETA: 883827.6s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.576s, learning 0.198s)
               Value function loss: 5.1334
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 275.04
               Mean episode length: 237.37
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 8.77s
                        Total time: 34624.56s
                               ETA: 883807.9s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.719s, learning 0.178s)
               Value function loss: 4.4932
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 281.92
               Mean episode length: 241.69
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 8.90s
                        Total time: 34633.45s
                               ETA: 883791.4s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.876s, learning 0.219s)
               Value function loss: 5.2266
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 285.01
               Mean episode length: 244.76
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 9.10s
                        Total time: 34642.55s
                               ETA: 883780.0s

################################################################################
                    [1m Learning iteration 3772/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.807s, learning 0.213s)
               Value function loss: 6.8582
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 284.86
               Mean episode length: 244.11
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61816832
                    Iteration time: 9.02s
                        Total time: 34651.57s
                               ETA: 883766.6s

################################################################################
                    [1m Learning iteration 3773/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.877s, learning 0.235s)
               Value function loss: 6.3725
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 275.21
               Mean episode length: 237.03
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 9.11s
                        Total time: 34660.68s
                               ETA: 883755.6s

################################################################################
                    [1m Learning iteration 3774/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.275s, learning 0.181s)
               Value function loss: 6.4124
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 273.58
               Mean episode length: 237.20
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61849600
                    Iteration time: 8.46s
                        Total time: 34669.14s
                               ETA: 883727.8s

################################################################################
                    [1m Learning iteration 3775/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.395s, learning 0.197s)
               Value function loss: 6.8006
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 283.31
               Mean episode length: 243.03
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61865984
                    Iteration time: 8.59s
                        Total time: 34677.73s
                               ETA: 883703.5s

################################################################################
                    [1m Learning iteration 3776/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.549s, learning 0.198s)
               Value function loss: 5.4318
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 286.57
               Mean episode length: 245.43
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61882368
                    Iteration time: 8.75s
                        Total time: 34686.48s
                               ETA: 883683.2s

################################################################################
                    [1m Learning iteration 3777/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.695s, learning 0.181s)
               Value function loss: 7.2170
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 281.07
               Mean episode length: 239.36
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61898752
                    Iteration time: 8.88s
                        Total time: 34695.35s
                               ETA: 883666.2s

################################################################################
                    [1m Learning iteration 3778/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.337s, learning 0.217s)
               Value function loss: 6.1821
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 275.08
               Mean episode length: 235.85
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61915136
                    Iteration time: 8.55s
                        Total time: 34703.91s
                               ETA: 883641.0s

################################################################################
                    [1m Learning iteration 3779/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.494s, learning 0.183s)
               Value function loss: 6.3738
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 274.24
               Mean episode length: 234.02
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 8.68s
                        Total time: 34712.58s
                               ETA: 883618.9s

################################################################################
                    [1m Learning iteration 3780/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.525s, learning 0.179s)
               Value function loss: 5.4876
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 280.65
               Mean episode length: 238.74
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61947904
                    Iteration time: 8.70s
                        Total time: 34721.29s
                               ETA: 883597.5s

################################################################################
                    [1m Learning iteration 3781/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.568s, learning 0.204s)
               Value function loss: 4.8730
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 274.67
               Mean episode length: 235.04
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61964288
                    Iteration time: 8.77s
                        Total time: 34730.06s
                               ETA: 883577.9s

################################################################################
                    [1m Learning iteration 3782/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.387s, learning 0.180s)
               Value function loss: 5.3043
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 271.59
               Mean episode length: 233.71
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61980672
                    Iteration time: 8.57s
                        Total time: 34738.63s
                               ETA: 883553.0s

################################################################################
                    [1m Learning iteration 3783/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.424s, learning 0.205s)
               Value function loss: 3.4837
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 279.15
               Mean episode length: 241.99
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61997056
                    Iteration time: 8.63s
                        Total time: 34747.26s
                               ETA: 883529.8s

################################################################################
                    [1m Learning iteration 3784/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.568s, learning 0.203s)
               Value function loss: 5.2325
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 284.20
               Mean episode length: 245.12
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62013440
                    Iteration time: 8.77s
                        Total time: 34756.03s
                               ETA: 883510.1s

################################################################################
                    [1m Learning iteration 3785/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.378s, learning 0.184s)
               Value function loss: 5.1374
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 276.43
               Mean episode length: 242.60
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 8.56s
                        Total time: 34764.59s
                               ETA: 883485.2s

################################################################################
                    [1m Learning iteration 3786/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.800s, learning 0.212s)
               Value function loss: 5.1520
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 281.98
               Mean episode length: 246.29
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62046208
                    Iteration time: 9.01s
                        Total time: 34773.60s
                               ETA: 883471.6s

################################################################################
                    [1m Learning iteration 3787/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.608s, learning 0.179s)
               Value function loss: 5.4220
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 292.00
               Mean episode length: 247.59
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62062592
                    Iteration time: 8.79s
                        Total time: 34782.39s
                               ETA: 883452.4s

################################################################################
                    [1m Learning iteration 3788/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.570s, learning 0.194s)
               Value function loss: 6.3035
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 285.85
               Mean episode length: 244.95
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62078976
                    Iteration time: 8.76s
                        Total time: 34791.15s
                               ETA: 883432.6s

################################################################################
                    [1m Learning iteration 3789/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.462s, learning 0.213s)
               Value function loss: 5.1726
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 286.85
               Mean episode length: 245.76
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62095360
                    Iteration time: 8.67s
                        Total time: 34799.83s
                               ETA: 883410.6s

################################################################################
                    [1m Learning iteration 3790/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.625s, learning 0.207s)
               Value function loss: 5.5782
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 286.09
               Mean episode length: 242.16
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62111744
                    Iteration time: 8.83s
                        Total time: 34808.66s
                               ETA: 883392.5s

################################################################################
                    [1m Learning iteration 3791/100000 [0m                    

                       Computation: 1774 steps/s (collection: 9.014s, learning 0.219s)
               Value function loss: 5.6444
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 288.38
               Mean episode length: 244.10
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 9.23s
                        Total time: 34817.89s
                               ETA: 883384.6s

################################################################################
                    [1m Learning iteration 3792/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.637s, learning 0.219s)
               Value function loss: 5.3463
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 288.61
               Mean episode length: 246.86
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62144512
                    Iteration time: 8.86s
                        Total time: 34826.75s
                               ETA: 883367.2s

################################################################################
                    [1m Learning iteration 3793/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.300s, learning 0.228s)
               Value function loss: 4.8136
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 286.67
               Mean episode length: 244.98
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62160896
                    Iteration time: 8.53s
                        Total time: 34835.27s
                               ETA: 883341.4s

################################################################################
                    [1m Learning iteration 3794/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.426s, learning 0.208s)
               Value function loss: 5.1425
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 268.90
               Mean episode length: 233.65
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62177280
                    Iteration time: 8.63s
                        Total time: 34843.91s
                               ETA: 883318.3s

################################################################################
                    [1m Learning iteration 3795/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.549s, learning 0.225s)
               Value function loss: 5.9207
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 282.02
               Mean episode length: 243.19
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62193664
                    Iteration time: 8.77s
                        Total time: 34852.68s
                               ETA: 883298.8s

################################################################################
                    [1m Learning iteration 3796/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.525s, learning 0.183s)
               Value function loss: 5.3821
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 279.37
               Mean episode length: 243.31
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62210048
                    Iteration time: 8.71s
                        Total time: 34861.39s
                               ETA: 883277.7s

################################################################################
                    [1m Learning iteration 3797/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.732s, learning 0.278s)
               Value function loss: 5.2875
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 285.25
               Mean episode length: 247.26
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 9.01s
                        Total time: 34870.40s
                               ETA: 883264.1s

################################################################################
                    [1m Learning iteration 3798/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.459s, learning 0.179s)
               Value function loss: 5.8168
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 276.88
               Mean episode length: 241.05
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62242816
                    Iteration time: 8.64s
                        Total time: 34879.04s
                               ETA: 883241.2s

################################################################################
                    [1m Learning iteration 3799/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.445s, learning 0.180s)
               Value function loss: 5.7528
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 286.01
               Mean episode length: 244.22
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62259200
                    Iteration time: 8.62s
                        Total time: 34887.66s
                               ETA: 883217.9s

################################################################################
                    [1m Learning iteration 3800/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.419s, learning 0.222s)
               Value function loss: 4.5705
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 274.92
               Mean episode length: 239.25
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62275584
                    Iteration time: 8.64s
                        Total time: 34896.30s
                               ETA: 883195.1s

################################################################################
                    [1m Learning iteration 3801/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.809s, learning 0.215s)
               Value function loss: 4.8143
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 273.84
               Mean episode length: 241.57
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62291968
                    Iteration time: 9.02s
                        Total time: 34905.33s
                               ETA: 883181.9s

################################################################################
                    [1m Learning iteration 3802/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.663s, learning 0.202s)
               Value function loss: 4.9652
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 273.57
               Mean episode length: 240.65
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62308352
                    Iteration time: 8.87s
                        Total time: 34914.19s
                               ETA: 883164.8s

################################################################################
                    [1m Learning iteration 3803/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.415s, learning 0.286s)
               Value function loss: 5.9004
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 259.91
               Mean episode length: 231.14
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 8.70s
                        Total time: 34922.90s
                               ETA: 883143.5s

################################################################################
                    [1m Learning iteration 3804/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.284s, learning 0.174s)
               Value function loss: 7.1925
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 282.05
               Mean episode length: 244.51
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62341120
                    Iteration time: 8.46s
                        Total time: 34931.35s
                               ETA: 883116.0s

################################################################################
                    [1m Learning iteration 3805/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.655s, learning 0.175s)
               Value function loss: 5.8669
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 276.49
               Mean episode length: 244.36
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62357504
                    Iteration time: 8.83s
                        Total time: 34940.18s
                               ETA: 883098.0s

################################################################################
                    [1m Learning iteration 3806/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.778s, learning 0.193s)
               Value function loss: 4.7731
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 282.65
               Mean episode length: 249.25
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62373888
                    Iteration time: 8.97s
                        Total time: 34949.16s
                               ETA: 883083.5s

################################################################################
                    [1m Learning iteration 3807/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.363s, learning 0.184s)
               Value function loss: 4.8919
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 275.87
               Mean episode length: 243.33
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62390272
                    Iteration time: 8.55s
                        Total time: 34957.70s
                               ETA: 883058.4s

################################################################################
                    [1m Learning iteration 3808/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.714s, learning 0.175s)
               Value function loss: 5.4584
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 273.60
               Mean episode length: 241.66
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62406656
                    Iteration time: 8.89s
                        Total time: 34966.59s
                               ETA: 883041.8s

################################################################################
                    [1m Learning iteration 3809/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.504s, learning 0.171s)
               Value function loss: 6.8481
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 273.10
               Mean episode length: 240.06
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 8.68s
                        Total time: 34975.27s
                               ETA: 883019.9s

################################################################################
                    [1m Learning iteration 3810/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.455s, learning 0.202s)
               Value function loss: 5.0617
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 272.42
               Mean episode length: 240.01
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62439424
                    Iteration time: 8.66s
                        Total time: 34983.92s
                               ETA: 882997.5s

################################################################################
                    [1m Learning iteration 3811/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.496s, learning 0.214s)
               Value function loss: 4.7366
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 274.75
               Mean episode length: 244.22
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62455808
                    Iteration time: 8.71s
                        Total time: 34992.63s
                               ETA: 882976.5s

################################################################################
                    [1m Learning iteration 3812/100000 [0m                    

                       Computation: 1776 steps/s (collection: 8.946s, learning 0.278s)
               Value function loss: 5.2402
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 271.36
               Mean episode length: 243.29
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62472192
                    Iteration time: 9.22s
                        Total time: 35001.86s
                               ETA: 882968.4s

################################################################################
                    [1m Learning iteration 3813/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.958s, learning 0.209s)
               Value function loss: 6.6606
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 279.89
               Mean episode length: 246.83
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62488576
                    Iteration time: 9.17s
                        Total time: 35011.02s
                               ETA: 882958.9s

################################################################################
                    [1m Learning iteration 3814/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.420s, learning 0.287s)
               Value function loss: 3.0137
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 274.83
               Mean episode length: 245.35
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62504960
                    Iteration time: 8.71s
                        Total time: 35019.73s
                               ETA: 882937.8s

################################################################################
                    [1m Learning iteration 3815/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.404s, learning 0.203s)
               Value function loss: 4.8379
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 271.90
               Mean episode length: 243.72
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 8.61s
                        Total time: 35028.34s
                               ETA: 882914.2s

################################################################################
                    [1m Learning iteration 3816/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.759s, learning 0.226s)
               Value function loss: 4.5607
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 268.79
               Mean episode length: 240.34
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62537728
                    Iteration time: 8.98s
                        Total time: 35037.32s
                               ETA: 882900.1s

################################################################################
                    [1m Learning iteration 3817/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.330s, learning 0.195s)
               Value function loss: 5.5680
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 269.29
               Mean episode length: 237.23
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62554112
                    Iteration time: 8.52s
                        Total time: 35045.85s
                               ETA: 882874.5s

################################################################################
                    [1m Learning iteration 3818/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.787s, learning 0.304s)
               Value function loss: 4.3846
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 271.41
               Mean episode length: 241.74
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62570496
                    Iteration time: 9.09s
                        Total time: 35054.94s
                               ETA: 882863.1s

################################################################################
                    [1m Learning iteration 3819/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.659s, learning 0.263s)
               Value function loss: 6.1954
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 277.09
               Mean episode length: 247.89
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62586880
                    Iteration time: 8.92s
                        Total time: 35063.86s
                               ETA: 882847.4s

################################################################################
                    [1m Learning iteration 3820/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.509s, learning 0.204s)
               Value function loss: 4.7437
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 277.42
               Mean episode length: 246.18
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62603264
                    Iteration time: 8.71s
                        Total time: 35072.57s
                               ETA: 882826.5s

################################################################################
                    [1m Learning iteration 3821/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.469s, learning 0.198s)
               Value function loss: 4.7401
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 275.29
               Mean episode length: 244.69
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 8.67s
                        Total time: 35081.24s
                               ETA: 882804.4s

################################################################################
                    [1m Learning iteration 3822/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.525s, learning 0.205s)
               Value function loss: 4.5150
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 275.86
               Mean episode length: 247.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62636032
                    Iteration time: 8.73s
                        Total time: 35089.97s
                               ETA: 882783.9s

################################################################################
                    [1m Learning iteration 3823/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.691s, learning 0.186s)
               Value function loss: 4.1683
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 272.47
               Mean episode length: 246.49
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62652416
                    Iteration time: 8.88s
                        Total time: 35098.85s
                               ETA: 882767.2s

################################################################################
                    [1m Learning iteration 3824/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.430s, learning 0.181s)
               Value function loss: 4.5464
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 270.63
               Mean episode length: 243.07
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62668800
                    Iteration time: 8.61s
                        Total time: 35107.46s
                               ETA: 882743.7s

################################################################################
                    [1m Learning iteration 3825/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.671s, learning 0.193s)
               Value function loss: 4.5823
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 275.27
               Mean episode length: 245.28
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62685184
                    Iteration time: 8.86s
                        Total time: 35116.32s
                               ETA: 882726.7s

################################################################################
                    [1m Learning iteration 3826/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.854s, learning 0.213s)
               Value function loss: 5.5031
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 276.38
               Mean episode length: 245.19
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62701568
                    Iteration time: 9.07s
                        Total time: 35125.39s
                               ETA: 882714.7s

################################################################################
                    [1m Learning iteration 3827/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.305s, learning 0.177s)
               Value function loss: 5.0024
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 277.93
               Mean episode length: 247.39
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 8.48s
                        Total time: 35133.87s
                               ETA: 882688.0s

################################################################################
                    [1m Learning iteration 3828/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.654s, learning 0.202s)
               Value function loss: 4.8027
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 280.15
               Mean episode length: 249.81
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62734336
                    Iteration time: 8.86s
                        Total time: 35142.73s
                               ETA: 882670.7s

################################################################################
                    [1m Learning iteration 3829/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.535s, learning 0.172s)
               Value function loss: 5.9370
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 275.23
               Mean episode length: 248.51
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62750720
                    Iteration time: 8.71s
                        Total time: 35151.43s
                               ETA: 882649.7s

################################################################################
                    [1m Learning iteration 3830/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.669s, learning 0.175s)
               Value function loss: 5.6627
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 272.23
               Mean episode length: 245.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62767104
                    Iteration time: 8.84s
                        Total time: 35160.28s
                               ETA: 882632.2s

################################################################################
                    [1m Learning iteration 3831/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.543s, learning 0.283s)
               Value function loss: 5.6806
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 270.83
               Mean episode length: 243.34
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62783488
                    Iteration time: 8.83s
                        Total time: 35169.10s
                               ETA: 882614.2s

################################################################################
                    [1m Learning iteration 3832/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.433s, learning 0.186s)
               Value function loss: 4.0709
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 270.59
               Mean episode length: 243.97
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62799872
                    Iteration time: 8.62s
                        Total time: 35177.72s
                               ETA: 882591.0s

################################################################################
                    [1m Learning iteration 3833/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.212s, learning 0.174s)
               Value function loss: 5.3560
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 271.50
               Mean episode length: 248.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 8.39s
                        Total time: 35186.11s
                               ETA: 882562.0s

################################################################################
                    [1m Learning iteration 3834/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.713s, learning 0.178s)
               Value function loss: 5.4168
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 278.56
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62832640
                    Iteration time: 8.89s
                        Total time: 35195.00s
                               ETA: 882545.6s

################################################################################
                    [1m Learning iteration 3835/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.563s, learning 0.174s)
               Value function loss: 7.2415
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 274.98
               Mean episode length: 246.30
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62849024
                    Iteration time: 8.74s
                        Total time: 35203.74s
                               ETA: 882525.4s

################################################################################
                    [1m Learning iteration 3836/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.806s, learning 0.228s)
               Value function loss: 5.0253
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 274.53
               Mean episode length: 247.61
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62865408
                    Iteration time: 9.03s
                        Total time: 35212.77s
                               ETA: 882512.6s

################################################################################
                    [1m Learning iteration 3837/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.408s, learning 0.258s)
               Value function loss: 4.5512
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 274.27
               Mean episode length: 249.45
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62881792
                    Iteration time: 8.67s
                        Total time: 35221.44s
                               ETA: 882490.6s

################################################################################
                    [1m Learning iteration 3838/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.353s, learning 0.219s)
               Value function loss: 5.0165
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 273.78
               Mean episode length: 249.02
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62898176
                    Iteration time: 8.57s
                        Total time: 35230.01s
                               ETA: 882466.3s

################################################################################
                    [1m Learning iteration 3839/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.449s, learning 0.194s)
               Value function loss: 4.2554
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 273.76
               Mean episode length: 249.02
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 8.64s
                        Total time: 35238.65s
                               ETA: 882443.8s

################################################################################
                    [1m Learning iteration 3840/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.242s, learning 0.180s)
               Value function loss: 6.2573
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 276.40
               Mean episode length: 249.48
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62930944
                    Iteration time: 8.42s
                        Total time: 35247.07s
                               ETA: 882415.7s

################################################################################
                    [1m Learning iteration 3841/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.213s, learning 0.261s)
               Value function loss: 5.8803
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 270.23
               Mean episode length: 248.49
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62947328
                    Iteration time: 8.47s
                        Total time: 35255.55s
                               ETA: 882388.9s

################################################################################
                    [1m Learning iteration 3842/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.591s, learning 0.223s)
               Value function loss: 6.1627
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 270.70
               Mean episode length: 246.80
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62963712
                    Iteration time: 8.81s
                        Total time: 35264.36s
                               ETA: 882370.7s

################################################################################
                    [1m Learning iteration 3843/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.492s, learning 0.206s)
               Value function loss: 4.5163
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 263.91
               Mean episode length: 241.70
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62980096
                    Iteration time: 8.70s
                        Total time: 35273.06s
                               ETA: 882349.5s

################################################################################
                    [1m Learning iteration 3844/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.385s, learning 0.184s)
               Value function loss: 4.5161
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 268.41
               Mean episode length: 245.71
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62996480
                    Iteration time: 8.57s
                        Total time: 35281.63s
                               ETA: 882325.2s

################################################################################
                    [1m Learning iteration 3845/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.776s, learning 0.226s)
               Value function loss: 3.6626
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 275.71
               Mean episode length: 248.21
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 9.00s
                        Total time: 35290.63s
                               ETA: 882311.7s

################################################################################
                    [1m Learning iteration 3846/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.317s, learning 0.249s)
               Value function loss: 3.7378
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 270.80
               Mean episode length: 245.50
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63029248
                    Iteration time: 8.57s
                        Total time: 35299.20s
                               ETA: 882287.3s

################################################################################
                    [1m Learning iteration 3847/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.326s, learning 0.183s)
               Value function loss: 3.9527
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 265.90
               Mean episode length: 247.02
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63045632
                    Iteration time: 8.51s
                        Total time: 35307.71s
                               ETA: 882261.5s

################################################################################
                    [1m Learning iteration 3848/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.741s, learning 0.174s)
               Value function loss: 4.7205
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 256.31
               Mean episode length: 238.77
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63062016
                    Iteration time: 8.92s
                        Total time: 35316.62s
                               ETA: 882245.8s

################################################################################
                    [1m Learning iteration 3849/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.692s, learning 0.224s)
               Value function loss: 4.3690
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 256.50
               Mean episode length: 240.62
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63078400
                    Iteration time: 8.92s
                        Total time: 35325.54s
                               ETA: 882230.1s

################################################################################
                    [1m Learning iteration 3850/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.619s, learning 0.180s)
               Value function loss: 4.7991
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 261.52
               Mean episode length: 246.05
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63094784
                    Iteration time: 8.80s
                        Total time: 35334.34s
                               ETA: 882211.6s

################################################################################
                    [1m Learning iteration 3851/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.421s, learning 0.167s)
               Value function loss: 5.3961
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 272.15
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 8.59s
                        Total time: 35342.93s
                               ETA: 882187.7s

################################################################################
                    [1m Learning iteration 3852/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.614s, learning 0.175s)
               Value function loss: 5.4187
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 269.58
               Mean episode length: 246.32
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63127552
                    Iteration time: 8.79s
                        Total time: 35351.72s
                               ETA: 882168.9s

################################################################################
                    [1m Learning iteration 3853/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.790s, learning 0.172s)
               Value function loss: 5.0174
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 270.88
               Mean episode length: 248.17
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63143936
                    Iteration time: 8.96s
                        Total time: 35360.68s
                               ETA: 882154.4s

################################################################################
                    [1m Learning iteration 3854/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.389s, learning 0.213s)
               Value function loss: 4.6493
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 270.68
               Mean episode length: 249.99
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63160320
                    Iteration time: 8.60s
                        Total time: 35369.28s
                               ETA: 882130.9s

################################################################################
                    [1m Learning iteration 3855/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.468s, learning 0.187s)
               Value function loss: 4.6158
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 266.60
               Mean episode length: 246.62
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63176704
                    Iteration time: 8.66s
                        Total time: 35377.94s
                               ETA: 882108.8s

################################################################################
                    [1m Learning iteration 3856/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.259s, learning 0.176s)
               Value function loss: 5.6993
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 266.46
               Mean episode length: 245.24
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63193088
                    Iteration time: 8.43s
                        Total time: 35386.37s
                               ETA: 882081.2s

################################################################################
                    [1m Learning iteration 3857/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.360s, learning 0.238s)
               Value function loss: 6.7972
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 267.33
               Mean episode length: 244.72
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 8.60s
                        Total time: 35394.97s
                               ETA: 882057.6s

################################################################################
                    [1m Learning iteration 3858/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.530s, learning 0.181s)
               Value function loss: 5.2287
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 262.39
               Mean episode length: 244.94
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63225856
                    Iteration time: 8.71s
                        Total time: 35403.68s
                               ETA: 882036.9s

################################################################################
                    [1m Learning iteration 3859/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.638s, learning 0.195s)
               Value function loss: 5.3660
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 267.91
               Mean episode length: 245.09
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63242240
                    Iteration time: 8.83s
                        Total time: 35412.51s
                               ETA: 882019.2s

################################################################################
                    [1m Learning iteration 3860/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.458s, learning 0.175s)
               Value function loss: 6.1973
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 272.26
               Mean episode length: 248.97
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63258624
                    Iteration time: 8.63s
                        Total time: 35421.14s
                               ETA: 881996.6s

################################################################################
                    [1m Learning iteration 3861/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.511s, learning 0.182s)
               Value function loss: 6.0366
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 266.67
               Mean episode length: 247.01
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63275008
                    Iteration time: 8.69s
                        Total time: 35429.84s
                               ETA: 881975.4s

################################################################################
                    [1m Learning iteration 3862/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.634s, learning 0.183s)
               Value function loss: 6.0832
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 261.65
               Mean episode length: 240.79
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63291392
                    Iteration time: 8.82s
                        Total time: 35438.65s
                               ETA: 881957.4s

################################################################################
                    [1m Learning iteration 3863/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.507s, learning 0.168s)
               Value function loss: 4.0924
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 266.84
               Mean episode length: 243.56
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 8.68s
                        Total time: 35447.33s
                               ETA: 881935.8s

################################################################################
                    [1m Learning iteration 3864/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.238s, learning 0.238s)
               Value function loss: 4.7692
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 270.86
               Mean episode length: 247.23
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63324160
                    Iteration time: 8.48s
                        Total time: 35455.80s
                               ETA: 881909.2s

################################################################################
                    [1m Learning iteration 3865/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.544s, learning 0.169s)
               Value function loss: 5.9918
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 271.07
               Mean episode length: 247.19
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63340544
                    Iteration time: 8.71s
                        Total time: 35464.52s
                               ETA: 881888.6s

################################################################################
                    [1m Learning iteration 3866/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.304s, learning 0.189s)
               Value function loss: 7.0420
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 272.32
               Mean episode length: 249.22
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63356928
                    Iteration time: 8.49s
                        Total time: 35473.01s
                               ETA: 881862.5s

################################################################################
                    [1m Learning iteration 3867/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.765s, learning 0.183s)
               Value function loss: 5.3432
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 272.50
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63373312
                    Iteration time: 8.95s
                        Total time: 35481.96s
                               ETA: 881847.8s

################################################################################
                    [1m Learning iteration 3868/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.544s, learning 0.203s)
               Value function loss: 5.8698
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 274.64
               Mean episode length: 248.28
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63389696
                    Iteration time: 8.75s
                        Total time: 35490.71s
                               ETA: 881828.0s

################################################################################
                    [1m Learning iteration 3869/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.567s, learning 0.177s)
               Value function loss: 5.7676
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 271.59
               Mean episode length: 248.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 8.74s
                        Total time: 35499.45s
                               ETA: 881808.2s

################################################################################
                    [1m Learning iteration 3870/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.458s, learning 0.200s)
               Value function loss: 5.0402
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 264.62
               Mean episode length: 242.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63422464
                    Iteration time: 8.66s
                        Total time: 35508.11s
                               ETA: 881786.2s

################################################################################
                    [1m Learning iteration 3871/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.636s, learning 0.263s)
               Value function loss: 7.2260
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 271.27
               Mean episode length: 243.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63438848
                    Iteration time: 8.90s
                        Total time: 35517.01s
                               ETA: 881770.2s

################################################################################
                    [1m Learning iteration 3872/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.702s, learning 0.180s)
               Value function loss: 6.6217
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 275.54
               Mean episode length: 248.56
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63455232
                    Iteration time: 8.88s
                        Total time: 35525.89s
                               ETA: 881753.9s

################################################################################
                    [1m Learning iteration 3873/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.672s, learning 0.261s)
               Value function loss: 6.0597
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 271.67
               Mean episode length: 244.99
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63471616
                    Iteration time: 8.93s
                        Total time: 35534.82s
                               ETA: 881738.8s

################################################################################
                    [1m Learning iteration 3874/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.775s, learning 0.197s)
               Value function loss: 5.5962
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 273.54
               Mean episode length: 248.64
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63488000
                    Iteration time: 8.97s
                        Total time: 35543.80s
                               ETA: 881724.6s

################################################################################
                    [1m Learning iteration 3875/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.836s, learning 0.276s)
               Value function loss: 6.0453
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 274.16
               Mean episode length: 247.47
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 9.11s
                        Total time: 35552.91s
                               ETA: 881713.9s

################################################################################
                    [1m Learning iteration 3876/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.632s, learning 0.199s)
               Value function loss: 5.1220
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 274.58
               Mean episode length: 247.72
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63520768
                    Iteration time: 8.83s
                        Total time: 35561.74s
                               ETA: 881696.3s

################################################################################
                    [1m Learning iteration 3877/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.610s, learning 0.227s)
               Value function loss: 3.6928
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 276.54
               Mean episode length: 248.89
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63537152
                    Iteration time: 8.84s
                        Total time: 35570.58s
                               ETA: 881678.8s

################################################################################
                    [1m Learning iteration 3878/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.293s, learning 0.186s)
               Value function loss: 4.3636
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 275.83
               Mean episode length: 247.68
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63553536
                    Iteration time: 8.48s
                        Total time: 35579.05s
                               ETA: 881652.4s

################################################################################
                    [1m Learning iteration 3879/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.487s, learning 0.182s)
               Value function loss: 5.6787
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 268.27
               Mean episode length: 245.75
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63569920
                    Iteration time: 8.67s
                        Total time: 35587.72s
                               ETA: 881630.8s

################################################################################
                    [1m Learning iteration 3880/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.478s, learning 0.272s)
               Value function loss: 5.3636
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 267.04
               Mean episode length: 247.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63586304
                    Iteration time: 8.75s
                        Total time: 35596.47s
                               ETA: 881611.1s

################################################################################
                    [1m Learning iteration 3881/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.817s, learning 0.217s)
               Value function loss: 5.1088
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 266.85
               Mean episode length: 248.12
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 9.03s
                        Total time: 35605.51s
                               ETA: 881598.6s

################################################################################
                    [1m Learning iteration 3882/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.610s, learning 0.228s)
               Value function loss: 6.1406
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 273.13
               Mean episode length: 247.52
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63619072
                    Iteration time: 8.84s
                        Total time: 35614.34s
                               ETA: 881581.1s

################################################################################
                    [1m Learning iteration 3883/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.420s, learning 0.183s)
               Value function loss: 6.4196
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 269.82
               Mean episode length: 249.01
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63635456
                    Iteration time: 8.60s
                        Total time: 35622.95s
                               ETA: 881557.9s

################################################################################
                    [1m Learning iteration 3884/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.873s, learning 0.175s)
               Value function loss: 5.2904
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 270.48
               Mean episode length: 248.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63651840
                    Iteration time: 9.05s
                        Total time: 35631.99s
                               ETA: 881545.6s

################################################################################
                    [1m Learning iteration 3885/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.394s, learning 0.179s)
               Value function loss: 5.1781
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 273.58
               Mean episode length: 248.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63668224
                    Iteration time: 8.57s
                        Total time: 35640.57s
                               ETA: 881521.7s

################################################################################
                    [1m Learning iteration 3886/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.718s, learning 0.185s)
               Value function loss: 5.0640
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 270.47
               Mean episode length: 246.85
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63684608
                    Iteration time: 8.90s
                        Total time: 35649.47s
                               ETA: 881505.8s

################################################################################
                    [1m Learning iteration 3887/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.278s, learning 0.206s)
               Value function loss: 5.6139
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 268.23
               Mean episode length: 245.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 8.48s
                        Total time: 35657.95s
                               ETA: 881479.7s

################################################################################
                    [1m Learning iteration 3888/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.580s, learning 0.202s)
               Value function loss: 5.6794
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 269.68
               Mean episode length: 247.72
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63717376
                    Iteration time: 8.78s
                        Total time: 35666.74s
                               ETA: 881460.9s

################################################################################
                    [1m Learning iteration 3889/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.344s, learning 0.184s)
               Value function loss: 6.9660
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 275.22
               Mean episode length: 248.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63733760
                    Iteration time: 8.53s
                        Total time: 35675.27s
                               ETA: 881435.8s

################################################################################
                    [1m Learning iteration 3890/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.577s, learning 0.228s)
               Value function loss: 5.3278
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 273.33
               Mean episode length: 248.11
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63750144
                    Iteration time: 8.81s
                        Total time: 35684.07s
                               ETA: 881417.6s

################################################################################
                    [1m Learning iteration 3891/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.159s, learning 0.170s)
               Value function loss: 5.4766
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 273.65
               Mean episode length: 247.53
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63766528
                    Iteration time: 8.33s
                        Total time: 35692.40s
                               ETA: 881387.7s

################################################################################
                    [1m Learning iteration 3892/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.442s, learning 0.176s)
               Value function loss: 5.8247
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 273.91
               Mean episode length: 248.25
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63782912
                    Iteration time: 8.62s
                        Total time: 35701.02s
                               ETA: 881364.9s

################################################################################
                    [1m Learning iteration 3893/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.174s)
               Value function loss: 6.0883
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 273.99
               Mean episode length: 245.92
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 8.59s
                        Total time: 35709.61s
                               ETA: 881341.4s

################################################################################
                    [1m Learning iteration 3894/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.296s, learning 0.178s)
               Value function loss: 4.3778
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 279.37
               Mean episode length: 249.10
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63815680
                    Iteration time: 8.47s
                        Total time: 35718.08s
                               ETA: 881315.0s

################################################################################
                    [1m Learning iteration 3895/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.555s, learning 0.190s)
               Value function loss: 4.2257
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 280.21
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63832064
                    Iteration time: 8.75s
                        Total time: 35726.83s
                               ETA: 881295.4s

################################################################################
                    [1m Learning iteration 3896/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.654s, learning 0.178s)
               Value function loss: 5.0171
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 276.09
               Mean episode length: 247.39
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63848448
                    Iteration time: 8.83s
                        Total time: 35735.66s
                               ETA: 881277.9s

################################################################################
                    [1m Learning iteration 3897/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.305s, learning 0.179s)
               Value function loss: 6.8171
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 275.13
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63864832
                    Iteration time: 8.48s
                        Total time: 35744.14s
                               ETA: 881251.8s

################################################################################
                    [1m Learning iteration 3898/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.321s, learning 0.181s)
               Value function loss: 5.4843
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 275.53
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63881216
                    Iteration time: 8.50s
                        Total time: 35752.64s
                               ETA: 881226.1s

################################################################################
                    [1m Learning iteration 3899/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.547s, learning 0.191s)
               Value function loss: 5.6764
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 273.16
               Mean episode length: 248.65
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 8.74s
                        Total time: 35761.38s
                               ETA: 881206.3s

################################################################################
                    [1m Learning iteration 3900/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.534s, learning 0.207s)
               Value function loss: 5.9447
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 278.38
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63913984
                    Iteration time: 8.74s
                        Total time: 35770.12s
                               ETA: 881186.6s

################################################################################
                    [1m Learning iteration 3901/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.620s, learning 0.203s)
               Value function loss: 5.1247
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 276.78
               Mean episode length: 247.86
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63930368
                    Iteration time: 8.82s
                        Total time: 35778.95s
                               ETA: 881168.9s

################################################################################
                    [1m Learning iteration 3902/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.422s, learning 0.309s)
               Value function loss: 6.7334
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 274.81
               Mean episode length: 244.67
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63946752
                    Iteration time: 8.73s
                        Total time: 35787.68s
                               ETA: 881148.9s

################################################################################
                    [1m Learning iteration 3903/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.473s, learning 0.176s)
               Value function loss: 6.0642
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 278.96
               Mean episode length: 249.02
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63963136
                    Iteration time: 8.65s
                        Total time: 35796.33s
                               ETA: 881126.9s

################################################################################
                    [1m Learning iteration 3904/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.528s, learning 0.215s)
               Value function loss: 5.7900
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 271.37
               Mean episode length: 244.31
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63979520
                    Iteration time: 8.74s
                        Total time: 35805.07s
                               ETA: 881107.3s

################################################################################
                    [1m Learning iteration 3905/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.604s, learning 0.210s)
               Value function loss: 4.8416
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 274.25
               Mean episode length: 247.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 8.81s
                        Total time: 35813.88s
                               ETA: 881089.4s

################################################################################
                    [1m Learning iteration 3906/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.789s, learning 0.180s)
               Value function loss: 4.9408
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 277.48
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64012288
                    Iteration time: 8.97s
                        Total time: 35822.85s
                               ETA: 881075.3s

################################################################################
                    [1m Learning iteration 3907/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.175s, learning 0.178s)
               Value function loss: 5.2527
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 279.77
               Mean episode length: 249.90
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64028672
                    Iteration time: 8.35s
                        Total time: 35831.21s
                               ETA: 881046.1s

################################################################################
                    [1m Learning iteration 3908/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.854s, learning 0.178s)
               Value function loss: 3.7784
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 275.25
               Mean episode length: 247.23
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64045056
                    Iteration time: 9.03s
                        Total time: 35840.24s
                               ETA: 881033.5s

################################################################################
                    [1m Learning iteration 3909/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.271s, learning 0.205s)
               Value function loss: 4.2593
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 273.55
               Mean episode length: 246.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64061440
                    Iteration time: 8.48s
                        Total time: 35848.71s
                               ETA: 881007.3s

################################################################################
                    [1m Learning iteration 3910/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.718s, learning 0.223s)
               Value function loss: 5.7719
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 275.26
               Mean episode length: 247.87
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64077824
                    Iteration time: 8.94s
                        Total time: 35857.65s
                               ETA: 880992.6s

################################################################################
                    [1m Learning iteration 3911/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.639s, learning 0.187s)
               Value function loss: 5.3498
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 271.41
               Mean episode length: 247.89
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 8.83s
                        Total time: 35866.48s
                               ETA: 880975.0s

################################################################################
                    [1m Learning iteration 3912/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.448s, learning 0.218s)
               Value function loss: 4.9449
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 272.94
               Mean episode length: 248.90
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64110592
                    Iteration time: 8.67s
                        Total time: 35875.15s
                               ETA: 880953.5s

################################################################################
                    [1m Learning iteration 3913/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.772s, learning 0.186s)
               Value function loss: 6.5721
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 273.96
               Mean episode length: 248.07
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64126976
                    Iteration time: 8.96s
                        Total time: 35884.10s
                               ETA: 880939.2s

################################################################################
                    [1m Learning iteration 3914/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.774s, learning 0.271s)
               Value function loss: 6.0267
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 273.83
               Mean episode length: 246.86
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64143360
                    Iteration time: 9.05s
                        Total time: 35893.15s
                               ETA: 880927.0s

################################################################################
                    [1m Learning iteration 3915/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.872s, learning 0.211s)
               Value function loss: 4.7062
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 276.86
               Mean episode length: 248.27
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64159744
                    Iteration time: 9.08s
                        Total time: 35902.23s
                               ETA: 880915.7s

################################################################################
                    [1m Learning iteration 3916/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.298s, learning 0.175s)
               Value function loss: 4.8914
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 278.12
               Mean episode length: 248.05
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64176128
                    Iteration time: 8.47s
                        Total time: 35910.70s
                               ETA: 880889.5s

################################################################################
                    [1m Learning iteration 3917/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.343s, learning 0.185s)
               Value function loss: 4.7568
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 274.92
               Mean episode length: 246.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 8.53s
                        Total time: 35919.23s
                               ETA: 880864.6s

################################################################################
                    [1m Learning iteration 3918/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.450s, learning 0.273s)
               Value function loss: 4.7718
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 274.62
               Mean episode length: 247.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64208896
                    Iteration time: 8.72s
                        Total time: 35927.96s
                               ETA: 880844.6s

################################################################################
                    [1m Learning iteration 3919/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.384s, learning 0.184s)
               Value function loss: 5.4049
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 275.43
               Mean episode length: 247.71
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64225280
                    Iteration time: 8.57s
                        Total time: 35936.52s
                               ETA: 880820.7s

################################################################################
                    [1m Learning iteration 3920/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.398s, learning 0.226s)
               Value function loss: 6.7689
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 275.23
               Mean episode length: 246.43
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64241664
                    Iteration time: 8.62s
                        Total time: 35945.15s
                               ETA: 880798.2s

################################################################################
                    [1m Learning iteration 3921/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.598s, learning 0.180s)
               Value function loss: 5.6576
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 276.71
               Mean episode length: 249.47
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64258048
                    Iteration time: 8.78s
                        Total time: 35953.92s
                               ETA: 880779.5s

################################################################################
                    [1m Learning iteration 3922/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.517s, learning 0.240s)
               Value function loss: 5.3177
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 278.02
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64274432
                    Iteration time: 8.76s
                        Total time: 35962.68s
                               ETA: 880760.3s

################################################################################
                    [1m Learning iteration 3923/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.875s, learning 0.199s)
               Value function loss: 5.5029
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 274.57
               Mean episode length: 248.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 9.07s
                        Total time: 35971.76s
                               ETA: 880748.8s

################################################################################
                    [1m Learning iteration 3924/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.463s, learning 0.245s)
               Value function loss: 5.3860
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 271.96
               Mean episode length: 247.70
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64307200
                    Iteration time: 8.71s
                        Total time: 35980.46s
                               ETA: 880728.4s

################################################################################
                    [1m Learning iteration 3925/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.674s, learning 0.206s)
               Value function loss: 4.3099
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 272.17
               Mean episode length: 248.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64323584
                    Iteration time: 8.88s
                        Total time: 35989.34s
                               ETA: 880712.2s

################################################################################
                    [1m Learning iteration 3926/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.386s, learning 0.175s)
               Value function loss: 4.3664
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 271.97
               Mean episode length: 247.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64339968
                    Iteration time: 8.56s
                        Total time: 35997.91s
                               ETA: 880688.2s

################################################################################
                    [1m Learning iteration 3927/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.524s, learning 0.216s)
               Value function loss: 4.7038
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 270.06
               Mean episode length: 245.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64356352
                    Iteration time: 8.74s
                        Total time: 36006.65s
                               ETA: 880668.7s

################################################################################
                    [1m Learning iteration 3928/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.700s, learning 0.203s)
               Value function loss: 5.4452
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 269.56
               Mean episode length: 246.50
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64372736
                    Iteration time: 8.90s
                        Total time: 36015.55s
                               ETA: 880653.0s

################################################################################
                    [1m Learning iteration 3929/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.430s, learning 0.180s)
               Value function loss: 5.9951
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 267.80
               Mean episode length: 245.56
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 8.61s
                        Total time: 36024.16s
                               ETA: 880630.3s

################################################################################
                    [1m Learning iteration 3930/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.806s, learning 0.279s)
               Value function loss: 5.9983
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 270.80
               Mean episode length: 247.40
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64405504
                    Iteration time: 9.09s
                        Total time: 36033.24s
                               ETA: 880619.1s

################################################################################
                    [1m Learning iteration 3931/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.370s, learning 0.176s)
               Value function loss: 4.3522
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 270.92
               Mean episode length: 245.59
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64421888
                    Iteration time: 8.55s
                        Total time: 36041.79s
                               ETA: 880594.8s

################################################################################
                    [1m Learning iteration 3932/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.767s, learning 0.191s)
               Value function loss: 5.0561
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 271.13
               Mean episode length: 246.54
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64438272
                    Iteration time: 8.96s
                        Total time: 36050.75s
                               ETA: 880580.5s

################################################################################
                    [1m Learning iteration 3933/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.741s, learning 0.175s)
               Value function loss: 5.2807
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 271.22
               Mean episode length: 248.05
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64454656
                    Iteration time: 8.92s
                        Total time: 36059.66s
                               ETA: 880565.3s

################################################################################
                    [1m Learning iteration 3934/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.619s, learning 0.204s)
               Value function loss: 6.8533
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 269.62
               Mean episode length: 244.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64471040
                    Iteration time: 8.82s
                        Total time: 36068.49s
                               ETA: 880547.7s

################################################################################
                    [1m Learning iteration 3935/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.609s, learning 0.199s)
               Value function loss: 5.8106
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 270.76
               Mean episode length: 247.34
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 8.81s
                        Total time: 36077.30s
                               ETA: 880529.8s

################################################################################
                    [1m Learning iteration 3936/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.667s, learning 0.202s)
               Value function loss: 4.7232
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 268.55
               Mean episode length: 244.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64503808
                    Iteration time: 8.87s
                        Total time: 36086.17s
                               ETA: 880513.4s

################################################################################
                    [1m Learning iteration 3937/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.561s, learning 0.199s)
               Value function loss: 5.2177
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 273.97
               Mean episode length: 247.61
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64520192
                    Iteration time: 8.76s
                        Total time: 36094.93s
                               ETA: 880494.4s

################################################################################
                    [1m Learning iteration 3938/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.744s, learning 0.183s)
               Value function loss: 6.4090
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 275.02
               Mean episode length: 246.79
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64536576
                    Iteration time: 8.93s
                        Total time: 36103.85s
                               ETA: 880479.4s

################################################################################
                    [1m Learning iteration 3939/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.691s, learning 0.175s)
               Value function loss: 3.5775
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 270.74
               Mean episode length: 243.73
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64552960
                    Iteration time: 8.87s
                        Total time: 36112.72s
                               ETA: 880462.9s

################################################################################
                    [1m Learning iteration 3940/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.757s, learning 0.182s)
               Value function loss: 5.0135
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 260.24
               Mean episode length: 238.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64569344
                    Iteration time: 8.94s
                        Total time: 36121.66s
                               ETA: 880448.2s

################################################################################
                    [1m Learning iteration 3941/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.405s, learning 0.179s)
               Value function loss: 5.2185
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 261.25
               Mean episode length: 239.25
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 8.58s
                        Total time: 36130.24s
                               ETA: 880424.9s

################################################################################
                    [1m Learning iteration 3942/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.363s, learning 0.187s)
               Value function loss: 6.0620
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 264.23
               Mean episode length: 241.15
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64602112
                    Iteration time: 8.55s
                        Total time: 36138.79s
                               ETA: 880400.8s

################################################################################
                    [1m Learning iteration 3943/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.390s, learning 0.186s)
               Value function loss: 4.7076
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 260.11
               Mean episode length: 239.94
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64618496
                    Iteration time: 8.58s
                        Total time: 36147.37s
                               ETA: 880377.2s

################################################################################
                    [1m Learning iteration 3944/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.528s, learning 0.170s)
               Value function loss: 6.8913
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 269.11
               Mean episode length: 243.78
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64634880
                    Iteration time: 8.70s
                        Total time: 36156.07s
                               ETA: 880356.7s

################################################################################
                    [1m Learning iteration 3945/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.539s, learning 0.168s)
               Value function loss: 4.7269
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 269.93
               Mean episode length: 245.99
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64651264
                    Iteration time: 8.71s
                        Total time: 36164.78s
                               ETA: 880336.4s

################################################################################
                    [1m Learning iteration 3946/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.515s, learning 0.189s)
               Value function loss: 6.3910
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 269.44
               Mean episode length: 246.17
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64667648
                    Iteration time: 8.70s
                        Total time: 36173.48s
                               ETA: 880316.0s

################################################################################
                    [1m Learning iteration 3947/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.692s, learning 0.217s)
               Value function loss: 4.7553
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 265.31
               Mean episode length: 242.13
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 8.91s
                        Total time: 36182.39s
                               ETA: 880300.6s

################################################################################
                    [1m Learning iteration 3948/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.631s, learning 0.188s)
               Value function loss: 5.5534
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 269.54
               Mean episode length: 245.41
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64700416
                    Iteration time: 8.82s
                        Total time: 36191.21s
                               ETA: 880283.1s

################################################################################
                    [1m Learning iteration 3949/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.541s, learning 0.183s)
               Value function loss: 5.6128
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 260.22
               Mean episode length: 236.88
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64716800
                    Iteration time: 8.72s
                        Total time: 36199.93s
                               ETA: 880263.2s

################################################################################
                    [1m Learning iteration 3950/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.842s, learning 0.176s)
               Value function loss: 5.5887
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 268.20
               Mean episode length: 241.79
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64733184
                    Iteration time: 9.02s
                        Total time: 36208.95s
                               ETA: 880250.5s

################################################################################
                    [1m Learning iteration 3951/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.521s, learning 0.175s)
               Value function loss: 6.6157
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 272.24
               Mean episode length: 245.34
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64749568
                    Iteration time: 8.70s
                        Total time: 36217.65s
                               ETA: 880229.9s

################################################################################
                    [1m Learning iteration 3952/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.324s, learning 0.187s)
               Value function loss: 6.0770
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 272.07
               Mean episode length: 246.35
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64765952
                    Iteration time: 8.51s
                        Total time: 36226.16s
                               ETA: 880204.9s

################################################################################
                    [1m Learning iteration 3953/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.283s, learning 0.181s)
               Value function loss: 5.4315
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 268.29
               Mean episode length: 244.18
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 8.46s
                        Total time: 36234.62s
                               ETA: 880178.7s

################################################################################
                    [1m Learning iteration 3954/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.849s, learning 0.213s)
               Value function loss: 5.7448
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 261.77
               Mean episode length: 241.37
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64798720
                    Iteration time: 9.06s
                        Total time: 36243.68s
                               ETA: 880167.1s

################################################################################
                    [1m Learning iteration 3955/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.590s, learning 0.175s)
               Value function loss: 5.2832
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 268.52
               Mean episode length: 243.46
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64815104
                    Iteration time: 8.76s
                        Total time: 36252.45s
                               ETA: 880148.2s

################################################################################
                    [1m Learning iteration 3956/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.374s, learning 0.168s)
               Value function loss: 5.6496
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 271.58
               Mean episode length: 245.41
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64831488
                    Iteration time: 8.54s
                        Total time: 36260.99s
                               ETA: 880124.0s

################################################################################
                    [1m Learning iteration 3957/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.319s, learning 0.176s)
               Value function loss: 3.2012
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 270.78
               Mean episode length: 243.22
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64847872
                    Iteration time: 8.50s
                        Total time: 36269.49s
                               ETA: 880098.6s

################################################################################
                    [1m Learning iteration 3958/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.712s, learning 0.182s)
               Value function loss: 4.8711
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 266.62
               Mean episode length: 244.38
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64864256
                    Iteration time: 8.89s
                        Total time: 36278.38s
                               ETA: 880082.9s

################################################################################
                    [1m Learning iteration 3959/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.627s, learning 0.207s)
               Value function loss: 5.2256
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 265.00
               Mean episode length: 242.49
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 8.83s
                        Total time: 36287.21s
                               ETA: 880065.7s

################################################################################
                    [1m Learning iteration 3960/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.442s, learning 0.212s)
               Value function loss: 6.8189
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 267.56
               Mean episode length: 240.92
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64897024
                    Iteration time: 8.65s
                        Total time: 36295.87s
                               ETA: 880044.2s

################################################################################
                    [1m Learning iteration 3961/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.457s, learning 0.179s)
               Value function loss: 5.4322
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 269.61
               Mean episode length: 244.89
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64913408
                    Iteration time: 8.64s
                        Total time: 36304.50s
                               ETA: 880022.3s

################################################################################
                    [1m Learning iteration 3962/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.110s, learning 0.173s)
               Value function loss: 4.5281
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 264.08
               Mean episode length: 241.89
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64929792
                    Iteration time: 8.28s
                        Total time: 36312.79s
                               ETA: 879991.8s

################################################################################
                    [1m Learning iteration 3963/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.556s, learning 0.179s)
               Value function loss: 4.9632
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 263.77
               Mean episode length: 242.83
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64946176
                    Iteration time: 8.73s
                        Total time: 36321.52s
                               ETA: 879972.2s

################################################################################
                    [1m Learning iteration 3964/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.342s, learning 0.184s)
               Value function loss: 4.1870
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 252.77
               Mean episode length: 236.13
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64962560
                    Iteration time: 8.53s
                        Total time: 36330.05s
                               ETA: 879947.6s

################################################################################
                    [1m Learning iteration 3965/100000 [0m                    

                       Computation: 1777 steps/s (collection: 8.994s, learning 0.223s)
               Value function loss: 6.3466
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 257.07
               Mean episode length: 242.19
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 9.22s
                        Total time: 36339.26s
                               ETA: 879939.8s

################################################################################
                    [1m Learning iteration 3966/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.770s, learning 0.180s)
               Value function loss: 5.2715
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 258.36
               Mean episode length: 241.49
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64995328
                    Iteration time: 8.95s
                        Total time: 36348.21s
                               ETA: 879925.5s

################################################################################
                    [1m Learning iteration 3967/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.499s, learning 0.201s)
               Value function loss: 5.6518
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 256.82
               Mean episode length: 237.18
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65011712
                    Iteration time: 8.70s
                        Total time: 36356.92s
                               ETA: 879905.2s

################################################################################
                    [1m Learning iteration 3968/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.129s, learning 0.203s)
               Value function loss: 4.3460
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 256.67
               Mean episode length: 239.27
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65028096
                    Iteration time: 8.33s
                        Total time: 36365.25s
                               ETA: 879875.9s

################################################################################
                    [1m Learning iteration 3969/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.676s, learning 0.196s)
               Value function loss: 5.0794
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 247.55
               Mean episode length: 233.98
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65044480
                    Iteration time: 8.87s
                        Total time: 36374.12s
                               ETA: 879859.7s

################################################################################
                    [1m Learning iteration 3970/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.207s, learning 0.176s)
               Value function loss: 3.7969
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 246.00
               Mean episode length: 232.53
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65060864
                    Iteration time: 8.38s
                        Total time: 36382.50s
                               ETA: 879831.7s

################################################################################
                    [1m Learning iteration 3971/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.681s, learning 0.190s)
               Value function loss: 4.3483
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 253.66
               Mean episode length: 239.93
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 8.87s
                        Total time: 36391.37s
                               ETA: 879815.5s

################################################################################
                    [1m Learning iteration 3972/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.664s, learning 0.223s)
               Value function loss: 4.2782
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 255.64
               Mean episode length: 239.71
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65093632
                    Iteration time: 8.89s
                        Total time: 36400.26s
                               ETA: 879799.7s

################################################################################
                    [1m Learning iteration 3973/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.770s, learning 0.189s)
               Value function loss: 4.7199
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 256.79
               Mean episode length: 240.85
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65110016
                    Iteration time: 8.96s
                        Total time: 36409.22s
                               ETA: 879785.6s

################################################################################
                    [1m Learning iteration 3974/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.803s, learning 0.197s)
               Value function loss: 4.6927
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 255.94
               Mean episode length: 242.13
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65126400
                    Iteration time: 9.00s
                        Total time: 36418.22s
                               ETA: 879772.6s

################################################################################
                    [1m Learning iteration 3975/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.589s, learning 0.225s)
               Value function loss: 5.5110
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 247.85
               Mean episode length: 233.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65142784
                    Iteration time: 8.81s
                        Total time: 36427.03s
                               ETA: 879755.0s

################################################################################
                    [1m Learning iteration 3976/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.463s, learning 0.187s)
               Value function loss: 4.9025
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 247.65
               Mean episode length: 237.38
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65159168
                    Iteration time: 8.65s
                        Total time: 36435.68s
                               ETA: 879733.5s

################################################################################
                    [1m Learning iteration 3977/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.467s, learning 0.182s)
               Value function loss: 5.1508
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 258.89
               Mean episode length: 247.66
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 8.65s
                        Total time: 36444.33s
                               ETA: 879712.0s

################################################################################
                    [1m Learning iteration 3978/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.487s, learning 0.176s)
               Value function loss: 4.6385
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 253.41
               Mean episode length: 246.54
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65191936
                    Iteration time: 8.66s
                        Total time: 36453.00s
                               ETA: 879690.8s

################################################################################
                    [1m Learning iteration 3979/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.400s, learning 0.184s)
               Value function loss: 4.3376
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 244.08
               Mean episode length: 239.08
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65208320
                    Iteration time: 8.58s
                        Total time: 36461.58s
                               ETA: 879667.7s

################################################################################
                    [1m Learning iteration 3980/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.780s, learning 0.195s)
               Value function loss: 5.4484
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 239.71
               Mean episode length: 232.50
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65224704
                    Iteration time: 8.98s
                        Total time: 36470.56s
                               ETA: 879654.1s

################################################################################
                    [1m Learning iteration 3981/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.532s, learning 0.193s)
               Value function loss: 5.4051
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 241.14
               Mean episode length: 233.35
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65241088
                    Iteration time: 8.73s
                        Total time: 36479.28s
                               ETA: 879634.4s

################################################################################
                    [1m Learning iteration 3982/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.844s, learning 0.188s)
               Value function loss: 6.4684
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 250.43
               Mean episode length: 239.96
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65257472
                    Iteration time: 9.03s
                        Total time: 36488.31s
                               ETA: 879622.1s

################################################################################
                    [1m Learning iteration 3983/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.280s, learning 0.191s)
               Value function loss: 5.1181
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 243.58
               Mean episode length: 234.95
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 8.47s
                        Total time: 36496.79s
                               ETA: 879596.3s

################################################################################
                    [1m Learning iteration 3984/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.660s, learning 0.186s)
               Value function loss: 4.2593
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 257.67
               Mean episode length: 243.60
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65290240
                    Iteration time: 8.85s
                        Total time: 36505.63s
                               ETA: 879579.6s

################################################################################
                    [1m Learning iteration 3985/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.565s, learning 0.179s)
               Value function loss: 4.8104
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 254.83
               Mean episode length: 242.53
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65306624
                    Iteration time: 8.74s
                        Total time: 36514.38s
                               ETA: 879560.4s

################################################################################
                    [1m Learning iteration 3986/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.473s, learning 0.188s)
               Value function loss: 5.1421
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 248.27
               Mean episode length: 241.53
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65323008
                    Iteration time: 8.66s
                        Total time: 36523.04s
                               ETA: 879539.2s

################################################################################
                    [1m Learning iteration 3987/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.543s, learning 0.176s)
               Value function loss: 5.6513
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 246.08
               Mean episode length: 238.81
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65339392
                    Iteration time: 8.72s
                        Total time: 36531.76s
                               ETA: 879519.4s

################################################################################
                    [1m Learning iteration 3988/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.466s, learning 0.302s)
               Value function loss: 2.9855
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 244.95
               Mean episode length: 236.89
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65355776
                    Iteration time: 8.77s
                        Total time: 36540.52s
                               ETA: 879500.8s

################################################################################
                    [1m Learning iteration 3989/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.532s, learning 0.336s)
               Value function loss: 4.6890
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 252.51
               Mean episode length: 240.98
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 8.87s
                        Total time: 36549.39s
                               ETA: 879484.6s

################################################################################
                    [1m Learning iteration 3990/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.272s, learning 0.184s)
               Value function loss: 5.3457
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 256.54
               Mean episode length: 243.15
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65388544
                    Iteration time: 8.46s
                        Total time: 36557.85s
                               ETA: 879458.5s

################################################################################
                    [1m Learning iteration 3991/100000 [0m                    

                       Computation: 1780 steps/s (collection: 8.976s, learning 0.226s)
               Value function loss: 6.2707
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 250.11
               Mean episode length: 239.53
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65404928
                    Iteration time: 9.20s
                        Total time: 36567.05s
                               ETA: 879450.4s

################################################################################
                    [1m Learning iteration 3992/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.355s, learning 0.268s)
               Value function loss: 4.4343
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 250.49
               Mean episode length: 240.50
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65421312
                    Iteration time: 8.62s
                        Total time: 36575.67s
                               ETA: 879428.3s

################################################################################
                    [1m Learning iteration 3993/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.409s, learning 0.180s)
               Value function loss: 4.3092
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 247.81
               Mean episode length: 237.44
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65437696
                    Iteration time: 8.59s
                        Total time: 36584.26s
                               ETA: 879405.4s

################################################################################
                    [1m Learning iteration 3994/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.849s, learning 0.174s)
               Value function loss: 4.5542
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 248.09
               Mean episode length: 236.23
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65454080
                    Iteration time: 9.02s
                        Total time: 36593.29s
                               ETA: 879393.0s

################################################################################
                    [1m Learning iteration 3995/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.343s, learning 0.183s)
               Value function loss: 4.9631
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 264.38
               Mean episode length: 248.09
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 8.53s
                        Total time: 36601.81s
                               ETA: 879368.6s

################################################################################
                    [1m Learning iteration 3996/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.414s, learning 0.189s)
               Value function loss: 5.9967
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 263.19
               Mean episode length: 243.46
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65486848
                    Iteration time: 8.60s
                        Total time: 36610.42s
                               ETA: 879346.1s

################################################################################
                    [1m Learning iteration 3997/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.385s, learning 0.189s)
               Value function loss: 5.5427
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 260.20
               Mean episode length: 244.83
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65503232
                    Iteration time: 8.57s
                        Total time: 36618.99s
                               ETA: 879322.9s

################################################################################
                    [1m Learning iteration 3998/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.313s, learning 0.170s)
               Value function loss: 5.6787
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 257.85
               Mean episode length: 242.87
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65519616
                    Iteration time: 8.48s
                        Total time: 36627.47s
                               ETA: 879297.5s

################################################################################
                    [1m Learning iteration 3999/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.460s, learning 0.180s)
               Value function loss: 4.3789
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 252.55
               Mean episode length: 238.14
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536000
                    Iteration time: 8.64s
                        Total time: 36636.11s
                               ETA: 879275.9s

################################################################################
                    [1m Learning iteration 4000/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.413s, learning 0.206s)
               Value function loss: 4.8932
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 257.24
               Mean episode length: 245.15
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65552384
                    Iteration time: 8.62s
                        Total time: 36644.73s
                               ETA: 879253.7s

################################################################################
                    [1m Learning iteration 4001/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.223s, learning 0.301s)
               Value function loss: 4.7615
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 263.71
               Mean episode length: 246.41
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 8.52s
                        Total time: 36653.26s
                               ETA: 879229.4s

################################################################################
                    [1m Learning iteration 4002/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.233s, learning 0.200s)
               Value function loss: 3.7699
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 262.00
               Mean episode length: 243.91
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65585152
                    Iteration time: 8.43s
                        Total time: 36661.69s
                               ETA: 879202.8s

################################################################################
                    [1m Learning iteration 4003/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.425s, learning 0.191s)
               Value function loss: 4.7503
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 251.45
               Mean episode length: 235.69
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65601536
                    Iteration time: 8.62s
                        Total time: 36670.31s
                               ETA: 879180.6s

################################################################################
                    [1m Learning iteration 4004/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.516s, learning 0.196s)
               Value function loss: 5.1191
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 252.41
               Mean episode length: 238.31
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65617920
                    Iteration time: 8.71s
                        Total time: 36679.02s
                               ETA: 879160.8s

################################################################################
                    [1m Learning iteration 4005/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.604s, learning 0.167s)
               Value function loss: 4.3905
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 258.04
               Mean episode length: 240.37
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65634304
                    Iteration time: 8.77s
                        Total time: 36687.79s
                               ETA: 879142.3s

################################################################################
                    [1m Learning iteration 4006/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.626s, learning 0.191s)
               Value function loss: 5.1084
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 265.54
               Mean episode length: 244.40
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65650688
                    Iteration time: 8.82s
                        Total time: 36696.60s
                               ETA: 879125.0s

################################################################################
                    [1m Learning iteration 4007/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.571s, learning 0.170s)
               Value function loss: 5.2410
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 259.57
               Mean episode length: 242.60
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 8.74s
                        Total time: 36705.35s
                               ETA: 879105.8s

################################################################################
                    [1m Learning iteration 4008/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.286s, learning 0.187s)
               Value function loss: 5.1288
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 254.91
               Mean episode length: 242.66
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65683456
                    Iteration time: 8.47s
                        Total time: 36713.82s
                               ETA: 879080.3s

################################################################################
                    [1m Learning iteration 4009/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.741s, learning 0.208s)
               Value function loss: 4.6850
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 261.86
               Mean episode length: 244.50
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65699840
                    Iteration time: 8.95s
                        Total time: 36722.77s
                               ETA: 879066.1s

################################################################################
                    [1m Learning iteration 4010/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.531s, learning 0.175s)
               Value function loss: 5.0105
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 264.87
               Mean episode length: 245.14
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65716224
                    Iteration time: 8.71s
                        Total time: 36731.47s
                               ETA: 879046.1s

################################################################################
                    [1m Learning iteration 4011/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.650s, learning 0.217s)
               Value function loss: 4.3486
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 260.08
               Mean episode length: 241.98
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65732608
                    Iteration time: 8.87s
                        Total time: 36740.34s
                               ETA: 879030.0s

################################################################################
                    [1m Learning iteration 4012/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.325s, learning 0.223s)
               Value function loss: 5.6584
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 257.85
               Mean episode length: 241.14
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65748992
                    Iteration time: 8.55s
                        Total time: 36748.89s
                               ETA: 879006.3s

################################################################################
                    [1m Learning iteration 4013/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.598s, learning 0.176s)
               Value function loss: 6.0542
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 263.40
               Mean episode length: 245.03
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 8.77s
                        Total time: 36757.66s
                               ETA: 878988.0s

################################################################################
                    [1m Learning iteration 4014/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.242s, learning 0.171s)
               Value function loss: 5.2826
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 264.12
               Mean episode length: 245.97
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65781760
                    Iteration time: 8.41s
                        Total time: 36766.07s
                               ETA: 878961.0s

################################################################################
                    [1m Learning iteration 4015/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.465s, learning 0.175s)
               Value function loss: 4.6368
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 259.23
               Mean episode length: 245.22
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65798144
                    Iteration time: 8.64s
                        Total time: 36774.71s
                               ETA: 878939.5s

################################################################################
                    [1m Learning iteration 4016/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.490s, learning 0.167s)
               Value function loss: 4.1031
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 258.25
               Mean episode length: 240.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65814528
                    Iteration time: 8.66s
                        Total time: 36783.37s
                               ETA: 878918.4s

################################################################################
                    [1m Learning iteration 4017/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.440s, learning 0.175s)
               Value function loss: 5.1929
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 266.59
               Mean episode length: 247.16
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65830912
                    Iteration time: 8.61s
                        Total time: 36791.99s
                               ETA: 878896.3s

################################################################################
                    [1m Learning iteration 4018/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.548s, learning 0.201s)
               Value function loss: 4.8983
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 266.69
               Mean episode length: 247.88
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65847296
                    Iteration time: 8.75s
                        Total time: 36800.73s
                               ETA: 878877.4s

################################################################################
                    [1m Learning iteration 4019/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.283s, learning 0.180s)
               Value function loss: 3.5970
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 262.60
               Mean episode length: 245.41
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 8.46s
                        Total time: 36809.20s
                               ETA: 878851.6s

################################################################################
                    [1m Learning iteration 4020/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.630s, learning 0.193s)
               Value function loss: 3.6572
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 258.01
               Mean episode length: 242.09
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65880064
                    Iteration time: 8.82s
                        Total time: 36818.02s
                               ETA: 878834.5s

################################################################################
                    [1m Learning iteration 4021/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.752s, learning 0.219s)
               Value function loss: 4.5590
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 258.31
               Mean episode length: 242.16
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65896448
                    Iteration time: 8.97s
                        Total time: 36826.99s
                               ETA: 878820.9s

################################################################################
                    [1m Learning iteration 4022/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.621s, learning 0.173s)
               Value function loss: 6.5511
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 256.67
               Mean episode length: 239.88
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65912832
                    Iteration time: 8.79s
                        Total time: 36835.79s
                               ETA: 878803.2s

################################################################################
                    [1m Learning iteration 4023/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.703s, learning 0.165s)
               Value function loss: 5.3526
                    Surrogate loss: 0.0018
             Mean action noise std: 0.75
                       Mean reward: 254.31
               Mean episode length: 238.11
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65929216
                    Iteration time: 8.87s
                        Total time: 36844.65s
                               ETA: 878787.1s

################################################################################
                    [1m Learning iteration 4024/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.471s, learning 0.307s)
               Value function loss: 4.3857
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 253.60
               Mean episode length: 238.70
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65945600
                    Iteration time: 8.78s
                        Total time: 36853.43s
                               ETA: 878768.9s

################################################################################
                    [1m Learning iteration 4025/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.866s, learning 0.177s)
               Value function loss: 4.5143
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 254.06
               Mean episode length: 239.84
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 8.04s
                        Total time: 36861.48s
                               ETA: 878733.3s

################################################################################
                    [1m Learning iteration 4026/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.360s, learning 0.168s)
               Value function loss: 4.8159
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 251.82
               Mean episode length: 238.64
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65978368
                    Iteration time: 8.53s
                        Total time: 36870.00s
                               ETA: 878709.1s

################################################################################
                    [1m Learning iteration 4027/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.341s, learning 0.239s)
               Value function loss: 5.5573
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 245.98
               Mean episode length: 233.34
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65994752
                    Iteration time: 8.58s
                        Total time: 36878.58s
                               ETA: 878686.2s

################################################################################
                    [1m Learning iteration 4028/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.514s, learning 0.211s)
               Value function loss: 5.1956
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 260.50
               Mean episode length: 244.06
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66011136
                    Iteration time: 8.72s
                        Total time: 36887.31s
                               ETA: 878666.8s

################################################################################
                    [1m Learning iteration 4029/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.122s, learning 0.198s)
               Value function loss: 4.9564
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 261.38
               Mean episode length: 244.14
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66027520
                    Iteration time: 8.32s
                        Total time: 36895.63s
                               ETA: 878637.8s

################################################################################
                    [1m Learning iteration 4030/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.654s, learning 0.182s)
               Value function loss: 3.8273
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 256.16
               Mean episode length: 239.90
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66043904
                    Iteration time: 8.84s
                        Total time: 36904.46s
                               ETA: 878621.0s

################################################################################
                    [1m Learning iteration 4031/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.616s, learning 0.210s)
               Value function loss: 4.4229
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 262.31
               Mean episode length: 244.38
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 8.83s
                        Total time: 36913.29s
                               ETA: 878604.0s

################################################################################
                    [1m Learning iteration 4032/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.377s, learning 0.170s)
               Value function loss: 4.6303
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 252.42
               Mean episode length: 241.77
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66076672
                    Iteration time: 8.55s
                        Total time: 36921.84s
                               ETA: 878580.4s

################################################################################
                    [1m Learning iteration 4033/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.364s, learning 0.187s)
               Value function loss: 3.1153
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 262.37
               Mean episode length: 248.63
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66093056
                    Iteration time: 8.55s
                        Total time: 36930.39s
                               ETA: 878556.9s

################################################################################
                    [1m Learning iteration 4034/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.456s, learning 0.261s)
               Value function loss: 4.3757
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 258.51
               Mean episode length: 242.11
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66109440
                    Iteration time: 8.72s
                        Total time: 36939.10s
                               ETA: 878537.3s

################################################################################
                    [1m Learning iteration 4035/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.650s, learning 0.249s)
               Value function loss: 4.4207
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 251.35
               Mean episode length: 238.49
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66125824
                    Iteration time: 8.90s
                        Total time: 36948.00s
                               ETA: 878522.1s

################################################################################
                    [1m Learning iteration 4036/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.335s, learning 0.215s)
               Value function loss: 5.1027
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 255.39
               Mean episode length: 239.06
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66142208
                    Iteration time: 8.55s
                        Total time: 36956.55s
                               ETA: 878498.5s

################################################################################
                    [1m Learning iteration 4037/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.575s, learning 0.182s)
               Value function loss: 4.5082
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 255.96
               Mean episode length: 238.59
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 8.76s
                        Total time: 36965.31s
                               ETA: 878479.9s

################################################################################
                    [1m Learning iteration 4038/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.495s, learning 0.253s)
               Value function loss: 5.6041
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 266.34
               Mean episode length: 248.35
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66174976
                    Iteration time: 8.75s
                        Total time: 36974.06s
                               ETA: 878461.1s

################################################################################
                    [1m Learning iteration 4039/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.449s, learning 0.206s)
               Value function loss: 4.4916
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 262.60
               Mean episode length: 242.77
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66191360
                    Iteration time: 8.66s
                        Total time: 36982.71s
                               ETA: 878440.1s

################################################################################
                    [1m Learning iteration 4040/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.318s, learning 0.172s)
               Value function loss: 4.6521
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 264.98
               Mean episode length: 245.83
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66207744
                    Iteration time: 8.49s
                        Total time: 36991.20s
                               ETA: 878415.2s

################################################################################
                    [1m Learning iteration 4041/100000 [0m                    

                       Computation: 1786 steps/s (collection: 9.001s, learning 0.172s)
               Value function loss: 4.6633
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 263.36
               Mean episode length: 244.67
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66224128
                    Iteration time: 9.17s
                        Total time: 37000.38s
                               ETA: 878406.5s

################################################################################
                    [1m Learning iteration 4042/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.403s, learning 0.186s)
               Value function loss: 4.3706
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 260.27
               Mean episode length: 240.88
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66240512
                    Iteration time: 8.59s
                        Total time: 37008.97s
                               ETA: 878383.9s

################################################################################
                    [1m Learning iteration 4043/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.608s, learning 0.177s)
               Value function loss: 4.4888
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 256.65
               Mean episode length: 237.41
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 8.79s
                        Total time: 37017.75s
                               ETA: 878366.0s

################################################################################
                    [1m Learning iteration 4044/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.578s, learning 0.246s)
               Value function loss: 5.0851
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 270.18
               Mean episode length: 248.43
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66273280
                    Iteration time: 8.82s
                        Total time: 37026.57s
                               ETA: 878349.1s

################################################################################
                    [1m Learning iteration 4045/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.301s, learning 0.172s)
               Value function loss: 5.8602
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 261.02
               Mean episode length: 242.60
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66289664
                    Iteration time: 8.47s
                        Total time: 37035.05s
                               ETA: 878323.8s

################################################################################
                    [1m Learning iteration 4046/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.504s, learning 0.182s)
               Value function loss: 4.7021
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 259.63
               Mean episode length: 242.99
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66306048
                    Iteration time: 8.69s
                        Total time: 37043.73s
                               ETA: 878303.5s

################################################################################
                    [1m Learning iteration 4047/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.813s, learning 0.197s)
               Value function loss: 5.0802
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 259.02
               Mean episode length: 240.32
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66322432
                    Iteration time: 9.01s
                        Total time: 37052.74s
                               ETA: 878290.9s

################################################################################
                    [1m Learning iteration 4048/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.212s, learning 0.189s)
               Value function loss: 5.1672
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 261.75
               Mean episode length: 240.99
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66338816
                    Iteration time: 8.40s
                        Total time: 37061.14s
                               ETA: 878264.0s

################################################################################
                    [1m Learning iteration 4049/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.343s, learning 0.289s)
               Value function loss: 4.5111
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 259.36
               Mean episode length: 240.08
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 8.63s
                        Total time: 37069.78s
                               ETA: 878242.5s

################################################################################
                    [1m Learning iteration 4050/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.864s, learning 0.276s)
               Value function loss: 4.6241
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 264.98
               Mean episode length: 246.57
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66371584
                    Iteration time: 9.14s
                        Total time: 37078.91s
                               ETA: 878233.0s

################################################################################
                    [1m Learning iteration 4051/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.573s, learning 0.171s)
               Value function loss: 4.0450
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 258.56
               Mean episode length: 240.97
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66387968
                    Iteration time: 8.74s
                        Total time: 37087.66s
                               ETA: 878214.1s

################################################################################
                    [1m Learning iteration 4052/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.225s, learning 0.177s)
               Value function loss: 5.4078
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 262.57
               Mean episode length: 242.47
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66404352
                    Iteration time: 8.40s
                        Total time: 37096.06s
                               ETA: 878187.2s

################################################################################
                    [1m Learning iteration 4053/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.592s, learning 0.175s)
               Value function loss: 6.2985
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 259.99
               Mean episode length: 242.04
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66420736
                    Iteration time: 8.77s
                        Total time: 37104.83s
                               ETA: 878168.9s

################################################################################
                    [1m Learning iteration 4054/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.581s, learning 0.213s)
               Value function loss: 7.0382
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 268.62
               Mean episode length: 248.08
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66437120
                    Iteration time: 8.79s
                        Total time: 37113.62s
                               ETA: 878151.3s

################################################################################
                    [1m Learning iteration 4055/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.508s, learning 0.209s)
               Value function loss: 5.7249
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 266.48
               Mean episode length: 245.48
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 8.72s
                        Total time: 37122.34s
                               ETA: 878131.8s

################################################################################
                    [1m Learning iteration 4056/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.663s, learning 0.193s)
               Value function loss: 4.6555
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 268.18
               Mean episode length: 246.59
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66469888
                    Iteration time: 8.86s
                        Total time: 37131.19s
                               ETA: 878115.7s

################################################################################
                    [1m Learning iteration 4057/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.553s, learning 0.170s)
               Value function loss: 4.6057
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 263.34
               Mean episode length: 242.66
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66486272
                    Iteration time: 8.72s
                        Total time: 37139.92s
                               ETA: 878096.4s

################################################################################
                    [1m Learning iteration 4058/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.593s, learning 0.221s)
               Value function loss: 5.7050
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 266.36
               Mean episode length: 246.03
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66502656
                    Iteration time: 8.81s
                        Total time: 37148.73s
                               ETA: 878079.2s

################################################################################
                    [1m Learning iteration 4059/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.444s, learning 0.168s)
               Value function loss: 6.0883
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 269.16
               Mean episode length: 247.13
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66519040
                    Iteration time: 8.61s
                        Total time: 37157.34s
                               ETA: 878057.3s

################################################################################
                    [1m Learning iteration 4060/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.624s, learning 0.277s)
               Value function loss: 6.0539
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 267.75
               Mean episode length: 247.02
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66535424
                    Iteration time: 8.90s
                        Total time: 37166.25s
                               ETA: 878042.2s

################################################################################
                    [1m Learning iteration 4061/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.161s, learning 0.180s)
               Value function loss: 4.1747
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 263.89
               Mean episode length: 245.25
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 8.34s
                        Total time: 37174.59s
                               ETA: 878013.9s

################################################################################
                    [1m Learning iteration 4062/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.756s, learning 0.168s)
               Value function loss: 5.4203
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 264.17
               Mean episode length: 247.89
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66568192
                    Iteration time: 8.92s
                        Total time: 37183.51s
                               ETA: 877999.4s

################################################################################
                    [1m Learning iteration 4063/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.656s, learning 0.177s)
               Value function loss: 6.3280
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 262.10
               Mean episode length: 243.58
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66584576
                    Iteration time: 8.83s
                        Total time: 37192.34s
                               ETA: 877982.7s

################################################################################
                    [1m Learning iteration 4064/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.537s, learning 0.179s)
               Value function loss: 2.8553
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 264.09
               Mean episode length: 246.53
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66600960
                    Iteration time: 8.72s
                        Total time: 37201.06s
                               ETA: 877963.3s

################################################################################
                    [1m Learning iteration 4065/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.179s, learning 0.199s)
               Value function loss: 5.1885
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 253.89
               Mean episode length: 243.36
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66617344
                    Iteration time: 8.38s
                        Total time: 37209.44s
                               ETA: 877935.9s

################################################################################
                    [1m Learning iteration 4066/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.000s, learning 0.176s)
               Value function loss: 4.8047
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 256.86
               Mean episode length: 241.71
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66633728
                    Iteration time: 9.18s
                        Total time: 37218.61s
                               ETA: 877927.3s

################################################################################
                    [1m Learning iteration 4067/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.480s, learning 0.173s)
               Value function loss: 5.2746
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 263.45
               Mean episode length: 244.75
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 8.65s
                        Total time: 37227.27s
                               ETA: 877906.4s

################################################################################
                    [1m Learning iteration 4068/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.654s, learning 0.176s)
               Value function loss: 4.8405
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 259.57
               Mean episode length: 240.21
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66666496
                    Iteration time: 8.83s
                        Total time: 37236.10s
                               ETA: 877889.7s

################################################################################
                    [1m Learning iteration 4069/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.559s, learning 0.170s)
               Value function loss: 5.4752
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 254.73
               Mean episode length: 242.75
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66682880
                    Iteration time: 8.73s
                        Total time: 37244.82s
                               ETA: 877870.6s

################################################################################
                    [1m Learning iteration 4070/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.420s, learning 0.171s)
               Value function loss: 4.6705
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 267.31
               Mean episode length: 249.87
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66699264
                    Iteration time: 8.59s
                        Total time: 37253.42s
                               ETA: 877848.3s

################################################################################
                    [1m Learning iteration 4071/100000 [0m                    

                       Computation: 1779 steps/s (collection: 8.863s, learning 0.342s)
               Value function loss: 5.0419
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 266.71
               Mean episode length: 246.73
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66715648
                    Iteration time: 9.21s
                        Total time: 37262.62s
                               ETA: 877840.4s

################################################################################
                    [1m Learning iteration 4072/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.521s, learning 0.199s)
               Value function loss: 4.9629
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 262.67
               Mean episode length: 244.49
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66732032
                    Iteration time: 8.72s
                        Total time: 37271.34s
                               ETA: 877821.1s

################################################################################
                    [1m Learning iteration 4073/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.464s, learning 0.168s)
               Value function loss: 3.8938
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 256.15
               Mean episode length: 247.22
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 8.63s
                        Total time: 37279.97s
                               ETA: 877799.7s

################################################################################
                    [1m Learning iteration 4074/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.470s, learning 0.197s)
               Value function loss: 4.4897
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 255.84
               Mean episode length: 245.19
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66764800
                    Iteration time: 8.67s
                        Total time: 37288.64s
                               ETA: 877779.2s

################################################################################
                    [1m Learning iteration 4075/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.277s, learning 0.188s)
               Value function loss: 4.8992
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 260.88
               Mean episode length: 247.88
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66781184
                    Iteration time: 8.47s
                        Total time: 37297.11s
                               ETA: 877753.9s

################################################################################
                    [1m Learning iteration 4076/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.551s, learning 0.203s)
               Value function loss: 5.9565
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 261.50
               Mean episode length: 248.07
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66797568
                    Iteration time: 8.75s
                        Total time: 37305.86s
                               ETA: 877735.4s

################################################################################
                    [1m Learning iteration 4077/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.257s, learning 0.199s)
               Value function loss: 4.4753
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 256.08
               Mean episode length: 244.13
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66813952
                    Iteration time: 8.46s
                        Total time: 37314.32s
                               ETA: 877709.9s

################################################################################
                    [1m Learning iteration 4078/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.694s, learning 0.193s)
               Value function loss: 4.9837
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 256.27
               Mean episode length: 244.89
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66830336
                    Iteration time: 8.89s
                        Total time: 37323.20s
                               ETA: 877694.6s

################################################################################
                    [1m Learning iteration 4079/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.904s, learning 0.230s)
               Value function loss: 5.7291
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 263.35
               Mean episode length: 248.69
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 9.13s
                        Total time: 37332.34s
                               ETA: 877685.1s

################################################################################
                    [1m Learning iteration 4080/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.665s, learning 0.201s)
               Value function loss: 5.4356
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 258.51
               Mean episode length: 244.97
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66863104
                    Iteration time: 8.87s
                        Total time: 37341.20s
                               ETA: 877669.2s

################################################################################
                    [1m Learning iteration 4081/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.381s, learning 0.181s)
               Value function loss: 4.9007
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 263.10
               Mean episode length: 246.75
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66879488
                    Iteration time: 8.56s
                        Total time: 37349.76s
                               ETA: 877646.2s

################################################################################
                    [1m Learning iteration 4082/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.581s, learning 0.180s)
               Value function loss: 3.2344
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 260.14
               Mean episode length: 244.83
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66895872
                    Iteration time: 8.76s
                        Total time: 37358.52s
                               ETA: 877628.0s

################################################################################
                    [1m Learning iteration 4083/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.262s, learning 0.172s)
               Value function loss: 4.5426
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 259.86
               Mean episode length: 245.19
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66912256
                    Iteration time: 8.43s
                        Total time: 37366.96s
                               ETA: 877602.0s

################################################################################
                    [1m Learning iteration 4084/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.568s, learning 0.205s)
               Value function loss: 5.9306
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 269.71
               Mean episode length: 248.41
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66928640
                    Iteration time: 8.77s
                        Total time: 37375.73s
                               ETA: 877584.0s

################################################################################
                    [1m Learning iteration 4085/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.670s, learning 0.172s)
               Value function loss: 6.5356
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 265.91
               Mean episode length: 248.79
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 8.84s
                        Total time: 37384.57s
                               ETA: 877567.6s

################################################################################
                    [1m Learning iteration 4086/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.756s, learning 0.170s)
               Value function loss: 5.4174
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 259.38
               Mean episode length: 244.46
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66961408
                    Iteration time: 8.93s
                        Total time: 37393.50s
                               ETA: 877553.2s

################################################################################
                    [1m Learning iteration 4087/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.254s, learning 0.177s)
               Value function loss: 3.8638
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 263.27
               Mean episode length: 246.97
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66977792
                    Iteration time: 8.43s
                        Total time: 37401.93s
                               ETA: 877527.2s

################################################################################
                    [1m Learning iteration 4088/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.916s, learning 0.172s)
               Value function loss: 4.8770
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 266.17
               Mean episode length: 248.28
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66994176
                    Iteration time: 9.09s
                        Total time: 37411.02s
                               ETA: 877516.6s

################################################################################
                    [1m Learning iteration 4089/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.637s, learning 0.168s)
               Value function loss: 4.8838
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 260.40
               Mean episode length: 244.45
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67010560
                    Iteration time: 8.80s
                        Total time: 37419.82s
                               ETA: 877499.4s

################################################################################
                    [1m Learning iteration 4090/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.562s, learning 0.167s)
               Value function loss: 6.3745
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 269.52
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67026944
                    Iteration time: 8.73s
                        Total time: 37428.55s
                               ETA: 877480.4s

################################################################################
                    [1m Learning iteration 4091/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.586s, learning 0.208s)
               Value function loss: 5.7378
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 268.13
               Mean episode length: 249.04
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 8.79s
                        Total time: 37437.34s
                               ETA: 877462.9s

################################################################################
                    [1m Learning iteration 4092/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.733s, learning 0.199s)
               Value function loss: 5.2243
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 266.49
               Mean episode length: 246.22
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67059712
                    Iteration time: 8.93s
                        Total time: 37446.28s
                               ETA: 877448.7s

################################################################################
                    [1m Learning iteration 4093/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.151s, learning 0.166s)
               Value function loss: 4.5225
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 266.20
               Mean episode length: 247.82
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67076096
                    Iteration time: 8.32s
                        Total time: 37454.59s
                               ETA: 877420.1s

################################################################################
                    [1m Learning iteration 4094/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.700s, learning 0.203s)
               Value function loss: 5.6486
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 268.88
               Mean episode length: 249.45
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67092480
                    Iteration time: 8.90s
                        Total time: 37463.50s
                               ETA: 877405.2s

################################################################################
                    [1m Learning iteration 4095/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.389s, learning 0.183s)
               Value function loss: 3.4832
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 264.28
               Mean episode length: 246.24
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67108864
                    Iteration time: 8.57s
                        Total time: 37472.07s
                               ETA: 877382.5s

################################################################################
                    [1m Learning iteration 4096/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.728s, learning 0.194s)
               Value function loss: 4.7256
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 261.77
               Mean episode length: 245.16
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67125248
                    Iteration time: 8.92s
                        Total time: 37480.99s
                               ETA: 877368.1s

################################################################################
                    [1m Learning iteration 4097/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.774s, learning 0.213s)
               Value function loss: 5.4323
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 260.65
               Mean episode length: 242.62
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 8.99s
                        Total time: 37489.98s
                               ETA: 877355.2s

################################################################################
                    [1m Learning iteration 4098/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.538s, learning 0.211s)
               Value function loss: 5.0207
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 268.29
               Mean episode length: 247.58
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67158016
                    Iteration time: 8.75s
                        Total time: 37498.73s
                               ETA: 877336.7s

################################################################################
                    [1m Learning iteration 4099/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.319s, learning 0.219s)
               Value function loss: 5.0216
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 267.03
               Mean episode length: 245.90
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67174400
                    Iteration time: 8.54s
                        Total time: 37507.26s
                               ETA: 877313.2s

################################################################################
                    [1m Learning iteration 4100/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.525s, learning 0.251s)
               Value function loss: 5.3739
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 265.68
               Mean episode length: 247.40
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67190784
                    Iteration time: 8.78s
                        Total time: 37516.04s
                               ETA: 877295.4s

################################################################################
                    [1m Learning iteration 4101/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.549s, learning 0.191s)
               Value function loss: 4.8903
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 267.62
               Mean episode length: 247.01
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67207168
                    Iteration time: 8.74s
                        Total time: 37524.78s
                               ETA: 877276.7s

################################################################################
                    [1m Learning iteration 4102/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.732s, learning 0.188s)
               Value function loss: 4.6090
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 256.84
               Mean episode length: 238.45
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67223552
                    Iteration time: 8.92s
                        Total time: 37533.70s
                               ETA: 877262.2s

################################################################################
                    [1m Learning iteration 4103/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.682s, learning 0.187s)
               Value function loss: 5.1193
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 261.94
               Mean episode length: 242.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 8.87s
                        Total time: 37542.57s
                               ETA: 877246.5s

################################################################################
                    [1m Learning iteration 4104/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.750s, learning 0.250s)
               Value function loss: 4.4569
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 265.39
               Mean episode length: 246.07
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67256320
                    Iteration time: 9.00s
                        Total time: 37551.57s
                               ETA: 877233.9s

################################################################################
                    [1m Learning iteration 4105/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.645s, learning 0.217s)
               Value function loss: 5.9734
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 266.51
               Mean episode length: 245.22
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67272704
                    Iteration time: 8.86s
                        Total time: 37560.43s
                               ETA: 877218.1s

################################################################################
                    [1m Learning iteration 4106/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.907s, learning 0.228s)
               Value function loss: 5.1240
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 258.89
               Mean episode length: 239.65
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67289088
                    Iteration time: 9.13s
                        Total time: 37569.57s
                               ETA: 877208.7s

################################################################################
                    [1m Learning iteration 4107/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.294s, learning 0.207s)
               Value function loss: 6.5138
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 253.59
               Mean episode length: 240.81
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67305472
                    Iteration time: 9.50s
                        Total time: 37579.07s
                               ETA: 877207.8s

################################################################################
                    [1m Learning iteration 4108/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.096s, learning 0.175s)
               Value function loss: 5.3370
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 267.51
               Mean episode length: 249.38
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67321856
                    Iteration time: 9.27s
                        Total time: 37588.34s
                               ETA: 877201.5s

################################################################################
                    [1m Learning iteration 4109/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.174s)
               Value function loss: 3.8569
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 265.49
               Mean episode length: 248.40
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 8.86s
                        Total time: 37597.20s
                               ETA: 877185.7s

################################################################################
                    [1m Learning iteration 4110/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.774s, learning 0.319s)
               Value function loss: 4.8070
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 264.65
               Mean episode length: 246.26
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67354624
                    Iteration time: 9.09s
                        Total time: 37606.29s
                               ETA: 877175.3s

################################################################################
                    [1m Learning iteration 4111/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.902s, learning 0.192s)
               Value function loss: 6.6625
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 272.39
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67371008
                    Iteration time: 9.09s
                        Total time: 37615.39s
                               ETA: 877164.9s

################################################################################
                    [1m Learning iteration 4112/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.886s, learning 0.266s)
               Value function loss: 5.1261
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 266.16
               Mean episode length: 247.96
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67387392
                    Iteration time: 9.15s
                        Total time: 37624.54s
                               ETA: 877155.8s

################################################################################
                    [1m Learning iteration 4113/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.360s, learning 0.178s)
               Value function loss: 3.7465
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 262.65
               Mean episode length: 245.92
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67403776
                    Iteration time: 8.54s
                        Total time: 37633.08s
                               ETA: 877132.5s

################################################################################
                    [1m Learning iteration 4114/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.370s, learning 0.186s)
               Value function loss: 4.2490
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 260.15
               Mean episode length: 244.23
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67420160
                    Iteration time: 8.56s
                        Total time: 37641.63s
                               ETA: 877109.5s

################################################################################
                    [1m Learning iteration 4115/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.383s, learning 0.170s)
               Value function loss: 5.1515
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 265.12
               Mean episode length: 247.87
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 8.55s
                        Total time: 37650.19s
                               ETA: 877086.5s

################################################################################
                    [1m Learning iteration 4116/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.772s, learning 0.193s)
               Value function loss: 5.8195
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 259.50
               Mean episode length: 244.44
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67452928
                    Iteration time: 8.97s
                        Total time: 37659.15s
                               ETA: 877073.1s

################################################################################
                    [1m Learning iteration 4117/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.590s, learning 0.169s)
               Value function loss: 4.8031
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 259.89
               Mean episode length: 243.60
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67469312
                    Iteration time: 8.76s
                        Total time: 37667.91s
                               ETA: 877054.9s

################################################################################
                    [1m Learning iteration 4118/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.525s, learning 0.210s)
               Value function loss: 4.2870
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 256.27
               Mean episode length: 243.04
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67485696
                    Iteration time: 8.73s
                        Total time: 37676.64s
                               ETA: 877036.2s

################################################################################
                    [1m Learning iteration 4119/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.709s, learning 0.174s)
               Value function loss: 4.9233
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 255.03
               Mean episode length: 242.65
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67502080
                    Iteration time: 8.88s
                        Total time: 37685.53s
                               ETA: 877020.9s

################################################################################
                    [1m Learning iteration 4120/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.539s, learning 0.176s)
               Value function loss: 5.3831
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 262.00
               Mean episode length: 247.93
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67518464
                    Iteration time: 8.72s
                        Total time: 37694.24s
                               ETA: 877001.7s

################################################################################
                    [1m Learning iteration 4121/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.780s, learning 0.165s)
               Value function loss: 6.3895
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 258.42
               Mean episode length: 242.01
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 8.95s
                        Total time: 37703.19s
                               ETA: 876987.9s

################################################################################
                    [1m Learning iteration 4122/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.753s, learning 0.156s)
               Value function loss: 5.9314
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 262.03
               Mean episode length: 244.08
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67551232
                    Iteration time: 8.91s
                        Total time: 37712.10s
                               ETA: 876973.2s

################################################################################
                    [1m Learning iteration 4123/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.658s, learning 0.164s)
               Value function loss: 5.1725
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 259.17
               Mean episode length: 243.78
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67567616
                    Iteration time: 8.82s
                        Total time: 37720.92s
                               ETA: 876956.5s

################################################################################
                    [1m Learning iteration 4124/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.792s, learning 0.201s)
               Value function loss: 4.0518
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 251.18
               Mean episode length: 239.20
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67584000
                    Iteration time: 8.99s
                        Total time: 37729.91s
                               ETA: 876943.8s

################################################################################
                    [1m Learning iteration 4125/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.772s, learning 0.165s)
               Value function loss: 5.3013
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 256.35
               Mean episode length: 243.71
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67600384
                    Iteration time: 8.94s
                        Total time: 37738.85s
                               ETA: 876929.8s

################################################################################
                    [1m Learning iteration 4126/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.501s, learning 0.168s)
               Value function loss: 4.6561
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 259.53
               Mean episode length: 247.35
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67616768
                    Iteration time: 8.67s
                        Total time: 37747.52s
                               ETA: 876909.5s

################################################################################
                    [1m Learning iteration 4127/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.829s, learning 0.272s)
               Value function loss: 4.2424
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 257.65
               Mean episode length: 241.96
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 9.10s
                        Total time: 37756.62s
                               ETA: 876899.3s

################################################################################
                    [1m Learning iteration 4128/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.907s, learning 0.160s)
               Value function loss: 5.6030
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 257.65
               Mean episode length: 240.06
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67649536
                    Iteration time: 9.07s
                        Total time: 37765.69s
                               ETA: 876888.3s

################################################################################
                    [1m Learning iteration 4129/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.293s, learning 0.171s)
               Value function loss: 5.2392
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 258.83
               Mean episode length: 241.08
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67665920
                    Iteration time: 8.46s
                        Total time: 37774.15s
                               ETA: 876863.3s

################################################################################
                    [1m Learning iteration 4130/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.819s, learning 0.188s)
               Value function loss: 4.3554
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 257.59
               Mean episode length: 242.69
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67682304
                    Iteration time: 9.01s
                        Total time: 37783.16s
                               ETA: 876851.0s

################################################################################
                    [1m Learning iteration 4131/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.453s, learning 0.171s)
               Value function loss: 5.2718
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 259.32
               Mean episode length: 244.83
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67698688
                    Iteration time: 8.62s
                        Total time: 37791.78s
                               ETA: 876829.7s

################################################################################
                    [1m Learning iteration 4132/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.632s, learning 0.210s)
               Value function loss: 5.7934
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 258.28
               Mean episode length: 242.55
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67715072
                    Iteration time: 8.84s
                        Total time: 37800.62s
                               ETA: 876813.5s

################################################################################
                    [1m Learning iteration 4133/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.856s, learning 0.262s)
               Value function loss: 5.8823
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 261.43
               Mean episode length: 242.94
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 9.12s
                        Total time: 37809.74s
                               ETA: 876803.7s

################################################################################
                    [1m Learning iteration 4134/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.920s, learning 0.187s)
               Value function loss: 6.0617
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 256.98
               Mean episode length: 241.76
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67747840
                    Iteration time: 9.11s
                        Total time: 37818.85s
                               ETA: 876793.7s

################################################################################
                    [1m Learning iteration 4135/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.509s, learning 0.174s)
               Value function loss: 5.0411
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 257.28
               Mean episode length: 243.35
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67764224
                    Iteration time: 8.68s
                        Total time: 37827.53s
                               ETA: 876773.8s

################################################################################
                    [1m Learning iteration 4136/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.637s, learning 0.173s)
               Value function loss: 5.1313
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 265.05
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67780608
                    Iteration time: 8.81s
                        Total time: 37836.34s
                               ETA: 876756.9s

################################################################################
                    [1m Learning iteration 4137/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.348s, learning 0.194s)
               Value function loss: 5.6753
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 258.00
               Mean episode length: 246.32
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67796992
                    Iteration time: 8.54s
                        Total time: 37844.88s
                               ETA: 876733.7s

################################################################################
                    [1m Learning iteration 4138/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.464s, learning 0.268s)
               Value function loss: 6.8090
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 268.83
               Mean episode length: 248.61
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67813376
                    Iteration time: 8.73s
                        Total time: 37853.62s
                               ETA: 876715.0s

################################################################################
                    [1m Learning iteration 4139/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.573s, learning 0.162s)
               Value function loss: 5.8662
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 265.75
               Mean episode length: 246.65
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 8.73s
                        Total time: 37862.35s
                               ETA: 876696.3s

################################################################################
                    [1m Learning iteration 4140/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.481s, learning 0.164s)
               Value function loss: 4.6090
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 261.36
               Mean episode length: 245.52
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67846144
                    Iteration time: 8.65s
                        Total time: 37871.00s
                               ETA: 876675.6s

################################################################################
                    [1m Learning iteration 4141/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.576s, learning 0.184s)
               Value function loss: 5.0100
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 264.46
               Mean episode length: 246.36
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67862528
                    Iteration time: 8.76s
                        Total time: 37879.76s
                               ETA: 876657.5s

################################################################################
                    [1m Learning iteration 4142/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.430s, learning 0.205s)
               Value function loss: 6.1309
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 260.73
               Mean episode length: 245.40
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67878912
                    Iteration time: 8.64s
                        Total time: 37888.39s
                               ETA: 876636.6s

################################################################################
                    [1m Learning iteration 4143/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.688s, learning 0.160s)
               Value function loss: 4.8669
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 254.08
               Mean episode length: 240.86
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67895296
                    Iteration time: 8.85s
                        Total time: 37897.24s
                               ETA: 876620.6s

################################################################################
                    [1m Learning iteration 4144/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.896s, learning 0.167s)
               Value function loss: 4.4320
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 257.73
               Mean episode length: 242.17
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67911680
                    Iteration time: 9.06s
                        Total time: 37906.30s
                               ETA: 876609.6s

################################################################################
                    [1m Learning iteration 4145/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.687s, learning 0.244s)
               Value function loss: 3.6459
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 261.77
               Mean episode length: 243.08
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 8.93s
                        Total time: 37915.24s
                               ETA: 876595.5s

################################################################################
                    [1m Learning iteration 4146/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.436s, learning 0.172s)
               Value function loss: 4.9758
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 260.59
               Mean episode length: 242.93
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67944448
                    Iteration time: 8.61s
                        Total time: 37923.84s
                               ETA: 876573.9s

################################################################################
                    [1m Learning iteration 4147/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.210s, learning 0.173s)
               Value function loss: 6.8117
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 268.47
               Mean episode length: 248.60
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67960832
                    Iteration time: 8.38s
                        Total time: 37932.23s
                               ETA: 876547.2s

################################################################################
                    [1m Learning iteration 4148/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.509s, learning 0.180s)
               Value function loss: 5.9530
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 259.65
               Mean episode length: 242.70
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67977216
                    Iteration time: 8.69s
                        Total time: 37940.91s
                               ETA: 876527.5s

################################################################################
                    [1m Learning iteration 4149/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.558s, learning 0.258s)
               Value function loss: 4.8750
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 260.09
               Mean episode length: 240.74
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67993600
                    Iteration time: 8.82s
                        Total time: 37949.73s
                               ETA: 876510.8s

################################################################################
                    [1m Learning iteration 4150/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.530s, learning 0.172s)
               Value function loss: 5.4552
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 269.37
               Mean episode length: 247.95
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68009984
                    Iteration time: 8.70s
                        Total time: 37958.43s
                               ETA: 876491.4s

################################################################################
                    [1m Learning iteration 4151/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.896s, learning 0.183s)
               Value function loss: 5.0307
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 268.58
               Mean episode length: 247.87
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 9.08s
                        Total time: 37967.51s
                               ETA: 876480.7s

################################################################################
                    [1m Learning iteration 4152/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.551s, learning 0.169s)
               Value function loss: 7.3375
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 267.89
               Mean episode length: 246.69
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68042752
                    Iteration time: 8.72s
                        Total time: 37976.23s
                               ETA: 876461.8s

################################################################################
                    [1m Learning iteration 4153/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.661s, learning 0.176s)
               Value function loss: 6.2794
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 261.11
               Mean episode length: 238.63
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68059136
                    Iteration time: 8.84s
                        Total time: 37985.07s
                               ETA: 876445.6s

################################################################################
                    [1m Learning iteration 4154/100000 [0m                    

                       Computation: 1770 steps/s (collection: 9.035s, learning 0.217s)
               Value function loss: 6.6139
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 262.18
               Mean episode length: 239.78
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68075520
                    Iteration time: 9.25s
                        Total time: 37994.32s
                               ETA: 876438.9s

################################################################################
                    [1m Learning iteration 4155/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.670s, learning 0.204s)
               Value function loss: 4.7560
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 263.47
               Mean episode length: 245.78
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68091904
                    Iteration time: 8.87s
                        Total time: 38003.20s
                               ETA: 876423.5s

################################################################################
                    [1m Learning iteration 4156/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.598s, learning 0.167s)
               Value function loss: 4.7310
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 261.29
               Mean episode length: 246.36
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68108288
                    Iteration time: 8.76s
                        Total time: 38011.96s
                               ETA: 876405.6s

################################################################################
                    [1m Learning iteration 4157/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.500s, learning 0.187s)
               Value function loss: 5.4372
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 260.55
               Mean episode length: 245.64
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 8.69s
                        Total time: 38020.65s
                               ETA: 876386.0s

################################################################################
                    [1m Learning iteration 4158/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.382s, learning 0.172s)
               Value function loss: 3.8805
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 257.41
               Mean episode length: 241.89
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68141056
                    Iteration time: 8.55s
                        Total time: 38029.20s
                               ETA: 876363.2s

################################################################################
                    [1m Learning iteration 4159/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.508s, learning 0.165s)
               Value function loss: 5.2696
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 261.59
               Mean episode length: 241.47
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68157440
                    Iteration time: 8.67s
                        Total time: 38037.87s
                               ETA: 876343.2s

################################################################################
                    [1m Learning iteration 4160/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.573s, learning 0.159s)
               Value function loss: 5.1912
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 264.84
               Mean episode length: 244.73
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68173824
                    Iteration time: 8.73s
                        Total time: 38046.61s
                               ETA: 876324.6s

################################################################################
                    [1m Learning iteration 4161/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.248s, learning 0.188s)
               Value function loss: 5.0446
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 257.94
               Mean episode length: 240.65
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68190208
                    Iteration time: 9.44s
                        Total time: 38056.04s
                               ETA: 876322.2s

################################################################################
                    [1m Learning iteration 4162/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.505s, learning 0.164s)
               Value function loss: 4.8629
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 263.07
               Mean episode length: 244.82
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68206592
                    Iteration time: 8.67s
                        Total time: 38064.71s
                               ETA: 876302.1s

################################################################################
                    [1m Learning iteration 4163/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.735s, learning 0.176s)
               Value function loss: 4.9063
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 267.73
               Mean episode length: 249.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 8.91s
                        Total time: 38073.62s
                               ETA: 876287.6s

################################################################################
                    [1m Learning iteration 4164/100000 [0m                    

                       Computation: 1785 steps/s (collection: 8.963s, learning 0.214s)
               Value function loss: 5.4250
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 256.69
               Mean episode length: 239.33
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68239360
                    Iteration time: 9.18s
                        Total time: 38082.80s
                               ETA: 876279.2s

################################################################################
                    [1m Learning iteration 4165/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.794s, learning 0.178s)
               Value function loss: 5.3958
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 264.77
               Mean episode length: 243.24
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68255744
                    Iteration time: 8.97s
                        Total time: 38091.77s
                               ETA: 876266.1s

################################################################################
                    [1m Learning iteration 4166/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.752s, learning 0.175s)
               Value function loss: 4.5898
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 264.32
               Mean episode length: 245.52
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68272128
                    Iteration time: 8.93s
                        Total time: 38100.70s
                               ETA: 876252.0s

################################################################################
                    [1m Learning iteration 4167/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.516s, learning 0.178s)
               Value function loss: 4.9016
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 268.71
               Mean episode length: 249.17
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68288512
                    Iteration time: 8.69s
                        Total time: 38109.39s
                               ETA: 876232.6s

################################################################################
                    [1m Learning iteration 4168/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.434s, learning 0.162s)
               Value function loss: 5.5609
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 264.68
               Mean episode length: 246.48
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68304896
                    Iteration time: 8.60s
                        Total time: 38117.99s
                               ETA: 876210.9s

################################################################################
                    [1m Learning iteration 4169/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.150s, learning 0.160s)
               Value function loss: 5.9267
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 270.05
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 8.31s
                        Total time: 38126.30s
                               ETA: 876182.6s

################################################################################
                    [1m Learning iteration 4170/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.913s, learning 0.165s)
               Value function loss: 5.6982
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 267.53
               Mean episode length: 247.92
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68337664
                    Iteration time: 9.08s
                        Total time: 38135.38s
                               ETA: 876171.9s

################################################################################
                    [1m Learning iteration 4171/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.551s, learning 0.163s)
               Value function loss: 5.1585
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 264.34
               Mean episode length: 245.93
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68354048
                    Iteration time: 8.71s
                        Total time: 38144.09s
                               ETA: 876152.9s

################################################################################
                    [1m Learning iteration 4172/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.361s, learning 0.193s)
               Value function loss: 4.4131
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 263.29
               Mean episode length: 244.71
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68370432
                    Iteration time: 8.55s
                        Total time: 38152.64s
                               ETA: 876130.3s

################################################################################
                    [1m Learning iteration 4173/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.645s, learning 0.190s)
               Value function loss: 4.9666
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 258.51
               Mean episode length: 240.25
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68386816
                    Iteration time: 8.83s
                        Total time: 38161.48s
                               ETA: 876114.0s

################################################################################
                    [1m Learning iteration 4174/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.740s, learning 0.232s)
               Value function loss: 5.0619
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 258.40
               Mean episode length: 240.95
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68403200
                    Iteration time: 8.97s
                        Total time: 38170.45s
                               ETA: 876101.0s

################################################################################
                    [1m Learning iteration 4175/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.645s, learning 0.164s)
               Value function loss: 4.1670
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 267.02
               Mean episode length: 247.85
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 8.81s
                        Total time: 38179.26s
                               ETA: 876084.2s

################################################################################
                    [1m Learning iteration 4176/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.587s, learning 0.163s)
               Value function loss: 4.5509
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 268.38
               Mean episode length: 246.56
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68435968
                    Iteration time: 8.75s
                        Total time: 38188.01s
                               ETA: 876066.0s

################################################################################
                    [1m Learning iteration 4177/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.426s, learning 0.239s)
               Value function loss: 4.6926
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 261.86
               Mean episode length: 240.98
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68452352
                    Iteration time: 8.66s
                        Total time: 38196.67s
                               ETA: 876045.9s

################################################################################
                    [1m Learning iteration 4178/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.718s, learning 0.163s)
               Value function loss: 6.2583
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 266.46
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68468736
                    Iteration time: 8.88s
                        Total time: 38205.55s
                               ETA: 876030.8s

################################################################################
                    [1m Learning iteration 4179/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.428s, learning 0.192s)
               Value function loss: 6.6203
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 260.97
               Mean episode length: 245.61
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68485120
                    Iteration time: 8.62s
                        Total time: 38214.17s
                               ETA: 876009.7s

################################################################################
                    [1m Learning iteration 4180/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.413s, learning 0.215s)
               Value function loss: 5.2413
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 261.39
               Mean episode length: 246.33
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68501504
                    Iteration time: 8.63s
                        Total time: 38222.80s
                               ETA: 875988.7s

################################################################################
                    [1m Learning iteration 4181/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.423s, learning 0.340s)
               Value function loss: 4.2237
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 263.30
               Mean episode length: 248.62
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 8.76s
                        Total time: 38231.57s
                               ETA: 875970.9s

################################################################################
                    [1m Learning iteration 4182/100000 [0m                    

                       Computation: 1788 steps/s (collection: 9.003s, learning 0.159s)
               Value function loss: 4.7052
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 263.16
               Mean episode length: 244.77
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68534272
                    Iteration time: 9.16s
                        Total time: 38240.73s
                               ETA: 875962.2s

################################################################################
                    [1m Learning iteration 4183/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.947s, learning 0.177s)
               Value function loss: 6.0982
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 259.19
               Mean episode length: 243.35
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68550656
                    Iteration time: 9.12s
                        Total time: 38249.85s
                               ETA: 875952.7s

################################################################################
                    [1m Learning iteration 4184/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.608s, learning 0.210s)
               Value function loss: 7.8368
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 255.78
               Mean episode length: 239.25
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68567040
                    Iteration time: 8.82s
                        Total time: 38258.67s
                               ETA: 875936.1s

################################################################################
                    [1m Learning iteration 4185/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.277s, learning 0.162s)
               Value function loss: 6.3608
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 252.61
               Mean episode length: 240.77
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68583424
                    Iteration time: 8.44s
                        Total time: 38267.11s
                               ETA: 875910.9s

################################################################################
                    [1m Learning iteration 4186/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.407s, learning 0.193s)
               Value function loss: 4.9493
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 263.15
               Mean episode length: 246.52
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68599808
                    Iteration time: 8.60s
                        Total time: 38275.71s
                               ETA: 875889.4s

################################################################################
                    [1m Learning iteration 4187/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.955s, learning 0.157s)
               Value function loss: 5.2725
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 254.88
               Mean episode length: 244.13
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 9.11s
                        Total time: 38284.82s
                               ETA: 875879.5s

################################################################################
                    [1m Learning iteration 4188/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.621s, learning 0.163s)
               Value function loss: 5.8434
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 251.17
               Mean episode length: 241.60
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68632576
                    Iteration time: 8.78s
                        Total time: 38293.60s
                               ETA: 875862.2s

################################################################################
                    [1m Learning iteration 4189/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.683s, learning 0.201s)
               Value function loss: 4.2373
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 253.16
               Mean episode length: 239.17
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68648960
                    Iteration time: 8.88s
                        Total time: 38302.49s
                               ETA: 875847.2s

################################################################################
                    [1m Learning iteration 4190/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.753s, learning 0.163s)
               Value function loss: 6.0403
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 256.18
               Mean episode length: 238.85
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68665344
                    Iteration time: 8.92s
                        Total time: 38311.40s
                               ETA: 875832.9s

################################################################################
                    [1m Learning iteration 4191/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.742s, learning 0.185s)
               Value function loss: 5.3530
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 260.03
               Mean episode length: 242.50
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68681728
                    Iteration time: 8.93s
                        Total time: 38320.33s
                               ETA: 875818.9s

################################################################################
                    [1m Learning iteration 4192/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.801s, learning 0.196s)
               Value function loss: 5.2866
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 259.46
               Mean episode length: 244.28
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68698112
                    Iteration time: 9.00s
                        Total time: 38329.33s
                               ETA: 875806.4s

################################################################################
                    [1m Learning iteration 4193/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.554s, learning 0.161s)
               Value function loss: 5.1956
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 262.48
               Mean episode length: 244.21
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 8.72s
                        Total time: 38338.04s
                               ETA: 875787.6s

################################################################################
                    [1m Learning iteration 4194/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.535s, learning 0.361s)
               Value function loss: 6.2363
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 264.49
               Mean episode length: 246.31
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68730880
                    Iteration time: 8.90s
                        Total time: 38346.94s
                               ETA: 875772.8s

################################################################################
                    [1m Learning iteration 4195/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.585s, learning 0.219s)
               Value function loss: 5.5174
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 254.14
               Mean episode length: 243.12
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68747264
                    Iteration time: 8.80s
                        Total time: 38355.75s
                               ETA: 875756.0s

################################################################################
                    [1m Learning iteration 4196/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.821s, learning 0.233s)
               Value function loss: 5.8766
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 254.50
               Mean episode length: 245.21
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68763648
                    Iteration time: 9.05s
                        Total time: 38364.80s
                               ETA: 875744.9s

################################################################################
                    [1m Learning iteration 4197/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.890s, learning 0.172s)
               Value function loss: 5.2303
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 256.61
               Mean episode length: 242.66
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68780032
                    Iteration time: 9.06s
                        Total time: 38373.86s
                               ETA: 875733.9s

################################################################################
                    [1m Learning iteration 4198/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.894s, learning 0.167s)
               Value function loss: 4.3120
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 262.06
               Mean episode length: 245.41
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68796416
                    Iteration time: 9.06s
                        Total time: 38382.92s
                               ETA: 875722.9s

################################################################################
                    [1m Learning iteration 4199/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.838s, learning 0.195s)
               Value function loss: 4.4120
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 260.70
               Mean episode length: 246.77
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 9.03s
                        Total time: 38391.95s
                               ETA: 875711.3s

################################################################################
                    [1m Learning iteration 4200/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.567s, learning 0.213s)
               Value function loss: 5.2850
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 263.97
               Mean episode length: 248.24
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68829184
                    Iteration time: 8.78s
                        Total time: 38400.73s
                               ETA: 875694.0s

################################################################################
                    [1m Learning iteration 4201/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.578s, learning 0.219s)
               Value function loss: 6.2946
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 260.32
               Mean episode length: 244.81
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68845568
                    Iteration time: 8.80s
                        Total time: 38409.53s
                               ETA: 875677.0s

################################################################################
                    [1m Learning iteration 4202/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.655s, learning 0.192s)
               Value function loss: 5.1668
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 264.98
               Mean episode length: 247.72
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68861952
                    Iteration time: 8.85s
                        Total time: 38418.38s
                               ETA: 875661.1s

################################################################################
                    [1m Learning iteration 4203/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.259s, learning 0.176s)
               Value function loss: 4.5384
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 267.38
               Mean episode length: 249.64
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68878336
                    Iteration time: 8.44s
                        Total time: 38426.81s
                               ETA: 875635.9s

################################################################################
                    [1m Learning iteration 4204/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.467s, learning 0.229s)
               Value function loss: 4.7693
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 266.72
               Mean episode length: 247.79
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68894720
                    Iteration time: 8.70s
                        Total time: 38435.51s
                               ETA: 875616.7s

################################################################################
                    [1m Learning iteration 4205/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.164s)
               Value function loss: 5.5341
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 268.80
               Mean episode length: 248.44
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 8.58s
                        Total time: 38444.09s
                               ETA: 875594.7s

################################################################################
                    [1m Learning iteration 4206/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.221s, learning 0.215s)
               Value function loss: 4.4979
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 258.65
               Mean episode length: 241.95
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68927488
                    Iteration time: 8.44s
                        Total time: 38452.52s
                               ETA: 875569.5s

################################################################################
                    [1m Learning iteration 4207/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.855s, learning 0.230s)
               Value function loss: 3.0828
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 256.14
               Mean episode length: 239.90
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68943872
                    Iteration time: 9.09s
                        Total time: 38461.61s
                               ETA: 875559.1s

################################################################################
                    [1m Learning iteration 4208/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.012s, learning 0.198s)
               Value function loss: 4.0770
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 264.91
               Mean episode length: 246.39
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68960256
                    Iteration time: 9.21s
                        Total time: 38470.82s
                               ETA: 875551.6s

################################################################################
                    [1m Learning iteration 4209/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.908s, learning 0.251s)
               Value function loss: 6.2320
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 272.44
               Mean episode length: 249.04
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68976640
                    Iteration time: 9.16s
                        Total time: 38479.98s
                               ETA: 875542.8s

################################################################################
                    [1m Learning iteration 4210/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.710s, learning 0.191s)
               Value function loss: 6.7542
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 256.34
               Mean episode length: 238.15
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68993024
                    Iteration time: 8.90s
                        Total time: 38488.88s
                               ETA: 875528.3s

################################################################################
                    [1m Learning iteration 4211/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.558s, learning 0.268s)
               Value function loss: 5.0303
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 259.38
               Mean episode length: 241.41
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 8.83s
                        Total time: 38497.70s
                               ETA: 875512.0s

################################################################################
                    [1m Learning iteration 4212/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.367s, learning 0.174s)
               Value function loss: 3.8213
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 263.65
               Mean episode length: 245.16
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69025792
                    Iteration time: 8.54s
                        Total time: 38506.24s
                               ETA: 875489.2s

################################################################################
                    [1m Learning iteration 4213/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.581s, learning 0.160s)
               Value function loss: 5.1471
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 268.18
               Mean episode length: 247.81
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69042176
                    Iteration time: 8.74s
                        Total time: 38514.99s
                               ETA: 875471.0s

################################################################################
                    [1m Learning iteration 4214/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.585s, learning 0.222s)
               Value function loss: 4.4957
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 264.68
               Mean episode length: 243.74
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69058560
                    Iteration time: 8.81s
                        Total time: 38523.79s
                               ETA: 875454.3s

################################################################################
                    [1m Learning iteration 4215/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.538s, learning 0.156s)
               Value function loss: 7.5276
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 266.64
               Mean episode length: 245.62
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69074944
                    Iteration time: 8.69s
                        Total time: 38532.49s
                               ETA: 875435.0s

################################################################################
                    [1m Learning iteration 4216/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.780s, learning 0.158s)
               Value function loss: 6.0224
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 264.81
               Mean episode length: 242.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69091328
                    Iteration time: 8.94s
                        Total time: 38541.42s
                               ETA: 875421.3s

################################################################################
                    [1m Learning iteration 4217/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.729s, learning 0.165s)
               Value function loss: 5.6276
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 273.62
               Mean episode length: 248.61
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 8.89s
                        Total time: 38550.32s
                               ETA: 875406.6s

################################################################################
                    [1m Learning iteration 4218/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.651s, learning 0.206s)
               Value function loss: 4.4759
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 275.95
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69124096
                    Iteration time: 8.86s
                        Total time: 38559.18s
                               ETA: 875391.1s

################################################################################
                    [1m Learning iteration 4219/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.714s, learning 0.169s)
               Value function loss: 6.4936
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 270.56
               Mean episode length: 245.23
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69140480
                    Iteration time: 8.88s
                        Total time: 38568.06s
                               ETA: 875376.1s

################################################################################
                    [1m Learning iteration 4220/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.843s, learning 0.163s)
               Value function loss: 4.0395
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 270.73
               Mean episode length: 245.23
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69156864
                    Iteration time: 9.01s
                        Total time: 38577.07s
                               ETA: 875364.0s

################################################################################
                    [1m Learning iteration 4221/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.617s, learning 0.167s)
               Value function loss: 4.9745
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 270.85
               Mean episode length: 248.33
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69173248
                    Iteration time: 8.78s
                        Total time: 38585.85s
                               ETA: 875346.8s

################################################################################
                    [1m Learning iteration 4222/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.566s, learning 0.183s)
               Value function loss: 6.2717
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 268.83
               Mean episode length: 245.03
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69189632
                    Iteration time: 8.75s
                        Total time: 38594.60s
                               ETA: 875328.8s

################################################################################
                    [1m Learning iteration 4223/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.530s, learning 0.208s)
               Value function loss: 4.5086
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 266.80
               Mean episode length: 244.40
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 8.74s
                        Total time: 38603.34s
                               ETA: 875310.5s

################################################################################
                    [1m Learning iteration 4224/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.579s, learning 0.268s)
               Value function loss: 5.6148
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 271.33
               Mean episode length: 246.80
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69222400
                    Iteration time: 8.85s
                        Total time: 38612.18s
                               ETA: 875294.8s

################################################################################
                    [1m Learning iteration 4225/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.961s, learning 0.179s)
               Value function loss: 5.7679
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 273.49
               Mean episode length: 248.19
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69238784
                    Iteration time: 9.14s
                        Total time: 38621.32s
                               ETA: 875285.6s

################################################################################
                    [1m Learning iteration 4226/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.695s, learning 0.186s)
               Value function loss: 7.0495
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 266.00
               Mean episode length: 241.06
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69255168
                    Iteration time: 8.88s
                        Total time: 38630.20s
                               ETA: 875270.7s

################################################################################
                    [1m Learning iteration 4227/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.714s, learning 0.160s)
               Value function loss: 5.5904
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 266.70
               Mean episode length: 242.18
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69271552
                    Iteration time: 8.87s
                        Total time: 38639.08s
                               ETA: 875255.5s

################################################################################
                    [1m Learning iteration 4228/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.688s, learning 0.166s)
               Value function loss: 6.7803
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 275.17
               Mean episode length: 248.91
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69287936
                    Iteration time: 8.85s
                        Total time: 38647.93s
                               ETA: 875239.9s

################################################################################
                    [1m Learning iteration 4229/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.106s, learning 0.176s)
               Value function loss: 4.8838
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 271.78
               Mean episode length: 247.16
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 9.28s
                        Total time: 38657.21s
                               ETA: 875234.0s

################################################################################
                    [1m Learning iteration 4230/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.611s, learning 0.161s)
               Value function loss: 5.6513
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 268.29
               Mean episode length: 246.49
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69320704
                    Iteration time: 8.77s
                        Total time: 38665.99s
                               ETA: 875216.6s

################################################################################
                    [1m Learning iteration 4231/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.699s, learning 0.165s)
               Value function loss: 5.4572
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 270.79
               Mean episode length: 246.49
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69337088
                    Iteration time: 8.86s
                        Total time: 38674.85s
                               ETA: 875201.3s

################################################################################
                    [1m Learning iteration 4232/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.356s, learning 0.168s)
               Value function loss: 6.7851
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 275.65
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69353472
                    Iteration time: 8.52s
                        Total time: 38683.37s
                               ETA: 875178.2s

################################################################################
                    [1m Learning iteration 4233/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.529s, learning 0.163s)
               Value function loss: 5.4278
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 279.15
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69369856
                    Iteration time: 8.69s
                        Total time: 38692.07s
                               ETA: 875159.0s

################################################################################
                    [1m Learning iteration 4234/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.677s, learning 0.186s)
               Value function loss: 4.1073
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 278.97
               Mean episode length: 248.26
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69386240
                    Iteration time: 8.86s
                        Total time: 38700.93s
                               ETA: 875143.6s

################################################################################
                    [1m Learning iteration 4235/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.695s, learning 0.155s)
               Value function loss: 5.0958
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 274.27
               Mean episode length: 243.53
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 8.85s
                        Total time: 38709.78s
                               ETA: 875127.9s

################################################################################
                    [1m Learning iteration 4236/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.537s, learning 0.186s)
               Value function loss: 6.4255
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 278.77
               Mean episode length: 247.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69419008
                    Iteration time: 8.72s
                        Total time: 38718.50s
                               ETA: 875109.4s

################################################################################
                    [1m Learning iteration 4237/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.656s, learning 0.282s)
               Value function loss: 5.5084
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 278.90
               Mean episode length: 247.86
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69435392
                    Iteration time: 8.94s
                        Total time: 38727.44s
                               ETA: 875095.7s

################################################################################
                    [1m Learning iteration 4238/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.475s, learning 0.242s)
               Value function loss: 3.5591
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 278.51
               Mean episode length: 247.86
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69451776
                    Iteration time: 8.72s
                        Total time: 38736.16s
                               ETA: 875077.1s

################################################################################
                    [1m Learning iteration 4239/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.358s, learning 0.233s)
               Value function loss: 4.5975
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 279.62
               Mean episode length: 248.13
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69468160
                    Iteration time: 8.59s
                        Total time: 38744.75s
                               ETA: 875055.6s

################################################################################
                    [1m Learning iteration 4240/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.724s, learning 0.161s)
               Value function loss: 4.9835
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 280.90
               Mean episode length: 248.13
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69484544
                    Iteration time: 8.89s
                        Total time: 38753.63s
                               ETA: 875040.7s

################################################################################
                    [1m Learning iteration 4241/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.409s, learning 0.262s)
               Value function loss: 6.7832
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 278.78
               Mean episode length: 248.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 8.67s
                        Total time: 38762.30s
                               ETA: 875021.1s

################################################################################
                    [1m Learning iteration 4242/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.720s, learning 0.229s)
               Value function loss: 5.3121
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 271.62
               Mean episode length: 243.79
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69517312
                    Iteration time: 8.95s
                        Total time: 38771.25s
                               ETA: 875007.7s

################################################################################
                    [1m Learning iteration 4243/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.556s, learning 0.210s)
               Value function loss: 4.5902
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 269.65
               Mean episode length: 240.39
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69533696
                    Iteration time: 8.77s
                        Total time: 38780.02s
                               ETA: 874990.1s

################################################################################
                    [1m Learning iteration 4244/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.638s, learning 0.157s)
               Value function loss: 5.0731
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 274.45
               Mean episode length: 244.59
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69550080
                    Iteration time: 8.80s
                        Total time: 38788.81s
                               ETA: 874973.3s

################################################################################
                    [1m Learning iteration 4245/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.657s, learning 0.264s)
               Value function loss: 5.5953
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 282.52
               Mean episode length: 249.77
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69566464
                    Iteration time: 8.92s
                        Total time: 38797.73s
                               ETA: 874959.3s

################################################################################
                    [1m Learning iteration 4246/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.459s, learning 0.167s)
               Value function loss: 6.5394
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 281.84
               Mean episode length: 248.43
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69582848
                    Iteration time: 8.63s
                        Total time: 38806.36s
                               ETA: 874938.6s

################################################################################
                    [1m Learning iteration 4247/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.821s, learning 0.163s)
               Value function loss: 5.7512
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 273.87
               Mean episode length: 244.84
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 8.98s
                        Total time: 38815.34s
                               ETA: 874926.0s

################################################################################
                    [1m Learning iteration 4248/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.530s, learning 0.171s)
               Value function loss: 5.2670
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 280.65
               Mean episode length: 248.49
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69615616
                    Iteration time: 8.70s
                        Total time: 38824.04s
                               ETA: 874907.0s

################################################################################
                    [1m Learning iteration 4249/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.711s, learning 0.261s)
               Value function loss: 4.0590
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 279.80
               Mean episode length: 246.79
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69632000
                    Iteration time: 8.97s
                        Total time: 38833.02s
                               ETA: 874894.2s

################################################################################
                    [1m Learning iteration 4250/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.570s, learning 0.203s)
               Value function loss: 5.3498
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 279.46
               Mean episode length: 248.36
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69648384
                    Iteration time: 8.77s
                        Total time: 38841.79s
                               ETA: 874876.8s

################################################################################
                    [1m Learning iteration 4251/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.660s, learning 0.192s)
               Value function loss: 4.7424
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 275.82
               Mean episode length: 244.74
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69664768
                    Iteration time: 8.85s
                        Total time: 38850.64s
                               ETA: 874861.3s

################################################################################
                    [1m Learning iteration 4252/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.782s, learning 0.178s)
               Value function loss: 4.7387
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 270.38
               Mean episode length: 240.68
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69681152
                    Iteration time: 8.96s
                        Total time: 38859.60s
                               ETA: 874848.2s

################################################################################
                    [1m Learning iteration 4253/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.529s, learning 0.163s)
               Value function loss: 5.7737
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 278.38
               Mean episode length: 247.54
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 8.69s
                        Total time: 38868.29s
                               ETA: 874829.0s

################################################################################
                    [1m Learning iteration 4254/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.866s, learning 0.168s)
               Value function loss: 5.3790
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 277.27
               Mean episode length: 247.84
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69713920
                    Iteration time: 9.03s
                        Total time: 38877.33s
                               ETA: 874817.5s

################################################################################
                    [1m Learning iteration 4255/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.534s, learning 0.195s)
               Value function loss: 4.3277
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 281.42
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69730304
                    Iteration time: 8.73s
                        Total time: 38886.06s
                               ETA: 874799.2s

################################################################################
                    [1m Learning iteration 4256/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.511s, learning 0.194s)
               Value function loss: 4.7062
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 277.89
               Mean episode length: 247.70
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69746688
                    Iteration time: 8.70s
                        Total time: 38894.76s
                               ETA: 874780.4s

################################################################################
                    [1m Learning iteration 4257/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.472s, learning 0.216s)
               Value function loss: 5.8547
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 277.77
               Mean episode length: 247.99
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69763072
                    Iteration time: 8.69s
                        Total time: 38903.45s
                               ETA: 874761.2s

################################################################################
                    [1m Learning iteration 4258/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.507s, learning 0.159s)
               Value function loss: 6.1414
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 280.26
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69779456
                    Iteration time: 8.67s
                        Total time: 38912.12s
                               ETA: 874741.5s

################################################################################
                    [1m Learning iteration 4259/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.165s)
               Value function loss: 5.4981
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 276.34
               Mean episode length: 247.93
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 8.66s
                        Total time: 38920.78s
                               ETA: 874721.7s

################################################################################
                    [1m Learning iteration 4260/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.805s, learning 0.164s)
               Value function loss: 4.5713
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 279.03
               Mean episode length: 248.60
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69812224
                    Iteration time: 8.97s
                        Total time: 38929.75s
                               ETA: 874708.8s

################################################################################
                    [1m Learning iteration 4261/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.867s, learning 0.255s)
               Value function loss: 4.7311
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 278.17
               Mean episode length: 248.60
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69828608
                    Iteration time: 9.12s
                        Total time: 38938.87s
                               ETA: 874699.3s

################################################################################
                    [1m Learning iteration 4262/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.599s, learning 0.186s)
               Value function loss: 5.1139
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 277.49
               Mean episode length: 248.92
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69844992
                    Iteration time: 8.78s
                        Total time: 38947.65s
                               ETA: 874682.3s

################################################################################
                    [1m Learning iteration 4263/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.501s, learning 0.173s)
               Value function loss: 6.3207
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 278.94
               Mean episode length: 248.84
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69861376
                    Iteration time: 8.67s
                        Total time: 38956.33s
                               ETA: 874662.8s

################################################################################
                    [1m Learning iteration 4264/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.897s, learning 0.232s)
               Value function loss: 5.7100
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 277.97
               Mean episode length: 248.33
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69877760
                    Iteration time: 9.13s
                        Total time: 38965.46s
                               ETA: 874653.5s

################################################################################
                    [1m Learning iteration 4265/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.713s, learning 0.164s)
               Value function loss: 4.2444
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 275.94
               Mean episode length: 246.79
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 8.88s
                        Total time: 38974.33s
                               ETA: 874638.5s

################################################################################
                    [1m Learning iteration 4266/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.716s, learning 0.155s)
               Value function loss: 4.7540
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 266.12
               Mean episode length: 238.51
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69910528
                    Iteration time: 8.87s
                        Total time: 38983.20s
                               ETA: 874623.4s

################################################################################
                    [1m Learning iteration 4267/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.725s, learning 0.162s)
               Value function loss: 5.5259
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 272.31
               Mean episode length: 243.62
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69926912
                    Iteration time: 8.89s
                        Total time: 38992.09s
                               ETA: 874608.7s

################################################################################
                    [1m Learning iteration 4268/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.819s, learning 0.192s)
               Value function loss: 4.7102
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 279.02
               Mean episode length: 247.95
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69943296
                    Iteration time: 9.01s
                        Total time: 39001.10s
                               ETA: 874596.8s

################################################################################
                    [1m Learning iteration 4269/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.750s, learning 0.167s)
               Value function loss: 4.1133
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 277.74
               Mean episode length: 247.99
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69959680
                    Iteration time: 8.92s
                        Total time: 39010.02s
                               ETA: 874582.7s

################################################################################
                    [1m Learning iteration 4270/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.394s, learning 0.161s)
               Value function loss: 3.6082
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 279.09
               Mean episode length: 247.99
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69976064
                    Iteration time: 8.55s
                        Total time: 39018.57s
                               ETA: 874560.6s

################################################################################
                    [1m Learning iteration 4271/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.470s, learning 0.164s)
               Value function loss: 5.2409
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 281.42
               Mean episode length: 249.95
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 8.63s
                        Total time: 39027.21s
                               ETA: 874540.2s

################################################################################
                    [1m Learning iteration 4272/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.731s, learning 0.165s)
               Value function loss: 6.8142
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 274.08
               Mean episode length: 245.90
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70008832
                    Iteration time: 8.90s
                        Total time: 39036.10s
                               ETA: 874525.7s

################################################################################
                    [1m Learning iteration 4273/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.753s, learning 0.166s)
               Value function loss: 6.2260
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 273.42
               Mean episode length: 245.78
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70025216
                    Iteration time: 8.92s
                        Total time: 39045.02s
                               ETA: 874511.7s

################################################################################
                    [1m Learning iteration 4274/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.658s, learning 0.156s)
               Value function loss: 5.1311
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 275.94
               Mean episode length: 247.89
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70041600
                    Iteration time: 8.81s
                        Total time: 39053.84s
                               ETA: 874495.4s

################################################################################
                    [1m Learning iteration 4275/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.944s, learning 0.161s)
               Value function loss: 5.6234
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 274.69
               Mean episode length: 245.71
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70057984
                    Iteration time: 9.11s
                        Total time: 39062.94s
                               ETA: 874485.6s

################################################################################
                    [1m Learning iteration 4276/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.687s, learning 0.163s)
               Value function loss: 5.4734
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 271.63
               Mean episode length: 243.64
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70074368
                    Iteration time: 8.85s
                        Total time: 39071.79s
                               ETA: 874470.0s

################################################################################
                    [1m Learning iteration 4277/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.866s, learning 0.185s)
               Value function loss: 7.0060
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 272.67
               Mean episode length: 247.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 9.05s
                        Total time: 39080.85s
                               ETA: 874459.0s

################################################################################
                    [1m Learning iteration 4278/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.563s, learning 0.215s)
               Value function loss: 6.6169
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 276.62
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70107136
                    Iteration time: 8.78s
                        Total time: 39089.62s
                               ETA: 874441.9s

################################################################################
                    [1m Learning iteration 4279/100000 [0m                    

                       Computation: 1778 steps/s (collection: 8.814s, learning 0.399s)
               Value function loss: 5.4179
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 273.02
               Mean episode length: 247.90
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70123520
                    Iteration time: 9.21s
                        Total time: 39098.84s
                               ETA: 874434.5s

################################################################################
                    [1m Learning iteration 4280/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.578s, learning 0.190s)
               Value function loss: 4.7911
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 273.73
               Mean episode length: 247.90
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70139904
                    Iteration time: 8.77s
                        Total time: 39107.60s
                               ETA: 874417.2s

################################################################################
                    [1m Learning iteration 4281/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.775s, learning 0.165s)
               Value function loss: 5.7470
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 273.91
               Mean episode length: 246.03
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70156288
                    Iteration time: 8.94s
                        Total time: 39116.54s
                               ETA: 874403.7s

################################################################################
                    [1m Learning iteration 4282/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.358s, learning 0.162s)
               Value function loss: 6.5756
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 273.26
               Mean episode length: 245.55
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70172672
                    Iteration time: 8.52s
                        Total time: 39125.06s
                               ETA: 874380.8s

################################################################################
                    [1m Learning iteration 4283/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.920s, learning 0.166s)
               Value function loss: 4.5403
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 267.96
               Mean episode length: 243.10
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 9.09s
                        Total time: 39134.15s
                               ETA: 874370.5s

################################################################################
                    [1m Learning iteration 4284/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.623s, learning 0.160s)
               Value function loss: 5.5455
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 267.04
               Mean episode length: 243.98
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70205440
                    Iteration time: 8.78s
                        Total time: 39142.93s
                               ETA: 874353.5s

################################################################################
                    [1m Learning iteration 4285/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.662s, learning 0.208s)
               Value function loss: 4.7823
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 271.19
               Mean episode length: 246.58
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70221824
                    Iteration time: 8.87s
                        Total time: 39151.80s
                               ETA: 874338.5s

################################################################################
                    [1m Learning iteration 4286/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.446s, learning 0.170s)
               Value function loss: 4.9891
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 275.22
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70238208
                    Iteration time: 8.62s
                        Total time: 39160.42s
                               ETA: 874317.8s

################################################################################
                    [1m Learning iteration 4287/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.451s, learning 0.161s)
               Value function loss: 4.6623
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 276.18
               Mean episode length: 248.91
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70254592
                    Iteration time: 8.61s
                        Total time: 39169.03s
                               ETA: 874297.0s

################################################################################
                    [1m Learning iteration 4288/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.110s, learning 0.168s)
               Value function loss: 4.8250
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 273.46
               Mean episode length: 249.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70270976
                    Iteration time: 9.28s
                        Total time: 39178.31s
                               ETA: 874291.0s

################################################################################
                    [1m Learning iteration 4289/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.786s, learning 0.169s)
               Value function loss: 6.4781
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 274.75
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 8.96s
                        Total time: 39187.26s
                               ETA: 874277.9s

################################################################################
                    [1m Learning iteration 4290/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.679s, learning 0.191s)
               Value function loss: 5.8015
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 276.60
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70303744
                    Iteration time: 8.87s
                        Total time: 39196.13s
                               ETA: 874262.9s

################################################################################
                    [1m Learning iteration 4291/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.410s, learning 0.168s)
               Value function loss: 4.4546
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 276.69
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70320128
                    Iteration time: 8.58s
                        Total time: 39204.71s
                               ETA: 874241.3s

################################################################################
                    [1m Learning iteration 4292/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.584s, learning 0.170s)
               Value function loss: 4.4018
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 274.32
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70336512
                    Iteration time: 8.75s
                        Total time: 39213.47s
                               ETA: 874223.7s

################################################################################
                    [1m Learning iteration 4293/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.497s, learning 0.174s)
               Value function loss: 4.8820
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 274.59
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70352896
                    Iteration time: 8.67s
                        Total time: 39222.14s
                               ETA: 874204.3s

################################################################################
                    [1m Learning iteration 4294/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.282s, learning 0.167s)
               Value function loss: 5.3737
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 272.12
               Mean episode length: 247.90
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70369280
                    Iteration time: 8.45s
                        Total time: 39230.59s
                               ETA: 874179.9s

################################################################################
                    [1m Learning iteration 4295/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.241s, learning 0.174s)
               Value function loss: 6.0099
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 272.87
               Mean episode length: 246.59
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 8.41s
                        Total time: 39239.00s
                               ETA: 874154.7s

################################################################################
                    [1m Learning iteration 4296/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.801s, learning 0.227s)
               Value function loss: 4.6955
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 270.88
               Mean episode length: 247.25
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70402048
                    Iteration time: 9.03s
                        Total time: 39248.03s
                               ETA: 874143.2s

################################################################################
                    [1m Learning iteration 4297/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.867s, learning 0.298s)
               Value function loss: 4.7075
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 269.00
               Mean episode length: 246.31
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70418432
                    Iteration time: 9.16s
                        Total time: 39257.19s
                               ETA: 874134.8s

################################################################################
                    [1m Learning iteration 4298/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.415s, learning 0.162s)
               Value function loss: 5.3878
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 269.28
               Mean episode length: 245.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70434816
                    Iteration time: 8.58s
                        Total time: 39265.77s
                               ETA: 874113.2s

################################################################################
                    [1m Learning iteration 4299/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.588s, learning 0.168s)
               Value function loss: 4.7541
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 266.80
               Mean episode length: 245.11
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70451200
                    Iteration time: 8.76s
                        Total time: 39274.53s
                               ETA: 874095.7s

################################################################################
                    [1m Learning iteration 4300/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.676s, learning 0.162s)
               Value function loss: 3.9841
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 269.35
               Mean episode length: 246.63
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70467584
                    Iteration time: 8.84s
                        Total time: 39283.37s
                               ETA: 874080.0s

################################################################################
                    [1m Learning iteration 4301/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.713s, learning 0.257s)
               Value function loss: 3.9488
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 273.58
               Mean episode length: 246.88
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 8.97s
                        Total time: 39292.33s
                               ETA: 874067.2s

################################################################################
                    [1m Learning iteration 4302/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.780s, learning 0.172s)
               Value function loss: 4.0632
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 275.49
               Mean episode length: 248.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70500352
                    Iteration time: 8.95s
                        Total time: 39301.29s
                               ETA: 874054.0s

################################################################################
                    [1m Learning iteration 4303/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.611s, learning 0.160s)
               Value function loss: 5.9408
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 274.51
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70516736
                    Iteration time: 8.77s
                        Total time: 39310.06s
                               ETA: 874036.9s

################################################################################
                    [1m Learning iteration 4304/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.803s, learning 0.163s)
               Value function loss: 6.1370
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 273.19
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70533120
                    Iteration time: 8.97s
                        Total time: 39319.02s
                               ETA: 874024.0s

################################################################################
                    [1m Learning iteration 4305/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.583s, learning 0.162s)
               Value function loss: 5.2369
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 270.41
               Mean episode length: 249.55
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70549504
                    Iteration time: 8.74s
                        Total time: 39327.77s
                               ETA: 874006.2s

################################################################################
                    [1m Learning iteration 4306/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.599s, learning 0.161s)
               Value function loss: 4.2399
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 268.36
               Mean episode length: 247.02
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70565888
                    Iteration time: 8.76s
                        Total time: 39336.53s
                               ETA: 873988.8s

################################################################################
                    [1m Learning iteration 4307/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.763s, learning 0.195s)
               Value function loss: 4.1593
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 264.34
               Mean episode length: 244.40
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 8.96s
                        Total time: 39345.49s
                               ETA: 873975.8s

################################################################################
                    [1m Learning iteration 4308/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.617s, learning 0.186s)
               Value function loss: 5.4340
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 269.18
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70598656
                    Iteration time: 8.80s
                        Total time: 39354.29s
                               ETA: 873959.3s

################################################################################
                    [1m Learning iteration 4309/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.665s, learning 0.158s)
               Value function loss: 7.8077
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 271.19
               Mean episode length: 249.44
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70615040
                    Iteration time: 8.82s
                        Total time: 39363.11s
                               ETA: 873943.3s

################################################################################
                    [1m Learning iteration 4310/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.659s, learning 0.162s)
               Value function loss: 5.7862
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 271.96
               Mean episode length: 247.29
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70631424
                    Iteration time: 8.82s
                        Total time: 39371.93s
                               ETA: 873927.3s

################################################################################
                    [1m Learning iteration 4311/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.171s)
               Value function loss: 4.7451
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 274.87
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70647808
                    Iteration time: 8.55s
                        Total time: 39380.49s
                               ETA: 873905.3s

################################################################################
                    [1m Learning iteration 4312/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.432s, learning 0.167s)
               Value function loss: 5.0248
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 270.13
               Mean episode length: 248.13
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70664192
                    Iteration time: 8.60s
                        Total time: 39389.09s
                               ETA: 873884.3s

################################################################################
                    [1m Learning iteration 4313/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.439s, learning 0.166s)
               Value function loss: 6.7865
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 272.15
               Mean episode length: 247.86
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 8.60s
                        Total time: 39397.69s
                               ETA: 873863.5s

################################################################################
                    [1m Learning iteration 4314/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.714s, learning 0.183s)
               Value function loss: 4.1805
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 273.59
               Mean episode length: 248.22
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70696960
                    Iteration time: 8.90s
                        Total time: 39406.59s
                               ETA: 873849.1s

################################################################################
                    [1m Learning iteration 4315/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.847s, learning 0.163s)
               Value function loss: 5.9545
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 273.08
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70713344
                    Iteration time: 9.01s
                        Total time: 39415.60s
                               ETA: 873837.3s

################################################################################
                    [1m Learning iteration 4316/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.215s, learning 0.236s)
               Value function loss: 5.8553
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 270.60
               Mean episode length: 248.35
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70729728
                    Iteration time: 8.45s
                        Total time: 39424.05s
                               ETA: 873813.0s

################################################################################
                    [1m Learning iteration 4317/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.551s, learning 0.161s)
               Value function loss: 5.4970
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 271.14
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70746112
                    Iteration time: 8.71s
                        Total time: 39432.76s
                               ETA: 873794.6s

################################################################################
                    [1m Learning iteration 4318/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.822s, learning 0.186s)
               Value function loss: 5.3337
                    Surrogate loss: 0.0138
             Mean action noise std: 0.75
                       Mean reward: 268.96
               Mean episode length: 248.90
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70762496
                    Iteration time: 9.01s
                        Total time: 39441.77s
                               ETA: 873782.7s

################################################################################
                    [1m Learning iteration 4319/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.525s, learning 0.175s)
               Value function loss: 5.1783
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 274.59
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 8.70s
                        Total time: 39450.47s
                               ETA: 873764.0s

################################################################################
                    [1m Learning iteration 4320/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.945s, learning 0.159s)
               Value function loss: 5.6364
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 266.24
               Mean episode length: 244.84
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70795264
                    Iteration time: 9.10s
                        Total time: 39459.57s
                               ETA: 873754.2s

################################################################################
                    [1m Learning iteration 4321/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.346s, learning 0.185s)
               Value function loss: 5.0139
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 262.69
               Mean episode length: 242.46
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70811648
                    Iteration time: 8.53s
                        Total time: 39468.11s
                               ETA: 873731.8s

################################################################################
                    [1m Learning iteration 4322/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.529s, learning 0.188s)
               Value function loss: 5.3319
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 269.53
               Mean episode length: 246.98
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70828032
                    Iteration time: 8.72s
                        Total time: 39476.82s
                               ETA: 873713.5s

################################################################################
                    [1m Learning iteration 4323/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.611s, learning 0.158s)
               Value function loss: 3.8082
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 272.00
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70844416
                    Iteration time: 8.77s
                        Total time: 39485.59s
                               ETA: 873696.3s

################################################################################
                    [1m Learning iteration 4324/100000 [0m                    

                       Computation: 1782 steps/s (collection: 9.028s, learning 0.163s)
               Value function loss: 4.7552
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 269.91
               Mean episode length: 249.78
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70860800
                    Iteration time: 9.19s
                        Total time: 39494.78s
                               ETA: 873688.5s

################################################################################
                    [1m Learning iteration 4325/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.161s)
               Value function loss: 5.2827
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 270.26
               Mean episode length: 248.39
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 8.43s
                        Total time: 39503.21s
                               ETA: 873663.8s

################################################################################
                    [1m Learning iteration 4326/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.334s, learning 0.165s)
               Value function loss: 5.6574
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 265.57
               Mean episode length: 245.52
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70893568
                    Iteration time: 8.50s
                        Total time: 39511.71s
                               ETA: 873640.7s

################################################################################
                    [1m Learning iteration 4327/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.160s)
               Value function loss: 4.4896
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 273.44
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70909952
                    Iteration time: 8.40s
                        Total time: 39520.11s
                               ETA: 873615.4s

################################################################################
                    [1m Learning iteration 4328/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.656s, learning 0.230s)
               Value function loss: 4.5123
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 269.15
               Mean episode length: 247.47
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70926336
                    Iteration time: 8.89s
                        Total time: 39528.99s
                               ETA: 873600.8s

################################################################################
                    [1m Learning iteration 4329/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.834s, learning 0.176s)
               Value function loss: 5.0408
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 267.09
               Mean episode length: 245.98
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70942720
                    Iteration time: 9.01s
                        Total time: 39538.00s
                               ETA: 873589.0s

################################################################################
                    [1m Learning iteration 4330/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.288s, learning 0.169s)
               Value function loss: 4.3134
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 270.49
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70959104
                    Iteration time: 8.46s
                        Total time: 39546.46s
                               ETA: 873565.0s

################################################################################
                    [1m Learning iteration 4331/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.697s, learning 0.166s)
               Value function loss: 3.8341
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 270.30
               Mean episode length: 249.58
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 8.86s
                        Total time: 39555.32s
                               ETA: 873549.9s

################################################################################
                    [1m Learning iteration 4332/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.732s, learning 0.170s)
               Value function loss: 3.2653
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 268.86
               Mean episode length: 248.39
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70991872
                    Iteration time: 8.90s
                        Total time: 39564.23s
                               ETA: 873535.7s

################################################################################
                    [1m Learning iteration 4333/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.698s, learning 0.163s)
               Value function loss: 3.9801
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 258.53
               Mean episode length: 242.28
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71008256
                    Iteration time: 8.86s
                        Total time: 39573.09s
                               ETA: 873520.6s

################################################################################
                    [1m Learning iteration 4334/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.628s, learning 0.164s)
               Value function loss: 5.1006
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 262.96
               Mean episode length: 247.30
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71024640
                    Iteration time: 8.79s
                        Total time: 39581.88s
                               ETA: 873504.0s

################################################################################
                    [1m Learning iteration 4335/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.755s, learning 0.193s)
               Value function loss: 5.5716
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 264.54
               Mean episode length: 247.26
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71041024
                    Iteration time: 8.95s
                        Total time: 39590.83s
                               ETA: 873490.9s

################################################################################
                    [1m Learning iteration 4336/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.729s, learning 0.157s)
               Value function loss: 4.7096
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 259.11
               Mean episode length: 247.83
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71057408
                    Iteration time: 8.89s
                        Total time: 39599.71s
                               ETA: 873476.3s

################################################################################
                    [1m Learning iteration 4337/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.623s, learning 0.171s)
               Value function loss: 3.5213
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 259.78
               Mean episode length: 246.13
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 8.79s
                        Total time: 39608.51s
                               ETA: 873459.8s

################################################################################
                    [1m Learning iteration 4338/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.192s, learning 0.221s)
               Value function loss: 4.0907
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 265.13
               Mean episode length: 246.62
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71090176
                    Iteration time: 8.41s
                        Total time: 39616.92s
                               ETA: 873434.8s

################################################################################
                    [1m Learning iteration 4339/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.574s, learning 0.193s)
               Value function loss: 3.6959
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 266.56
               Mean episode length: 248.78
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71106560
                    Iteration time: 8.77s
                        Total time: 39625.69s
                               ETA: 873417.7s

################################################################################
                    [1m Learning iteration 4340/100000 [0m                    

                       Computation: 1772 steps/s (collection: 9.079s, learning 0.163s)
               Value function loss: 5.4149
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 261.92
               Mean episode length: 247.01
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71122944
                    Iteration time: 9.24s
                        Total time: 39634.93s
                               ETA: 873411.0s

################################################################################
                    [1m Learning iteration 4341/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.510s, learning 0.187s)
               Value function loss: 4.3784
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 264.19
               Mean episode length: 248.57
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71139328
                    Iteration time: 8.70s
                        Total time: 39643.63s
                               ETA: 873392.4s

################################################################################
                    [1m Learning iteration 4342/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.620s, learning 0.261s)
               Value function loss: 3.9833
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 258.16
               Mean episode length: 246.14
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71155712
                    Iteration time: 8.88s
                        Total time: 39652.51s
                               ETA: 873377.7s

################################################################################
                    [1m Learning iteration 4343/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.731s, learning 0.160s)
               Value function loss: 4.0311
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 254.57
               Mean episode length: 240.90
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 8.89s
                        Total time: 39661.40s
                               ETA: 873363.4s

################################################################################
                    [1m Learning iteration 4344/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.723s, learning 0.156s)
               Value function loss: 4.5854
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 258.82
               Mean episode length: 247.01
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71188480
                    Iteration time: 8.88s
                        Total time: 39670.28s
                               ETA: 873348.7s

################################################################################
                    [1m Learning iteration 4345/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.761s, learning 0.156s)
               Value function loss: 3.3279
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 256.25
               Mean episode length: 243.80
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71204864
                    Iteration time: 8.92s
                        Total time: 39679.20s
                               ETA: 873334.9s

################################################################################
                    [1m Learning iteration 4346/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.887s, learning 0.195s)
               Value function loss: 4.3472
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 253.78
               Mean episode length: 241.60
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71221248
                    Iteration time: 9.08s
                        Total time: 39688.28s
                               ETA: 873324.7s

################################################################################
                    [1m Learning iteration 4347/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.738s, learning 0.193s)
               Value function loss: 4.9173
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 258.15
               Mean episode length: 243.33
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71237632
                    Iteration time: 8.93s
                        Total time: 39697.21s
                               ETA: 873311.2s

################################################################################
                    [1m Learning iteration 4348/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.305s, learning 0.193s)
               Value function loss: 3.6451
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 257.97
               Mean episode length: 247.30
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71254016
                    Iteration time: 8.50s
                        Total time: 39705.71s
                               ETA: 873288.2s

################################################################################
                    [1m Learning iteration 4349/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.612s, learning 0.188s)
               Value function loss: 4.7636
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 254.71
               Mean episode length: 242.70
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 8.80s
                        Total time: 39714.51s
                               ETA: 873271.8s

################################################################################
                    [1m Learning iteration 4350/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.827s, learning 0.173s)
               Value function loss: 4.7391
                    Surrogate loss: -0.0012
             Mean action noise std: 0.75
                       Mean reward: 248.57
               Mean episode length: 236.69
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71286784
                    Iteration time: 9.00s
                        Total time: 39723.51s
                               ETA: 873259.8s

################################################################################
                    [1m Learning iteration 4351/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.554s, learning 0.167s)
               Value function loss: 5.3516
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 255.89
               Mean episode length: 243.46
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71303168
                    Iteration time: 8.72s
                        Total time: 39732.23s
                               ETA: 873241.7s

################################################################################
                    [1m Learning iteration 4352/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.744s, learning 0.170s)
               Value function loss: 4.3714
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 258.72
               Mean episode length: 245.66
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71319552
                    Iteration time: 8.91s
                        Total time: 39741.14s
                               ETA: 873227.8s

################################################################################
                    [1m Learning iteration 4353/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.708s, learning 0.158s)
               Value function loss: 4.7517
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 259.67
               Mean episode length: 247.44
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71335936
                    Iteration time: 8.87s
                        Total time: 39750.01s
                               ETA: 873212.9s

################################################################################
                    [1m Learning iteration 4354/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.667s, learning 0.160s)
               Value function loss: 3.7877
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 255.68
               Mean episode length: 248.23
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71352320
                    Iteration time: 8.83s
                        Total time: 39758.83s
                               ETA: 873197.1s

################################################################################
                    [1m Learning iteration 4355/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.494s, learning 0.188s)
               Value function loss: 3.9297
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 251.86
               Mean episode length: 243.42
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 8.68s
                        Total time: 39767.52s
                               ETA: 873178.2s

################################################################################
                    [1m Learning iteration 4356/100000 [0m                    

                       Computation: 1780 steps/s (collection: 9.019s, learning 0.183s)
               Value function loss: 4.0763
                    Surrogate loss: 0.0014
             Mean action noise std: 0.75
                       Mean reward: 249.68
               Mean episode length: 244.50
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71385088
                    Iteration time: 9.20s
                        Total time: 39776.72s
                               ETA: 873170.6s

################################################################################
                    [1m Learning iteration 4357/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.495s, learning 0.274s)
               Value function loss: 5.3391
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 252.56
               Mean episode length: 244.53
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71401472
                    Iteration time: 9.77s
                        Total time: 39786.49s
                               ETA: 873175.6s

################################################################################
                    [1m Learning iteration 4358/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.594s, learning 0.262s)
               Value function loss: 4.1097
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 250.33
               Mean episode length: 239.71
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71417856
                    Iteration time: 8.86s
                        Total time: 39795.34s
                               ETA: 873160.4s

################################################################################
                    [1m Learning iteration 4359/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.056s, learning 0.163s)
               Value function loss: 2.8810
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 250.89
               Mean episode length: 238.68
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71434240
                    Iteration time: 9.22s
                        Total time: 39804.56s
                               ETA: 873153.3s

################################################################################
                    [1m Learning iteration 4360/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.568s, learning 0.160s)
               Value function loss: 3.7739
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 252.98
               Mean episode length: 243.24
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71450624
                    Iteration time: 8.73s
                        Total time: 39813.29s
                               ETA: 873135.3s

################################################################################
                    [1m Learning iteration 4361/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.637s, learning 0.158s)
               Value function loss: 4.4651
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 249.53
               Mean episode length: 246.26
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 8.79s
                        Total time: 39822.09s
                               ETA: 873118.9s

################################################################################
                    [1m Learning iteration 4362/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.293s, learning 0.169s)
               Value function loss: 3.3817
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 250.80
               Mean episode length: 247.46
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71483392
                    Iteration time: 8.46s
                        Total time: 39830.55s
                               ETA: 873095.1s

################################################################################
                    [1m Learning iteration 4363/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.389s, learning 0.172s)
               Value function loss: 2.4457
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 242.13
               Mean episode length: 245.71
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71499776
                    Iteration time: 8.56s
                        Total time: 39839.11s
                               ETA: 873073.5s

################################################################################
                    [1m Learning iteration 4364/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.778s, learning 0.277s)
               Value function loss: 3.3895
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 227.59
               Mean episode length: 242.21
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71516160
                    Iteration time: 9.05s
                        Total time: 39848.16s
                               ETA: 873062.7s

################################################################################
                    [1m Learning iteration 4365/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.587s, learning 0.194s)
               Value function loss: 3.3279
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 223.65
               Mean episode length: 236.72
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71532544
                    Iteration time: 8.78s
                        Total time: 39856.94s
                               ETA: 873046.0s

################################################################################
                    [1m Learning iteration 4366/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.634s, learning 0.163s)
               Value function loss: 4.0325
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 234.67
               Mean episode length: 244.19
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71548928
                    Iteration time: 8.80s
                        Total time: 39865.74s
                               ETA: 873029.6s

################################################################################
                    [1m Learning iteration 4367/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.478s, learning 0.176s)
               Value function loss: 3.3506
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 227.23
               Mean episode length: 246.36
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 8.65s
                        Total time: 39874.39s
                               ETA: 873010.1s

################################################################################
                    [1m Learning iteration 4368/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.579s, learning 0.193s)
               Value function loss: 2.9699
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 226.16
               Mean episode length: 245.64
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71581696
                    Iteration time: 8.77s
                        Total time: 39883.17s
                               ETA: 872993.1s

################################################################################
                    [1m Learning iteration 4369/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.791s, learning 0.156s)
               Value function loss: 3.4584
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 226.05
               Mean episode length: 242.16
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71598080
                    Iteration time: 8.95s
                        Total time: 39892.11s
                               ETA: 872980.0s

################################################################################
                    [1m Learning iteration 4370/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.848s, learning 0.160s)
               Value function loss: 3.6933
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 225.39
               Mean episode length: 243.02
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71614464
                    Iteration time: 9.01s
                        Total time: 39901.12s
                               ETA: 872968.3s

################################################################################
                    [1m Learning iteration 4371/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.666s, learning 0.160s)
               Value function loss: 3.8970
                    Surrogate loss: 0.0034
             Mean action noise std: 0.75
                       Mean reward: 222.00
               Mean episode length: 245.52
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71630848
                    Iteration time: 8.83s
                        Total time: 39909.95s
                               ETA: 872952.5s

################################################################################
                    [1m Learning iteration 4372/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.870s, learning 0.160s)
               Value function loss: 3.4915
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 220.28
               Mean episode length: 247.25
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71647232
                    Iteration time: 9.03s
                        Total time: 39918.98s
                               ETA: 872941.2s

################################################################################
                    [1m Learning iteration 4373/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.326s, learning 0.161s)
               Value function loss: 3.2958
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 224.02
               Mean episode length: 247.50
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 8.49s
                        Total time: 39927.46s
                               ETA: 872918.1s

################################################################################
                    [1m Learning iteration 4374/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.365s, learning 0.192s)
               Value function loss: 3.4104
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 226.42
               Mean episode length: 246.02
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71680000
                    Iteration time: 8.56s
                        Total time: 39936.02s
                               ETA: 872896.5s

################################################################################
                    [1m Learning iteration 4375/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.729s, learning 0.162s)
               Value function loss: 3.7057
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 216.75
               Mean episode length: 245.06
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71696384
                    Iteration time: 8.89s
                        Total time: 39944.91s
                               ETA: 872882.1s

################################################################################
                    [1m Learning iteration 4376/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.664s, learning 0.161s)
               Value function loss: 2.9941
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 211.71
               Mean episode length: 244.39
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71712768
                    Iteration time: 8.83s
                        Total time: 39953.74s
                               ETA: 872866.4s

################################################################################
                    [1m Learning iteration 4377/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.548s, learning 0.169s)
               Value function loss: 2.7564
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 218.66
               Mean episode length: 247.39
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71729152
                    Iteration time: 8.72s
                        Total time: 39962.46s
                               ETA: 872848.3s

################################################################################
                    [1m Learning iteration 4378/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.632s, learning 0.167s)
               Value function loss: 3.8152
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 223.76
               Mean episode length: 246.57
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71745536
                    Iteration time: 8.80s
                        Total time: 39971.25s
                               ETA: 872832.0s

################################################################################
                    [1m Learning iteration 4379/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.897s, learning 0.251s)
               Value function loss: 3.8168
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 219.17
               Mean episode length: 239.72
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 9.15s
                        Total time: 39980.40s
                               ETA: 872823.3s

################################################################################
                    [1m Learning iteration 4380/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.745s, learning 0.194s)
               Value function loss: 3.3349
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 223.21
               Mean episode length: 242.30
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71778304
                    Iteration time: 8.94s
                        Total time: 39989.34s
                               ETA: 872810.1s

################################################################################
                    [1m Learning iteration 4381/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.531s, learning 0.164s)
               Value function loss: 3.6835
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 218.51
               Mean episode length: 240.48
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71794688
                    Iteration time: 8.70s
                        Total time: 39998.04s
                               ETA: 872791.5s

################################################################################
                    [1m Learning iteration 4382/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.363s, learning 0.169s)
               Value function loss: 4.0679
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 220.10
               Mean episode length: 245.92
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71811072
                    Iteration time: 8.53s
                        Total time: 40006.57s
                               ETA: 872769.4s

################################################################################
                    [1m Learning iteration 4383/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.750s, learning 0.162s)
               Value function loss: 4.3064
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 223.44
               Mean episode length: 246.55
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71827456
                    Iteration time: 8.91s
                        Total time: 40015.48s
                               ETA: 872755.5s

################################################################################
                    [1m Learning iteration 4384/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.252s, learning 0.194s)
               Value function loss: 3.6657
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 228.07
               Mean episode length: 246.17
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71843840
                    Iteration time: 8.45s
                        Total time: 40023.93s
                               ETA: 872731.5s

################################################################################
                    [1m Learning iteration 4385/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.639s, learning 0.163s)
               Value function loss: 3.1694
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 224.29
               Mean episode length: 248.61
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 8.80s
                        Total time: 40032.73s
                               ETA: 872715.3s

################################################################################
                    [1m Learning iteration 4386/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.544s, learning 0.179s)
               Value function loss: 3.0414
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 217.35
               Mean episode length: 243.20
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71876608
                    Iteration time: 8.72s
                        Total time: 40041.45s
                               ETA: 872697.4s

################################################################################
                    [1m Learning iteration 4387/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.545s, learning 0.164s)
               Value function loss: 3.4007
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 219.37
               Mean episode length: 247.07
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71892992
                    Iteration time: 8.71s
                        Total time: 40050.16s
                               ETA: 872679.1s

################################################################################
                    [1m Learning iteration 4388/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.753s, learning 0.167s)
               Value function loss: 4.0174
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 218.66
               Mean episode length: 247.13
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71909376
                    Iteration time: 8.92s
                        Total time: 40059.08s
                               ETA: 872665.5s

################################################################################
                    [1m Learning iteration 4389/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.837s, learning 0.163s)
               Value function loss: 3.5957
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 225.01
               Mean episode length: 245.43
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71925760
                    Iteration time: 9.00s
                        Total time: 40068.08s
                               ETA: 872653.6s

################################################################################
                    [1m Learning iteration 4390/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.610s, learning 0.243s)
               Value function loss: 2.8408
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 214.47
               Mean episode length: 240.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71942144
                    Iteration time: 8.85s
                        Total time: 40076.93s
                               ETA: 872638.5s

################################################################################
                    [1m Learning iteration 4391/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.390s, learning 0.164s)
               Value function loss: 3.5284
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 221.06
               Mean episode length: 243.33
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 8.55s
                        Total time: 40085.49s
                               ETA: 872616.9s

################################################################################
                    [1m Learning iteration 4392/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.645s, learning 0.163s)
               Value function loss: 3.3694
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 221.82
               Mean episode length: 246.61
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71974912
                    Iteration time: 8.81s
                        Total time: 40094.30s
                               ETA: 872600.8s

################################################################################
                    [1m Learning iteration 4393/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.654s, learning 0.157s)
               Value function loss: 2.9719
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 220.47
               Mean episode length: 242.01
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71991296
                    Iteration time: 8.81s
                        Total time: 40103.11s
                               ETA: 872584.8s

################################################################################
                    [1m Learning iteration 4394/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.530s, learning 0.279s)
               Value function loss: 2.7189
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 225.47
               Mean episode length: 243.53
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72007680
                    Iteration time: 8.81s
                        Total time: 40111.91s
                               ETA: 872568.8s

################################################################################
                    [1m Learning iteration 4395/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.520s, learning 0.167s)
               Value function loss: 2.4929
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 224.22
               Mean episode length: 248.55
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72024064
                    Iteration time: 8.69s
                        Total time: 40120.60s
                               ETA: 872550.1s

################################################################################
                    [1m Learning iteration 4396/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.676s, learning 0.169s)
               Value function loss: 2.6032
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 219.98
               Mean episode length: 247.93
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72040448
                    Iteration time: 8.84s
                        Total time: 40129.45s
                               ETA: 872534.8s

################################################################################
                    [1m Learning iteration 4397/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.483s, learning 0.166s)
               Value function loss: 3.6947
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 213.82
               Mean episode length: 247.15
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 8.65s
                        Total time: 40138.10s
                               ETA: 872515.3s

################################################################################
                    [1m Learning iteration 4398/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.600s, learning 0.178s)
               Value function loss: 3.8603
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 218.16
               Mean episode length: 247.35
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72073216
                    Iteration time: 8.78s
                        Total time: 40146.87s
                               ETA: 872498.6s

################################################################################
                    [1m Learning iteration 4399/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.402s, learning 0.160s)
               Value function loss: 2.6228
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 216.51
               Mean episode length: 247.70
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72089600
                    Iteration time: 8.56s
                        Total time: 40155.44s
                               ETA: 872477.2s

################################################################################
                    [1m Learning iteration 4400/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.500s, learning 0.174s)
               Value function loss: 2.8232
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 212.22
               Mean episode length: 247.66
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72105984
                    Iteration time: 8.67s
                        Total time: 40164.11s
                               ETA: 872458.3s

################################################################################
                    [1m Learning iteration 4401/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.731s, learning 0.167s)
               Value function loss: 2.6616
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 214.11
               Mean episode length: 247.50
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72122368
                    Iteration time: 8.90s
                        Total time: 40173.01s
                               ETA: 872444.2s

################################################################################
                    [1m Learning iteration 4402/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.468s, learning 0.159s)
               Value function loss: 2.9657
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 219.25
               Mean episode length: 246.89
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72138752
                    Iteration time: 8.63s
                        Total time: 40181.63s
                               ETA: 872424.2s
